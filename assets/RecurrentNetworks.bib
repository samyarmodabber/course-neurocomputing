@article{Asabuki2018,
  title = {Interactive Reservoir Computing for Chunking Information Streams},
  author = {Asabuki, Toshitake and Hiratani, Naoki and Fukai, Tomoki},
  editor = {Graham, Lyle J.},
  date = {2018-10},
  journaltitle = {PLOS Computational Biology},
  volume = {14},
  number = {10},
  pages = {e1006400-e1006400},
  doi = {10.1371/journal.pcbi.1006400},
  url = {https://dx.plos.org/10.1371/journal.pcbi.1006400},
  abstract = {Chunking is the process by which frequently repeated segments of temporal inputs are concatenated into single units that are easy to process. Such a process is fundamental to time-series analysis in biological and artificial information processing systems. The brain efficiently acquires chunks from various information streams in an unsupervised manner; however, the underlying mechanisms of this process remain elusive. A widely-adopted statistical method for chunking consists of predicting frequently repeated contiguous elements in an input sequence based on unequal transition probabilities over sequence elements. However, recent experimental findings suggest that the brain is unlikely to adopt this method, as human subjects can chunk sequences with uniform transition probabilities. In this study, we propose a novel conceptual framework to overcome this limitation. In this process, neural networks learn to predict dynamical response patterns to sequence input rather than to directly learn transition patterns. Using a mutually supervising pair of reservoir computing modules, we demonstrate how this mechanism works in chunking sequences of letters or visual images with variable regularity and complexity. In addition, we demonstrate that background noise plays a crucial role in correctly learning chunks in this model. In particular, the model can successfully chunk sequences that conventional statistical approaches fail to chunk due to uniform transition probabilities. In addition, the neural responses of the model exhibit an interesting similarity to those of the basal ganglia observed after motor habit formation.},
  file = {/Users/vitay/Documents/Zotero/storage/I2C55L5T/Asabuki et al_2018_Interactive reservoir computing for chunking information streams.pdf}
}

@article{Aswolinskiy2015,
  title = {{{RM-SORN}}: A Reward-Modulated Self-Organizing Recurrent Neural Network},
  author = {Aswolinskiy, Witali and Pipa, Gordon},
  date = {2015-03},
  journaltitle = {Frontiers in Computational Neuroscience},
  volume = {9},
  pages = {36--36},
  doi = {10.3389/fncom.2015.00036},
  url = {http://www.frontiersin.org/Computational_Neuroscience/10.3389/fncom.2015.00036/abstract},
  abstract = {Neural plasticity plays an important role in learning and memory. Reward-modulation of plasticity offers an explanation for the ability of the brain to adapt its neural activity to achieve a rewarded goal. Here, we define a neural network model that learns through the interaction of Intrinsic Plasticity (IP) and reward-modulated Spike-Timing-Dependent Plasticity (STDP). IP enables the network to explore possible output sequences and STDP, modulated by reward, reinforces the creation of the rewarded output sequences. The model is tested on tasks for prediction, recall, non-linear computation, pattern recognition and sequence generation. It achieves performance comparable to networks trained with supervised learning, while using simple, biologically motivated plasticity rules and rewarding strategies. The results confirm the importance of investigating the interaction of several plasticity rules in the context of reward-modulated learning and whether reward-modulated self-organization can explain the amazing capabilities of the brain.},
  keywords = {Hebbian Learning,Intrinsic Plasticity,plasticity,recurrent neural networks,reward-modulated STDP,self-organization},
  file = {/Users/vitay/Documents/Zotero/storage/E9YLZ8LW/Aswolinskiy_Pipa_2015_RM-SORN.pdf}
}

@article{Atiya2000,
  title = {New Results on Recurrent Network Training: Unifying the Algorithms and Accelerating Convergence},
  author = {Atiya, A.F. and Parlos, A.G.},
  date = {2000-05},
  journaltitle = {IEEE Transactions on Neural Networks},
  volume = {11},
  number = {3},
  pages = {697--709},
  doi = {10.1109/72.846741},
  url = {http://ieeexplore.ieee.org/document/846741/},
  file = {/Users/vitay/Documents/Zotero/storage/NBAQ3FKD/Atiya_Parlos_2000_New results on recurrent network training.pdf}
}

@inproceedings{Babinec2007,
  title = {Improving the {{Prediction Accuracy}} of {{Echo State Neural Networks}} by {{Anti-Oja}}’s {{Learning}}},
  booktitle = {Artificial {{Neural Networks}} – {{ICANN}} 2007},
  author = {Babinec, Štefan and Pospíchal, Jiří},
  editor = {family=Sá, given=Joaquim Marques, prefix=de, useprefix=true and Alexandre, Luís A. and Duch, Włodzisław and Mandic, Danilo},
  date = {2007},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {19--28},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-74690-4_3},
  abstract = {Echo state neural networks, which are a special case of recurrent neural networks, are studied from the viewpoint of their learning ability, with a goal to achieve their greater prediction ability. A standard training of these neural networks uses pseudoinverse matrix for one-step learning of weights from hidden to output neurons. This regular adaptation of Echo State neural networks was optimized by updating the weights of the dynamic reservoir with Anti-Oja’s learning. Echo State neural networks use dynamics of this massive and randomly initialized dynamic reservoir to extract interesting properties of incoming sequences. This approach was tested in laser fluctuations and Mackey-Glass time series prediction. The prediction error achieved by this approach was substantially smaller in comparison with prediction error achieved by a standard algorithm.},
  isbn = {978-3-540-74690-4},
  langid = {english}
}

@article{Balafrej2022,
  title = {P-{{CRITICAL}}: A Reservoir Autoregulation Plasticity Rule for Neuromorphic Hardware},
  shorttitle = {P-{{CRITICAL}}},
  author = {Balafrej, Ismael and Alibart, Fabien and Rouat, Jean},
  date = {2022-06-01},
  journaltitle = {Neuromorphic Computing and Engineering},
  shortjournal = {Neuromorph. Comput. Eng.},
  volume = {2},
  number = {2},
  pages = {024007},
  issn = {2634-4386},
  doi = {10.1088/2634-4386/ac6533},
  url = {https://iopscience.iop.org/article/10.1088/2634-4386/ac6533},
  urldate = {2023-02-10},
  abstract = {Backpropagation algorithms on recurrent artificial neural networks require an unfolding of accumulated states over time. These states must be kept in memory for an undefined period of time which is task-dependent and costly for edge devices. This paper uses the reservoir computing paradigm where an untrained recurrent pool of neurons is used as a preprocessor for temporally structured inputs and with a limited number of training data samples. These so-called reservoirs usually require either extensive fine-tuning or neuroplasticity. We propose a new local and unsupervised plasticity rule named P-CRITICAL designed for automatic reservoir tuning that translates well to physical and digital neuromorphic processors. The spiking neuronal architecture implementation is simulated on the Loihi research chip from Intel and on a conventional CPU. Comparisons on state-of-the-art machine learning datasets are given. Improved performance on visual and auditory tasks are observed. There is no need to a priori tune the reservoir when switching between tasks, making this approach suitable for physical implementations. Furthermore, such plastic behaviour of the reservoir is a key to end-to-end energy-efficient neuromorphic-based machine learning on edge devices.},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/CUIQCTKG/Balafrej_et_al_2022_P-CRITICAL.pdf}
}

@article{Barak2013,
  title = {From Fixed Points to Chaos: {{Three}} Models of Delayed Discrimination},
  shorttitle = {From Fixed Points to Chaos},
  author = {Barak, Omri and Sussillo, David and Romo, Ranulfo and Tsodyks, Misha and Abbott, L.F.},
  date = {2013-04},
  journaltitle = {Progress in Neurobiology},
  volume = {103},
  pages = {214--222},
  issn = {03010082},
  doi = {10.1016/j.pneurobio.2013.02.002},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0301008213000129},
  urldate = {2019-04-30},
  abstract = {Working memory is a crucial component of most cognitive tasks. Its neuronal mechanisms are still unclear despite intensive experimental and theoretical explorations. Most theoretical models of working memory assume both time-invariant neural representations and precise connectivity schemes based on the tuning properties of network neurons. A different, more recent class of models assumes randomly connected neurons that have no tuning to any particular task, and bases task performance purely on adjustment of network readout. Intermediate between these schemes are networks that start out random but are trained by a learning scheme. Experimental studies of a delayed vibrotactile discrimination task indicate that some of the neurons in prefrontal cortex are persistently tuned to the frequency of a remembered stimulus, but the majority exhibit more complex relationships to the stimulus that vary considerably across time. We compare three models, ranging from a highly organized linear attractor model to a randomly connected network with chaotic activity, with data recorded during this task. The random network does a surprisingly good job of both performing the task and matching certain aspects of the data. The intermediate model, in which an initially random network is partially trained to perform the working memory task by tuning its recurrent and readout connections, provides a better description, although none of the models matches all features of the data. Our results suggest that prefrontal networks may begin in a random state relative to the task and initially rely on modified readout for task performance. With further training, however, more tuned neurons with less time-varying responses should emerge as the networks become more structured.},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/A742US2F/Barak et al_2013_From fixed points to chaos.pdf}
}

@article{Bellec2020a,
  title = {A Solution to the Learning Dilemma for Recurrent Networks of Spiking Neurons},
  author = {Bellec, Guillaume and Scherr, Franz and Subramoney, Anand and Hajek, Elias and Salaj, Darjan and Legenstein, Robert and Maass, Wolfgang},
  date = {2020-07-17},
  journaltitle = {Nature Communications},
  shortjournal = {Nat Commun},
  volume = {11},
  number = {1},
  pages = {3625},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/s41467-020-17236-y},
  url = {https://www.nature.com/articles/s41467-020-17236-y},
  urldate = {2023-08-11},
  abstract = {Recurrently connected networks of spiking neurons underlie the astounding information processing capabilities of the brain. Yet in spite of extensive research, how they can learn through synaptic plasticity to carry out complex network computations~remains unclear. We argue that two pieces of this puzzle were provided by experimental data from neuroscience. A mathematical result tells us how these pieces need to be combined to enable biologically plausible online network learning through gradient descent, in particular deep reinforcement learning. This learning method–called e-prop–approaches the performance of backpropagation through time (BPTT), the best-known method for training recurrent neural networks in machine learning. In addition, it suggests a method for powerful on-chip learning in energy-efficient spike-based hardware for artificial intelligence.},
  issue = {1},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/KQXCNGH3/Bellec_et_al_2020_A_solution_to_the_learning_dilemma_for_recurrent_networks_of_spiking_neurons.pdf}
}

@article{Berger2017,
  title = {Spatial Features of Synaptic Adaptation Affecting Learning Performance},
  author = {Berger, Damian L. and family=Arcangelis, given=Lucilla, prefix=de, useprefix=true and Herrmann, Hans J.},
  date = {2017-12},
  journaltitle = {Scientific Reports},
  volume = {7},
  number = {1},
  pages = {11016--11016},
  doi = {10.1038/s41598-017-11424-5},
  url = {http://www.nature.com/articles/s41598-017-11424-5},
  abstract = {Recent studies have proposed that the diffusion of messenger molecules, such as monoamines, can mediate the plastic adaptation of synapses in supervised learning of neural networks. Based on these findings we developed a model for neural learning, where the signal for plastic adaptation is assumed to propagate through the extracellular space. We investigate the conditions allowing learning of Boolean rules in a neural network. Even fully excitatory networks show very good learning performances. Moreover, the investigation of the plastic adaptation features optimizing the performance suggests that learning is very sensitive to the extent of the plastic adaptation and the spatial range of synaptic connections.},
  keywords = {Biological physics,Complex networks,Complexity,Criticality,Learning algorithms},
  file = {/Users/vitay/Documents/Zotero/storage/2UJZ4KS8/Berger et al_2017_Spatial features of synaptic adaptation affecting learning performance.pdf}
}

@article{Bertschinger2004,
  title = {Real-{{Time Computation}} at the {{Edge}} of {{Chaos}} in {{Recurrent Neural Networks}}},
  author = {Bertschinger, Nils and Natschläger, Thomas},
  date = {2004-07},
  journaltitle = {Neural Computation},
  volume = {16},
  number = {7},
  pages = {1413--1436},
  doi = {10.1162/089976604323057443},
  url = {http://www.mitpressjournals.org/doi/10.1162/089976604323057443}
}

@article{Bi2020,
  title = {Understanding the Computation of Time Using Neural Network Models},
  author = {Bi, Zedong and Zhou, Changsong},
  date = {2020-05-12},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  volume = {117},
  number = {19},
  eprint = {32341153},
  eprinttype = {pmid},
  pages = {10530--10540},
  publisher = {National Academy of Sciences},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1921609117},
  url = {https://www.pnas.org/content/117/19/10530},
  urldate = {2020-08-21},
  abstract = {To maximize future rewards in this ever-changing world, animals must be able to discover the temporal structure of stimuli and then anticipate or act correctly at the right time. How do animals perceive, maintain, and use time intervals ranging from hundreds of milliseconds to multiseconds in working memory? How is temporal information processed concurrently with spatial information and decision making? Why are there strong neuronal temporal signals in tasks in which temporal information is not required? A systematic understanding of the underlying neural mechanisms is still lacking. Here, we addressed these problems using supervised training of recurrent neural network models. We revealed that neural networks perceive elapsed time through state evolution along stereotypical trajectory, maintain time intervals in working memory in the monotonic increase or decrease of the firing rates of interval-tuned neurons, and compare or produce time intervals by scaling state evolution speed. Temporal and nontemporal information is coded in subspaces orthogonal with each other, and the state trajectories with time at different nontemporal information are quasiparallel and isomorphic. Such coding geometry facilitates the decoding generalizability of temporal and nontemporal information across each other. The network structure exhibits multiple feedforward sequences that mutually excite or inhibit depending on whether their preferences of nontemporal information are similar or not. We identified four factors that facilitate strong temporal signals in nontiming tasks, including the anticipation of coming events. Our work discloses fundamental computational principles of temporal processing, and it is supported by and gives predictions to a number of experimental phenomena.},
  isbn = {9781921609114},
  langid = {english},
  keywords = {interval timing,neural network model,population coding},
  file = {/Users/vitay/Documents/Zotero/storage/B3X4W5FT/Bi_Zhou_2020_Understanding_the_computation_of_time_using_neural_network_models.pdf;/Users/vitay/Documents/Zotero/storage/WPNGEGXZ/10530.html}
}

@article{Bianchi2021,
  title = {Reservoir {{Computing Approaches}} for {{Representation}} and {{Classification}} of {{Multivariate Time Series}}},
  author = {Bianchi, Filippo Maria and Scardapane, Simone and Løkse, Sigurd and Jenssen, Robert},
  date = {2021-05},
  journaltitle = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {32},
  number = {5},
  pages = {2169--2179},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2020.3001377},
  abstract = {Classification of multivariate time series (MTS) has been tackled with a large variety of methodologies and applied to a wide range of scenarios. Reservoir computing (RC) provides efficient tools to generate a vectorial, fixed-size representation of the MTS that can be further processed by standard classifiers. Despite their unrivaled training speed, MTS classifiers based on a standard RC architecture fail to achieve the same accuracy of fully trainable neural networks. In this article, we introduce the reservoir model space, an unsupervised approach based on RC to learn vectorial representations of MTS. Each MTS is encoded within the parameters of a linear model trained to predict a low-dimensional embedding of the reservoir dynamics. Compared with other RC methods, our model space yields better representations and attains comparable computational performance due to an intermediate dimensionality reduction procedure. As a second contribution, we propose a modular RC framework for MTS classification, with an associated open-source Python library. The framework provides different modules to seamlessly implement advanced RC architectures. The architectures are compared with other MTS classifiers, including deep learning models and time series kernels. Results obtained on the benchmark and real-world MTS data sets show that RC classifiers are dramatically faster and, when implemented using our proposed representation, also achieve superior classification accuracy.},
  eventtitle = {{{IEEE Transactions}} on {{Neural Networks}} and {{Learning Systems}}},
  file = {/Users/vitay/Documents/Zotero/storage/5KYFBK7M/Bianchi_et_al_2021_Reservoir_Computing_Approaches_for_Representation_and_Classification_of.pdf;/Users/vitay/Documents/Zotero/storage/YCMPPA5X/9127499.html}
}

@article{Borges2017,
  title = {Synchronised Firing Patterns in a Random Network of Adaptive Exponential Integrate-and-Fire Neuron Model},
  author = {Borges, F. S. and Protachevicz, P. R. and Lameu, E. L. and Bonetti, R. C. and Iarosz, K. C. and Caldas, I. L. and Baptista, M. S. and Batista, A. M.},
  date = {2017-06-01},
  journaltitle = {Neural Networks},
  shortjournal = {Neural Networks},
  volume = {90},
  pages = {1--7},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2017.03.005},
  url = {http://www.sciencedirect.com/science/article/pii/S0893608017300588},
  urldate = {2019-04-14},
  abstract = {We have studied neuronal synchronisation in a random network of adaptive exponential integrate-and-fire neurons. We study how spiking or bursting synchronous behaviour appears as a function of the coupling strength and the probability of connections, by constructing parameter spaces that identify these synchronous behaviours from measurements of the inter-spike interval and the calculation of the order parameter. Moreover, we verify the robustness of synchronisation by applying an external perturbation to each neuron. The simulations show that bursting synchronisation is more robust than spike synchronisation.},
  keywords = {Integrate-and-fire,Network,Synchronisation},
  file = {/Users/vitay/Documents/Zotero/storage/7EJPBBNY/Borges et al_2017_Synchronised firing patterns in a random network of adaptive exponential.pdf;/Users/vitay/Documents/Zotero/storage/QMYIZ4DY/S0893608017300588.html}
}

@inproceedings{Bourdoukan2015,
  title = {Enforcing Balance Allows Local Supervised Learning in Spiking Recurrent Networks},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Bourdoukan, Ralph and Denève, Sophie},
  date = {2015},
  volume = {28},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper/2015/hash/3871bd64012152bfb53fdf04b401193f-Abstract.html},
  urldate = {2024-06-18},
  abstract = {To predict sensory inputs or control motor trajectories, the brain must constantly learn temporal dynamics based on error feedback. However, it remains unclear how such supervised learning is implemented in biological neural networks. Learning in recurrent spiking networks is notoriously difficult because local changes in connectivity may have an unpredictable effect on the global dynamics. The most commonly used learning rules, such as temporal back-propagation, are not local and thus not biologically plausible. Furthermore, reproducing the Poisson-like statistics of neural responses requires the use of networks with balanced excitation and inhibition. Such balance is easily destroyed during learning. Using a top-down approach, we show how networks of integrate-and-fire neurons can learn arbitrary linear dynamical systems by feeding back their error as a feed-forward input. The network uses two types of recurrent connections: fast and slow. The fast connections learn to balance excitation and inhibition using a voltage-based plasticity rule. The slow connections are trained to minimize the error feedback using a current-based Hebbian learning rule. Importantly, the balance maintained by fast connections is crucial to ensure that global error signals are available locally in each neuron, in turn resulting in a local learning rule for the slow connections. This demonstrates that spiking networks can learn complex dynamics using purely local learning rules, using E/I balance as the key rather than an additional constraint. The resulting network implements a given function within the predictive coding scheme, with minimal dimensions and activity.},
  file = {/Users/vitay/Documents/Zotero/storage/RVZFW94H/Bourdoukan and Denève - 2015 - Enforcing balance allows local supervised learning in spiking recurrent networks.pdf}
}

@article{Bourjaily2011,
  title = {Excitatory, {{Inhibitory}}, and {{Structural Plasticity Produce Correlated Connectivity}} in {{Random Networks Trained}} to {{Solve Paired-Stimulus Tasks}}},
  author = {Bourjaily, Mark A. and Miller, Paul},
  date = {2011-09},
  journaltitle = {Frontiers in Computational Neuroscience},
  volume = {5},
  pages = {37--37},
  doi = {10.3389/fncom.2011.00037},
  url = {http://journal.frontiersin.org/article/10.3389/fncom.2011.00037/abstract},
  abstract = {The pattern of connections among cortical excitatory cells with overlapping arbors is non-random. In particular, correlations among connections produce clustering – cells in cliques connect to each other with high probability, but with lower probability to cells in other spatially intertwined cliques. In this study, we model initially randomly connected sparse recurrent networks of spiking neurons with random, overlapping inputs, to investigate what functional and structural synaptic plasticity mechanisms sculpt network connections into the patterns measured in vitro. Our Hebbian implementation of structural plasticity causes a removal of connections between uncorrelated excitatory cells, followed by their random replacement. To model a biconditional discrimination task, we stimulate the network via pairs (A+B, C+D, A+D and C+B) of four inputs (A, B, C and D). We find networks that produce neurons most responsive to specific paired-inputs – a building block of computation and essential role for cortex – contain the excessive clustering of excitatory synaptic connections observed in cortical slices. The same networks produce the best performance in a behavioral readout of the networks' ability to complete the task. A plasticity mechanism operating on inhibitory connections, long-term potentiation of inhibition (LTPi), when combined with structural plasticity, indirectly enhances clustering of excitatory cells via excitatory connections. A rate-dependent (triplet) form of spike-timing-dependent plasticity (STDP) between excitatory cells is less effective and basic STDP is detrimental. Clustering also arises in networks stimulated with single stimuli and in networks undergoing raised levels of spontaneous activity when structural plasticity is combined with functional plasticity. In conclusion, spatially intertwined clusters or cliques of connected excitatory cells can arise via a Hebbian-form of structural plasticity operating in initially randomly connected networks.},
  keywords = {Correlation,inhibition,LTPi,network,plasticity,spike-timing dependent plasticity,structural plasticity,Structure},
  file = {/Users/vitay/Documents/Zotero/storage/LJY8TNVJ/Bourjaily_Miller_2011_Excitatory, Inhibitory, and Structural Plasticity Produce Correlated.pdf}
}

@article{Brosch2015,
  title = {Reinforcement {{Learning}} of {{Linking}} and {{Tracing Contours}} in {{Recurrent Neural Networks}}},
  author = {Brosch, Tobias and Neumann, Heiko and Roelfsema, Pieter R.},
  editor = {Bethge, Matthias},
  date = {2015-10},
  journaltitle = {PLOS Computational Biology},
  volume = {11},
  number = {10},
  pages = {e1004489-e1004489},
  doi = {10.1371/journal.pcbi.1004489},
  url = {https://dx.plos.org/10.1371/journal.pcbi.1004489},
  abstract = {The processing of a visual stimulus can be subdivided into a number of stages. Upon stimulus presentation there is an early phase of feedforward processing where the visual information is propagated from lower to higher visual areas for the extraction of basic and complex stimulus features. This is followed by a later phase where horizontal connections within areas and feedback connections from higher areas back to lower areas come into play. In this later phase, image elements that are behaviorally relevant are grouped by Gestalt grouping rules and are labeled in the cortex with enhanced neuronal activity (object-based attention in psychology). Recent neurophysiological studies revealed that reward-based learning influences these recurrent grouping processes, but it is not well understood how rewards train recurrent circuits for perceptual organization. This paper examines the mechanisms for reward-based learning of new grouping rules. We derive a learning rule that can explain how rewards influence the information flow through feedforward, horizontal and feedback connections. We illustrate the efficiency with two tasks that have been used to study the neuronal correlates of perceptual organization in early visual cortex. The first task is called contour-integration and demands the integration of collinear contour elements into an elongated curve. We show how reward-based learning causes an enhancement of the representation of the to-be-grouped elements at early levels of a recurrent neural network, just as is observed in the visual cortex of monkeys. The second task is curve-tracing where the aim is to determine the endpoint of an elongated curve composed of connected image elements. If trained with the new learning rule, neural networks learn to propagate enhanced activity over the curve, in accordance with neurophysiological data. We close the paper with a number of model predictions that can be tested in future neurophysiological and computational studies.},
  file = {/Users/vitay/Documents/Zotero/storage/YR6SHZFF/Brosch et al_2015_Reinforcement Learning of Linking and Tracing Contours in Recurrent Neural.pdf}
}

@article{Buonomano2009,
  title = {Harnessing {{Chaos}} in {{Recurrent Neural Networks}}},
  author = {Buonomano, Dean V.},
  date = {2009-08},
  journaltitle = {Neuron},
  volume = {63},
  number = {4},
  pages = {423--425},
  issn = {08966273},
  doi = {10.1016/j.neuron.2009.08.003},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0896627309005881},
  urldate = {2019-03-03},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/SW4LEMA6/Buonomano_2009_Harnessing Chaos in Recurrent Neural Networks.pdf}
}

@article{Cayco-Gajic2019,
  title = {Re-Evaluating {{Circuit Mechanisms Underlying Pattern Separation}}},
  author = {Cayco-Gajic, N. Alex and Silver, R. Angus},
  date = {2019-02-20},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  volume = {101},
  number = {4},
  pages = {584--602},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2019.01.044},
  url = {http://www.sciencedirect.com/science/article/pii/S0896627319300716},
  urldate = {2020-01-25},
  abstract = {When animals interact with complex environments, their neural circuits must separate overlapping patterns of activity that represent sensory and motor information. Pattern separation is thought to be a key function of several brain regions, including the cerebellar cortex, insect mushroom body, and dentate gyrus. However, recent findings have questioned long-held ideas on how these circuits perform this fundamental computation. Here, we re-evaluate the functional and structural mechanisms underlying pattern separation. We argue that the dimensionality of the space available for population codes representing sensory and motor information provides a common framework for understanding pattern separation. We then discuss how these three circuits use different strategies to separate activity patterns and facilitate associative learning in the presence of trial-to-trial variability.},
  langid = {english},
  keywords = {cerebellum,decorrelation,dimensionality,hippocampus,insect mushroom body,neural circuits,pattern separation,sensorimotor processing,sparse coding,sparse connectivity},
  file = {/Users/vitay/Documents/Zotero/storage/ZX7HZX7K/S0896627319300716.html}
}

@article{Cazin2019,
  title = {Reservoir Computing Model of Prefrontal Cortex Creates Novel Combinations of Previous Navigation Sequences from Hippocampal Place-Cell Replay with Spatial Reward Propagation},
  author = {Cazin, Nicolas and Alonso, Martin Llofriu and Chiodi, Pablo Scleidorovich and Pelc, Tatiana and Harland, Bruce and Weitzenfeld, Alfredo and Fellous, Jean-Marc and Dominey, Peter Ford},
  date = {2019-07-15},
  journaltitle = {PLOS Computational Biology},
  shortjournal = {PLOS Computational Biology},
  volume = {15},
  number = {7},
  pages = {e1006624},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1006624},
  url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006624},
  urldate = {2019-07-30},
  abstract = {As rats learn to search for multiple sources of food or water in a complex environment, they generate increasingly efficient trajectories between reward sites. Such spatial navigation capacity involves the replay of hippocampal place-cells during awake states, generating small sequences of spatially related place-cell activity that we call “snippets”. These snippets occur primarily during sharp-wave-ripples (SWRs). Here we focus on the role of such replay events, as the animal is learning a traveling salesperson task (TSP) across multiple trials. We hypothesize that snippet replay generates synthetic data that can substantially expand and restructure the experience available and make learning more optimal. We developed a model of snippet generation that is modulated by reward, propagated in the forward and reverse directions. This implements a form of spatial credit assignment for reinforcement learning. We use a biologically motivated computational framework known as ‘reservoir computing’ to model prefrontal cortex (PFC) in sequence learning, in which large pools of prewired neural elements process information dynamically through reverberations. This PFC model consolidates snippets into larger spatial sequences that may be later recalled by subsets of the original sequences. Our simulation experiments provide neurophysiological explanations for two pertinent observations related to navigation. Reward modulation allows the system to reject non-optimal segments of experienced trajectories, and reverse replay allows the system to “learn” trajectories that is has not physically experienced, both of which significantly contribute to the TSP behavior.},
  langid = {english},
  keywords = {Algorithms,Hippocampus,Learning,Neostriatum,Neurons,Prefrontal cortex,Problem solving,Rats},
  file = {/Users/vitay/Documents/Zotero/storage/9W4J43LV/Cazin et al_2019_Reservoir computing model of prefrontal cortex creates novel combinations of.pdf;/Users/vitay/Documents/Zotero/storage/ZBGBYGDN/article.html}
}

@article{Cazin2020,
  title = {Real-Time Sensory–Motor Integration of Hippocampal Place Cell Replay and Prefrontal Sequence Learning in Simulated and Physical Rat Robots for Novel Path Optimization},
  author = {Cazin, Nicolas and Scleidorovich, Pablo and Weitzenfeld, Alfredo and Dominey, Peter Ford},
  date = {2020-04-01},
  journaltitle = {Biological Cybernetics},
  shortjournal = {Biol Cybern},
  volume = {114},
  number = {2},
  pages = {249--268},
  issn = {1432-0770},
  doi = {10.1007/s00422-020-00820-2},
  url = {https://doi.org/10.1007/s00422-020-00820-2},
  urldate = {2020-06-28},
  abstract = {An open problem in the cognitive dimensions of navigation concerns how previous exploratory experience is reorganized in order to allow the creation of novel efficient navigation trajectories. This behavior is revealed in the “traveling salesrat problem” (TSP) when rats discover the shortest path linking baited food wells after a few exploratory traversals. We have recently published a model of navigation sequence learning, where sharp wave ripple replay of hippocampal place cells transmit “snippets” of the recent trajectories that the animal has explored to the prefrontal cortex (PFC) (Cazin et al. in PLoS Comput Biol 15:e1006624, 2019). PFC is modeled as a recurrent reservoir network that is able to assemble these snippets into the efficient sequence (trajectory of spatial locations coded by place cell activation). The model of hippocampal replay generates a distribution of snippets as a function of their proximity to a reward, thus implementing a form of spatial credit assignment that solves the TSP task.The integrative PFC reservoir reconstructs the efficient TSP sequence based on exposure to this distribution of snippets that favors paths that are most proximal to rewards. While this demonstrates the theoretical feasibility of the PFC–HIPP interaction, the integration of such a dynamic system into a real-time sensory–motor system remains a challenge. In the current research, we test the hypothesis that the PFC reservoir model can operate in a real-time sensory–motor loop. Thus, the main goal of the paper is to validate the model in simulated and real robot scenarios. Place cell activation encoding the current position of the simulated and physical rat robot feeds the PFC reservoir which generates the successor place cell activation that represents the next step in the reproduced sequence in the readout. This is input to the robot, which advances to the coded location and then generates de novo the current place cell activation. This allows demonstration of the crucial role of embodiment. If the spatial code readout from PFC is played back directly into PFC, error can accumulate, and the system can diverge from desired trajectories. This required a spatial filter to decode the PFC code to a location and then recode a new place cell code for that location. In the robot, the place cell vector output of PFC is used to physically displace the robot and then generate a new place cell coded input to the PFC, replacing part of the software recoding procedure that was required otherwise. We demonstrate how this integrated sensory–motor system can learn simple navigation sequences and then, importantly, how it can synthesize novel efficient sequences based on prior experience, as previously demonstrated (Cazin et al. 2019). This contributes to the understanding of hippocampal replay in novel navigation sequence formation and the important role of embodiment.},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/F592M2MA/Cazin_et_al_2020_Real-time_sensory–motor_integration_of_hippocampal_place_cell_replay_and.pdf}
}

@inproceedings{Ceolini2016,
  title = {Temporal Sequence Recognition in a Self-Organizing Recurrent Network},
  booktitle = {2016 {{Second International Conference}} on {{Event-based Control}}, {{Communication}}, and {{Signal Processing}} ({{EBCCSP}})},
  author = {Ceolini, Enea and Neil, Daniel and Delbruck, Tobi and Liu, Shih-Chii},
  date = {2016-06},
  pages = {1--4},
  publisher = {IEEE},
  doi = {10.1109/EBCCSP.2016.7605258},
  url = {http://ieeexplore.ieee.org/document/7605258/},
  isbn = {978-1-5090-4196-1},
  file = {/Users/vitay/Documents/Zotero/storage/HWUEPJPM/Ceolini et al_2016_Temporal sequence recognition in a self-organizing recurrent network.pdf}
}

@article{Chrol-Cannon2014,
  title = {On the {{Correlation}} between {{Reservoir Metrics}} and {{Performance}} for {{Time Series Classification}} under the {{Influence}} of {{Synaptic Plasticity}}},
  author = {Chrol-Cannon, Joseph and Jin, Yaochu},
  date = {2014-07-10},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {9},
  number = {7},
  pages = {e101792},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0101792},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0101792},
  urldate = {2023-02-01},
  abstract = {Reservoir computing provides a simpler paradigm of training recurrent networks by initialising and adapting the recurrent connections separately to a supervised linear readout. This creates a problem, though. As the recurrent weights and topology are now separated from adapting to the task, there is a burden on the reservoir designer to construct an effective network that happens to produce state vectors that can be mapped linearly into the desired outputs. Guidance in forming a reservoir can be through the use of some established metrics which link a number of theoretical properties of the reservoir computing paradigm to quantitative measures that can be used to evaluate the effectiveness of a given design. We provide a comprehensive empirical study of four metrics; class separation, kernel quality, Lyapunov's exponent and spectral radius. These metrics are each compared over a number of repeated runs, for different reservoir computing set-ups that include three types of network topology and three mechanisms of weight adaptation through synaptic plasticity. Each combination of these methods is tested on two time-series classification problems. We find that the two metrics that correlate most strongly with the classification performance are Lyapunov's exponent and kernel quality. It is also evident in the comparisons that these two metrics both measure a similar property of the reservoir dynamics. We also find that class separation and spectral radius are both less reliable and less effective in predicting performance.},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/2WBDXDJF/Chrol-Cannon_Jin_2014_On_the_Correlation_between_Reservoir_Metrics_and_Performance_for_Time_Series.pdf}
}

@article{Cramer2020,
  title = {Control of Criticality and Computation in Spiking Neuromorphic Networks with Plasticity},
  author = {Cramer, Benjamin and Stöckel, David and Kreft, Markus and Wibral, Michael and Schemmel, Johannes and Meier, Karlheinz and Priesemann, Viola},
  date = {2020-06-05},
  journaltitle = {Nature Communications},
  shortjournal = {Nat Commun},
  volume = {11},
  number = {1},
  pages = {2853},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/s41467-020-16548-3},
  url = {https://www.nature.com/articles/s41467-020-16548-3},
  urldate = {2023-11-10},
  abstract = {The critical state is assumed to be optimal for any computation in recurrent neural networks, because criticality maximizes a number of abstract computational properties. We challenge this assumption by evaluating the performance of a spiking recurrent neural network on a set of tasks of varying complexity at - and away from critical network dynamics. To that end, we developed a plastic spiking network on a neuromorphic chip. We show that the distance to criticality can be easily adapted by changing the input strength, and then demonstrate a clear relation between criticality, task-performance and information-theoretic fingerprint. Whereas the information-theoretic measures all show that network capacity is maximal at criticality, only the complex tasks profit from criticality, whereas simple tasks suffer. Thereby, we challenge the general assumption that criticality would be beneficial for any task, and provide instead an understanding of how the collective network state should be tuned to task requirement.},
  issue = {1},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/UFXDDR3V/Cramer_et_al_2020_Control_of_criticality_and_computation_in_spiking_neuromorphic_networks_with.pdf}
}

@article{Cucchi2022,
  title = {Hands-on Reservoir Computing: A Tutorial for Practical Implementation},
  shorttitle = {Hands-on Reservoir Computing},
  author = {Cucchi, Matteo and Abreu, Steven and Ciccone, Giuseppe and Brunner, Daniel and Kleemann, Hans},
  date = {2022-08-05},
  journaltitle = {Neuromorphic Computing and Engineering},
  shortjournal = {Neuromorph. Comput. Eng.},
  volume = {2},
  number = {3},
  pages = {032002},
  publisher = {IOP Publishing},
  issn = {2634-4386},
  doi = {10.1088/2634-4386/ac7db7},
  url = {https://iopscience.iop.org/article/10.1088/2634-4386/ac7db7/meta},
  urldate = {2022-09-08},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/YL5CHS5K/Cucchi_et_al_2022_Hands-on_reservoir_computing.pdf;/Users/vitay/Documents/Zotero/storage/8N5I2AF9/ac7db7.html}
}

@article{Cueva2020,
  title = {Low-Dimensional Dynamics for Working Memory and Time Encoding},
  author = {Cueva, Christopher J. and Saez, Alex and Marcos, Encarni and Genovesio, Aldo and Jazayeri, Mehrdad and Romo, Ranulfo and Salzman, C. Daniel and Shadlen, Michael N. and Fusi, Stefano},
  date = {2020-09-15},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume = {117},
  number = {37},
  pages = {23021--23032},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.1915984117},
  url = {https://www.pnas.org/doi/10.1073/pnas.1915984117},
  urldate = {2024-06-11},
  abstract = {Our decisions often depend on multiple sensory experiences separated by time delays. The brain can remember these experiences and, simultaneously, estimate the timing between events. To understand the mechanisms underlying working memory and time encoding, we analyze neural activity recorded during delays in four experiments on nonhuman primates. To disambiguate potential mechanisms, we propose two analyses, namely, decoding the passage of time from neural data and computing the cumulative dimensionality of the neural trajectory over time. Time can be decoded with high precision in tasks where timing information is relevant and with lower precision when irrelevant for performing the task. Neural trajectories are always observed to be low-dimensional. In addition, our results further constrain the mechanisms underlying time encoding as we find that the linear “ramping” component of each neuron’s firing rate strongly contributes to the slow timescale variations that make decoding time possible. These constraints rule out working memory models that rely on constant, sustained activity and neural networks with high-dimensional trajectories, like reservoir networks. Instead, recurrent networks trained with backpropagation capture the time-encoding properties and the dimensionality observed in the data.},
  file = {/Users/vitay/Documents/Zotero/storage/5RQMDCEX/Cueva et al. - 2020 - Low-dimensional dynamics for working memory and time encoding.pdf}
}

@article{Dai2021,
  title = {Computational {{Efficiency}} of a {{Modular Reservoir Network}} for {{Image Recognition}}},
  author = {Dai, Yifan and Yamamoto, Hideaki and Sakuraba, Masao and Sato, Shigeo},
  date = {2021},
  journaltitle = {Frontiers in Computational Neuroscience},
  volume = {15},
  issn = {1662-5188},
  url = {https://www.frontiersin.org/article/10.3389/fncom.2021.594337},
  urldate = {2022-01-17},
  abstract = {Liquid state machine (LSM) is a type of recurrent spiking network with a strong relationship to neurophysiology and has achieved great success in time series processing. However, the computational cost of simulations and complex dynamics with time dependency limit the size and functionality of LSMs. This paper presents a large-scale bioinspired LSM with modular topology. We integrate the findings on the visual cortex that specifically designed input synapses can fit the activation of the real cortex and perform the Hough transform, a feature extraction algorithm used in digital image processing, without additional cost. We experimentally verify that such a combination can significantly improve the network functionality. The network performance is evaluated using the MNIST dataset where the image data are encoded into spiking series by Poisson coding. We show that the proposed structure can not only significantly reduce the computational complexity but also achieve higher performance compared to the structure of previous reported networks of a similar size. We also show that the proposed structure has better robustness against system damage than the small-world and random structures. We believe that the proposed computationally efficient method can greatly contribute to future applications of reservoir computing.},
  file = {/Users/vitay/Documents/Zotero/storage/2LACT6AI/Dai_et_al_2021_Computational_Efficiency_of_a_Modular_Reservoir_Network_for_Image_Recognition.pdf}
}

@inproceedings{Dale2017,
  title = {Reservoir Computing in Materio: {{A}} Computational Framework for in Materio Computing},
  shorttitle = {Reservoir Computing in Materio},
  booktitle = {2017 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {Dale, Matthew and Stepney, Susan and Miller, Julian F. and Trefzer, Martin},
  date = {2017-05},
  pages = {2178--2185},
  issn = {2161-4407},
  doi = {10.1109/IJCNN.2017.7966119},
  abstract = {The Reservoir Computing (RC) framework is said to have the potential to transfer onto any input-driven dynamical system, provided two properties are present: (i) a fading memory, and (ii) input separability. A typical reservoir consists of a fixed network of recurrently connected processing units; however recent hardware implementations have shown reservoirs are not ultimately bound by this architecture. Previously, we have demonstrated how the RC framework can be applied to randomly-formed carbon nanotube composites to solve computational tasks. Here, we apply the RC framework to an evolvable substrate and compare performance to an already established in materia training technique, referred to as evolution in materia. The results show that by adding the programmable reservoir layer, reservoir computing in materia can significantly outperform the original evolution in materia implementation. This suggests the RC framework offers improved performance, even across non-temporal tasks, when combined with the evolution in materia technique.},
  eventtitle = {2017 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})}
}

@article{Dale2021,
  title = {Reservoir Computing Quality: Connectivity and Topology},
  shorttitle = {Reservoir Computing Quality},
  author = {Dale, Matthew and O’Keefe, Simon and Sebald, Angelika and Stepney, Susan and Trefzer, Martin A.},
  date = {2021-06-01},
  journaltitle = {Natural Computing},
  shortjournal = {Nat Comput},
  volume = {20},
  number = {2},
  pages = {205--216},
  issn = {1572-9796},
  doi = {10.1007/s11047-020-09823-1},
  url = {https://doi.org/10.1007/s11047-020-09823-1},
  urldate = {2023-02-09},
  abstract = {We explore the effect of connectivity and topology on the dynamical behaviour of Reservoir Computers. At present, considerable effort is taken to design and hand-craft physical reservoir computers. Both structure and physical complexity are often pivotal to task performance, however, assessing their overall importance is challenging. Using a recently developed framework, we evaluate and compare the dynamical freedom (referring to quality) of neural network structures, as an analogy for physical systems. The results quantify how structure affects the behavioural range of networks. It demonstrates how high quality reached by more complex structures is often also achievable in simpler structures with greater network size. Alternatively, quality is often improved in smaller networks by adding greater connection complexity. This work demonstrates the benefits of using dynamical behaviour to assess the quality of computing substrates, rather than evaluation through benchmark tasks that often provide a narrow and biased insight into the computing quality of physical systems.},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/7H9PRYNH/Dale_et_al_2021_Reservoir_computing_quality.pdf}
}

@article{Darshan2017,
  title = {A Canonical Neural Mechanism for Behavioral Variability},
  author = {Darshan, Ran and Wood, William E. and Peters, Susan and Leblois, Arthur and Hansel, David},
  date = {2017-05-22},
  journaltitle = {Nature Communications},
  shortjournal = {Nat Commun},
  volume = {8},
  number = {1},
  pages = {15415},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/ncomms15415},
  url = {https://www.nature.com/articles/ncomms15415},
  urldate = {2024-02-28},
  abstract = {The ability to generate variable movements is essential for learning and adjusting complex behaviours. This variability has been linked to the temporal irregularity of neuronal activity in the central nervous system. However, how neuronal irregularity actually translates into behavioural variability is unclear. Here we combine modelling, electrophysiological and behavioural studies to address this issue. We demonstrate that a model circuit comprising topographically organized and strongly recurrent neural networks can autonomously generate irregular motor behaviours. Simultaneous recordings of neurons in singing finches reveal that neural correlations increase across the circuit driving song variability, in agreement with the model predictions. Analysing behavioural data, we find remarkable similarities in the babbling statistics of 5–6-month-old human infants and juveniles from three songbird species and show that our model naturally accounts for these ‘universal’ statistics.},
  issue = {1},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/CA45DX7P/Darshan_et_al_2017_A_canonical_neural_mechanism_for_behavioral_variability.pdf}
}

@article{DePasquale2018,
  title = {Full-{{FORCE}}: {{A}} Target-Based Method for Training Recurrent Networks},
  shorttitle = {Full-{{FORCE}}},
  author = {DePasquale, Brian and Cueva, Christopher J. and Rajan, Kanaka and Escola, G. Sean and Abbott, L. F.},
  date = {2018-02-07},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {13},
  number = {2},
  pages = {e0191527},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0191527},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0191527},
  urldate = {2021-06-21},
  abstract = {Trained recurrent networks are powerful tools for modeling dynamic neural computations. We present a target-based method for modifying the full connectivity matrix of a recurrent network to train it to perform tasks involving temporally complex input/output transformations. The method introduces a second network during training to provide suitable “target” dynamics useful for performing the task. Because it exploits the full recurrent connectivity, the method produces networks that perform tasks with fewer neurons and greater noise robustness than traditional least-squares (FORCE) approaches. In addition, we show how introducing additional input signals into the target-generating network, which act as task hints, greatly extends the range of tasks that can be learned and provides control over the complexity and nature of the dynamics of the trained, task-performing network.},
  langid = {english},
  keywords = {Algorithms,Eigenvalues,Learning,Machine learning,Network analysis,Neural networks,Signaling networks,White noise},
  file = {/Users/vitay/Documents/Zotero/storage/YZCWMXHE/DePasquale_et_al_2018_full-FORCE.pdf;/Users/vitay/Documents/Zotero/storage/BFCI5DC2/article.html}
}

@article{Dominey1995a,
  title = {Complex Sensory-Motor Sequence Learning Based on Recurrent State Representation and Reinforcement Learning},
  author = {Dominey, Peter F.},
  date = {1995-08},
  journaltitle = {Biological Cybernetics},
  volume = {73},
  number = {3},
  pages = {265--274},
  doi = {10.1007/BF00201428},
  url = {http://link.springer.com/10.1007/BF00201428},
  file = {/Users/vitay/Documents/Zotero/storage/P29BF753/Dominey_1995_Complex sensory-motor sequence learning based on recurrent state representation.pdf}
}

@article{Dominey2021,
  title = {Narrative Event Segmentation in the Cortical Reservoir},
  author = {Dominey, Peter Ford},
  date = {2021-10-07},
  journaltitle = {PLOS Computational Biology},
  shortjournal = {PLOS Computational Biology},
  volume = {17},
  number = {10},
  pages = {e1008993},
  publisher = {Public Library of Science},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1008993},
  url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008993},
  urldate = {2022-01-17},
  abstract = {Recent research has revealed that during continuous perception of movies or stories, humans display cortical activity patterns that reveal hierarchical segmentation of event structure. Thus, sensory areas like auditory cortex display high frequency segmentation related to the stimulus, while semantic areas like posterior middle cortex display a lower frequency segmentation related to transitions between events. These hierarchical levels of segmentation are associated with different time constants for processing. Likewise, when two groups of participants heard the same sentence in a narrative, preceded by different contexts, neural responses for the groups were initially different and then gradually aligned. The time constant for alignment followed the segmentation hierarchy: sensory cortices aligned most quickly, followed by mid-level regions, while some higher-order cortical regions took more than 10 seconds to align. These hierarchical segmentation phenomena can be considered in the context of processing related to comprehension. In a recently described model of discourse comprehension word meanings are modeled by a language model pre-trained on a billion word corpus. During discourse comprehension, word meanings are continuously integrated in a recurrent cortical network. The model demonstrates novel discourse and inference processing, in part because of two fundamental characteristics: real-world event semantics are represented in the word embeddings, and these are integrated in a reservoir network which has an inherent gradient of functional time constants due to the recurrent connections. Here we demonstrate how this model displays hierarchical narrative event segmentation properties beyond the embeddings alone, or their linear integration. The reservoir produces activation patterns that are segmented by a hidden Markov model (HMM) in a manner that is comparable to that of humans. Context construction displays a continuum of time constants across reservoir neuron subsets, while context forgetting has a fixed time constant across these subsets. Importantly, virtual areas formed by subgroups of reservoir neurons with faster time constants segmented with shorter events, while those with longer time constants preferred longer events. This neurocomputational recurrent neural network simulates narrative event processing as revealed by the fMRI event segmentation algorithm provides a novel explanation of the asymmetry in narrative forgetting and construction. The model extends the characterization of online integration processes in discourse to more extended narrative, and demonstrates how reservoir computing provides a useful model of cortical processing of narrative structure.},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/G99HQ8UX/Dominey_2021_Narrative_event_segmentation_in_the_cortical_reservoir.pdf;/Users/vitay/Documents/Zotero/storage/S7PY7RQH/article.html}
}

@article{Enel2016,
  title = {Reservoir {{Computing Properties}} of {{Neural Dynamics}} in {{Prefrontal Cortex}}},
  author = {Enel, Pierre and Procyk, Emmanuel and Quilodran, René and Dominey, Peter Ford},
  editor = {O'Reilly, Jill X},
  date = {2016-06},
  journaltitle = {PLOS Computational Biology},
  volume = {12},
  number = {6},
  pages = {e1004967-e1004967},
  doi = {10.1371/journal.pcbi.1004967},
  url = {https://dx.plos.org/10.1371/journal.pcbi.1004967},
  abstract = {Primates display a remarkable ability to adapt to novel situations. Determining what is most pertinent in these situations is not always possible based only on the current sensory inputs, and often also depends on recent inputs and behavioral outputs that contribute to internal states. Thus, one can ask how cortical dynamics generate representations of these complex situations. It has been observed that mixed selectivity in cortical neurons contributes to represent diverse situations defined by a combination of the current stimuli, and that mixed selectivity is readily obtained in randomly connected recurrent networks. In this context, these reservoir networks reproduce the highly recurrent nature of local cortical connectivity. Recombining present and past inputs, random recurrent networks from the reservoir computing framework generate mixed selectivity which provides pre-coded representations of an essentially universal set of contexts. These representations can then be selectively amplified through learning to solve the task at hand. We thus explored their representational power and dynamical properties after training a reservoir to perform a complex cognitive task initially developed for monkeys. The reservoir model inherently displayed a dynamic form of mixed selectivity, key to the representation of the behavioral context over time. The pre-coded representation of context was amplified by training a feedback neuron to explicitly represent this context, thereby reproducing the effect of learning and allowing the model to perform more robustly. This second version of the model demonstrates how a hybrid dynamical regime combining spatio-temporal processing of reservoirs, and input driven attracting dynamics generated by the feedback neuron, can be used to solve a complex cognitive task. We compared reservoir activity to neural activity of dorsal anterior cingulate cortex of monkeys which revealed similar network dynamics. We argue that reservoir computing is a pertinent framework to model local cortical dynamics and their contribution to higher cognitive function.},
  file = {/Users/vitay/Documents/Zotero/storage/V2EW2YDP/Enel et al_2016_Reservoir Computing Properties of Neural Dynamics in Prefrontal Cortex.pdf}
}

@online{Evanusa2020,
  title = {Deep {{Reservoir Networks}} with {{Learned Hidden Reservoir Weights}} Using {{Direct Feedback Alignment}}},
  author = {Evanusa, Matthew and Fermüller, Cornelia and Aloimonos, Yiannis},
  date = {2020-10-14},
  eprint = {2010.06209},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2010.06209},
  urldate = {2023-02-01},
  abstract = {Deep Reservoir Computing has emerged as a new paradigm for deep learning, which is based around the reservoir computing principle of maintaining random pools of neurons combined with hierarchical deep learning. The reservoir paradigm reflects and respects the high degree of recurrence in biological brains, and the role that neuronal dynamics play in learning. However, one issue hampering deep reservoir network development is that one cannot backpropagate through the reservoir layers. Recent deep reservoir architectures do not learn hidden or hierarchical representations in the same manner as deep artificial neural networks, but rather concatenate all hidden reservoirs together to perform traditional regression. Here we present a novel Deep Reservoir Network for time series prediction and classification that learns through the non-differentiable hidden reservoir layers using a biologically-inspired backpropagation alternative called Direct Feedback Alignment, which resembles global dopamine signal broadcasting in the brain. We demonstrate its efficacy on two real world multidimensional time series datasets.},
  pubstate = {prepublished},
  file = {/Users/vitay/Documents/Zotero/storage/AB7NFCKV/Evanusa_et_al_2020_Deep_Reservoir_Networks_with_Learned_Hidden_Reservoir_Weights_using_Direct.pdf}
}

@inproceedings{Fernando2003,
  title = {Pattern {{Recognition}} in a {{Bucket}}},
  booktitle = {Advances in {{Artificial Life}}},
  author = {Fernando, Chrisantha and Sojakka, Sampsa},
  editor = {Banzhaf, Wolfgang and Ziegler, Jens and Christaller, Thomas and Dittrich, Peter and Kim, Jan T.},
  date = {2003},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {588--597},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-39432-7_63},
  abstract = {This paper demonstrates that the waves produced on the surface of water can be used as the medium for a “Liquid State Machine” that pre-processes inputs so allowing a simple perceptron to solve the XOR problem and undertake speech recognition. Interference between waves allows non-linear parallel computation upon simultaneous sensory inputs. Temporal patterns of stimulation are converted to spatial patterns of water waves upon which a linear discrimination can be made. Whereas Wolfgang Maass’ Liquid State Machine requires fine tuning of the spiking neural network parameters, water has inherent self-organising properties such as strong local interactions, time-dependent spread of activation to distant areas, inherent stability to a wide variety of inputs, and high complexity. Water achieves this “for free”, and does so without the time-consuming computation required by realistic neural models. An analogy is made between water molecules and neurons in a recurrent neural network.},
  isbn = {978-3-540-39432-7},
  langid = {english},
  keywords = {Neural Network Parameter,Recurrent Neural Network,Separation Property,Sound Sample,Speech Recognition},
  file = {/Users/vitay/Documents/Zotero/storage/F6EGQSKL/Fernando_Sojakka_2003_Pattern_Recognition_in_a_Bucket.pdf}
}

@article{Fiete2006,
  title = {Gradient {{Learning}} in {{Spiking Neural Networks}} by {{Dynamic Perturbation}} of {{Conductances}}},
  author = {Fiete, Ila R. and Seung, H. Sebastian},
  date = {2006-07-28},
  journaltitle = {Physical Review Letters},
  shortjournal = {Phys. Rev. Lett.},
  volume = {97},
  number = {4},
  pages = {048104},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.97.048104},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.97.048104},
  urldate = {2023-05-15},
  abstract = {We present a method of estimating the gradient of an objective function with respect to the synaptic weights of a spiking neural network. The method works by measuring the fluctuations in the objective function in response to dynamic perturbation of the membrane conductances of the neurons. It is compatible with recurrent networks of conductance-based model neurons with dynamic synapses. The method can be interpreted as a biologically plausible synaptic learning rule, if the dynamic perturbations are generated by a special class of “empiric” synapses driven by random spike trains from an external source.},
  file = {/Users/vitay/Documents/Zotero/storage/D3VS5RRN/Fiete_Seung_2006_Gradient_Learning_in_Spiking_Neural_Networks_by_Dynamic_Perturbation_of.pdf}
}

@thesis{Fonlladosa2013,
  title = {Machine {{Learning}} with {{Chaotic Recurrent Neural Networks}}},
  author = {Fonlladosa, Thomas},
  date = {2013},
  institution = {Imperial College London},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/XMQGIIXY/Fonlladosa_2013_Machine Learning with Chaotic Recurrent Neural Networks.pdf}
}

@article{Frega2014,
  title = {Network Dynamics of {{3D}} Engineered Neuronal Cultures: A New Experimental Model for in-Vitro Electrophysiology},
  shorttitle = {Network Dynamics of {{3D}} Engineered Neuronal Cultures},
  author = {Frega, Monica and Tedesco, Mariateresa and Massobrio, Paolo and Pesce, Mattia and Martinoia, Sergio},
  date = {2014-06-30},
  journaltitle = {Scientific Reports},
  volume = {4},
  number = {1},
  pages = {1--14},
  issn = {2045-2322},
  doi = {10.1038/srep05489},
  url = {https://www.nature.com/articles/srep05489},
  urldate = {2020-01-25},
  abstract = {Despite the extensive use of in-vitro models for neuroscientific investigations and notwithstanding the growing field of network electrophysiology, all studies on cultured cells devoted to elucidate neurophysiological mechanisms and computational properties, are based on 2D neuronal networks. These networks are usually grown onto specific rigid substrates (also with embedded electrodes) and lack of most of the constituents of the in-vivo like environment: cell morphology, cell-to-cell interaction and neuritic outgrowth in all directions. Cells in a brain region develop in a 3D space and interact with a complex multi-cellular environment and extracellular matrix. Under this perspective, 3D networks coupled to micro-transducer arrays, represent a new and powerful in-vitro model capable of better emulating in-vivo physiology. In this work, we present a new experimental paradigm constituted by 3D hippocampal networks coupled to Micro-Electrode-Arrays (MEAs) and we show how the features of the recorded network dynamics differ from the corresponding 2D network model. Further development of the proposed 3D in-vitro model by adding embedded functionalized scaffolds might open new prospects for manipulating, stimulating and recording the neuronal activity to elucidate neurophysiological mechanisms and to design bio-hybrid microsystems.},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/SU77RWWJ/Frega_et_al_2014_Network_dynamics_of_3D_engineered_neuronal_cultures.pdf;/Users/vitay/Documents/Zotero/storage/X8HS8QLP/srep05489.html}
}

@inproceedings{Gallicchio2016,
  title = {Deep {{Reservoir Computing}}: {{A Critical Analysis}}},
  booktitle = {European {{Symposium}} on {{Artificial Neural Networks}}, {{Computational Intelligence}} and {{Machine Learning}}},
  author = {Gallicchio, Claudio and Micheli, Alessio},
  date = {2016},
  pages = {27--29},
  abstract = {In this paper we propose an empirical analysis of deep recur-rent neural networks (RNNs) with stacked layers. The analysis aims at the study and proposal of approaches to develop and enhance multiple time-scale and hierarchical dynamics in deep recurrent architectures, within the efficient Reservoir Computing (RC) approach for RNN modeling. Results point out the actual relevance of layering and RC parameters aspects on the diversification of temporal representations in deep recurrent models.},
  isbn = {978-2-87587-027-8},
  file = {/Users/vitay/Documents/Zotero/storage/8M973746/Gallicchio_Micheli_2016_Deep Reservoir Computing.pdf}
}

@article{Gallicchio2017,
  title = {Deep Reservoir Computing: {{A}} Critical Experimental Analysis},
  shorttitle = {Deep Reservoir Computing},
  author = {Gallicchio, Claudio and Micheli, Alessio and Pedrelli, Luca},
  date = {2017-12-13},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  series = {Advances in Artificial Neural Networks, Machine Learning and Computational Intelligence},
  volume = {268},
  pages = {87--99},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2016.12.089},
  url = {http://www.sciencedirect.com/science/article/pii/S0925231217307567},
  urldate = {2019-04-16},
  abstract = {In this paper, we propose an empirical analysis of deep recurrent neural network (RNN) architectures with stacked layers. The main aim is to address some fundamental open research issues on the significance of creating deep layered architectures in RNN and to characterize the inherent hierarchical representation of time in such models, especially for efficient implementations. In particular, the analysis aims at the study and proposal of approaches to develop and enhance hierarchical dynamics in deep architectures within the efficient Reservoir Computing (RC) framework for RNN modeling. The effect of a deep layered organization of RC models is investigated in terms of both occurrence of multiple time-scale and increasing of richness of the dynamics. It turns out that a deep layering of recurrent models allows an effective diversification of temporal representations in the layers of the hierarchy, by amplifying the effects of the factors influencing the time-scales and the richness of the dynamics, measured as the entropy of recurrent units activations. The advantages of the proposed approach are also highlighted by measuring the increment of the short-term memory capacity of the RC models.},
  keywords = {Deep Learning,Deep neural networks,Echo State Networks,Multiple time-scale dynamics,Recurrent neural networks,Reservoir computing},
  file = {/Users/vitay/Documents/Zotero/storage/2FZ8SAQU/S0925231217307567.html}
}

@article{Gallicchio2017a,
  title = {Echo {{State Property}} of {{Deep Reservoir Computing Networks}}},
  author = {Gallicchio, Claudio and Micheli, Alessio},
  date = {2017-06-01},
  journaltitle = {Cognitive Computation},
  shortjournal = {Cogn Comput},
  volume = {9},
  number = {3},
  pages = {337--350},
  issn = {1866-9964},
  doi = {10.1007/s12559-017-9461-9},
  url = {https://doi.org/10.1007/s12559-017-9461-9},
  urldate = {2020-02-21},
  abstract = {In the last years, the Reservoir Computing (RC) framework has emerged as a state of-the-art approach for efficient learning in temporal domains. Recently, within the RC context, deep Echo State Network (ESN) models have been proposed. Being composed of a stack of multiple non-linear reservoir layers, deep ESNs potentially allow to exploit the advantages of a hierarchical temporal feature representation at different levels of abstraction, at the same time preserving the training efficiency typical of the RC methodology. In this paper, we generalize to the case of deep architectures the fundamental RC conditions related to the Echo State Property (ESP), based on the study of stability and contractivity of the resulting dynamical system. Besides providing a necessary condition and a sufficient condition for the ESP of layered RC networks, the results of our analysis provide also insights on the nature of the state dynamics in hierarchically organized recurrent models. In particular, we find out that by adding layers to a deep reservoir architecture, the regime of network’s dynamics can only be driven towards (equally or) less stable behaviors. Moreover, our investigation shows the intrinsic ability of temporal dynamics differentiation at the different levels in a deep recurrent architecture, with higher layers in the stack characterized by less contractive dynamics. Such theoretical insights are further supported by experimental results that show the effect of layering in terms of a progressively increased short-term memory capacity of the recurrent models.},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/LDE77KR8/Gallicchio_Micheli_2017_Echo_State_Property_of_Deep_Reservoir_Computing_Networks.pdf}
}

@article{Galtier2014,
  title = {Relative Entropy Minimizing Noisy Non-Linear Neural Network to Approximate Stochastic Processes},
  author = {Galtier, Mathieu N. and Marini, Camille and Wainrib, Gilles and Jaeger, Herbert},
  date = {2014-08},
  journaltitle = {Neural Networks},
  volume = {56},
  pages = {10--21},
  doi = {10.1016/J.NEUNET.2014.04.002},
  url = {https://www.sciencedirect.com/science/article/pii/S0893608014000860?via%3Dihub},
  abstract = {A method is provided for designing and training noise-driven recurrent neural networks as models of stochastic processes. The method unifies and generalizes two known separate modeling approaches, Echo State Networks (ESN) and Linear Inverse Modeling (LIM), under the common principle of relative entropy minimization. The power of the new method is demonstrated on a stochastic approximation of the El Niño phenomenon studied in climate research.},
  file = {/Users/vitay/Documents/Zotero/storage/7L8Z49IT/Galtier et al_2014_Relative entropy minimizing noisy non-linear neural network to approximate.pdf}
}

@article{Gauthier2021,
  title = {Next Generation Reservoir Computing},
  author = {Gauthier, Daniel J. and Bollt, Erik and Griffith, Aaron and Barbosa, Wendson A. S.},
  date = {2021-09-21},
  journaltitle = {Nature Communications},
  shortjournal = {Nat Commun},
  volume = {12},
  number = {1},
  pages = {5564},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/s41467-021-25801-2},
  url = {https://www.nature.com/articles/s41467-021-25801-2},
  urldate = {2021-09-24},
  abstract = {Reservoir computing is a best-in-class machine learning algorithm for processing information generated by dynamical systems using observed time-series data. Importantly, it requires very small training data sets, uses linear optimization, and thus requires minimal computing resources. However, the algorithm uses randomly sampled matrices to define the underlying recurrent neural network and has a multitude of metaparameters that must be optimized. Recent results demonstrate the equivalence of reservoir computing to nonlinear vector autoregression, which requires no random matrices, fewer metaparameters, and provides interpretable results. Here, we demonstrate that nonlinear vector autoregression excels at reservoir computing benchmark tasks and requires even shorter training data sets and training time, heralding the next generation of reservoir computing.},
  issue = {1},
  langid = {english},
  annotation = {Bandiera\_abtest: a\\
Cc\_license\_type: cc\_by\\
Cg\_type: Nature Research Journals\\
Primary\_atype: Research\\
Subject\_term: Computational science;Electrical and electronic engineering\\
Subject\_term\_id: computational-science;electrical-and-electronic-engineering},
  file = {/Users/vitay/Documents/Zotero/storage/IB8L7YFB/Gauthier_et_al_2021_Next_generation_reservoir_computing.pdf;/Users/vitay/Documents/Zotero/storage/I4AWYZVY/s41467-021-25801-2.html}
}

@article{Grigoryeva2018,
  title = {Echo State Networks Are Universal},
  author = {Grigoryeva, Lyudmila and Ortega, Juan-Pablo},
  date = {2018-12-01},
  journaltitle = {Neural Networks},
  shortjournal = {Neural Networks},
  volume = {108},
  pages = {495--508},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2018.08.025},
  url = {http://www.sciencedirect.com/science/article/pii/S089360801830251X},
  urldate = {2019-04-16},
  abstract = {This paper shows that echo state networks are universal uniform approximants in the context of discrete-time fading memory filters with uniformly bounded inputs defined on negative infinite times. This result guarantees that any fading memory input/output system in discrete time can be realized as a simple finite-dimensional neural network-type state-space model with a static linear readout map. This approximation is valid for infinite time intervals. The proof of this statement is based on fundamental results, also presented in this work, about the topological nature of the fading memory property and about reservoir computing systems generated by continuous reservoir maps.},
  keywords = {Echo state networks (ESN),Fading memory filters,Machine learning,Reservoir computing (RC),Uniform system approximation,Universality},
  file = {/Users/vitay/Documents/Zotero/storage/96HEJ3RL/Grigoryeva_Ortega_2018_Echo state networks are universal.pdf;/Users/vitay/Documents/Zotero/storage/BYYR32ZU/S089360801830251X.html}
}

@book{Haykin2002,
  title = {Adaptive Filter Theory},
  author = {Haykin, Simon S.},
  date = {2002},
  publisher = {Prentice Hall},
  abstract = {3rd ed. Ch. 1. Discrete-Time Signal Processing -- Ch. 2. Stationary Processes and Models -- Ch. 3. Spectrum Analyis -- Ch. 4. Eigenanalysis -- Ch. 5. Wiener Filters -- Ch. 6. Linear Prediction -- Ch. 7. Kalman Filters -- Ch. 8. Method of Steepest Descent -- Ch. 9. Least-Mean-Square Algorithm -- Ch. 10. Frequency-Domain Adaptive Filters -- Ch. 11. Method of Least Squares -- Ch. 12. Rotations and Reflections -- Ch. 13. Recursive Least-Squares Algorithm -- Ch. 14. Square-Root Adaptive Filters -- Ch. 15. Order-Recursive Adaptive Filters -- Ch. 16. Tracking of Time-Varying Systems -- Ch. 17. Fine-Precision Effects -- Ch. 18. Blind Deconvolution -- Ch. 19. Back-Propagation Learning -- Ch. 20. Radial Basis Funuction Networks -- Appendix A Complex Variables -- Appendix B Differentiation with Respect to a Vector -- Appendix C Method of Lagrange Multipliers -- Appendix D Estimation Theory -- Appendix E Maximum-Entropy Method.},
  isbn = {0-13-322760-X},
  pagetotal = {989}
}

@article{Hennequin2017,
  title = {Neural Networks Subtract and Conquer},
  author = {Hennequin, Guillaume},
  date = {2017-04-26},
  journaltitle = {eLife},
  volume = {6},
  issn = {2050-084X},
  doi = {10.7554/eLife.26157},
  url = {https://elifesciences.org/articles/26157},
  urldate = {2019-03-03},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/SGWNX2EH/Hennequin_2017_Neural networks subtract and conquer.pdf}
}

@article{Hinaut2013,
  title = {Real-{{Time Parallel Processing}} of {{Grammatical Structure}} in the {{Fronto-Striatal System}}: {{A Recurrent Network Simulation Study Using Reservoir Computing}}},
  shorttitle = {Real-{{Time Parallel Processing}} of {{Grammatical Structure}} in the {{Fronto-Striatal System}}},
  author = {Hinaut, Xavier and Dominey, Peter Ford},
  date = {2013-02-01},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {8},
  number = {2},
  pages = {e52946},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0052946},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0052946},
  urldate = {2020-01-25},
  abstract = {Sentence processing takes place in real-time. Previous words in the sentence can influence the processing of the current word in the timescale of hundreds of milliseconds. Recent neurophysiological studies in humans suggest that the fronto-striatal system (frontal cortex, and striatum – the major input locus of the basal ganglia) plays a crucial role in this process. The current research provides a possible explanation of how certain aspects of this real-time processing can occur, based on the dynamics of recurrent cortical networks, and plasticity in the cortico-striatal system. We simulate prefrontal area BA47 as a recurrent network that receives on-line input about word categories during sentence processing, with plastic connections between cortex and striatum. We exploit the homology between the cortico-striatal system and reservoir computing, where recurrent frontal cortical networks are the reservoir, and plastic cortico-striatal synapses are the readout. The system is trained on sentence-meaning pairs, where meaning is coded as activation in the striatum corresponding to the roles that different nouns and verbs play in the sentences. The model learns an extended set of grammatical constructions, and demonstrates the ability to generalize to novel constructions. It demonstrates how early in the sentence, a parallel set of predictions are made concerning the meaning, which are then confirmed or updated as the processing of the input sentence proceeds. It demonstrates how on-line responses to words are influenced by previous words in the sentence, and by previous sentences in the discourse, providing new insight into the neurophysiology of the P600 ERP scalp response to grammatical complexity. This demonstrates that a recurrent neural network can decode grammatical structure from sentences in real-time in order to generate a predictive representation of the meaning of the sentences. This can provide insight into the underlying mechanisms of human cortico-striatal function in sentence processing.},
  langid = {english},
  keywords = {Grammar,Lexical semantics,Morphology (linguistics),Neostriatum,Neurons,Semantics,Sentence processing,Syntax},
  file = {/Users/vitay/Documents/Zotero/storage/GN4XMQRJ/Hinaut_Dominey_2013_Real-Time_Parallel_Processing_of_Grammatical_Structure_in_the_Fronto-Striatal.pdf;/Users/vitay/Documents/Zotero/storage/5QSZLYHA/article.html}
}

@article{Hinaut2015,
  title = {Corticostriatal Response Selection in Sentence Production: {{Insights}} from Neural Network Simulation with Reservoir Computing},
  shorttitle = {Corticostriatal Response Selection in Sentence Production},
  author = {Hinaut, Xavier and Lance, Florian and Droin, Colas and Petit, Maxime and Pointeau, Gregoire and Dominey, Peter Ford},
  date = {2015-11-01},
  journaltitle = {Brain and Language},
  shortjournal = {Brain and Language},
  volume = {150},
  pages = {54--68},
  issn = {0093-934X},
  doi = {10.1016/j.bandl.2015.08.002},
  url = {http://www.sciencedirect.com/science/article/pii/S0093934X1500173X},
  urldate = {2020-04-06},
  abstract = {Language production requires selection of the appropriate sentence structure to accommodate the communication goal of the speaker – the transmission of a particular meaning. Here we consider event meanings, in terms of predicates and thematic roles, and we address the problem that a given event can be described from multiple perspectives, which poses a problem of response selection. We present a model of response selection in sentence production that is inspired by the primate corticostriatal system. The model is implemented in the context of reservoir computing where the reservoir – a recurrent neural network with fixed connections – corresponds to cortex, and the readout corresponds to the striatum. We demonstrate robust learning, and generalization properties of the model, and demonstrate its cross linguistic capabilities in English and Japanese. The results contribute to the argument that the corticostriatal system plays a role in response selection in language production, and to the stance that reservoir computing is a valid potential model of corticostriatal processing.},
  langid = {english},
  keywords = {Corticostriatal system,Language,Model,Neural network,Sentence production},
  file = {/Users/vitay/Documents/Zotero/storage/W9926XSU/Hinaut_et_al_2015_Corticostriatal_response_selection_in_sentence_production.pdf;/Users/vitay/Documents/Zotero/storage/73ICCPRR/S0093934X1500173X.html}
}

@article{Hoerzer2014,
  title = {Emergence of Complex Computational Structures from Chaotic Neural Networks through Reward-Modulated {{Hebbian}} Learning},
  author = {Hoerzer, Gregor M. and Legenstein, Robert and Maass, Wolfgang},
  date = {2014-03},
  journaltitle = {Cerebral Cortex (New York, N.Y.: 1991)},
  shortjournal = {Cereb Cortex},
  volume = {24},
  number = {3},
  eprint = {23146969},
  eprinttype = {pmid},
  pages = {677--690},
  issn = {1460-2199},
  doi = {10.1093/cercor/bhs348},
  abstract = {This paper addresses the question how generic microcircuits of neurons in different parts of the cortex can attain and maintain different computational specializations. We show that if stochastic variations in the dynamics of local microcircuits are correlated with signals related to functional improvements of the brain (e.g. in the control of behavior), the computational operation of these microcircuits can become optimized for specific tasks such as the generation of specific periodic signals and task-dependent routing of information. Furthermore, we show that working memory can autonomously emerge through reward-modulated Hebbian learning, if needed for specific tasks. Altogether, our results suggest that reward-modulated synaptic plasticity can not only optimize the network parameters for specific computational tasks, but also initiate a functional rewiring that re-programs microcircuits, thereby generating diverse computational functions in different generic cortical microcircuits. On a more general level, this work provides a new perspective for a standard model for computations in generic cortical microcircuits (liquid computing model). It shows that the arguably most problematic assumption of this model, the postulate of a teacher that trains neural readouts through supervised learning, can be eliminated. We show that generic networks of neurons can learn numerous biologically relevant computations through trial and error.},
  langid = {english},
  keywords = {Action Potentials,Computer Simulation,cortical microcircuit model,cortical plasticity,Humans,Learning,Models Neurological,Neural Networks Computer,Neurons,pattern generation,Reward,working memory},
  file = {/Users/vitay/Documents/Zotero/storage/YVVKC9EL/Hoerzer_et_al_2014_Emergence_of_complex_computational_structures_from_chaotic_neural_networks.pdf}
}

@article{Ingrosso2019,
  title = {Training Dynamically Balanced Excitatory-Inhibitory Networks},
  author = {Ingrosso, Alessandro and Abbott, L. F.},
  date = {2019-08-08},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {14},
  number = {8},
  pages = {e0220547},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0220547},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0220547},
  urldate = {2020-08-12},
  abstract = {The construction of biologically plausible models of neural circuits is crucial for understanding the computational properties of the nervous system. Constructing functional networks composed of separate excitatory and inhibitory neurons obeying Dale’s law presents a number of challenges. We show how a target-based approach, when combined with a fast online constrained optimization technique, is capable of building functional models of rate and spiking recurrent neural networks in which excitation and inhibition are balanced. Balanced networks can be trained to produce complicated temporal patterns and to solve input-output tasks while retaining biologically desirable features such as Dale’s law and response variability.},
  langid = {english},
  keywords = {Action potentials,Eigenvalues,Human learning,Learning,Neural networks,Neurons,Synapses,Teachers},
  file = {/Users/vitay/Documents/Zotero/storage/EDPLQJFA/Ingrosso_Abbott_2019_Training_dynamically_balanced_excitatory-inhibitory_networks.pdf;/Users/vitay/Documents/Zotero/storage/Z98EK5QA/article.html}
}

@article{Inubushi2017,
  title = {Reservoir {{Computing Beyond Memory-Nonlinearity Trade-off}}},
  author = {Inubushi, Masanobu and Yoshimura, Kazuyuki},
  date = {2017-08-31},
  journaltitle = {Scientific Reports},
  volume = {7},
  number = {1},
  pages = {10199},
  issn = {2045-2322},
  doi = {10.1038/s41598-017-10257-6},
  url = {https://www.nature.com/articles/s41598-017-10257-6},
  urldate = {2019-04-08},
  abstract = {Reservoir computing is a brain-inspired machine learning framework that employs a signal-driven dynamical system, in particular harnessing common-signal-induced synchronization which is a widely observed nonlinear phenomenon. Basic understanding of a working principle in reservoir computing can be expected to shed light on how information is stored and processed in nonlinear dynamical systems, potentially leading to progress in a broad range of nonlinear sciences. As a first step toward this goal, from the viewpoint of nonlinear physics and information theory, we study the memory-nonlinearity trade-off uncovered by Dambre et al. (2012). Focusing on a variational equation, we clarify a dynamical mechanism behind the trade-off, which illustrates why nonlinear dynamics degrades memory stored in dynamical system in general. Moreover, based on the trade-off, we propose a mixture reservoir endowed with both linear and nonlinear dynamics and show that it improves the performance of information processing. Interestingly, for some tasks, significant improvements are observed by adding a few linear dynamics to the nonlinear dynamical system. By employing the echo state network model, the effect of the mixture reservoir is numerically verified for a simple function approximation task and for more complex tasks.},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/3BC6MURA/Inubushi_Yoshimura_2017_Reservoir Computing Beyond Memory-Nonlinearity Trade-off.pdf;/Users/vitay/Documents/Zotero/storage/YTN8954X/s41598-017-10257-6.html}
}

@inproceedings{Ivanov2021,
  title = {Increasing {{Liquid State Machine Performance}} with {{Edge-of-Chaos Dynamics Organized}} by {{Astrocyte-modulated Plasticity}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Ivanov, Vladimir and Michmizos, Konstantinos},
  date = {2021},
  volume = {34},
  pages = {25703--25719},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper_files/paper/2021/hash/d79c8788088c2193f0244d8f1f36d2db-Abstract.html},
  urldate = {2024-07-18},
  file = {/Users/vitay/Documents/Zotero/storage/RDBGBKZR/Ivanov and Michmizos - 2021 - Increasing Liquid State Machine Performance with Edge-of-Chaos Dynamics Organized by Astrocyte-modul.pdf}
}

@report{Jaeger2001,
  title = {The "Echo State" Approach to Analysing and Training Recurrent Neural Networks},
  author = {Jaeger, Herbert},
  date = {2001},
  pages = {GMD Report 148-GMD Report 148},
  institution = {Jacobs Universität Bremen},
  url = {http://www.faculty.jacobs-university.de/hjaeger/pubs/EchoStatesTechRep.pdf}
}

@inproceedings{Jaeger2005,
  title = {Reservoir Riddles: Suggestions for Echo State Network Research (Extended Abstract)},
  booktitle = {Proceedings of {{International Joint Conference}} on {{Neural Networks}}},
  author = {Jaeger, H.},
  date = {2005},
  volume = {3},
  pages = {1460--1462},
  publisher = {IEEE},
  doi = {10.1109/IJCNN.2005.1556090},
  url = {http://ieeexplore.ieee.org/document/1556090/},
  isbn = {0-7803-9048-2},
  file = {/Users/vitay/Documents/Zotero/storage/LVHMUKFN/Jaeger_2005_Reservoir riddles.pdf}
}

@article{Jaeger2007,
  title = {Echo State Network},
  author = {Jaeger, Herbert},
  date = {2007},
  journaltitle = {Scholarpedia},
  volume = {2},
  number = {9},
  pages = {2330--2330},
  doi = {10.4249/scholarpedia.2330},
  url = {http://www.scholarpedia.org/article/Echo_state_network}
}

@article{Jaeger2017,
  title = {Using {{Conceptors}} to {{Manage Neural Long-Term Memories}} for {{Temporal Patterns}}},
  author = {Jaeger, Herbert},
  date = {2017},
  journaltitle = {Journal of Machine Learning Research},
  volume = {18},
  number = {13},
  pages = {1--43},
  url = {http://www.jmlr.org/papers/v18/15-449.html},
  file = {/Users/vitay/Documents/Zotero/storage/6ESYLJTX/Jaeger_2017_Using Conceptors to Manage Neural Long-Term Memories for Temporal Patterns.pdf}
}

@article{Jarvis2010,
  title = {Extending {{Stability Through Hierarchical Clusters}} in {{Echo State Networks}}},
  author = {Jarvis, Sarah and Rotter, Stefan and Egert, Ulrich},
  date = {2010-07-07},
  journaltitle = {Frontiers in Neuroinformatics},
  shortjournal = {Front. Neuroinform.},
  volume = {4},
  publisher = {Frontiers},
  issn = {1662-5196},
  doi = {10.3389/fninf.2010.00011},
  url = {https://www.frontiersin.org/articles/10.3389/fninf.2010.00011},
  urldate = {2024-06-03},
  abstract = {Echo State Networks (ESN) are reservoir networks that satisfy well-established criteria for stability when constructed as feedforward networks. Recent evidence suggests that stability criteria are altered in the presence of reservoir substructures, such as clusters. Understanding how the reservoir architecture affects stability is thus important for the appropriate design of any ESN. To quantitatively determine the influence of the most relevant network parameters, we analysed the impact of reservoir substructures on stability in hierarchically clustered ESNs (HESN), as they allow a smooth transition from highly structured to increasingly homogeneous reservoirs. Previous studies used the largest eigenvalue of the reservoir connectivity matrix (spectral radius) as a predictor for stable network dynamics. Here, we evaluate the impact of clusters, hierarchy and intercluster connectivity on the predictive power of the spectral radius for stability. Both hierarchy and low relative cluster sizes extend the range of spectral radius values, leading to stable networks, while increasing intercluster connectivity decreased maximal spectral radius.},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/SCHCYRJV/Jarvis et al. - 2010 - Extending Stability Through Hierarchical Clusters in Echo State Networks.pdf}
}

@incollection{Jiang2008,
  title = {Supervised and {{Evolutionary Learning}} of {{Echo State Networks}}},
  author = {Jiang, Fei and Berry, Hugues and Schoenauer, Marc},
  date = {2008},
  pages = {215--224},
  publisher = {Springer, Berlin, Heidelberg},
  doi = {10.1007/978-3-540-87700-4_22},
  url = {http://link.springer.com/10.1007/978-3-540-87700-4_22}
}

@inproceedings{Jones2007,
  title = {Is There a {{Liquid State Machine}} in the {{Bacterium Escherichia Coli}}?},
  booktitle = {2007 {{IEEE Symposium}} on {{Artificial Life}}},
  author = {Jones, Ben and Stekel, Dov and Rowe, Jon and Fernando, Chrisantha},
  date = {2007-04},
  pages = {187--191},
  issn = {2160-6382},
  doi = {10.1109/ALIFE.2007.367795},
  abstract = {The bacterium Escherichia coli has the capacity to respond to a wide range of environmental inputs, which have the potential to change suddenly and rapidly. Although the functions of many of its signal transduction and gene regulation networks have been identified, E.Coli's capacity for perceptual categorization, especially for discrimination between complex temporal patterns of chemical inputs, has been experimentally neglected. Real-time computations on time-varying inputs can be undertaken by a system possessing a high dimensional analog fading memory, i.e. a liquid-state machine (LSM). For example, the cortical microcolumn is hypothesized to be a LSM. A model of the gene regulation network (GRN) of E.Coli was assessed for its LSM properties for a range of increasingly complex stimuli. Cooperativity between transcription factors (TFs) is necessary for complex temporal discriminations. However, the low recurrence within the GRNs autonomous dynamics decreases its capacity for a rich fading memory, and hence for integrating temporal sequence information. We conclude that coupling of the GRN with signal transduction networks possessing cross-talk, and with metabolic networks is expected to increase the extent of non-autonomous recurrence and hence to facilitate enhanced LSM properties.},
  eventtitle = {2007 {{IEEE Symposium}} on {{Artificial Life}}},
  keywords = {bacterium Escherichia coli,biology computing,Chemical elements,Computer science,cortical microcolumn,cross-talk,DNA,environmental inputs,Fading,fading memory,gene regulation network,genetics,liquid state machine,metabolic networks,microorganisms,Network topology,Proteins,Real time systems,Sequences,Signal processing,signal transduction networks,transcription factors,Transfer functions},
  file = {/Users/vitay/Documents/Zotero/storage/425X8GTE/Jones_et_al_2007_Is_there_a_Liquid_State_Machine_in_the_Bacterium_Escherichia_Coli.pdf;/Users/vitay/Documents/Zotero/storage/T3JR9PBF/4218885.html}
}

@article{Jonke2017,
  title = {Feedback {{Inhibition Shapes Emergent Computational Properties}} of {{Cortical Microcircuit Motifs}}.},
  author = {Jonke, Zeno and Legenstein, Robert and Habenschuss, Stefan and Maass, Wolfgang},
  date = {2017-08},
  journaltitle = {The Journal of neuroscience : the official journal of the Society for Neuroscience},
  volume = {37},
  number = {35},
  eprint = {28760861},
  eprinttype = {pubmed},
  pages = {8511--8523},
  doi = {10.1523/JNEUROSCI.2078-16.2017},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/28760861},
  abstract = {Cortical microcircuits are very complex networks, but they are composed of a relatively small number of stereotypical motifs. Hence, one strategy for throwing light on the computational function of cortical microcircuits is to analyze emergent computational properties of these stereotypical microcircuit motifs. We are addressing here the question how spike timing-dependent plasticity shapes the computational properties of one motif that has frequently been studied experimentally: interconnected populations of pyramidal cells and parvalbumin-positive inhibitory cells in layer 2/3. Experimental studies suggest that these inhibitory neurons exert some form of divisive inhibition on the pyramidal cells. We show that this data-based form of feedback inhibition, which is softer than that of winner-take-all models that are commonly considered in theoretical analyses, contributes to the emergence of an important computational function through spike timing-dependent plasticity: The capability to disentangle superimposed firing patterns in upstream networks, and to represent their information content through a sparse assembly code.SIGNIFICANCE STATEMENT We analyze emergent computational properties of a ubiquitous cortical microcircuit motif: populations of pyramidal cells that are densely interconnected with inhibitory neurons. Simulations of this model predict that sparse assembly codes emerge in this microcircuit motif under spike timing-dependent plasticity. Furthermore, we show that different assemblies will represent different hidden sources of upstream firing activity. Hence, we propose that spike timing-dependent plasticity enables this microcircuit motif to perform a fundamental computational operation on neural activity patterns.},
  keywords = {computational function,cortical microcircuits,divisive inhibition,feedback inhibition,synaptic plasticity,winner-take-all},
  file = {/Users/vitay/Documents/Zotero/storage/RRWNDJQF/Jonke et al_2017_Feedback Inhibition Shapes Emergent Computational Properties of Cortical.pdf}
}

@inproceedings{Joshi2009a,
  title = {Optimizing {{Generic Neural Microcircuits}} through {{Reward Modulated STDP}}},
  booktitle = {Artificial {{Neural Networks}} – {{ICANN}} 2009},
  author = {Joshi, Prashant and Triesch, Jochen},
  editor = {Alippi, Cesare and Polycarpou, Marios and Panayiotou, Christos and Ellinas, Georgios},
  date = {2009},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {239--248},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-04274-4_25},
  abstract = {How can we characterize if a given neural circuit is optimal for the class of computational operations that it has to perform on a certain input distribution? We show that modifying the efficacies of recurrent synapses in a generic neural microcircuit via spike timing dependent plasticity (STDP) can optimize the circuit in an unsupervised fashion for a particular input distribution if STDP is modulated by a global reward signal. More precisely, optimizing microcircuits through reward modulated STDP leads to a lower eigen-value spread of the cross-correlation matrix, higher entropy, highly decorrelated neural activity, and tunes the circuit dynamics to a regime that requires a large number of principal components for representing the information contained in the liquid state as compared to randomly drawn microcircuits. Another set of results show that such optimization brings the mean firing rate into a realistic regime, while increasing the sparseness and the information content of the network. We also show that the performance of optimized circuits improves for several linear and non-linear tasks.},
  isbn = {978-3-642-04274-4},
  langid = {english}
}

@article{Kappel2015,
  title = {Network {{Plasticity}} as {{Bayesian Inference}}},
  author = {Kappel, David and Habenschuss, Stefan and Legenstein, Robert and Maass, Wolfgang},
  editor = {Beck, Jeff},
  date = {2015-11},
  journaltitle = {PLOS Computational Biology},
  volume = {11},
  number = {11},
  pages = {e1004485-e1004485},
  doi = {10.1371/journal.pcbi.1004485},
  url = {https://dx.plos.org/10.1371/journal.pcbi.1004485},
  abstract = {General results from statistical learning theory suggest to understand not only brain computations, but also brain plasticity as probabilistic inference. But a model for that has been missing. We propose that inherently stochastic features of synaptic plasticity and spine motility enable cortical networks of neurons to carry out probabilistic inference by sampling from a posterior distribution of network configurations. This model provides a viable alternative to existing models that propose convergence of parameters to maximum likelihood values. It explains how priors on weight distributions and connection probabilities can be merged optimally with learned experience, how cortical networks can generalize learned information so well to novel experiences, and how they can compensate continuously for unforeseen disturbances of the network. The resulting new theory of network plasticity explains from a functional perspective a number of experimental data on stochastic aspects of synaptic plasticity that previously appeared to be quite puzzling.},
  file = {/Users/vitay/Documents/Zotero/storage/5TEXWTQX/Kappel et al_2015_Network Plasticity as Bayesian Inference.pdf}
}

@article{Kappel2018,
  title = {A {{Dynamic Connectome Supports}} the {{Emergence}} of {{Stable Computational Function}} of {{Neural Circuits}} through {{Reward-Based Learning}}},
  author = {Kappel, David and Legenstein, Robert and Habenschuss, Stefan and Hsieh, Michael and Maass, Wolfgang},
  date = {2018-03},
  journaltitle = {eneuro},
  volume = {5},
  number = {2},
  pages = {ENEURO.0301-17.2018},
  doi = {10.1523/ENEURO.0301-17.2018},
  url = {http://eneuro.org/lookup/doi/10.1523/ENEURO.0301-17.2018},
  abstract = {![Figure][1] Synaptic connections between neurons in the brain are dynamic because of continuously ongoing spine dynamics, axonal sprouting, and other processes. In fact, it was recently shown that the spontaneous synapse-autonomous component of spine dynamics is at least as large as the component that depends on the history of pre- and postsynaptic neural activity. These data are inconsistent with common models for network plasticity and raise the following questions: how can neural circuits maintain a stable computational function in spite of these continuously ongoing processes, and what could be functional uses of these ongoing processes? Here, we present a rigorous theoretical framework for these seemingly stochastic spine dynamics and rewiring processes in the context of reward-based learning tasks. We show that spontaneous synapse-autonomous processes, in combination with reward signals such as dopamine, can explain the capability of networks of neurons in the brain to configure themselves for specific computational tasks, and to compensate automatically for later changes in the network or task. Furthermore, we show theoretically and through computer simulations that stable computational performance is compatible with continuously ongoing synapse-autonomous changes. After reaching good computational performance it causes primarily a slow drift of network architecture and dynamics in task-irrelevant dimensions, as observed for neural activity in motor cortex and other areas. On the more abstract level of reinforcement learning the resulting model gives rise to an understanding of reward-driven network plasticity as continuous sampling of network configurations. [1]: pending:yes},
  file = {/Users/vitay/Documents/Zotero/storage/RZE5TEJN/Kappel et al_2018_A Dynamic Connectome Supports the Emergence of Stable Computational Function of.pdf}
}

@article{Kawai2019,
  title = {A Small-World Topology Enhances the Echo State Property and Signal Propagation in Reservoir Computing},
  author = {Kawai, Yuji and Park, Jihoon and Asada, Minoru},
  date = {2019-04-01},
  journaltitle = {Neural Networks},
  shortjournal = {Neural Networks},
  volume = {112},
  pages = {15--23},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2019.01.002},
  url = {http://www.sciencedirect.com/science/article/pii/S0893608019300115},
  urldate = {2019-03-01},
  abstract = {Cortical neural connectivity has been shown to exhibit a small-world (SW) network topology. However, the role of the topology in neural information processing remains unclear. In this study, we investigated the learning performance of an echo state network (ESN) that includes the SW topology as a reservoir. To elucidate the potential of the SW topology, we limited the numbers of the input and output nodes in the ESN and spatially segregated the output nodes from the input nodes. We tested the ESNs in two benchmark tasks: memory capacity and nonlinear time-series prediction. The SW-ESN exhibited the best learning performance when the spectral radius of the weight matrix was large and when the input and output nodes were segregated. That is, the SW topology provided the ESN with a stable echo state property over a broad range of the weight matrix and efficiently propagated input signals to the output nodes. This result is the same as that of the ESN using a real human cortical connectivity. Thus, the results suggest that the SW topology is essential for maintaining the echo state property, which is the appropriate neural dynamics between input and output brain regions.},
  keywords = {Complex network,Connectome,Echo state network,Memory capacity,Small-world network},
  file = {/Users/vitay/Documents/Zotero/storage/79ER4I27/Kawai et al_2019_A small-world topology enhances the echo state property and signal propagation.pdf;/Users/vitay/Documents/Zotero/storage/Y5MA3EXJ/S0893608019300115.html}
}

@inproceedings{Kawai2022,
  title = {Self-Organization of~a~{{Dynamical Orthogonal Basis Acquiring Large Memory Capacity}} in~{{Modular Reservoir Computing}}},
  booktitle = {Artificial {{Neural Networks}} and {{Machine Learning}} – {{ICANN}} 2022},
  author = {Kawai, Yuji and Park, Jihoon and Tsuda, Ichiro and Asada, Minoru},
  editor = {Pimenidis, Elias and Angelov, Plamen and Jayne, Chrisina and Papaleonidas, Antonios and Aydin, Mehmet},
  date = {2022},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {635--646},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-031-15919-0_53},
  abstract = {The ability of the brain to generate complex spatiotemporal patterns with a specific timing is essential for motor learning and time series prediction. An approach that tries to replicate this ability using the self-sustained neural activity of a randomly connected recurrent neural network (reservoir) meets the difficulty of orbital instability. We propose a novel system that learns an arbitrary time series as the linear sum (readout) of stable trajectories produced by numerous small network modules. Our experimental results show that the trajectories of the module outputs are orthogonal to each other, that is, the reservoir self-organizes an orthogonal basis. Furthermore, the system can learn the timing of extremely long intervals, say tens of seconds for a millisecond computation unit and the complex time series of the Lorenz system.},
  isbn = {978-3-031-15919-0},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/ZYSPC9JE/Kawai_et_al_2022_Self-organization_of_a_Dynamical_Orthogonal_Basis_Acquiring_Large_Memory.pdf}
}

@inproceedings{Khajehabdollahi2021,
  title = {Reservoir Computing with Self-Organizing Neural Oscillators},
  author = {Khajehabdollahi, Sina and Giannakakis, Emmanouil and Prosi, Jan and Levina, Anna},
  date = {2021-07-19},
  publisher = {MIT Press},
  doi = {10.1162/isal_a_00409},
  url = {https://direct.mit.edu/isal/article/doi/10.1162/isal_a_00409/102967/Reservoir-computing-with-self-organizing-neural},
  urldate = {2021-11-10},
  eventtitle = {{{ALIFE}} 2021: {{The}} 2021 {{Conference}} on {{Artificial Life}}},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/4EESIEQX/Khajehabdollahi_et_al_2021_Reservoir_computing_with_self-organizing_neural_oscillators.pdf;/Users/vitay/Documents/Zotero/storage/8H9BDJF7/102967.html}
}

@article{Kim2018,
  title = {Learning Recurrent Dynamics in Spiking Networks},
  author = {Kim, Christopher M and Chow, Carson C},
  date = {2018-09},
  journaltitle = {eLife},
  volume = {7},
  doi = {10.7554/eLife.37124},
  url = {https://elifesciences.org/articles/37124},
  abstract = {Spiking activity of neurons engaged in learning and performing a task show complex spatiotemporal dynamics. While the output of recurrent network models can learn to perform various tasks, the possible range of recurrent dynamics that emerge after learning remains unknown. Here we show that modifying the recurrent connectivity with a recursive least squares algorithm provides sufficient flexibility for synaptic and spiking rate dynamics of spiking networks to produce a wide range of spatiotemporal activity. We apply the training method to learn arbitrary firing patterns, stabilize irregular spiking activity in a network of excitatory and inhibitory neurons respecting Dale’s law, and reproduce the heterogeneous spiking rate patterns of cortical neurons engaged in motor planning and movement. We identify sufficient conditions for successful learning, characterize two types of learning errors, and assess the network capacity. Our findings show that synaptically-coupled recurrent spiking networks possess a vast computational capability that can support the diverse activity patterns in the brain.},
  file = {/Users/vitay/Documents/Zotero/storage/KXMFX46W/Kim_Chow_2018_Learning recurrent dynamics in spiking networks.pdf}
}

@article{Klampfl2013,
  title = {Emergence of Dynamic Memory Traces in Cortical Microcircuit Models through {{STDP}}.},
  author = {Klampfl, Stefan and Maass, Wolfgang},
  date = {2013-07},
  journaltitle = {The Journal of neuroscience : the official journal of the Society for Neuroscience},
  volume = {33},
  number = {28},
  eprint = {23843522},
  eprinttype = {pubmed},
  pages = {11515--29},
  doi = {10.1523/JNEUROSCI.5044-12.2013},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/23843522},
  abstract = {Numerous experimental data suggest that simultaneously or sequentially activated assemblies of neurons play a key role in the storage and computational use of long-term memory in the brain. However, a model that elucidates how these memory traces could emerge through spike-timing-dependent plasticity (STDP) has been missing. We show here that stimulus-specific assemblies of neurons emerge automatically through STDP in a simple cortical microcircuit model. The model that we consider is a randomly connected network of well known microcircuit motifs: pyramidal cells with lateral inhibition. We show that the emergent assembly codes for repeatedly occurring spatiotemporal input patterns tend to fire in some loose, sequential manner that is reminiscent of experimentally observed stereotypical trajectories of network states. We also show that the emergent assembly codes add an important computational capability to standard models for online computations in cortical microcircuits: the capability to integrate information from long-term memory with information from novel spike inputs.},
  file = {/Users/vitay/Documents/Zotero/storage/NMEIN89J/Klampfl_Maass_2013_Emergence of dynamic memory traces in cortical microcircuit models through STDP.pdf}
}

@unpublished{Klos2019,
  title = {Dynamical Learning of Dynamics},
  author = {Klos, Christian and Kossio, Yaroslav Felipe Kalle and Goedeke, Sven and Gilra, Aditya and Memmesheimer, Raoul-Martin},
  date = {2019-02-07},
  eprint = {1902.02875},
  eprinttype = {arXiv},
  eprintclass = {q-bio},
  url = {http://arxiv.org/abs/1902.02875},
  urldate = {2019-08-06},
  abstract = {The ability of humans and animals to quickly adapt to novel tasks is difficult to reconcile with the standard paradigm of learning by slow synaptic weight modification. Here we show that already static neural networks can learn to generate required dynamics by imitation. After appropriate weight pretraining, the networks dynamically adapt to learn new tasks and thereafter continue to achieve them without further teacher feedback. We explain this ability and illustrate it with a variety of target dynamics, ranging from oscillatory trajectories to driven and chaotic dynamical systems.},
  keywords = {Quantitative Biology - Neurons and Cognition},
  file = {/Users/vitay/Documents/Zotero/storage/HJLRMTTU/Klos et al_2019_Dynamical learning of dynamics.pdf;/Users/vitay/Documents/Zotero/storage/LD4KVL5T/1902.html}
}

@online{Kokalj-Filipovic2021,
  title = {Reservoir-{{Based Distributed Machine Learning}} for {{Edge Operation}}},
  author = {Kokalj-Filipovic, Silvija and Toliver, Paul and Johnson, William and Miller, Rob},
  date = {2021-04-01},
  eprint = {2104.00751},
  eprinttype = {arXiv},
  eprintclass = {cs, eess},
  doi = {10.48550/arXiv.2104.00751},
  url = {http://arxiv.org/abs/2104.00751},
  urldate = {2023-02-09},
  abstract = {We introduce a novel design for in-situ training of machine learning algorithms built into smart sensors, and illustrate distributed training scenarios using radio frequency (RF) spectrum sensors. Current RF sensors at the Edge lack the computational resources to support practical, in-situ training for intelligent signal classification. We propose a solution using Deepdelay Loop Reservoir Computing (DLR), a processing architecture that supports machine learning algorithms on resource-constrained edge-devices by leveraging delayloop reservoir computing in combination with innovative hardware. DLR delivers reductions in form factor, hardware complexity and latency, compared to the State-ofthe- Art (SoA) neural nets. We demonstrate DLR for two applications: RF Specific Emitter Identification (SEI) and wireless protocol recognition. DLR enables mobile edge platforms to authenticate and then track emitters with fast SEI retraining. Once delay loops separate the data classes, traditionally complex, power-hungry classification models are no longer needed for the learning process. Yet, even with simple classifiers such as Ridge Regression (RR), the complexity grows at least quadratically with the input size. DLR with a RR classifier exceeds the SoA accuracy, while further reducing power consumption by leveraging the architecture of parallel (split) loops. To authenticate mobile devices across large regions, DLR can be trained in a distributed fashion with very little additional processing and a small communication cost, all while maintaining accuracy. We illustrate how to merge locally trained DLR classifiers in use cases of interest.},
  pubstate = {prepublished},
  file = {/Users/vitay/Documents/Zotero/storage/KWY48ZMA/Kokalj-Filipovic_et_al_2021_Reservoir-Based_Distributed_Machine_Learning_for_Edge_Operation.pdf}
}

@article{Kozachkov2020,
  title = {Achieving Stable Dynamics in Neural Circuits},
  author = {Kozachkov, Leo and Lundqvist, Mikael and Slotine, Jean-Jacques and Miller, Earl K.},
  date = {2020-08-07},
  journaltitle = {PLOS Computational Biology},
  shortjournal = {PLOS Computational Biology},
  volume = {16},
  number = {8},
  pages = {e1007659},
  publisher = {Public Library of Science},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1007659},
  url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1007659},
  urldate = {2022-01-27},
  abstract = {The brain consists of many interconnected networks with time-varying, partially autonomous activity. There are multiple sources of noise and variation yet activity has to eventually converge to a stable, reproducible state (or sequence of states) for its computations to make sense. We approached this problem from a control-theory perspective by applying contraction analysis to recurrent neural networks. This allowed us to find mechanisms for achieving stability in multiple connected networks with biologically realistic dynamics, including synaptic plasticity and time-varying inputs. These mechanisms included inhibitory Hebbian plasticity, excitatory anti-Hebbian plasticity, synaptic sparsity and excitatory-inhibitory balance. Our findings shed light on how stable computations might be achieved despite biological complexity. Crucially, our analysis is not limited to analyzing the stability of fixed geometric objects in state space (e.g points, lines, planes), but rather the stability of state trajectories which may be complex and time-varying.},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/J7EBA7J6/Kozachkov_et_al_2020_Achieving_stable_dynamics_in_neural_circuits.pdf;/Users/vitay/Documents/Zotero/storage/7SISQVY4/article.html}
}

@inproceedings{Kozachkov2022a,
  title = {{{RNNs}} of {{RNNs}}: {{Recursive Construction}} of {{Stable Assemblies}} of {{Recurrent Neural Networks}}},
  shorttitle = {{{RNNs}} of {{RNNs}}},
  author = {Kozachkov, Leo and Ennis, Michaela M. and Slotine, Jean-Jacques},
  date = {2022-10-31},
  url = {https://openreview.net/forum?id=2dgB38geVEU},
  urldate = {2023-02-10},
  abstract = {Recurrent neural networks (RNNs) are widely used throughout neuroscience as models of local neural activity. Many properties of single RNNs are well characterized theoretically, but experimental neuroscience has moved in the direction of studying multiple interacting areas, and RNN theory needs to be likewise extended. We take a constructive approach towards this problem, leveraging tools from nonlinear control theory and machine learning to characterize when combinations of stable RNNs will themselves be stable. Importantly, we derive conditions which allow for massive feedback connections between interacting RNNs. We parameterize these conditions for easy optimization using gradient-based techniques, and show that stability-constrained "networks of networks" can perform well on challenging sequential-processing benchmark tasks. Altogether, our results provide a principled approach towards understanding distributed, modular function in the brain.},
  eventtitle = {Advances in {{Neural Information Processing Systems}}},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/9MKF34D2/Kozachkov_et_al_2022_RNNs_of_RNNs.pdf}
}

@online{Kozachkov2023,
  title = {{{RNNs}} of {{RNNs}}: {{Recursive Construction}} of {{Stable Assemblies}} of {{Recurrent Neural Networks}}},
  shorttitle = {{{RNNs}} of {{RNNs}}},
  author = {Kozachkov, Leo and Ennis, Michaela and Slotine, Jean-Jacques},
  date = {2023-01-29},
  eprint = {2106.08928},
  eprinttype = {arXiv},
  eprintclass = {cs, math, q-bio},
  doi = {10.48550/arXiv.2106.08928},
  url = {http://arxiv.org/abs/2106.08928},
  urldate = {2023-02-01},
  abstract = {Recurrent neural networks (RNNs) are widely used throughout neuroscience as models of local neural activity. Many properties of single RNNs are well characterized theoretically, but experimental neuroscience has moved in the direction of studying multiple interacting areas, and RNN theory needs to be likewise extended. We take a constructive approach towards this problem, leveraging tools from nonlinear control theory and machine learning to characterize when combinations of stable RNNs will themselves be stable. Importantly, we derive conditions which allow for massive feedback connections between interacting RNNs. We parameterize these conditions for easy optimization using gradient-based techniques, and show that stability-constrained "networks of networks" can perform well on challenging sequential-processing benchmark tasks. Altogether, our results provide a principled approach towards understanding distributed, modular function in the brain.},
  pubstate = {prepublished},
  file = {/Users/vitay/Documents/Zotero/storage/U2TPNAUS/Kozachkov_et_al_2023_RNNs_of_RNNs.pdf}
}

@article{Kuroki2017,
  title = {Common Features in Plastic Changes Rather than Constructed Structures in Recurrent Neural Network Prefrontal Cortex Models},
  author = {Kuroki, Satoshi and Isomura, Takuya},
  date = {2017-08},
  journaltitle = {bioRxiv},
  pages = {181297--181297},
  doi = {10.1101/181297},
  url = {https://www.biorxiv.org/content/early/2017/08/28/181297},
  abstract = {We have flexible control over our cognition depending on the context or surrounding environments. The prefrontal cortex (PFC) controls this cognitive flexibility; however, the detailed underlying mechanisms remain unclear. Recent developments in machine learning techniques have allowed simple recurrent neural network PFC models to perform human- or animal-like behavioral tasks. These systems allow us to acquire parameters, which we could not in biological experiments, for performing the tasks. We compared four models, in which a flexible cognition task, called context-dependent integration task, was performed; subsequently, we searched for common features. In all the models, we observed that high plastic synapses were concentrated in the small neuronal population and the more concentrated neuronal units contributed further to the performance. However, there were no common properties in the constructed structures. These results suggest that plastic changes can be more general and important to accomplish cognitive tasks than features of the constructed structures.},
  file = {/Users/vitay/Documents/Zotero/storage/3GKIVLUH/Kuroki_Isomura_2017_Common features in plastic changes rather than constructed structures in.pdf}
}

@article{Kusmierz2017a,
  title = {Learning with Three Factors: Modulating {{Hebbian}} Plasticity with Errors},
  shorttitle = {Learning with Three Factors},
  author = {Kuśmierz, Łukasz and Isomura, Takuya and Toyoizumi, Taro},
  date = {2017-10-01},
  journaltitle = {Current Opinion in Neurobiology},
  shortjournal = {Current Opinion in Neurobiology},
  series = {Computational {{Neuroscience}}},
  volume = {46},
  pages = {170--177},
  issn = {0959-4388},
  doi = {10.1016/j.conb.2017.08.020},
  url = {https://www.sciencedirect.com/science/article/pii/S0959438817300612},
  urldate = {2024-05-28},
  abstract = {Synaptic plasticity is a central theme in neuroscience. A framework of three-factor learning rules provides a powerful abstraction, helping to navigate through the abundance of models of synaptic plasticity. It is well-known that the dopamine modulation of learning is related to reward, but theoretical models predict other functional roles of the modulatory third factor; it may encode errors for supervised learning, summary statistics of the population activity for unsupervised learning or attentional feedback. Specialized structures may be needed in order to generate and propagate third factors in the neural network.},
  file = {/Users/vitay/Documents/Zotero/storage/2NMTY96T/Kuśmierz et al. - 2017 - Learning with three factors modulating Hebbian plasticity with errors.pdf}
}

@article{Laje2013,
  title = {Robust Timing and Motor Patterns by Taming Chaos in Recurrent Neural Networks.},
  author = {Laje, Rodrigo and Buonomano, Dean V},
  date = {2013-07},
  journaltitle = {Nature neuroscience},
  volume = {16},
  number = {7},
  eprint = {23708144},
  eprinttype = {pubmed},
  pages = {925--33},
  doi = {10.1038/nn.3405},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/23708144},
  abstract = {The brain's ability to tell time and produce complex spatiotemporal motor patterns is critical for anticipating the next ring of a telephone or playing a musical instrument. One class of models proposes that these abilities emerge from dynamically changing patterns of neural activity generated in recurrent neural networks. However, the relevant dynamic regimes of recurrent networks are highly sensitive to noise; that is, chaotic. We developed a firing rate model that tells time on the order of seconds and generates complex spatiotemporal patterns in the presence of high levels of noise. This is achieved through the tuning of the recurrent connections. The network operates in a dynamic regime that exhibits coexisting chaotic and locally stable trajectories. These stable patterns function as 'dynamic attractors' and provide a feature that is characteristic of biological systems: the ability to 'return' to the pattern being generated in the face of perturbations.},
  file = {/Users/vitay/Documents/Zotero/storage/JRPQVJGJ/Laje_Buonomano_2013_Robust_timing_and_motor_patterns_by_taming_chaos_in_recurrent_neural_networks.pdf}
}

@article{Lazar2009,
  title = {{{SORN}}: A {{Self-organizing Recurrent Neural Network}}},
  author = {Lazar, Andreea and Pipa, Gordon and Triesch, Jochen},
  date = {2009-10},
  journaltitle = {Frontiers in Computational Neuroscience},
  volume = {3},
  pages = {23--23},
  doi = {10.3389/neuro.10.023.2009},
  url = {http://journal.frontiersin.org/article/10.3389/neuro.10.023.2009/abstract},
  abstract = {Understanding the dynamics of recurrent neural networks is crucial for explaining how the brain processes information. In the neocortex, a range of different plasticity mechanisms are shaping recurrent networks into effective information processing circuits that learn appropriate representations for time-varying sensory stimuli. However, it has been difficult to mimic these abilities in artificial neural network models. Here we introduce SORN, a self-organizing recurrent network. It combines three distinct forms of local plasticity to learn spatio-temporal patterns in its input while maintaining its dynamics in a healthy regime suitable for learning. The SORN learns to encode information in the form of trajectories through its high-dimensional state space reminiscent of recent biological findings on cortical coding. All three forms of plasticity are shown to be essential for the network's success.},
  keywords = {Intrinsic Plasticity,recurrent neural networks,reservoir computing,synaptic plasticity,time series prediction}
}

@incollection{Lazar2011,
  title = {Emerging {{Bayesian Priors}} in a {{Self-Organizing Recurrent Network}}},
  author = {Lazar, Andreea and Pipa, Gordon and Triesch, Jochen},
  date = {2011},
  pages = {127--134},
  publisher = {Springer, Berlin, Heidelberg},
  doi = {10.1007/978-3-642-21738-8_17},
  url = {http://link.springer.com/10.1007/978-3-642-21738-8_17},
  file = {/Users/vitay/Documents/Zotero/storage/N8HCBT8C/Lazar et al_2011_Emerging Bayesian Priors in a Self-Organizing Recurrent Network.pdf}
}

@article{Legenstein2007,
  title = {Edge of Chaos and Prediction of Computational Performance for Neural Circuit Models},
  author = {Legenstein, Robert and Maass, Wolfgang},
  date = {2007-04},
  journaltitle = {Neural Networks},
  volume = {20},
  number = {3},
  pages = {323--334},
  issn = {08936080},
  doi = {10.1016/j.neunet.2007.04.017},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0893608007000433},
  urldate = {2019-03-03},
  abstract = {We analyze in this article the significance of the edge of chaos for real-time computations in neural microcircuit models consisting of spiking neurons and dynamic synapses. We find that the edge of chaos predicts quite well those values of circuit parameters that yield maximal computational performance. But obviously it makes no prediction of their computational performance for other parameter values. Therefore, we propose a new method for predicting the computational performance of neural microcircuit models. The new measure estimates directly the kernel property and the generalization capability of a neural microcircuit. We validate the proposed measure by comparing its prediction with direct evaluations of the computational performance of various neural microcircuit models. The proposed method also allows us to quantify differences in the computational performance and generalization capability of neural circuits in different dynamic regimes (UP- and DOWN-states) that have been demonstrated through intracellular recordings in vivo.},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/SIGZLBGD/Legenstein_Maass_2007_Edge of chaos and prediction of computational performance for neural circuit.pdf}
}

@article{Legenstein2010,
  title = {A {{Reward-Modulated Hebbian Learning Rule Can Explain Experimentally Observed Network Reorganization}} in a {{Brain Control Task}}},
  author = {Legenstein, Robert and Chase, Steven M and Schwartz, Andrew B and Maass, Wolfgang},
  date = {2010-06},
  journaltitle = {The Journal of Neuroscience},
  volume = {30},
  number = {25},
  pages = {8400 LP-8410},
  url = {http://www.jneurosci.org/content/30/25/8400.abstract},
  abstract = {It has recently been shown in a brain–computer interface experiment that motor cortical neurons change their tuning properties selectively to compensate for errors induced by displaced decoding parameters. In particular, it was shown that the three-dimensional tuning curves of neurons whose decoding parameters were reassigned changed more than those of neurons whose decoding parameters had not been reassigned. In this article, we propose a simple learning rule that can reproduce this effect. Our learning rule uses Hebbian weight updates driven by a global reward signal and neuronal noise. In contrast to most previously proposed learning rules, this approach does not require extrinsic information to separate noise from signal. The learning rule is able to optimize the performance of a model system within biologically realistic periods of time under high noise levels. Furthermore, when the model parameters are matched to data recorded during the brain–computer interface learning experiments described above, the model produces learning effects strikingly similar to those found in the experiments.},
  file = {/Users/vitay/Documents/Zotero/storage/DB3FHWFV/Legenstein et al_2010_A Reward-Modulated Hebbian Learning Rule Can Explain Experimentally Observed.pdf}
}

@article{Liu2022d,
  title = {Tension: {{A Python}} Package for {{FORCE}} Learning},
  shorttitle = {Tension},
  author = {Liu, Lu Bin and Losonczy, Attila and Liao, Zhenrui},
  date = {2022-12-19},
  journaltitle = {PLOS Computational Biology},
  shortjournal = {PLOS Computational Biology},
  volume = {18},
  number = {12},
  pages = {e1010722},
  publisher = {Public Library of Science},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1010722},
  url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1010722},
  urldate = {2022-12-28},
  abstract = {First-Order, Reduced and Controlled Error (FORCE) learning and its variants are widely used to train chaotic recurrent neural networks (RNNs), and outperform gradient methods on certain tasks. However, there is currently no standard software framework for FORCE learning. We present tension, an object-oriented, open-source Python package that implements a TensorFlow / Keras API for FORCE. We show how rate networks, spiking networks, and networks constrained by biological data can all be trained using a shared, easily extensible high-level API. With the same resources, our implementation outperforms a conventional RNN in loss and published FORCE implementations in runtime. Our work here makes FORCE training chaotic RNNs accessible and simple to iterate, and facilitates modeling of how behaviors of interest emerge from neural dynamics.},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/DWPYCTNT/Liu_et_al_2022_tension.pdf}
}

@article{LiXiumin2017,
  title = {Biological Modelling of a Computational Spiking Neural Network with Neuronal Avalanches},
  author = {{Li Xiumin} and {Chen Qing} and {Xue Fangzheng}},
  date = {2017-06-28},
  journaltitle = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  shortjournal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume = {375},
  number = {2096},
  pages = {20160286},
  doi = {10.1098/rsta.2016.0286},
  url = {https://royalsocietypublishing.org/doi/abs/10.1098/rsta.2016.0286},
  urldate = {2019-04-14},
  abstract = {In recent years, an increasing number of studies have demonstrated that networks in the brain can self-organize into a critical state where dynamics exhibit a mixture of ordered and disordered patterns. This critical branching phenomenon is termed neuronal avalanches. It has been hypothesized that the homeostatic level balanced between stability and plasticity of this critical state may be the optimal state for performing diverse neural computational tasks. However, the critical region for high performance is narrow and sensitive for spiking neural networks (SNNs). In this paper, we investigated the role of the critical state in neural computations based on liquid-state machines, a biologically plausible computational neural network model for real-time computing. The computational performance of an SNN when operating at the critical state and, in particular, with spike-timing-dependent plasticity for updating synaptic weights is investigated. The network is found to show the best computational performance when it is subjected to critical dynamic states. Moreover, the active-neuron-dominant structure refined from synaptic learning can remarkably enhance the robustness of the critical state and further improve computational accuracy. These results may have important implications in the modelling of spiking neural networks with optimal computational performance.This article is part of the themed issue ‘Mathematical methods in medicine: neuroscience, cardiology and pathology’.},
  file = {/Users/vitay/Documents/Zotero/storage/EN9BZ2YL/rsta.2016.html}
}

@article{Loeffler2021,
  title = {Modularity and Multitasking in Neuro-Memristive Reservoir Networks},
  author = {Loeffler, Alon and Zhu, Ruomin and Hochstetter, Joel and Diaz-Alvarez, Adrian and Nakayama, Tomonobu and Shine, James M. and Kuncic, Zdenka},
  date = {2021-08},
  journaltitle = {Neuromorphic Computing and Engineering},
  shortjournal = {Neuromorph. Comput. Eng.},
  volume = {1},
  number = {1},
  pages = {014003},
  publisher = {IOP Publishing},
  issn = {2634-4386},
  doi = {10.1088/2634-4386/ac156f},
  url = {https://dx.doi.org/10.1088/2634-4386/ac156f},
  urldate = {2023-02-01},
  abstract = {The human brain seemingly effortlessly performs multiple concurrent and elaborate tasks in response to complex, dynamic sensory input from our environment. This capability has been attributed to the highly modular structure of the brain, enabling specific task assignment among different regions and limiting interference between them. Here, we compare the structure and functional capabilities of different bio-physically inspired and biological networks. We then focus on the influence of topological properties on the functional performance of highly modular, bio-physically inspired neuro-memristive nanowire networks (NWNs). We perform two benchmark reservoir computing tasks (memory capacity and nonlinear transformation) on simulated networks and show that while random networks outperform NWNs on independent tasks, NWNs with highly segregated modules achieve the best performance on simultaneous tasks. Conversely, networks that share too many resources, such as networks with random structure, perform poorly in multitasking. Overall, our results show that structural properties such as modularity play a critical role in trafficking information flow, preventing information from spreading indiscriminately throughout NWNs.},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/GIYBDBNV/Loeffler_et_al_2021_Modularity_and_multitasking_in_neuro-memristive_reservoir_networks.pdf}
}

@article{Logiaco2021,
  title = {Thalamic Control of Cortical Dynamics in a Model of Flexible Motor Sequencing},
  author = {Logiaco, Laureline and Abbott, L. F. and Escola, Sean},
  date = {2021-06-01},
  journaltitle = {Cell Reports},
  shortjournal = {Cell Reports},
  volume = {35},
  number = {9},
  pages = {109090},
  issn = {2211-1247},
  doi = {10.1016/j.celrep.2021.109090},
  url = {https://www.sciencedirect.com/science/article/pii/S2211124721004241},
  urldate = {2024-02-28},
  abstract = {The neural mechanisms that generate an extensible library of motor motifs and flexibly string them into arbitrary sequences are unclear. We developed a model in which inhibitory basal ganglia output neurons project to thalamic units that are themselves bidirectionally connected to a recurrent cortical network. We model the basal ganglia inhibitory patterns as silencing some thalamic neurons while leaving others disinhibited and free to interact with cortex during specific motifs. We show that a small number of disinhibited thalamic neurons can control cortical dynamics to generate specific motor output in a noise-robust way. Additionally, a single “preparatory” thalamocortical network can produce fast cortical dynamics that support rapid transitions between any pair of learned motifs. If the thalamic units associated with each sequence component are segregated, many motor outputs can be learned without interference and then combined in arbitrary orders for the flexible production of long and complex motor sequences.},
  file = {/Users/vitay/Documents/Zotero/storage/A676GY75/Logiaco_et_al_2021_Thalamic_control_of_cortical_dynamics_in_a_model_of_flexible_motor_sequencing.pdf}
}

@article{Logiaco2021a,
  title = {Thalamic Control of Cortical Dynamics in a Model of Flexible Motor Sequencing},
  author = {Logiaco, Laureline and Abbott, L. F. and Escola, Sean},
  date = {2021-06-01},
  journaltitle = {Cell Reports},
  shortjournal = {Cell Reports},
  volume = {35},
  number = {9},
  pages = {109090},
  issn = {2211-1247},
  doi = {10.1016/j.celrep.2021.109090},
  url = {https://www.sciencedirect.com/science/article/pii/S2211124721004241},
  urldate = {2024-05-27},
  abstract = {The neural mechanisms that generate an extensible library of motor motifs and flexibly string them into arbitrary sequences are unclear. We developed a model in which inhibitory basal ganglia output neurons project to thalamic units that are themselves bidirectionally connected to a recurrent cortical network. We model the basal ganglia inhibitory patterns as silencing some thalamic neurons while leaving others disinhibited and free to interact with cortex during specific motifs. We show that a small number of disinhibited thalamic neurons can control cortical dynamics to generate specific motor output in a noise-robust way. Additionally, a single “preparatory” thalamocortical network can produce fast cortical dynamics that support rapid transitions between any pair of learned motifs. If the thalamic units associated with each sequence component are segregated, many motor outputs can be learned without interference and then combined in arbitrary orders for the flexible production of long and complex motor sequences.},
  file = {/Users/vitay/Documents/Zotero/storage/88CBX95U/Logiaco et al. - 2021 - Thalamic control of cortical dynamics in a model of flexible motor sequencing.pdf}
}

@inproceedings{Lowe2011,
  title = {Modelling {{Coordination}} of {{Learning Systems}}: {{A Reservoir Systems Approach}} to {{Dopamine Modulated Pavlovian Conditioning}}},
  booktitle = {Advances in {{Artificial Life}}. {{Darwin Meets}} von {{Neumann}}. {{ECAL}} 2009. {{Lecture Notes}} in {{Computer Science}}, Vol 5777},
  author = {Lowe, Robert and Mannella, Francesco and Ziemke, Tom and Baldassarre, Gianluca},
  editor = {{Kampis G.} and {Karsai I.} and {Szathmáry E.}},
  date = {2011},
  pages = {410--417},
  publisher = {Springer, Berlin, Heidelberg},
  doi = {10.1007/978-3-642-21283-3_51},
  url = {http://link.springer.com/10.1007/978-3-642-21283-3_51},
  abstract = {This paper presents a biologically constrained reward prediction model capable of learning cue-outcome associations involving temporally distant stimuli without using the commonly used temporal difference model. The model incorporates a novel use of an adapted echo state network to substitute the biologically implausible delay chains usually used, in relation to dopamine phenomena, for tackling temporally structured stimuli. Moreover, the model is based on a novel algorithm which successfully coordinates two sub systems: one providing Pavlovian conditioning, one providing timely inhibition of dopamine responses to salient anticipated stimuli. The model is validated against the typical profile of phasic dopamine in first and second order Pavlovian conditioning. The model is relevant not only to explaining the mechanisms underlying the biological regulation of dopamine signals, but also for applications in autonomous robotics involving reinforcement-based learning.},
  file = {/Users/vitay/Documents/Zotero/storage/346QZIVT/Lowe et al_2011_Modelling Coordination of Learning Systems.pdf}
}

@article{Lukosevicius2009,
  title = {Reservoir Computing Approaches to Recurrent Neural Network Training},
  author = {Lukoševičius, Mantas and Jaeger, Herbert},
  date = {2009-08},
  journaltitle = {Computer Science Review},
  volume = {3},
  number = {3},
  pages = {127--149},
  doi = {10.1016/J.COSREV.2009.03.005},
  url = {https://www.sciencedirect.com/science/article/pii/S1574013709000173?via%3Dihub},
  abstract = {Echo State Networks and Liquid State Machines introduced a new paradigm in artificial recurrent neural network (RNN) training, where an RNN (the reservoir) is generated randomly and only a readout is trained. The paradigm, becoming known as reservoir computing, greatly facilitated the practical application of RNNs and outperformed classical fully trained RNNs in many tasks. It has lately become a vivid research field with numerous extensions of the basic idea, including reservoir adaptation, thus broadening the initial paradigm to using different methods for training the reservoir and the readout. This review systematically surveys both current ways of generating/adapting the reservoirs and training different types of readouts. It offers a natural conceptual classification of the techniques, which transcends boundaries of the current “brand-names” of reservoir methods, and thus aims to help in unifying the field and providing the reader with a detailed “map” of it.}
}

@incollection{Lukosevicius2012,
  title = {A {{Practical Guide}} to {{Applying Echo State Networks}}},
  booktitle = {Neural {{Networks}}: {{Tricks}} of the {{Trade}}: {{Second Edition}}},
  author = {Lukoševičius, Mantas},
  editor = {Montavon, Grégoire and Orr, Geneviève B. and Müller, Klaus-Robert},
  date = {2012},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {659--686},
  publisher = {Springer Berlin Heidelberg},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-35289-8_36},
  url = {https://doi.org/10.1007/978-3-642-35289-8_36},
  urldate = {2019-04-16},
  abstract = {Reservoir computing has emerged in the last decade as an alternative to gradient descent methods for training recurrent neural networks. Echo State Network (ESN) is one of the key reservoir computing “flavors”. While being practical, conceptually simple, and easy to implement, ESNs require some experience and insight to achieve the hailed good performance in many tasks. Here we present practical techniques and recommendations for successfully applying ESNs, as well as some more advanced application-specific modifications.},
  isbn = {978-3-642-35289-8},
  langid = {english},
  keywords = {Little Mean Square,Output Feedback,Recurrent Neural Network,Ridge Regression,Spectral Radius},
  file = {/Users/vitay/Documents/Zotero/storage/24EL56TL/Lukoševičius_2012_A Practical Guide to Applying Echo State Networks.pdf}
}

@article{Lukosevicius2012a,
  title = {Reservoir {{Computing Trends}}},
  author = {Lukoševičius, Mantas and Jaeger, Herbert and Schrauwen, Benjamin},
  date = {2012-11},
  journaltitle = {KI - Künstliche Intelligenz},
  volume = {26},
  number = {4},
  pages = {365--371},
  doi = {10.1007/s13218-012-0204-5},
  url = {http://link.springer.com/10.1007/s13218-012-0204-5},
  file = {/Users/vitay/Documents/Zotero/storage/SQPPF3LI/Lukoševičius et al_2012_Reservoir Computing Trends.pdf}
}

@online{Ma2017,
  title = {Deep-{{ESN}}: {{A Multiple Projection-encoding Hierarchical Reservoir Computing Framework}}},
  shorttitle = {Deep-{{ESN}}},
  author = {Ma, Qianli and Shen, Lifeng and Cottrell, Garrison W.},
  date = {2017-11-13},
  eprint = {1711.05255},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1711.05255},
  url = {http://arxiv.org/abs/1711.05255},
  urldate = {2023-02-01},
  abstract = {As an efficient recurrent neural network (RNN) model, reservoir computing (RC) models, such as Echo State Networks, have attracted widespread attention in the last decade. However, while they have had great success with time series data [1], [2], many time series have a multiscale structure, which a single-hidden-layer RC model may have difficulty capturing. In this paper, we propose a novel hierarchical reservoir computing framework we call Deep Echo State Networks (Deep-ESNs). The most distinctive feature of a Deep-ESN is its ability to deal with time series through hierarchical projections. Specifically, when an input time series is projected into the high-dimensional echo-state space of a reservoir, a subsequent encoding layer (e.g., a PCA, autoencoder, or a random projection) can project the echo-state representations into a lower-dimensional space. These low-dimensional representations can then be processed by another ESN. By using projection layers and encoding layers alternately in the hierarchical framework, a Deep-ESN can not only attenuate the effects of the collinearity problem in ESNs, but also fully take advantage of the temporal kernel property of ESNs to explore multiscale dynamics of time series. To fuse the multiscale representations obtained by each reservoir, we add connections from each encoding layer to the last output layer. Theoretical analyses prove that stability of a Deep-ESN is guaranteed by the echo state property (ESP), and the time complexity is equivalent to a conventional ESN. Experimental results on some artificial and real world time series demonstrate that Deep-ESNs can capture multiscale dynamics, and outperform both standard ESNs and previous hierarchical ESN-based models.},
  pubstate = {prepublished},
  file = {/Users/vitay/Documents/Zotero/storage/Z5ZSXWNW/Ma_et_al_2017_Deep-ESN.pdf}
}

@article{Maass2002,
  title = {Real-Time Computing without Stable States: A New Framework for Neural Computation Based on Perturbations.},
  author = {Maass, Wolfgang and Natschläger, Thomas and Markram, Henry},
  date = {2002-11},
  journaltitle = {Neural computation},
  volume = {14},
  number = {11},
  eprint = {12433288},
  eprinttype = {pubmed},
  pages = {2531--60},
  doi = {10.1162/089976602760407955},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/12433288},
  abstract = {A key challenge for neural modeling is to explain how a continuous stream of multimodal input from a rapidly changing environment can be processed by stereotypical recurrent circuits of integrate-and-fire neurons in real time. We propose a new computational model for real-time computing on time-varying input that provides an alternative to paradigms based on Turing machines or attractor neural networks. It does not require a task-dependent construction of neural circuits. Instead, it is based on principles of high-dimensional dynamical systems in combination with statistical learning theory and can be implemented on generic evolved or found recurrent circuitry. It is shown that the inherent transient dynamics of the high-dimensional dynamical system formed by a sufficiently large and heterogeneous neural circuit may serve as universal analog fading memory. Readout neurons can learn to extract in real time from the current state of such recurrent neural circuit information about current and past inputs that may be needed for diverse tasks. Stable internal states are not required for giving a stable output, since transient internal states can be transformed by readout neurons into stable target outputs due to the high dimensionality of the dynamical system. Our approach is based on a rigorous computational model, the liquid state machine, that, unlike Turing machines, does not require sequential transitions between well-defined discrete internal states. It is supported, as the Turing machine is, by rigorous mathematical results that predict universal computational power under idealized conditions, but for the biologically more realistic scenario of real-time processing of time-varying inputs. Our approach provides new perspectives for the interpretation of neural coding, the design of experiments and data analysis in neurophysiology, and the solution of problems in robotics and neurotechnology.},
  file = {/Users/vitay/Documents/Zotero/storage/2VLNI3Z7/Maass_et_al_2002_Real-time_computing_without_stable_states.pdf}
}

@incollection{Maass2011,
  title = {Liquid {{State Machines}}: {{Motivation}}, {{Theory}}, and {{Applications}}},
  booktitle = {Computability in {{Context}}},
  author = {Maass, Wolfgang},
  date = {2011-02},
  pages = {275--296},
  publisher = {IMPERIAL COLLEGE PRESS},
  doi = {10.1142/9781848162778_0008},
  url = {http://www.worldscientific.com/doi/abs/10.1142/9781848162778_0008}
}

@article{Mannella2015,
  title = {Selection of Cortical Dynamics for Motor Behaviour by the Basal Ganglia},
  author = {Mannella, Francesco and Baldassarre, Gianluca},
  date = {2015-12},
  journaltitle = {Biological Cybernetics},
  volume = {109},
  number = {6},
  pages = {575--595},
  doi = {10.1007/s00422-015-0662-6},
  url = {http://link.springer.com/10.1007/s00422-015-0662-6},
  file = {/Users/vitay/Documents/Zotero/storage/CZ8HGJLW/Mannella_Baldassarre_2015_Selection of cortical dynamics for motor behaviour by the basal ganglia.pdf}
}

@online{Martinuzzi2022,
  title = {{{ReservoirComputing}}.Jl: {{An Efficient}} and {{Modular Library}} for {{Reservoir Computing Models}}},
  shorttitle = {{{ReservoirComputing}}.Jl},
  author = {Martinuzzi, Francesco and Rackauckas, Chris and Abdelrehim, Anas and Mahecha, Miguel D. and Mora, Karin},
  date = {2022-04-08},
  eprint = {2204.05117},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2204.05117},
  url = {http://arxiv.org/abs/2204.05117},
  urldate = {2023-02-01},
  abstract = {We introduce ReservoirComputing.jl, an open source Julia library for reservoir computing models. The software offers a great number of algorithms presented in the literature, and allows to expand on them with both internal and external tools in a simple way. The implementation is highly modular, fast and comes with a comprehensive documentation, which includes reproduced experiments from literature. The code and documentation are hosted on Github under an MIT license https://github.com/SciML/ReservoirComputing.jl.},
  pubstate = {prepublished},
  file = {/Users/vitay/Documents/Zotero/storage/V6T9B5NX/Martinuzzi_et_al_2022_ReservoirComputing.pdf}
}

@article{Marton2020,
  title = {Learning to Select Actions Shapes Recurrent Dynamics in the Corticostriatal System},
  author = {Márton, Christian D. and Schultz, Simon R. and Averbeck, Bruno B.},
  date = {2020-12-01},
  journaltitle = {Neural Networks},
  shortjournal = {Neural Networks},
  volume = {132},
  pages = {375--393},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2020.09.008},
  url = {http://www.sciencedirect.com/science/article/pii/S0893608020303312},
  urldate = {2020-11-11},
  abstract = {Learning to select appropriate actions based on their values is fundamental to adaptive behavior. This form of learning is supported by fronto-striatal systems. The dorsal-lateral prefrontal cortex (dlPFC) and the dorsal striatum (dSTR), which are strongly interconnected, are key nodes in this circuitry. Substantial experimental evidence, including neurophysiological recordings, have shown that neurons in these structures represent key aspects of learning. The computational mechanisms that shape the neurophysiological responses, however, are not clear. To examine this, we developed a recurrent neural network (RNN) model of the dlPFC-dSTR circuit and trained it on an oculomotor sequence learning task. We compared the activity generated by the model to activity recorded from monkey dlPFC and dSTR in the same task. This network consisted of a striatal component which encoded action values, and a prefrontal component which selected appropriate actions. After training, this system was able to autonomously represent and update action values and select actions, thus being able to closely approximate the representational structure in corticostriatal recordings. We found that learning to select the correct actions drove action-sequence representations further apart in activity space, both in the model and in the neural data. The model revealed that learning proceeds by increasing the distance between sequence-specific representations. This makes it more likely that the model will select the appropriate action sequence as learning develops. Our model thus supports the hypothesis that learning in networks drives the neural representations of actions further apart, increasing the probability that the network generates correct actions as learning proceeds. Altogether, this study advances our understanding of how neural circuit dynamics are involved in neural computation, revealing how dynamics in the corticostriatal system support task learning.},
  langid = {english},
  keywords = {Corticostriatal system,Dynamics,Learning,Recurrent neural network,Reinforcement learning},
  file = {/Users/vitay/Documents/Zotero/storage/4HEMBKVY/Márton_et_al_2020_Learning_to_select_actions_shapes_recurrent_dynamics_in_the_corticostriatal.pdf;/Users/vitay/Documents/Zotero/storage/IYGIE35B/S0893608020303312.html}
}

@article{Mastrogiuseppe2017,
  title = {Intrinsically-Generated Fluctuating Activity in Excitatory-Inhibitory Networks},
  author = {Mastrogiuseppe, Francesca and Ostojic, Srdjan},
  editor = {Latham, Peter E.},
  date = {2017-04},
  journaltitle = {PLOS Computational Biology},
  volume = {13},
  number = {4},
  pages = {e1005498-e1005498},
  doi = {10.1371/journal.pcbi.1005498},
  url = {https://dx.plos.org/10.1371/journal.pcbi.1005498},
  abstract = {Recurrent networks of non-linear units display a variety of dynamical regimes depending on the structure of their synaptic connectivity. A particularly remarkable phenomenon is the appearance of strongly fluctuating, chaotic activity in networks of deterministic, but randomly connected rate units. How this type of intrinsically generated fluctuations appears in more realistic networks of spiking neurons has been a long standing question. To ease the comparison between rate and spiking networks, recent works investigated the dynamical regimes of randomly-connected rate networks with segregated excitatory and inhibitory populations, and firing rates constrained to be positive. These works derived general dynamical mean field (DMF) equations describing the fluctuating dynamics, but solved these equations only in the case of purely inhibitory networks. Using a simplified excitatory-inhibitory architecture in which DMF equations are more easily tractable, here we show that the presence of excitation qualitatively modifies the fluctuating activity compared to purely inhibitory networks. In presence of excitation, intrinsically generated fluctuations induce a strong increase in mean firing rates, a phenomenon that is much weaker in purely inhibitory networks. Excitation moreover induces two different fluctuating regimes: for moderate overall coupling, recurrent inhibition is sufficient to stabilize fluctuations; for strong coupling, firing rates are stabilized solely by the upper bound imposed on activity, even if inhibition is stronger than excitation. These results extend to more general network architectures, and to rate networks receiving noisy inputs mimicking spiking activity. Finally, we show that signatures of the second dynamical regime appear in networks of integrate-and-fire neurons.},
  file = {/Users/vitay/Documents/Zotero/storage/IPHPAT2J/Mastrogiuseppe_Ostojic_2017_Intrinsically-generated fluctuating activity in excitatory-inhibitory networks.pdf}
}

@article{Mastrogiuseppe2018,
  title = {Linking {{Connectivity}}, {{Dynamics}}, and {{Computations}} in {{Low-Rank Recurrent Neural Networks}}},
  author = {Mastrogiuseppe, Francesca and Ostojic, Srdjan},
  date = {2018-08-08},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  volume = {99},
  number = {3},
  eprint = {30057201},
  eprinttype = {pmid},
  pages = {609-623.e29},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2018.07.003},
  url = {https://www.cell.com/neuron/abstract/S0896-6273(18)30543-9},
  urldate = {2019-04-30},
  langid = {english},
  keywords = {low dimensional dynamics,mixed selectivity,neural computations,recurrent neural networks},
  file = {/Users/vitay/Documents/Zotero/storage/5HLT5UIK/S0896-6273(18)30543-9.html}
}

@article{Matsuki2020,
  title = {Adaptive Balancing of Exploration and Exploitation around the Edge of Chaos in Internal-Chaos-Based Learning},
  author = {Matsuki, Toshitaka and Shibata, Katsunari},
  date = {2020-12-01},
  journaltitle = {Neural Networks},
  shortjournal = {Neural Networks},
  volume = {132},
  pages = {19--29},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2020.08.002},
  url = {http://www.sciencedirect.com/science/article/pii/S0893608020302914},
  urldate = {2020-11-25},
  abstract = {This paper addresses learning with exploration driven by chaotic internal dynamics of a neural network. Hoerzer et al. showed that a chaotic reservoir network (RN) can learn with exploration driven by external random noise and a sequential reward. In this paper, we demonstrate that a chaotic RN can learn without external noise because the output fluctuation originated from its internal chaotic dynamics functions as exploration. As learning progresses, the chaoticity decreases and the network can automatically switch from exploration mode to exploitation mode. Furthermore, the network can resume exploration when presented with a new situation. In addition, we found that even when the two parameters that influence the chaoticity are varied, learning performance always improves around the edge of chaos. From these results, we think that exploration is generated from internal chaotic dynamics, and exploitation appears in the process of forming attractors on the chaotic dynamics through learning. Consequently, exploration and exploitation are well-balanced around the edge of chaos, which leads to good learning performance.},
  langid = {english},
  keywords = {Chaotic neural network,Edge of chaos,Exploration–exploitation dilemma,Reservoir computing,Reward-modulated Hebbian learning},
  file = {/Users/vitay/Documents/Zotero/storage/2YXI9GNY/Matsuki_Shibata_2020_Adaptive_balancing_of_exploration_and_exploitation_around_the_edge_of_chaos_in.pdf;/Users/vitay/Documents/Zotero/storage/DWX5XNJ3/S0893608020302914.html}
}

@article{Miconi2017,
  title = {Biologically Plausible Learning in Recurrent Neural Networks Reproduces Neural Dynamics Observed during Cognitive Tasks},
  author = {Miconi, Thomas},
  date = {2017-02},
  journaltitle = {eLife},
  volume = {6},
  doi = {10.7554/elife.20899},
  url = {https://doi.org/10.7554%2Felife.20899},
  file = {/Users/vitay/Documents/Zotero/storage/BK8ZYSHJ/Miconi_2017_Biologically plausible learning in recurrent neural networks reproduces neural.pdf}
}

@report{Miner2019,
  type = {preprint},
  title = {Hey, Look over There: {{Distraction}} Effects on Rapid Sequence Recall},
  shorttitle = {Hey, Look over There},
  author = {Miner, Daniel and Tetzlaff, Christian},
  date = {2019-09-30},
  institution = {Neuroscience},
  doi = {10.1101/787861},
  url = {http://biorxiv.org/lookup/doi/10.1101/787861},
  urldate = {2019-10-26},
  abstract = {In the course of everyday life, the brain must store and recall a huge variety of representations of stimuli which are presented in an ordered or sequential way. The processes by which the ordering of these various things is stored and recalled are moderately well understood. We use here a computational model of a cortex-like recurrent neural network adapted by a multitude of plasticity mechanisms. We first demonstrate the learning of a sequence. Then, we examine the influence of different types of distractors on the network dynamics during the recall of the encoded ordered information being ordered in a sequence. We are able to broadly arrive at two distinct effect-categories for distractors, arrive at a basic understanding of why this is so, and predict what distractors will fall into each category.},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/X7YU6LIP/Miner_Tetzlaff_2019_Hey, look over there.pdf}
}

@article{Mollard2024,
  title = {Recurrent Neural Networks That Learn Multi-Step Visual Routines with Reinforcement Learning},
  author = {Mollard, Sami and Wacongne, Catherine and Bohte, Sander M. and Roelfsema, Pieter R.},
  date = {2024-04-29},
  journaltitle = {PLOS Computational Biology},
  shortjournal = {PLOS Computational Biology},
  volume = {20},
  number = {4},
  pages = {e1012030},
  publisher = {Public Library of Science},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1012030},
  url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1012030},
  urldate = {2024-05-22},
  abstract = {Many cognitive problems can be decomposed into series of subproblems that are solved sequentially by the brain. When subproblems are solved, relevant intermediate results need to be stored by neurons and propagated to the next subproblem, until the overarching goal has been completed. We will here consider visual tasks, which can be decomposed into sequences of elemental visual operations. Experimental evidence suggests that intermediate results of the elemental operations are stored in working memory as an enhancement of neural activity in the visual cortex. The focus of enhanced activity is then available for subsequent operations to act upon. The main question at stake is how the elemental operations and their sequencing can emerge in neural networks that are trained with only rewards, in a reinforcement learning setting. We here propose a new recurrent neural network architecture that can learn composite visual tasks that require the application of successive elemental operations. Specifically, we selected three tasks for which electrophysiological recordings of monkeys’ visual cortex are available. To train the networks, we used RELEARNN, a biologically plausible four-factor Hebbian learning rule, which is local both in time and space. We report that networks learn elemental operations, such as contour grouping and visual search, and execute sequences of operations, solely based on the characteristics of the visual stimuli and the reward structure of a task. After training was completed, the activity of the units of the neural network elicited by behaviorally relevant image items was stronger than that elicited by irrelevant ones, just as has been observed in the visual cortex of monkeys solving the same tasks. Relevant information that needed to be exchanged between subroutines was maintained as a focus of enhanced activity and passed on to the subsequent subroutines. Our results demonstrate how a biologically plausible learning rule can train a recurrent neural network on multistep visual tasks.},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/GLP4CP7T/Mollard et al. - 2024 - Recurrent neural networks that learn multi-step visual routines with reinforcement learning.pdf}
}

@article{Murray2017,
  title = {Learning Multiple Variable-Speed Sequences in Striatum via Cortical Tutoring},
  author = {Murray, James M. and Sean Escola, G.},
  date = {2017},
  journaltitle = {eLife},
  volume = {6},
  doi = {10.7554/eLife.26084.001},
  abstract = {Sparse, sequential patterns of neural activity have been observed in numerous brain areas during timekeeping and motor sequence tasks. Inspired by such observations, we construct a model of the striatum, an all-inhibitory circuit where sequential activity patterns are prominent, addressing the following key challenges: (i) obtaining control over temporal rescaling of the sequence speed, with the ability to generalize to new speeds; (ii) facilitating flexible expression of distinct sequences via selective activation, concatenation, and recycling of specific subsequences; and (iii) enabling the biologically plausible learning of sequences, consistent with the decoupling of learning and execution suggested by lesion studies showing that cortical circuits are necessary for learning, but that subcortical circuits are sufficient to drive learned behaviors. The same mechanisms that we describe can also be applied to circuits with both excitatory and inhibitory populations, and hence may underlie general features of sequential neural activity pattern generation in the brain.},
  file = {/Users/vitay/Documents/Zotero/storage/HJ8JPIGK/Murray_Sean Escola_2017_Learning multiple variable-speed sequences in striatum via cortical tutoring.pdf}
}

@article{Nakajima2021,
  title = {Scalable Reservoir Computing on Coherent Linear Photonic Processor},
  author = {Nakajima, Mitsumasa and Tanaka, Kenji and Hashimoto, Toshikazu},
  date = {2021-02-10},
  journaltitle = {Communications Physics},
  shortjournal = {Commun Phys},
  volume = {4},
  number = {1},
  pages = {1--12},
  publisher = {Nature Publishing Group},
  issn = {2399-3650},
  doi = {10.1038/s42005-021-00519-1},
  url = {https://www.nature.com/articles/s42005-021-00519-1},
  urldate = {2023-02-01},
  abstract = {Photonic neuromorphic computing is of particular interest due to its significant potential for ultrahigh computing speed and energy efficiency. The advantage of photonic computing hardware lies in its ultrawide bandwidth and parallel processing utilizing inherent parallelism. Here, we demonstrate a scalable on-chip photonic implementation of a simplified recurrent neural network, called a reservoir computer, using an integrated coherent linear photonic processor. In contrast to previous approaches, both the input and recurrent weights are encoded in the spatiotemporal domain by photonic linear processing, which enables scalable and ultrafast computing beyond the input electrical bandwidth. As the device can process multiple wavelength inputs over the telecom C-band simultaneously, we can use ultrawide optical bandwidth (\textasciitilde 5 terahertz) as a computational resource. Experiments for the standard benchmarks showed good performance for chaotic time-series forecasting and image classification. The device is considered to be able to perform 21.12 tera multiplication–accumulation operations per second (MAC\,∙\,s−1) for each wavelength and can reach petascale computation speed on a single photonic chip by using wavelength division multiplexing. Our results are challenging for conventional Turing–von Neumann machines, and they confirm the great potential of photonic neuromorphic processing towards peta-scale neuromorphic super-computing on a photonic chip.},
  issue = {1},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/ENFGJDNN/Nakajima_et_al_2021_Scalable_reservoir_computing_on_coherent_linear_photonic_processor.pdf}
}

@book{Nakajima2021a,
  title = {Reservoir {{Computing}}: {{Theory}}, {{Physical Implementations}}, and {{Applications}}},
  shorttitle = {Reservoir {{Computing}}},
  editor = {Nakajima, Kohei and Fischer, Ingo},
  date = {2021},
  series = {Natural {{Computing Series}}},
  publisher = {Springer Singapore},
  location = {Singapore},
  doi = {10.1007/978-981-13-1687-6},
  url = {https://link.springer.com/10.1007/978-981-13-1687-6},
  urldate = {2023-07-11},
  isbn = {9789811316869 9789811316876},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/BKDCTSEK/Nakajima and Fischer - 2021 - Reservoir Computing Theory, Physical Implementati.pdf}
}

@inproceedings{Ng2000,
  title = {Algorithms for {{Inverse Reinforcement Learning}}},
  booktitle = {Proceedings of the {{Seventeenth International Conference}} on {{Machine Learning}}},
  author = {Ng, Andrew Y. and Russell, Stuart J.},
  date = {2000-06-29},
  series = {{{ICML}} '00},
  pages = {663--670},
  publisher = {Morgan Kaufmann Publishers Inc.},
  location = {San Francisco, CA, USA},
  isbn = {978-1-55860-707-1}
}

@article{Nicola2017,
  title = {Supervised Learning in Spiking Neural Networks with {{FORCE}} Training},
  author = {Nicola, Wilten and Clopath, Claudia},
  date = {2017-12-20},
  journaltitle = {Nature Communications},
  volume = {8},
  number = {1},
  pages = {2208},
  issn = {2041-1723},
  doi = {10.1038/s41467-017-01827-3},
  url = {https://www.nature.com/articles/s41467-017-01827-3},
  urldate = {2019-04-14},
  abstract = {FORCE training is a . Here the authors implement FORCE training in models of spiking neuronal networks and demonstrate that these networks can be trained to exhibit different dynamic behaviours.},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/XIUH6PY7/Nicola_Clopath_2017_Supervised learning in spiking neural networks with FORCE training.pdf;/Users/vitay/Documents/Zotero/storage/NSN5AM2R/s41467-017-01827-3.html}
}

@article{Paassen2022,
  title = {Reservoir Stack Machines},
  author = {Paaßen, Benjamin and Schulz, Alexander and Hammer, Barbara},
  date = {2022-01-22},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {470},
  pages = {352--364},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2021.05.106},
  url = {https://www.sciencedirect.com/science/article/pii/S0925231221011103},
  urldate = {2023-02-16},
  abstract = {Memory-augmented neural networks equip a recurrent neural network with an explicit memory to support tasks that require information storage without interference over long times. A key motivation for such research is to perform classic computation tasks, such as parsing. However, memory-augmented neural networks are notoriously hard to train, requiring many backpropagation epochs and a lot of data. In this paper, we introduce the reservoir stack machine, a model which can provably recognize all deterministic context-free languages and circumvents the training problem by training only the output layer of a recurrent net and employing auxiliary information during training about the desired interaction with a stack. In our experiments, we validate the reservoir stack machine against deep and shallow networks from the literature on three benchmark tasks for Neural Turing machines and six deterministic context-free languages. Our results show that the reservoir stack machine achieves zero error, even on test sequences longer than the training data, requiring only a few seconds of training time and 100 training sequences.},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/627F9F5T/Paaßen_et_al_2022_Reservoir_stack_machines.pdf}
}

@article{Panda2017,
  title = {Learning to {{Generate Sequences}} with {{Combination}} of {{Hebbian}} and {{Non-hebbian Plasticity}} in {{Recurrent Spiking Neural Networks}}},
  author = {Panda, Priyadarshini and Roy, Kaushik},
  date = {2017},
  journaltitle = {Frontiers in Neuroscience},
  shortjournal = {Front. Neurosci.},
  volume = {11},
  issn = {1662-453X},
  doi = {10.3389/fnins.2017.00693},
  url = {https://www.frontiersin.org/articles/10.3389/fnins.2017.00693/full},
  urldate = {2019-04-14},
  abstract = {Synaptic Plasticity, the foundation for learning and memory formation in the human brain, manifests in various forms. Here, we combine the standard spike timing correlation based Hebbian plasticity with a non-Hebbian synaptic decay mechanism for training a recurrent spiking neural model to generate sequences. We show that inclusion of the adaptive decay of synaptic weights with standard STDP helps learn stable contextual dependencies between temporal sequences, while reducing the strong attractor states that emerge in recurrent models due to feedback loops. Furthermore, we show that the combined learning scheme suppresses the chaotic activity in the recurrent model substantially, thereby enhancing its’ ability to generate sequences consistently even in the presence of perturbations.},
  langid = {english},
  keywords = {attractor dynamics,Eigenvalue spectra,Hebbian plasticity,non-Hebbian learning,Reservoir model,sequence generation},
  file = {/Users/vitay/Documents/Zotero/storage/2IRMG53Y/Panda_Roy_2017_Learning to Generate Sequences with Combination of Hebbian and Non-hebbian.pdf}
}

@article{Panda2018,
  title = {Learning to {{Recognize Actions From Limited Training Examples Using}} a {{Recurrent Spiking Neural Model}}},
  author = {Panda, Priyadarshini and Srinivasa, Narayan},
  date = {2018},
  journaltitle = {Frontiers in Neuroscience},
  shortjournal = {Front. Neurosci.},
  volume = {12},
  publisher = {Frontiers},
  issn = {1662-453X},
  doi = {10.3389/fnins.2018.00126},
  url = {https://www.frontiersin.org/articles/10.3389/fnins.2018.00126/full},
  urldate = {2020-07-16},
  abstract = {A fundamental challenge in machine learning today is to build a model that can learn from few examples. Here, we describe a reservoir based spiking neural model for learning to recognize actions with a limited number of labeled videos. First, we propose a novel encoding, inspired by how microsaccades influence visual perception, to extract spike information from raw video data while preserving the temporal correlation across different frames. Using this encoding, we show that the reservoir generalizes its rich dynamical activity toward signature action/movements enabling it to learn from few training examples. We evaluate our approach on the UCF-101 dataset. Our experiments demonstrate that our proposed reservoir achieves 81.3\%/87\% Top-1/Top-5 accuracy, respectively, on the 101-class data while requiring just 8 video examples per class for training. Our results establish a new benchmark for action recognition from limited video examples for spiking neural models while yielding competetive accuracy with respect to state-of-the-art non-spiking neural models.},
  langid = {english},
  keywords = {Action recognition,Driven-Autonomous Construction,Eigenvalue spectra,Limited Training Example,Micro-Saccade Spike Encoding,Reservoir model,Supervised Plasticity},
  file = {/Users/vitay/Documents/Zotero/storage/DP96QQUI/Panda_Srinivasa_2018_Learning_to_Recognize_Actions_From_Limited_Training_Examples_Using_a_Recurrent.pdf}
}

@article{Pascanu2011,
  title = {A Neurodynamical Model for Working Memory},
  author = {Pascanu, Razvan and Jaeger, Herbert},
  date = {2011-03},
  journaltitle = {Neural Networks},
  volume = {24},
  number = {2},
  pages = {199--207},
  doi = {10.1016/J.NEUNET.2010.10.003},
  url = {https://www.sciencedirect.com/science/article/pii/S0893608010001899?via%3Dihub},
  abstract = {Neurodynamical models of working memory (WM) should provide mechanisms for storing, maintaining, retrieving, and deleting information. Many models address only a subset of these aspects. Here we present a rather simple WM model in which all of these performance modes are trained into a recurrent neural network (RNN) of the echo state network (ESN) type. The model is demonstrated on a bracket level parsing task with a stream of rich and noisy graphical script input. In terms of nonlinear dynamics, memory states correspond, intuitively, to attractors in an input-driven system. As a supplementary contribution, the article proposes a rigorous formal framework to describe such attractors, generalizing from the standard definition of attractors in autonomous (input-free) dynamical systems.},
  file = {/Users/vitay/Documents/Zotero/storage/BLLR2APE/Pascanu_Jaeger_2011_A neurodynamical model for working memory.pdf}
}

@article{Pascanu2013,
  title = {On the Difficulty of Training Recurrent Neural Networks},
  author = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
  date = {2013},
  pages = {9},
  abstract = {There are two widely known issues with properly training recurrent neural networks, the vanishing and the exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to improve the understanding of the underlying issues by exploring these problems from an analytical, a geometric and a dynamical systems perspective. Our analysis is used to justify a simple yet effective solution. We propose a gradient norm clipping strategy to deal with exploding gradients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/JL35FC4C/Pascanu et al_2013_On the diﬃculty of training recurrent neural networks.pdf}
}

@article{Pathak2018,
  title = {Model-{{Free Prediction}} of {{Large Spatiotemporally Chaotic Systems}} from {{Data}}: {{A Reservoir Computing Approach}}},
  author = {Pathak, Jaideep and Hunt, Brian and Girvan, Michelle and Lu, Zhixin and Ott, Edward},
  date = {2018-01},
  journaltitle = {Physical Review Letters},
  volume = {120},
  number = {2},
  pages = {024102--024102},
  doi = {10.1103/PhysRevLett.120.024102},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.120.024102},
  file = {/Users/vitay/Documents/Zotero/storage/BW5CUIDT/Pathak et al_2018_Model-Free Prediction of Large Spatiotemporally Chaotic Systems from Data.pdf}
}

@article{Pitti2017,
  title = {Iterative Free-Energy Optimization for Recurrent Neural Networks ({{INFERNO}})},
  author = {Pitti, Alexandre and Gaussier, Philippe and Quoy, Mathias},
  editor = {El-Deredy, Wael},
  date = {2017-03},
  journaltitle = {PLOS ONE},
  volume = {12},
  number = {3},
  pages = {e0173684-e0173684},
  doi = {10.1371/journal.pone.0173684},
  url = {http://dx.plos.org/10.1371/journal.pone.0173684},
  abstract = {The intra-parietal lobe coupled with the Basal Ganglia forms a working memory that demonstrates strong planning capabilities for generating robust yet flexible neuronal sequences. Neurocomputational models however, often fails to control long range neural synchrony in recurrent spiking networks due to spontaneous activity. As a novel framework based on the free-energy principle, we propose to see the problem of spikes’ synchrony as an optimization problem of the neurons sub-threshold activity for the generation of long neuronal chains. Using a stochastic gradient descent, a reinforcement signal (presumably dopaminergic) evaluates the quality of one input vector to move the recurrent neural network to a desired activity; depending on the error made, this input vector is strengthened to hill-climb the gradient or elicited to search for another solution. This vector can be learned then by one associative memory as a model of the basal-ganglia to control the recurrent neural network. Experiments on habit learning and on sequence retrieving demonstrate the capabilities of the dual system to generate very long and precise spatio-temporal sequences, above two hundred iterations. Its features are applied then to the sequential planning of arm movements. In line with neurobiological theories, we discuss its relevance for modeling the cortico-basal working memory to initiate flexible goal-directed neuronal chains of causation and its relation to novel architectures such as Deep Networks, Neural Turing Machines and the Free-Energy Principle.},
  file = {/Users/vitay/Documents/Zotero/storage/JQLJ8XYL/Pitti et al_2017_Iterative free-energy optimization for recurrent neural networks (INFERNO).pdf}
}

@article{Ponghiran2019,
  title = {Reinforcement {{Learning With Low-Complexity Liquid State Machines}}},
  author = {Ponghiran, Wachirawit and Srinivasan, Gopalakrishnan and Roy, Kaushik},
  date = {2019},
  journaltitle = {Frontiers in Neuroscience},
  shortjournal = {Front. Neurosci.},
  volume = {13},
  publisher = {Frontiers},
  issn = {1662-453X},
  doi = {10.3389/fnins.2019.00883},
  url = {https://www.frontiersin.org/articles/10.3389/fnins.2019.00883/full},
  urldate = {2020-07-16},
  abstract = {We propose reinforcement learning on simple networks consisting of random connections of spiking neurons (both recurrent and feed-forward) that can learn complex tasks with very little trainable parameters. Such sparse and randomly interconnected recurrent spiking networks exhibit highly non-linear dynamics that transform the inputs into rich high-dimensional representations based on the current and past context. The random input representations can be efficiently interpreted by an output (or readout) layer with trainable parameters. Systematic initialization of the random connections and training of the readout layer using Q-learning algorithm enable such small random spiking networks to learn optimally and achieve the same learning efficiency as humans on complex reinforcement learning (RL) tasks like Atari games. In fact, the sparse recurrent connections cause these networks to retain fading memory of past inputs, thereby enabling them to perform temporal integration across successive RL time-steps and learn with partial state inputs. The spike-based approach using small random recurrent networks provides a computationally efficient alternative to state-of-the-art deep reinforcement learning networks with several layers of trainable parameters.},
  langid = {english},
  keywords = {learning without stable states,Liquid state machine,Q-learning,recurrent SNN,spiking reinforcement learning},
  file = {/Users/vitay/Documents/Zotero/storage/7PDQETP4/Ponghiran_et_al_2019_Reinforcement_Learning_With_Low-Complexity_Liquid_State_Machines.pdf}
}

@article{Ponghiran2019a,
  title = {Reinforcement {{Learning With Low-Complexity Liquid State Machines}}},
  author = {Ponghiran, Wachirawit and Srinivasan, Gopalakrishnan and Roy, Kaushik},
  date = {2019},
  journaltitle = {Frontiers in Neuroscience},
  volume = {13},
  issn = {1662-453X},
  url = {https://www.frontiersin.org/articles/10.3389/fnins.2019.00883},
  urldate = {2023-08-11},
  abstract = {We propose reinforcement learning on simple networks consisting of random connections of spiking neurons (both recurrent and feed-forward) that can learn complex tasks with very little trainable parameters. Such sparse and randomly interconnected recurrent spiking networks exhibit highly non-linear dynamics that transform the inputs into rich high-dimensional representations based on the current and past context. The random input representations can be efficiently interpreted by an output (or readout) layer with trainable parameters. Systematic initialization of the random connections and training of the readout layer using Q-learning algorithm enable such small random spiking networks to learn optimally and achieve the same learning efficiency as humans on complex reinforcement learning (RL) tasks like Atari games. In fact, the sparse recurrent connections cause these networks to retain fading memory of past inputs, thereby enabling them to perform temporal integration across successive RL time-steps and learn with partial state inputs. The spike-based approach using small random recurrent networks provides a computationally efficient alternative to state-of-the-art deep reinforcement learning networks with several layers of trainable parameters.},
  file = {/Users/vitay/Documents/Zotero/storage/IY32E2HP/Ponghiran_et_al_2019_Reinforcement_Learning_With_Low-Complexity_Liquid_State_Machines.pdf}
}

@article{Pyle2019,
  title = {A {{Reservoir Computing Model}} of {{Reward-Modulated Motor Learning}} and {{Automaticity}}},
  author = {Pyle, Ryan and Rosenbaum, Robert},
  date = {2019-05-21},
  journaltitle = {Neural Computation},
  shortjournal = {Neural Computation},
  volume = {31},
  number = {7},
  pages = {1430--1461},
  issn = {0899-7667},
  doi = {10.1162/neco_a_01198},
  url = {https://doi.org/10.1162/neco_a_01198},
  urldate = {2019-06-15},
  abstract = {Reservoir computing is a biologically inspired class of learning algorithms in which the intrinsic dynamics of a recurrent neural network are mined to produce target time series. Most existing reservoir computing algorithms rely on fully supervised learning rules, which require access to an exact copy of the target response, greatly reducing the utility of the system. Reinforcement learning rules have been developed for reservoir computing, but we find that they fail to converge on complex motor tasks. Current theories of biological motor learning pose that early learning is controlled by dopamine-modulated plasticity in the basal ganglia that trains parallel cortical pathways through unsupervised plasticity as a motor task becomes well learned. We developed a novel learning algorithm for reservoir computing that models the interaction between reinforcement and unsupervised learning observed in experiments. This novel learning algorithm converges on simulated motor tasks on which previous reservoir computing algorithms fail and reproduces experimental findings that relate Parkinson's disease and its treatments to motor learning. Hence, incorporating biological theories of motor learning improves the effectiveness and biological relevance of reservoir computing models.},
  file = {/Users/vitay/Documents/Zotero/storage/8PB26VC5/Pyle_Rosenbaum_2019_A Reservoir Computing Model of Reward-Modulated Motor Learning and Automaticity.pdf;/Users/vitay/Documents/Zotero/storage/DVGT64UL/neco_a_01198.html}
}

@online{Pyle2019a,
  title = {A Model of Reward-Modulated Motor Learning with Parallelcortical and Basal Ganglia Pathways},
  author = {Pyle, Ryan and Rosenbaum, Robert},
  date = {2019-03-01},
  eprint = {1803.03304},
  eprinttype = {arXiv},
  eprintclass = {cs, q-bio},
  url = {http://arxiv.org/abs/1803.03304},
  urldate = {2023-07-26},
  abstract = {Reservoir computing is a biologically inspired class of learning algorithms in which the intrinsic dynamics of a recurrent neural network are mined to produce target time series. Most existing reservoir computing algorithms rely on fully supervised learning rules, which require access to an exact copy of the target response, greatly reducing the utility of the system. Reinforcement learning rules have been developed for reservoir computing, but we find that they fail to converge on complex motor tasks. Current theories of biological motor learning pose that early learning is controlled by dopamine modulated plasticity in the basal ganglia that trains parallel cortical pathways through unsupervised plasticity as a motor task becomes well-learned. We developed a novel learning algorithm for reservoir computing that models the interaction between reinforcement and unsupervised learning observed in experiments. This novel learning algorithm converges on simulated motor tasks on which previous reservoir computing algorithms fail, and reproduces experimental findings that relate Parkinson’s disease and its treatments to motor learning. Hence, incorporating biological theories of motor learning improves the effectiveness and biological relevance of reservoir computing models.},
  langid = {english},
  pubstate = {prepublished},
  file = {/Users/vitay/Documents/Zotero/storage/55SMQ9DV/Pyle and Rosenbaum - 2019 - A model of reward-modulated motor learning with pa.pdf}
}

@article{Rajan2016,
  title = {Recurrent {{Network Models}} of {{Sequence Generation}} and {{Memory}}},
  author = {Rajan, Kanaka and Harvey, Christopher D. and Tank, David W.},
  date = {2016-04},
  journaltitle = {Neuron},
  volume = {90},
  number = {1},
  pages = {128--142},
  doi = {10.1016/J.NEURON.2016.02.009},
  url = {https://www.sciencedirect.com/science/article/pii/S0896627316001021?via%3Dihub},
  abstract = {Sequential activation of neurons is a common feature of network activity during a variety of behaviors, including working memory and decision making. Previous network models for sequences and memory emphasized specialized architectures in which a principled mechanism is pre-wired into their connectivity. Here we demonstrate that, starting from random connectivity and modifying a small fraction of connections, a largely disordered recurrent network can produce sequences and implement working memory efficiently. We use this process, called Partial In-Network Training (PINning), to model and match cellular resolution imaging data from the posterior parietal cortex during a virtual memory-guided two-alternative forced-choice task. Analysis of the connectivity reveals that sequences propagate by the cooperation between recurrent synaptic interactions and external inputs, rather than through feedforward or asymmetric connections. Together our results suggest that neural sequences may emerge through learning from largely unstructured network architectures.},
  file = {/Users/vitay/Documents/Zotero/storage/7VV4CTPK/Rajan et al_2016_Recurrent Network Models of Sequence Generation and Memory.pdf}
}

@article{Recanatesi2019,
  title = {Predictive Learning Extracts Latent Space Representations from Sensory Observations},
  author = {Recanatesi, Stefano and Farrell, Matthew and Lajoie, Guillaume and Deneve, Sophie and Rigotti, Mattia and Shea-Brown, Eric},
  date = {2019-07-13},
  journaltitle = {bioRxiv},
  pages = {471987},
  doi = {10.1101/471987},
  url = {https://www.biorxiv.org/content/10.1101/471987v3},
  urldate = {2019-07-20},
  abstract = {{$<$}p{$>$}Neural networks have achieved many recent successes in solving sequential processing and planning tasks. Their success is often ascribed to the emergence of the task9s low-dimensional latent structure in the network activity - i.e., in the learned neural representations. Similarly, biological neural circuits and in particular the hippocampus may produce representations that organize semantically related episodes. Here, we investigate the hypothesis that representations with low-dimensional latent structure, reflecting such semantic organization, result from learning to predict observations about the world. Specifically, we ask whether and when network mechanisms for sensory prediction coincide with those for extracting the underlying latent variables. Using a recurrent neural network model trained to predict a sequence of observations in a simulated spatial navigation task, we show that network dynamics exhibit low-dimensional but nonlinearly transformed representations of sensory inputs that capture the latent structure of the sensory environment. We quantify these results using nonlinear measures of intrinsic dimensionality which highlight the importance of the predictive aspect of neural representations, and provide mathematical arguments for when and why these representations emerge. We focus throughout on how our results can aid the analysis and interpretation of experimental data.{$<$}/p{$>$}},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/HHKXZ3FF/Recanatesi et al_2019_Predictive learning extracts latent space representations from sensory.pdf;/Users/vitay/Documents/Zotero/storage/4I6CGX5D/471987v3.html}
}

@article{Rodriguez2019,
  title = {Optimal Modularity and Memory Capacity of Neural Reservoirs},
  author = {Rodriguez, Nathaniel and Izquierdo, Eduardo and Ahn, Yong-Yeol},
  date = {2019-05-01},
  journaltitle = {Network Neuroscience},
  shortjournal = {Network Neuroscience},
  volume = {3},
  number = {2},
  pages = {551--566},
  issn = {2472-1751},
  doi = {10.1162/netn_a_00082},
  url = {https://doi.org/10.1162/netn_a_00082},
  urldate = {2023-02-01},
  abstract = {The neural network is a powerful computing framework that has been exploited by biological evolution and by humans for solving diverse problems. Although the computational capabilities of neural networks are determined by their structure, the current understanding of the relationships between a neural network’s architecture and function is still primitive. Here we reveal that a neural network’s modular architecture plays a vital role in determining the neural dynamics and memory performance of the network of threshold neurons. In particular, we demonstrate that there exists an optimal modularity for memory performance, where a balance between local cohesion and global connectivity is established, allowing optimally modular networks to remember longer. Our results suggest that insights from dynamical analysis of neural networks and information-spreading processes can be leveraged to better design neural networks and may shed light on the brain’s modular organization.Understanding the inner workings of the human brain is one of the greatest scientific challenges. It will not only advance the science of the human mind, but also help us build more intelligent machines. In doing so, it is crucial to understand how the structural organization of the brain affects functional capabilities. Here we reveal a strong connection between the modularity of a neural network and its performance in memory tasks. Namely, we demonstrate that there is optimal modularity for memory performance. Our results suggest a design principle for artificial recurrent neural networks as well as a hypothesis that may explain not only the existence but also the strength of modularity in the brain.},
  file = {/Users/vitay/Documents/Zotero/storage/XXKARZG8/Rodriguez_et_al_2019_Optimal_modularity_and_memory_capacity_of_neural_reservoirs.pdf}
}

@article{Rossert2015,
  title = {At the {{Edge}} of {{Chaos}}: {{How Cerebellar Granular Layer Network Dynamics Can Provide}} the {{Basis}} for {{Temporal Filters}}},
  shorttitle = {At the {{Edge}} of {{Chaos}}},
  author = {Rössert, Christian and Dean, Paul and Porrill, John},
  editor = {Graham, Lyle J.},
  date = {2015-10-20},
  journaltitle = {PLOS Computational Biology},
  volume = {11},
  number = {10},
  pages = {e1004515},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1004515},
  url = {https://dx.plos.org/10.1371/journal.pcbi.1004515},
  urldate = {2019-01-18},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/GLVBF3SE/Rössert et al_2015_At the Edge of Chaos.pdf}
}

@article{Sankar2021,
  title = {[{{Re}}] {{A Reservoir Computing Model}} of {{Reward-Modulated Motor Learning}} and {{Automaticity}}},
  author = {Sankar, Remya and Thou, Nicolas and Rougier, Nicolas P. and Leblois, Arthur},
  date = {2021-11-22},
  publisher = {Zenodo},
  doi = {10.5281/ZENODO.5718075},
  url = {https://zenodo.org/record/5718075},
  urldate = {2023-08-18},
  abstract = {Replication},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/EACZXVSS/Sankar et al. - 2021 - [Re] A Reservoir Computing Model of Reward-Modulat.pdf}
}

@article{Savin2014,
  title = {Emergence of Task-Dependent Representations in Working Memory Circuits},
  author = {Savin, Cristina and Triesch, Jochen},
  date = {2014-05},
  journaltitle = {Frontiers in Computational Neuroscience},
  volume = {8},
  pages = {57--57},
  doi = {10.3389/fncom.2014.00057},
  url = {http://journal.frontiersin.org/article/10.3389/fncom.2014.00057/abstract},
  abstract = {A wealth of experimental evidence suggests that working memory circuits preferentially represent information that is behaviorally relevant. Still, we are missing a mechanistic account of how these representations come about. Here we provide a simple explanation for a range of experimental findings, in light of prefrontal circuits adapting to task constraints by reward-dependent learning. In particular, we model a neural network shaped by reward-modulated spike-timing dependent plasticity (r-STDP) and homeostatic plasticity (intrinsic excitability and synaptic scaling). We show that the experimentally-observed neural representations naturally emerge in an initially unstructured circuit as it learns to solve several working memory tasks. These results point to a critical, and previously unappreciated, role for reward-dependent learning in shaping prefrontal cortex activity.},
  keywords = {delayed categorization,Intrinsic Plasticity,Prefrontal Cortex,reward-dependent learning,STDP,synaptic scaling,working memory},
  file = {/Users/vitay/Documents/Zotero/storage/75EKIMJY/Savin_Triesch_2014_Emergence of task-dependent representations in working memory circuits.pdf}
}

@article{Scardapane2016,
  title = {Distributed {{Reservoir Computing}} with {{Sparse Readouts}} [{{Research Frontier}}]},
  author = {Scardapane, Simone and Panella, Massimo and Comminiello, Danilo and Hussain, Amir and Uncini, Aurelio},
  date = {2016-11},
  journaltitle = {IEEE Computational Intelligence Magazine},
  volume = {11},
  number = {4},
  pages = {59--70},
  issn = {1556-6048},
  doi = {10.1109/MCI.2016.2601759},
  abstract = {In a network of agents, a widespread problem is the need to estimate a common underlying function starting from locally distributed measurements. Real-world scenarios may not allow the presence of centralized fusion centers, requiring the development of distributed, message-passing implementations of the standard machine learning training algorithms. In this paper, we are concerned with the distributed training of a particular class of recurrent neural networks, namely echo state networks (ESNs). In the centralized case, ESNs have received considerable attention, due to the fact that they can be trained with standard linear regression routines. Based on this observation, in our previous work we have introduced a decentralized algorithm, framed in the distributed optimization field, in order to train an ESN. In this paper, we focus on an additional sparsity property of the output layer of ESNs, allowing for very efficient implementations of the resulting networks. In order to evaluate the proposed algorithm, we test it on two well-known prediction benchmarks, namely the Mackey-Glass chaotic time series and the 10th order nonlinear auto regressive moving average (NARMA) system.},
  eventtitle = {{{IEEE Computational Intelligence Magazine}}},
  file = {/Users/vitay/Documents/Zotero/storage/GA6VP97X/Scardapane_et_al_2016_Distributed_Reservoir_Computing_with_Sparse_Readouts_[Research_Frontier].pdf}
}

@inproceedings{Schmid2019,
  title = {Forward {{Models}} in the {{Cerebellum}} Using {{Reservoirs}} and {{Perturbation Learning}}},
  booktitle = {2019 {{Conference}} on {{Cognitive Computational Neuroscience}}},
  author = {Schmid, Katharina and Vitay, Julien and Hamker, Fred H.},
  date = {2019},
  publisher = {Cognitive Computational Neuroscience},
  location = {Berlin, Germany},
  doi = {10.32470/CCN.2019.1139-0},
  url = {https://ccneuro.org/2019/Papers/ViewPapers.asp?PaperNum=1139},
  urldate = {2020-03-02},
  abstract = {The cerebellum is thought to be able to learn forward models, which allow to predict the sensory consequences of planned movements and adapt behavior accordingly. Although classically considered as a feedforward structure learning in a supervised manner, recent proposals highlighted the importance of the internal recurrent connectivity of the cerebellum to produce rich dynamics, as well as the importance of reinforcement-like mechanisms for its plasticity. Based on these models, we propose a neuro-computational model of the cerebellum using an inhibitory reservoir architecture and biologically plausible learning mechanisms based on perturbation learning. The model is trained to predict the position of a simple robotic arm after ballistic movements. Understanding how the cerebellum is able to learn forward models might allow elucidating the biological basis of modelbased reinforcement learning.},
  eventtitle = {2019 {{Conference}} on {{Cognitive Computational Neuroscience}}},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/G9GQPMAK/Schmid_et_al_2019_Forward_Models_in_the_Cerebellum_using_Reservoirs_and_Perturbation_Learning.pdf}
}

@article{Schmidhuber2007,
  title = {Training {{Recurrent Networks}} by {{Evolino}}},
  author = {Schmidhuber, Jürgen and Wierstra, Daan and Gagliolo, Matteo and Gomez, Faustino},
  date = {2007-03},
  journaltitle = {Neural Computation},
  volume = {19},
  number = {3},
  pages = {757--779},
  doi = {10.1162/neco.2007.19.3.757},
  url = {http://www.mitpressjournals.org/doi/10.1162/neco.2007.19.3.757}
}

@article{Schrauwen2007,
  title = {An Overview of Reservoir Computing: Theory, Applications and Implementations},
  author = {Schrauwen, Benjamin and Verstraeten, David and Van Campenhout, Jan},
  date = {2007},
  journaltitle = {Proceedings of the 15th European Symposium on Artificial Neural Networks. p. 471-482 2007},
  pages = {471--482},
  url = {https://biblio.ugent.be/publication/416607},
  keywords = {reservoir computing,Technology and Engineering,tutorial}
}

@article{Schrauwen2008,
  title = {Improving Reservoirs Using Intrinsic Plasticity},
  author = {Schrauwen, Benjamin and Wardermann, Marion and Verstraeten, David and Steil, Jochen J. and Stroobandt, Dirk},
  date = {2008-03-01},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  series = {Progress in {{Modeling}}, {{Theory}}, and {{Application}} of {{Computational Intelligenc}}},
  volume = {71},
  number = {7},
  pages = {1159--1171},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2007.12.020},
  url = {https://www.sciencedirect.com/science/article/pii/S0925231208000519},
  urldate = {2022-11-03},
  abstract = {The benefits of using intrinsic plasticity (IP), an unsupervised, local, biologically inspired adaptation rule that tunes the probability density of a neuron's output towards an exponential distribution—thereby realizing an information maximization—have already been demonstrated. In this work, we extend the ideas of this adaptation method to a more commonly used non-linearity and a Gaussian output distribution. After deriving the learning rules, we show the effects of the bounded output of the transfer function on the moments of the actual output distribution. This allows us to show that the rule converges to the expected distributions, even in random recurrent networks. The IP rule is evaluated in a reservoir computing setting, which is a temporal processing technique which uses random, untrained recurrent networks as excitable media, where the network's state is fed to a linear regressor used to calculate the desired output. We present an experimental comparison of the different IP rules on three benchmark tasks with different characteristics. Furthermore, we show that this unsupervised reservoir adaptation is able to adapt networks with very constrained topologies, such as a 1D lattice which generally shows quite unsuitable dynamic behavior, to a reservoir that can be used to solve complex tasks. We clearly demonstrate that IP is able to make reservoir computing more robust: the internal dynamics can autonomously tune themselves—irrespective of initial weights or input scaling—to the dynamic regime which is optimal for a given task.},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/PS3PM63U/Schrauwen_et_al_2008_Improving_reservoirs_using_intrinsic_plasticity.pdf;/Users/vitay/Documents/Zotero/storage/GVJIJBP3/S0925231208000519.html}
}

@article{Schuessler2020,
  title = {Dynamics of Random Recurrent Networks with Correlated Low-Rank Structure},
  author = {Schuessler, Friedrich and Dubreuil, Alexis and Mastrogiuseppe, Francesca and Ostojic, Srdjan and Barak, Omri},
  date = {2020-02-03},
  journaltitle = {Physical Review Research},
  shortjournal = {Phys. Rev. Research},
  volume = {2},
  number = {1},
  pages = {013111},
  issn = {2643-1564},
  doi = {10.1103/PhysRevResearch.2.013111},
  url = {https://link.aps.org/doi/10.1103/PhysRevResearch.2.013111},
  urldate = {2023-06-06},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/V3LGCLI5/Schuessler et al. - 2020 - Dynamics of random recurrent networks with correla.pdf}
}

@article{Seoane2019,
  title = {Evolutionary Aspects of Reservoir Computing},
  author = {Seoane, Luís F.},
  date = {2019-06-10},
  journaltitle = {Philosophical Transactions of the Royal Society B},
  url = {https://royalsocietypublishing.org/doi/abs/10.1098/rstb.2018.0377},
  urldate = {2020-01-23},
  abstract = {Reservoir computing (RC) is a powerful computational paradigm that allows high versatility with cheap learning. While other artificial intelligence approaches need exhaustive resources to specify t...},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/9AIJ3B73/Seoane_2019_Evolutionary_aspects_of_reservoir_computing.pdf;/Users/vitay/Documents/Zotero/storage/L6ZDMAC5/rstb.2018.html}
}

@article{Shahi2022,
  title = {Prediction of Chaotic Time Series Using Recurrent Neural Networks and Reservoir Computing Techniques: {{A}} Comparative Study},
  shorttitle = {Prediction of Chaotic Time Series Using Recurrent Neural Networks and Reservoir Computing Techniques},
  author = {Shahi, Shahrokh and Fenton, Flavio H. and Cherry, Elizabeth M.},
  date = {2022-06-15},
  journaltitle = {Machine Learning with Applications},
  shortjournal = {Machine Learning with Applications},
  volume = {8},
  pages = {100300},
  issn = {2666-8270},
  doi = {10.1016/j.mlwa.2022.100300},
  url = {https://www.sciencedirect.com/science/article/pii/S2666827022000275},
  urldate = {2023-06-08},
  abstract = {In recent years, machine-learning techniques, particularly deep learning, have outperformed traditional time-series forecasting approaches in many contexts, including univariate and multivariate predictions. This study aims to investigate the capability of (i) gated recurrent neural networks, including long short-term memory (LSTM) and gated recurrent unit (GRU) networks, (ii) reservoir computing (RC) techniques, such as echo state networks (ESNs) and hybrid physics-informed ESNs, and (iii) the nonlinear vector autoregression (NVAR) approach, which has recently been introduced as the next generation RC, for the prediction of chaotic time series and to compare their performance in terms of accuracy, efficiency, and robustness. We apply the methods to predict time series obtained from two widely used chaotic benchmarks, the Mackey–Glass and Lorenz-63 models, as well as two other chaotic datasets representing a bursting neuron and the dynamics of the El Niño Southern Oscillation, and to one experimental dataset representing a time series of cardiac voltage with complex dynamics. We find that even though gated RNN techniques have been successful in forecasting time series generally, they can fall short in predicting chaotic time series for the methods, datasets, and ranges of hyperparameter values considered here. In contrast, for the chaotic datasets studied, we found that reservoir computing and NVAR techniques are more computationally efficient and offer more promise in long-term prediction of chaotic time series.},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/E6W7ISJF/Shahi_et_al_2022_Prediction_of_chaotic_time_series_using_recurrent_neural_networks_and_reservoir.pdf}
}

@unpublished{Shen2020,
  title = {Reservoir {{Transformer}}},
  author = {Shen, Sheng and Baevski, Alexei and Morcos, Ari S. and Keutzer, Kurt and Auli, Michael and Kiela, Douwe},
  date = {2020-12-30},
  eprint = {2012.15045},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2012.15045},
  urldate = {2021-02-21},
  abstract = {We demonstrate that transformers obtain impressive performance even when some of the layers are randomly initialized and never updated. Inspired by old and well-established ideas in machine learning, we explore a variety of non-linear "reservoir" layers interspersed with regular transformer layers, and show improvements in wall-clock compute time until convergence, as well as overall performance, on various machine translation and (masked) language modelling tasks.},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/vitay/Documents/Zotero/storage/IUYUNBJM/Shen_et_al_2020_Reservoir_Transformer.pdf;/Users/vitay/Documents/Zotero/storage/8QETC8GN/2012.html}
}

@article{Singer2013,
  title = {Cortical Dynamics Revisited},
  author = {Singer, Wolf},
  date = {2013-12-01},
  journaltitle = {Trends in Cognitive Sciences},
  shortjournal = {Trends in Cognitive Sciences},
  volume = {17},
  number = {12},
  eprint = {24139950},
  eprinttype = {pmid},
  pages = {616--626},
  publisher = {Elsevier},
  issn = {1364-6613, 1879-307X},
  doi = {10.1016/j.tics.2013.09.006},
  url = {https://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(13)00210-6},
  urldate = {2020-04-05},
  langid = {english},
  keywords = {cerebral cortex,nonlinear dynamics,oscillations,reservoir computing,synchrony},
  file = {/Users/vitay/Documents/Zotero/storage/DYHRX7LN/Singer_2013_Cortical_dynamics_revisited.pdf;/Users/vitay/Documents/Zotero/storage/599H7BDZ/S1364-6613(13)00210-6.html}
}

@article{Smith2022,
  title = {Learning Continuous Chaotic Attractors with a Reservoir Computer},
  author = {Smith, Lindsay M. and Kim, Jason Z. and Lu, Zhixin and Bassett, Dani S.},
  date = {2022-01},
  journaltitle = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  shortjournal = {Chaos},
  volume = {32},
  number = {1},
  pages = {011101},
  publisher = {American Institute of Physics},
  issn = {1054-1500},
  doi = {10.1063/5.0075572},
  url = {https://aip.scitation.org/doi/10.1063/5.0075572},
  urldate = {2022-10-14},
  abstract = {Neural systems are well known for their ability to learn and store information as memories. Even more impressive is their ability to abstract these memories to create complex internal representations, enabling advanced functions such as the spatial manipulation of mental representations. While recurrent neural networks (RNNs) are capable of representing complex information, the exact mechanisms of how dynamical neural systems perform abstraction are still not well-understood, thereby hindering the development of more advanced functions. Here, we train a 1000-neuron RNN—a reservoir computer (RC)—to abstract a continuous dynamical attractor memory from isolated examples of dynamical attractor memories. Furthermore, we explain the abstraction mechanism with a new theory. By training the RC on isolated and shifted examples of either stable limit cycles or chaotic Lorenz attractors, the RC learns a continuum of attractors as quantified by an extra Lyapunov exponent equal to zero. We propose a theoretical mechanism of this abstraction by combining ideas from differentiable generalized synchronization and feedback dynamics. Our results quantify abstraction in simple neural systems, enabling us to design artificial RNNs for abstraction and leading us toward a neural basis of abstraction.},
  file = {/Users/vitay/Documents/Zotero/storage/JD6LZIZE/Smith_et_al_2022_Learning_continuous_chaotic_attractors_with_a_reservoir_computer.pdf}
}

@article{Sompolinsky1988,
  title = {Chaos in {{Random Neural Networks}}},
  author = {Sompolinsky, H. and Crisanti, A. and Sommers, H. J.},
  date = {1988-07},
  journaltitle = {Physical Review Letters},
  volume = {61},
  number = {3},
  pages = {259--262},
  doi = {10.1103/PhysRevLett.61.259},
  url = {http://link.aps.org/doi/10.1103/PhysRevLett.61.259}
}

@article{Song2016,
  title = {Training {{Excitatory-Inhibitory Recurrent Neural Networks}} for {{Cognitive Tasks}}: {{A Simple}} and {{Flexible Framework}}},
  author = {Song, H. Francis and Yang, Guangyu R. and Wang, Xiao-Jing},
  editor = {Sporns, Olaf},
  date = {2016-02},
  journaltitle = {PLOS Computational Biology},
  volume = {12},
  number = {2},
  pages = {e1004792-e1004792},
  doi = {10.1371/journal.pcbi.1004792},
  url = {https://dx.plos.org/10.1371/journal.pcbi.1004792},
  abstract = {The ability to simultaneously record from large numbers of neurons in behaving animals has ushered in a new era for the study of the neural circuit mechanisms underlying cognitive functions. One promising approach to uncovering the dynamical and computational principles governing population responses is to analyze model recurrent neural networks (RNNs) that have been optimized to perform the same tasks as behaving animals. Because the optimization of network parameters specifies the desired output but not the manner in which to achieve this output, “trained” networks serve as a source of mechanistic hypotheses and a testing ground for data analyses that link neural computation to behavior. Complete access to the activity and connectivity of the circuit, and the ability to manipulate them arbitrarily, make trained networks a convenient proxy for biological circuits and a valuable platform for theoretical investigation. However, existing RNNs lack basic biological features such as the distinction between excitatory and inhibitory units (Dale’s principle), which are essential if RNNs are to provide insights into the operation of biological circuits. Moreover, trained networks can achieve the same behavioral performance but differ substantially in their structure and dynamics, highlighting the need for a simple and flexible framework for the exploratory training of RNNs. Here, we describe a framework for gradient descent-based training of excitatory-inhibitory RNNs that can incorporate a variety of biological knowledge. We provide an implementation based on the machine learning library Theano, whose automatic differentiation capabilities facilitate modifications and extensions. We validate this framework by applying it to well-known experimental paradigms such as perceptual decision-making, context-dependent integration, multisensory integration, parametric working memory, and motor sequence generation. Our results demonstrate the wide range of neural activity patterns and behavior that can be modeled, and suggest a unified setting in which diverse cognitive computations and mechanisms can be studied.},
  file = {/Users/vitay/Documents/Zotero/storage/5ARW2SA6/Song et al_2016_Training Excitatory-Inhibitory Recurrent Neural Networks for Cognitive Tasks.PDF}
}

@article{Song2017,
  title = {Reward-Based Training of Recurrent Neural Networks for Cognitive and Value-Based Tasks},
  author = {Song, H. Francis and Yang, Guangyu R. and Wang, Xiao Jing},
  date = {2017},
  journaltitle = {eLife},
  issn = {0106-9543},
  doi = {10.7554/eLife.21492},
  abstract = {Trained neural network models, which exhibit features of neural activity recorded from behaving animals, may provide insights into the circuit mechanisms of cognitive functions through systematic analysis of network activity and connectivity. However, in contrast to the graded error signals commonly used to train networks through supervised learning, animals learn from reward feedback on definite actions through reinforcement learning. Reward maximization is particularly relevant when optimal behavior depends on an animal's internal judgment of confidence or subjective preferences. Here, we implement reward-based training of recurrent neural networks in which a value network guides learning by using the activity of the decision network to predict future reward. We show that such models capture behavioral and electrophysiological findings from well-known experimental paradigms. Our work provides a unified framework for investigating diverse cognitive and value-based computations, and predicts a role for value representation that is essential for learning, but not executing, a task.},
  file = {/Users/vitay/Documents/Zotero/storage/5U5DC5NB/Song et al_2017_Reward-based training of recurrent neural networks for cognitive and.pdf}
}

@article{Soures2019,
  title = {Deep {{Liquid State Machines With Neural Plasticity}} for {{Video Activity Recognition}}},
  author = {Soures, Nicholas and Kudithipudi, Dhireesha},
  date = {2019},
  journaltitle = {Frontiers in Neuroscience},
  shortjournal = {Front. Neurosci.},
  volume = {13},
  issn = {1662-453X},
  doi = {10.3389/fnins.2019.00686},
  url = {https://www.frontiersin.org/articles/10.3389/fnins.2019.00686/full},
  urldate = {2020-02-23},
  abstract = {Real-world applications such as first-person video activity recognition require intelligent edge devices. However, size, weight, and power constraints of the embedded platforms cannot support resource intensive state-of-the-art algorithms. Machine learning lite algorithms, such as reservoir computing, with shallow 3-layer networks are computationally frugal as only the output layer is trained. By reducing network depth and plasticity, reservoir computing minimizes computational power and complexity, making the algorithms optimal for edge devices. However, as a trade-off for their frugal nature, reservoir computing sacrifices computational power compared to state-of-the-art methods. A good compromise between reservoir computing and fully supervised networks are the proposed deep-LSM networks. The deep-LSM is a deep spiking neural network which captures dynamic information over multiple time-scales with a combination of randomly connected layers and unsupervised layers. The deep-LSM processes the captured dynamic information through an attention modulated readout layer to perform classification. We demonstrate that the deep-LSM achieves an average of 84.78\textbackslash\% accuracy on the DogCentric video activity recognition task, beating state-of-the-art. The deep-LSM also shows up to 91.13\textbackslash\% memory savings and up to 91.55\textbackslash\% reduction in synaptic operations when compared to similar recurrent neural network models. Based on these results we claim that the deep-LSM is capable of overcoming limitations of traditional reservoir computing, while maintaining the low computational cost associated with reservoir computing.},
  langid = {english},
  keywords = {deep,Local learning,LSM,recurrent,spiking},
  file = {/Users/vitay/Documents/Zotero/storage/ZX4E4LX3/Soures_Kudithipudi_2019_Deep_Liquid_State_Machines_With_Neural_Plasticity_for_Video_Activity_Recognition.pdf}
}

@inproceedings{Steil2004,
  title = {Backpropagation-Decorrelation: Online Recurrent Learning with {{O}}({{N}}) Complexity},
  shorttitle = {Backpropagation-Decorrelation},
  booktitle = {2004 {{IEEE International Joint Conference}} on {{Neural Networks}} ({{IEEE Cat}}. {{No}}.{{04CH37541}})},
  author = {Steil, J.J.},
  date = {2004-07},
  volume = {2},
  pages = {843-848 vol.2},
  issn = {1098-7576},
  doi = {10.1109/IJCNN.2004.1380039},
  abstract = {We introduce a new learning rule for fully recurrent neural networks which we call backpropagation-decorrelation rule (BPDC). It combines important principles: one-step backpropagation of errors and the usage of temporal memory in the network dynamics by means of decorrelation of activations. The BPDC rule is derived and theoretically justified from regarding learning as a constraint optimization problem and applies uniformly in discrete and continuous time. It is very easy to implement, and has a minimal complexity of 2N multiplications per time-step in the single output case. Nevertheless we obtain fast tracking and excellent performance in some benchmark problems including the Mackey-Glass time-series.},
  eventtitle = {2004 {{IEEE International Joint Conference}} on {{Neural Networks}} ({{IEEE Cat}}. {{No}}.{{04CH37541}})},
  file = {/Users/vitay/Documents/Zotero/storage/59KV78N5/Steil_2004_Backpropagation-decorrelation.pdf;/Users/vitay/Documents/Zotero/storage/8DRUE3Q9/Steil_2004_Backpropagation-decorrelation2.pdf;/Users/vitay/Documents/Zotero/storage/5PKLD9A3/1380039.html}
}

@article{Steil2006,
  title = {Online Stability of Backpropagation–Decorrelation Recurrent Learning},
  author = {Steil, Jochen J.},
  date = {2006-03},
  journaltitle = {Neurocomputing},
  volume = {69},
  number = {7-9},
  pages = {642--650},
  doi = {10.1016/J.NEUCOM.2005.12.012},
  url = {https://www.sciencedirect.com/science/article/pii/S0925231205003073?via%3Dihub},
  abstract = {We provide a stability analysis based on nonlinear feedback theory for the recently introduced backpropagation–decorrelation (BPDC) recurrent learning algorithm which adapts only the output weights of a possibly large network and therefore can learn in O(N). Using a small gain criterion, we derive a simple sufficient stability inequality. The condition can be monitored online to assure that the recurrent network is stable and can in principle be applied to any network adapting only the output weights. Based on these results the BPDC learning is further enhanced with an efficient online rescaling algorithm to stabilize the network while adapting. In simulations we find that this mechanism improves learning in the provably stable domain. As byproduct we show that BPDC is highly competitive on standard data sets including the recently introduced CATS benchmark data [CATS data. URL: http://www.cis.hut.fi/lendasse/competition/competition.html].},
  file = {/Users/vitay/Documents/Zotero/storage/JTA9JAR9/Steil_2006_Online stability of backpropagation–decorrelation recurrent learning.pdf}
}

@article{Steiner2022,
  title = {{{PyRCN}}: {{A}} Toolbox for Exploration and Application of {{Reservoir Computing Networks}}},
  shorttitle = {{{PyRCN}}},
  author = {Steiner, Peter and Jalalvand, Azarakhsh and Stone, Simon and Birkholz, Peter},
  date = {2022-08-01},
  journaltitle = {Engineering Applications of Artificial Intelligence},
  shortjournal = {Engineering Applications of Artificial Intelligence},
  volume = {113},
  pages = {104964},
  issn = {0952-1976},
  doi = {10.1016/j.engappai.2022.104964},
  url = {https://www.sciencedirect.com/science/article/pii/S0952197622001713},
  urldate = {2023-08-28},
  abstract = {Reservoir Computing Networks (RCNs) belong to a group of machine learning techniques that project the input space non-linearly into a high-dimensional feature space, where the underlying task can be solved linearly. Popular variants of RCNs are capable of solving complex tasks equivalently to widely used deep neural networks, but with a substantially simpler training paradigm based on linear regression. In this paper, we show how to uniformly describe RCNs with small and clearly defined building blocks, and we introduce the Python toolbox PyRCN (Python Reservoir Computing Networks) for optimizing, training and analyzing RCNs on arbitrarily large datasets. The tool is based on widely-used scientific packages and complies with the scikit-learn interface specification. It provides a platform for educational and exploratory analyses of RCNs, as well as a framework to apply RCNs on complex tasks including sequence processing. With a small number of building blocks, the framework allows the implementation of numerous different RCN architectures. We provide code examples on how to set up RCNs for time series prediction and for sequence classification tasks. PyRCN is around ten times faster than reference toolboxes on a benchmark task while requiring substantially less boilerplate code.},
  file = {/Users/vitay/Documents/Zotero/storage/QPLQ7DBQ/Steiner_et_al_2022_PyRCN.pdf}
}

@article{Strock2019,
  title = {A {{Robust Model}} of {{Gated Working Memory}}},
  author = {Strock, Anthony and Hinaut, Xavier and Rougier, Nicolas P.},
  date = {2019-11-08},
  journaltitle = {Neural Computation},
  shortjournal = {Neural Computation},
  volume = {32},
  number = {1},
  pages = {153--181},
  issn = {0899-7667},
  doi = {10.1162/neco_a_01249},
  url = {https://doi.org/10.1162/neco_a_01249},
  urldate = {2020-01-15},
  abstract = {Gated working memory is defined as the capacity of holding arbitrary information at any time in order to be used at a later time. Based on electrophysiological recordings, several computational models have tackled the problem using dedicated and explicit mechanisms. We propose instead to consider an implicit mechanism based on a random recurrent neural network. We introduce a robust yet simple reservoir model of gated working memory with instantaneous updates. The model is able to store an arbitrary real value at random time over an extended period of time. The dynamics of the model is a line attractor that learns to exploit reentry and a nonlinearity during the training phase using only a few representative values. A deeper study of the model shows that there is actually a large range of hyperparameters for which the results hold (e.g., number of neurons, sparsity, global weight scaling) such that any large enough population, mixing excitatory and inhibitory neurons, can quickly learn to realize such gated working memory. In a nutshell, with a minimal set of hypotheses, we show that we can have a robust model of working memory. This suggests this property could be an implicit property of any random population, that can be acquired through learning. Furthermore, considering working memory to be a physically open but functionally closed system, we give account on some counterintuitive electrophysiological recordings.},
  file = {/Users/vitay/Documents/Zotero/storage/5L3WZTG5/Strock_et_al_2019_A_Robust_Model_of_Gated_Working_Memory.pdf;/Users/vitay/Documents/Zotero/storage/DVRBQ438/neco_a_01249.html}
}

@article{Sussillo2009,
  title = {Generating Coherent Patterns of Activity from Chaotic Neural Networks.},
  author = {Sussillo, David and Abbott, L F},
  date = {2009-08},
  journaltitle = {Neuron},
  volume = {63},
  number = {4},
  eprint = {19709635},
  eprinttype = {pubmed},
  pages = {544--57},
  doi = {10.1016/j.neuron.2009.07.018},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/19709635},
  abstract = {Neural circuits display complex activity patterns both spontaneously and when responding to a stimulus or generating a motor output. How are these two forms of activity related? We develop a procedure called FORCE learning for modifying synaptic strengths either external to or within a model neural network to change chaotic spontaneous activity into a wide variety of desired activity patterns. FORCE learning works even though the networks we train are spontaneously chaotic and we leave feedback loops intact and unclamped during learning. Using this approach, we construct networks that produce a wide variety of complex output patterns, input-output transformations that require memory, multiple outputs that can be switched by control inputs, and motor patterns matching human motion capture data. Our results reproduce data on premovement activity in motor and premotor cortex, and suggest that synaptic plasticity may be a more rapid and powerful modulator of network activity than generally appreciated.},
  file = {/Users/vitay/Documents/Zotero/storage/3RQV8GZY/Sussillo_Abbott_2009_Generating coherent patterns of activity from chaotic neural networks.pdf}
}

@inproceedings{Takimoto2017,
  title = {Self-Organization Based on Auditory Feedback Promotes Acquisition of Babbling},
  booktitle = {2017 {{Joint IEEE International Conference}} on {{Development}} and {{Learning}} and {{Epigenetic Robotics}} ({{ICDL-EpiRob}})},
  author = {Takimoto, Tomohiro and Kawai, Yuji and Park, Jihoon and Asada, Minoru},
  date = {2017-09},
  pages = {120--125},
  publisher = {IEEE},
  doi = {10.1109/DEVLRN.2017.8329796},
  url = {http://ieeexplore.ieee.org/document/8329796/},
  isbn = {978-1-5386-3715-9},
  file = {/Users/vitay/Documents/Zotero/storage/IQBKAHBU/Takimoto et al_2017_Self-organization based on auditory feedback promotes acquisition of babbling.pdf}
}

@inproceedings{Tamura2020,
  title = {Two-{{Step FORCE Learning Algorithm}} for {{Fast Convergence}} in {{Reservoir Computing}}},
  booktitle = {Artificial {{Neural Networks}} and {{Machine Learning}} – {{ICANN}} 2020},
  author = {Tamura, Hiroto and Tanaka, Gouhei},
  editor = {Farkaš, Igor and Masulli, Paolo and Wermter, Stefan},
  date = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {459--469},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-61616-8_37},
  abstract = {Reservoir computing devices are promising as energy-efficient machine learning hardware for real-time information processing. However, some online algorithms for reservoir computing are not simple enough for hardware implementation. In this study, we focus on the first order reduced and controlled error (FORCE) algorithm for online learning with reservoir computing models. We propose a two-step FORCE algorithm by simplifying the operations in the FORCE algorithm, which can reduce necessary memories. We analytically and numerically show that the proposed algorithm can converge faster than the original FORCE algorithm.},
  isbn = {978-3-030-61616-8},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/XMIYIAHD/Tamura_Tanaka_2020_Two-Step_FORCE_Learning_Algorithm_for_Fast_Convergence_in_Reservoir_Computing.pdf}
}

@article{Tamura2021a,
  title = {Transfer-{{RLS}} Method and Transfer-{{FORCE}} Learning for Simple and Fast Training of Reservoir Computing Models},
  author = {Tamura, Hiroto and Tanaka, Gouhei},
  date = {2021-11-01},
  journaltitle = {Neural Networks},
  shortjournal = {Neural Networks},
  volume = {143},
  pages = {550--563},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2021.06.031},
  url = {https://www.sciencedirect.com/science/article/pii/S089360802100263X},
  urldate = {2023-06-07},
  abstract = {Reservoir computing is a machine learning framework derived from a special type of recurrent neural network. Following recent advances in physical reservoir computing, some reservoir computing devices are thought to be promising as energy-efficient machine learning hardware for real-time information processing. To realize efficient online learning with low-power reservoir computing devices, it is beneficial to develop fast convergence learning methods with simpler operations. This study proposes a training method located in the middle between the recursive least squares (RLS) method and the least mean squares (LMS) method, which are standard online learning methods for reservoir computing models. The RLS method converges fast but requires updates of a huge matrix called a gain matrix, whereas the LMS method does not use a gain matrix but converges very slow. On the other hand, the proposed method called a transfer-RLS method does not require updates of the gain matrix in the main-training phase by updating that in advance (i.e., in a pre-training phase). As a result, the transfer-RLS method can work with simpler operations than the original RLS method without sacrificing much convergence speed. We numerically and analytically show that the transfer-RLS method converges much faster than the LMS method. Furthermore, we show that a modified version of the transfer-RLS method (called transfer-FORCE learning) can be applied to the first-order reduced and controlled error (FORCE) learning for a reservoir computing model with a closed-loop, which is challenging to train.},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/PKC3WRE6/Tamura_Tanaka_2021_Transfer-RLS_method_and_transfer-FORCE_learning_for_simple_and_fast_training_of.pdf}
}

@article{Tanaka2019a,
  ids = {Tanaka2018},
  title = {Recent Advances in Physical Reservoir Computing: {{A}} Review},
  shorttitle = {Recent Advances in Physical Reservoir Computing},
  author = {Tanaka, Gouhei and Yamane, Toshiyuki and Héroux, Jean Benoit and Nakane, Ryosho and Kanazawa, Naoki and Takeda, Seiji and Numata, Hidetoshi and Nakano, Daiju and Hirose, Akira},
  date = {2019-07-01},
  journaltitle = {Neural Networks},
  shortjournal = {Neural Networks},
  volume = {115},
  pages = {100--123},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2019.03.005},
  url = {http://www.sciencedirect.com/science/article/pii/S0893608019300784},
  urldate = {2019-05-24},
  abstract = {Reservoir computing is a computational framework suited for temporal/sequential data processing. It is derived from several recurrent neural network models, including echo state networks and liquid state machines. A reservoir computing system consists of a reservoir for mapping inputs into a high-dimensional space and a readout for pattern analysis from the high-dimensional states in the reservoir. The reservoir is fixed and only the readout is trained with a simple method such as linear regression and classification. Thus, the major advantage of reservoir computing compared to other recurrent neural networks is fast learning, resulting in low training cost. Another advantage is that the reservoir without adaptive updating is amenable to hardware implementation using a variety of physical systems, substrates, and devices. In fact, such physical reservoir computing has attracted increasing attention in diverse fields of research. The purpose of this review is to provide an overview of recent advances in physical reservoir computing by classifying them according to the type of the reservoir. We discuss the current issues and perspectives related to physical reservoir computing, in order to further expand its practical applications and develop next-generation machine learning systems.},
  keywords = {Machine learning,Neural networks,Neuromorphic device,Nonlinear dynamical systems,Reservoir computing},
  file = {/Users/vitay/Documents/Zotero/storage/NYDCDB62/Tanaka et al_2019_Recent advances in physical reservoir computing.pdf;/Users/vitay/Documents/Zotero/storage/WNDC8LK9/Tanaka et al_2018_Recent Advances in Physical Reservoir Computing.pdf;/Users/vitay/Documents/Zotero/storage/ZY7NYR7P/S0893608019300784.html}
}

@article{Tanaka2022,
  title = {Reservoir Computing with Diverse Timescales for Prediction of Multiscale Dynamics},
  author = {Tanaka, Gouhei and Matsumori, Tadayoshi and Yoshida, Hiroaki and Aihara, Kazuyuki},
  date = {2022-07-28},
  journaltitle = {Physical Review Research},
  shortjournal = {Phys. Rev. Res.},
  volume = {4},
  number = {3},
  pages = {L032014},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevResearch.4.L032014},
  url = {https://link.aps.org/doi/10.1103/PhysRevResearch.4.L032014},
  urldate = {2023-02-01},
  abstract = {Machine learning approaches have recently been leveraged as a substitute or an aid for physical/mathematical modeling approaches to dynamical systems. To develop an efficient machine learning method dedicated to modeling and prediction of multiscale dynamics, we propose a reservoir computing (RC) model with diverse timescales by using a recurrent network of heterogeneous leaky integrator (LI) neurons. We evaluate computational performance of the proposed model in two time series prediction tasks related to four chaotic fast-slow dynamical systems. In a one-step-ahead prediction task where input data are provided only from the fast subsystem, we show that the proposed model yields better performance than the standard RC model with identical LI neurons. Our analysis reveals that the timescale required for producing each component of target multiscale dynamics is appropriately and flexibly selected from the reservoir dynamics by model training. In a long-term prediction task, we demonstrate that a closed-loop version of the proposed model can achieve longer-term predictions compared to the counterpart with identical LI neurons depending on the hyperparameter setting.},
  file = {/Users/vitay/Documents/Zotero/storage/B8Q97NTV/Tanaka_et_al_2022_Reservoir_computing_with_diverse_timescales_for_prediction_of_multiscale.pdf}
}

@article{Tomizawa2021,
  title = {Combining Ensemble {{Kalman}} Filter and Reservoir Computing to Predict Spatiotemporal Chaotic Systems from Imperfect Observations and Models},
  author = {Tomizawa, Futo and Sawada, Yohei},
  date = {2021-09-13},
  journaltitle = {Geoscientific Model Development},
  shortjournal = {Geosci. Model Dev.},
  volume = {14},
  number = {9},
  pages = {5623--5635},
  issn = {1991-9603},
  doi = {10.5194/gmd-14-5623-2021},
  url = {https://gmd.copernicus.org/articles/14/5623/2021/},
  urldate = {2022-05-17},
  abstract = {Prediction of spatiotemporal chaotic systems is important in various fields, such as numerical weather prediction (NWP). While data assimilation methods have been applied in NWP, machine learning techniques, such as reservoir computing (RC), have recently been recognized as promising tools to predict spatiotemporal chaotic systems. However, the sensitivity of the skill of the machine-learning-based prediction to the imperfectness of observations is unclear. In this study, we evaluate the skill of RC with noisy and sparsely distributed observations. We intensively compare the performances of RC and local ensemble transform Kalman filter (LETKF) by applying them to the prediction of the Lorenz 96 system. In order to increase the scalability to larger systems, we applied a parallelized RC framework. Although RC can successfully predict the Lorenz 96 system if the system is perfectly observed, we find that RC is vulnerable to observation sparsity compared with LETKF. To overcome this limitation of RC, we propose to combine LETKF and RC. In our proposed method, the system is predicted by RC that learned the analysis time series estimated by LETKF. Our proposed method can successfully predict the Lorenz 96 system using noisy and sparsely distributed observations. Most importantly, our method can predict better than LETKF when the process-based model is imperfect.},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/KY3WCPM3/Tomizawa_Sawada_2021_Combining_ensemble_Kalman_filter_and_reservoir_computing_to_predict.pdf}
}

@inproceedings{Tong2018,
  title = {Reservoir {{Computing}} with {{Untrained Convolutional Neural Networks}} for {{Image Recognition}}},
  booktitle = {2018 24th {{International Conference}} on {{Pattern Recognition}} ({{ICPR}})},
  author = {Tong, Zhiqiang and Tanaka, Gouhei},
  date = {2018-08},
  pages = {1289--1294},
  issn = {1051-4651},
  doi = {10.1109/ICPR.2018.8545471},
  abstract = {Reservoir computing has attracted much attention for its easy training process as well as its ability to deal with temporal data. A reservoir computing system consists of a reservoir part represented as a sparsely connected recurrent neural network and a readout part represented as a simple regression model. In machine learning tasks, the reservoir part is fixed and only the readout part is trained. Although reservoir computing has been mainly applied to time series prediction and recognition, it can be applied to image recognition as well by considering an image data as a sequence of pixel values. However, to achieve a high performance in image recognition with raw image data, a large-scale reservoir including a large number of neurons is required. This is a bottleneck in terms of computer memory and computational cost. To overcome this bottleneck, we propose a new method which combines reservoir computing with untrained convolutional neural networks. We use an untrained convolutional neural network to transform raw image data into a set of smaller feature maps in a preprocessing step of the reservoir computing. We demonstrate that our method achieves a high classification accuracy in an image recognition task with a much smaller number of trainable parameters compared with a previous study.},
  eventtitle = {2018 24th {{International Conference}} on {{Pattern Recognition}} ({{ICPR}})},
  keywords = {Computational modeling,Convolution,Convolutional neural networks,Feature extraction,Image recognition,Reservoirs,Training},
  file = {/Users/vitay/Documents/Zotero/storage/RZQHK6KZ/Tong_Tanaka_2018_Reservoir_Computing_with_Untrained_Convolutional_Neural_Networks_for_Image.pdf;/Users/vitay/Documents/Zotero/storage/F5FR2SPL/8545471.html}
}

@article{Vasilaki2014,
  title = {Emergence of {{Connectivity Motifs}} in {{Networks}} of {{Model Neurons}} with {{Short-}} and {{Long-Term Plastic Synapses}}},
  author = {Vasilaki, Eleni and Giugliano, Michele},
  date = {2014-01-15},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {9},
  number = {1},
  pages = {e84626},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0084626},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0084626},
  urldate = {2019-04-23},
  abstract = {Recent experimental data from the rodent cerebral cortex and olfactory bulb indicate that specific connectivity motifs are correlated with short-term dynamics of excitatory synaptic transmission. It was observed that neurons with short-term facilitating synapses form predominantly reciprocal pairwise connections, while neurons with short-term depressing synapses form predominantly unidirectional pairwise connections. The cause of these structural differences in excitatory synaptic microcircuits is unknown. We show that these connectivity motifs emerge in networks of model neurons, from the interactions between short-term synaptic dynamics (SD) and long-term spike-timing dependent plasticity (STDP). While the impact of STDP on SD was shown in simultaneous neuronal pair recordings in vitro, the mutual interactions between STDP and SD in large networks are still the subject of intense research. Our approach combines an SD phenomenological model with an STDP model that faithfully captures long-term plasticity dependence on both spike times and frequency. As a proof of concept, we first simulate and analyze recurrent networks of spiking neurons with random initial connection efficacies and where synapses are either all short-term facilitating or all depressing. For identical external inputs to the network, and as a direct consequence of internally generated activity, we find that networks with depressing synapses evolve unidirectional connectivity motifs, while networks with facilitating synapses evolve reciprocal connectivity motifs. We then show that the same results hold for heterogeneous networks, including both facilitating and depressing synapses. This does not contradict a recent theory that proposes that motifs are shaped by external inputs, but rather complements it by examining the role of both the external inputs and the internally generated network activity. Our study highlights the conditions under which SD-STDP might explain the correlation between facilitation and reciprocal connectivity motifs, as well as between depression and unidirectional motifs.},
  langid = {english},
  keywords = {Action potentials,Depression,Network motifs,Neural networks,Neuronal plasticity,Neurons,Synapses,Synaptic plasticity},
  file = {/Users/vitay/Documents/Zotero/storage/EMZWYGGE/Vasilaki_Giugliano_2014_Emergence of Connectivity Motifs in Networks of Model Neurons with Short- and.pdf;/Users/vitay/Documents/Zotero/storage/YKGZZ2RC/article.html}
}

@article{Venkatesh2019,
  title = {Brain Dynamics and Temporal Trajectories during Task and Naturalistic Processing},
  author = {Venkatesh, Manasij and Jaja, Joseph and Pessoa, Luiz},
  date = {2019-02-01},
  journaltitle = {NeuroImage},
  shortjournal = {NeuroImage},
  volume = {186},
  pages = {410--423},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2018.11.016},
  url = {http://www.sciencedirect.com/science/article/pii/S105381191832086X},
  urldate = {2018-12-12},
  abstract = {Human functional Magnetic Resonance Imaging (fMRI) data are acquired while participants engage in diverse perceptual, motor, cognitive, and emotional tasks. Although data are acquired temporally, they are most often treated in a quasi-static manner. Yet, a fuller understanding of the mechanisms that support mental functions necessitates the characterization of dynamic properties. Here, we describe an approach employing a class of recurrent neural networks called reservoir computing, and show the feasibility and potential of using it for the analysis of temporal properties of brain data. We show that reservoirs can be used effectively both for condition classification and for characterizing lower-dimensional “trajectories” of temporal data. Classification accuracy was approximately 90\% for short clips of “social interactions” and around 70\% for clips extracted from movie segments. Data representations with 12 or fewer dimensions (from an original space with over 300) attained classification accuracy within 5\% of the full data. We hypothesize that such low-dimensional trajectories may provide “signatures” that can be associated with tasks and/or mental states. The approach was applied across participants (that is, training in one set of participants, and testing in a separate group), showing that representations generalized well to unseen participants. Taken together, we believe the present approach provides a promising framework to characterize dynamic fMRI information during both tasks and naturalistic conditions.},
  file = {/Users/vitay/Documents/Zotero/storage/AW6447WA/Venkatesh et al_2019_Brain dynamics and temporal trajectories during task and naturalistic processing.pdf}
}

@article{Viehweg2023,
  title = {Parameterizing Echo State Networks for Multi-Step Time Series Prediction},
  author = {Viehweg, Johannes and Worthmann, Karl and Mäder, Patrick},
  date = {2023-02-14},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {522},
  pages = {214--228},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2022.11.044},
  url = {https://www.sciencedirect.com/science/article/pii/S0925231222014291},
  urldate = {2024-06-20},
  abstract = {Prediction of multi-dimensional time-series data, which may represent such diverse phenomena as climate changes or financial markets, remains a challenging task in view of inherent nonlinearities and non-periodic behavior. In contrast to other recurrent neural networks, echo state networks (ESNs) are attractive for (online) learning due to lower requirements w.r.t.training data and computational power. However, the randomly-generated reservoir renders the choice of suitable hyper-parameters as an open research topic. We systematically derive and exemplarily demonstrate design guidelines for the hyper-parameter optimization of ESNs. For the evaluation, we focus on the prediction of chaotic time series, an especially challenging problem in machine learning. Our findings demonstrate the power of a hyper-parameter-tuned ESN when auto-regressively predicting time series over several hundred steps. We found that ESNs’ performance improved by 85.1\%-99.8\% over an already wisely chosen default parameter initialization. In addition, the fluctuation range is considerably reduced such that significantly worse performance becomes very unlikely across random reservoir seeds. Moreover, we report individual findings per hyper-parameter partly contradicting common knowledge to further, help researchers when training new models.}
}

@article{Vitay2016,
  title = {[{{Re}}] {{Robust Timing And Motor Patterns By Taming Chaos In Recurrent Neural Networks}}},
  author = {Vitay, Julien},
  date = {2016-10-07},
  journaltitle = {ReScience},
  volume = {2},
  number = {1},
  doi = {10.5281/zenodo.159545},
  url = {https://zenodo.org/record/159545},
  urldate = {2019-06-14},
  abstract = {A reference implementation of {$<$}strong{$>$}Laje, R. and Buonomano, D.V. (2013). Robust timing and motor patterns by taming chaos in recurrent neural networks. Nat Neurosci. 16(7) pp 925-33.{$<$}/strong{$>$}},
  keywords = {Chaos,Dynamical systems,Learning,Python,Recurrent neural networks,Reservoir computing}
}

@article{Vlachas2020,
  title = {Backpropagation Algorithms and {{Reservoir Computing}} in {{Recurrent Neural Networks}} for the Forecasting of Complex Spatiotemporal Dynamics},
  author = {Vlachas, P. R. and Pathak, J. and Hunt, B. R. and Sapsis, T. P. and Girvan, M. and Ott, E. and Koumoutsakos, P.},
  date = {2020-06-01},
  journaltitle = {Neural Networks},
  shortjournal = {Neural Networks},
  volume = {126},
  pages = {191--217},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2020.02.016},
  url = {http://www.sciencedirect.com/science/article/pii/S0893608020300708},
  urldate = {2020-05-29},
  abstract = {We examine the efficiency of Recurrent Neural Networks in forecasting the spatiotemporal dynamics of high dimensional and reduced order complex systems using Reservoir Computing (RC) and Backpropagation through time (BPTT) for gated network architectures. We highlight advantages and limitations of each method and discuss their implementation for parallel computing architectures. We quantify the relative prediction accuracy of these algorithms for the long-term forecasting of chaotic systems using as benchmarks the Lorenz-96 and the Kuramoto–Sivashinsky (KS) equations. We find that, when the full state dynamics are available for training, RC outperforms BPTT approaches in terms of predictive performance and in capturing of the long-term statistics, while at the same time requiring much less training time. However, in the case of reduced order data, large scale RC models can be unstable and more likely than the BPTT algorithms to diverge. In contrast, RNNs trained via BPTT show superior forecasting abilities and capture well the dynamics of reduced order systems. Furthermore, the present study quantifies for the first time the Lyapunov Spectrum of the KS equation with BPTT, achieving similar accuracy as RC. This study establishes that RNNs are a potent computational framework for the learning and forecasting of complex spatiotemporal systems.},
  langid = {english},
  keywords = {Complex systems,Kuramoto–Sivashinsky,Lorenz-96,Reservoir Computing,RNN LSTM GRU,Time series forecasting},
  file = {/Users/vitay/Documents/Zotero/storage/7L5RKDSG/Vlachas_et_al_2020_Backpropagation_algorithms_and_Reservoir_Computing_in_Recurrent_Neural_Networks.pdf;/Users/vitay/Documents/Zotero/storage/JUMHQ7DP/S0893608020300708.html}
}

@article{Werbos1990,
  title = {Backpropagation through Time: What It Does and How to Do It},
  author = {Werbos, P.J.},
  date = {1990},
  journaltitle = {Proceedings of the IEEE},
  volume = {78},
  number = {10},
  pages = {1550--1560},
  doi = {10.1109/5.58337},
  url = {http://ieeexplore.ieee.org/document/58337/},
  file = {/Users/vitay/Documents/Zotero/storage/QPBUKWXP/Werbos_1990_Backpropagation through time.pdf}
}

@online{Wringe2024,
  title = {Reservoir {{Computing Benchmarks}}: A Review, a Taxonomy, Some Best Practices},
  shorttitle = {Reservoir {{Computing Benchmarks}}},
  author = {Wringe, Chester and Trefzer, Martin and Stepney, Susan},
  date = {2024-05-10},
  eprint = {2405.06561},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2405.06561},
  url = {http://arxiv.org/abs/2405.06561},
  urldate = {2024-06-20},
  abstract = {Reservoir Computing is an Unconventional Computation model to perform computation on various different substrates, such as RNNs or physical materials. The method takes a "black-box" approach, training only the outputs of the system it is built on. As such, evaluating the computational capacity of these systems can be challenging. We review and critique the evaluation methods used in the field of Reservoir Computing. We introduce a categorisation of benchmark tasks. We review multiple examples of benchmarks from the literature as applied to reservoir computing, and note their strengths and shortcomings. We suggest ways in which benchmarks and their uses may be improved to the benefit of the reservoir computing community},
  pubstate = {prepublished},
  file = {/Users/vitay/Documents/Zotero/storage/9YWV9PJ5/Wringe et al. - 2024 - Reservoir Computing Benchmarks a review, a taxonomy, some best practices.pdf}
}

@article{Yamazaki2007,
  title = {The Cerebellum as a Liquid State Machine},
  author = {Yamazaki, Tadashi and Tanaka, Shigeru},
  date = {2007-04},
  journaltitle = {Neural Networks},
  volume = {20},
  number = {3},
  pages = {290--297},
  doi = {10.1016/J.NEUNET.2007.04.004},
  url = {https://www.sciencedirect.com/science/article/pii/S0893608007000366?via%3Dihub},
  abstract = {We examined closely the cerebellar circuit model that we have proposed previously. The model granular layer generates a finite but very long sequence of active neuron populations without recurrence, which is able to represent the passage of time. For all the possible binary patterns fed into mossy fibres, the circuit generates the same number of different sequences of active neuron populations. Model Purkinje cells that receive parallel fiber inputs from neurons in the granular layer learn to stop eliciting spikes at the timing instructed by the arrival of signals from the inferior olive. These functional roles of the granular layer and Purkinje cells are regarded as a liquid state generator and readout neurons, respectively. Thus, the cerebellum that has been considered to date as a biological counterpart of a perceptron is reinterpreted to be a liquid state machine that possesses powerful information processing capability more than a perceptron.},
  file = {/Users/vitay/Documents/Zotero/storage/644ZW33I/Yamazaki_Tanaka_2007_The cerebellum as a liquid state machine.pdf}
}

@article{Yusoff2016,
  title = {Modeling Neural Plasticity in Echo State Networks for Classification and Regression},
  author = {Yusoff, Mohd-Hanif and Chrol-Cannon, Joseph and Jin, Yaochu},
  date = {2016-10-10},
  journaltitle = {Information Sciences},
  shortjournal = {Information Sciences},
  volume = {364--365},
  pages = {184--196},
  issn = {0020-0255},
  doi = {10.1016/j.ins.2015.11.017},
  url = {https://www.sciencedirect.com/science/article/pii/S0020025515008270},
  urldate = {2023-02-01},
  abstract = {Echo state networks (ESNs) are one of two major neural network models belonging to the reservoir computing framework. Traditionally, only the weights connecting to the output neuron, termed read-out weights, are trained using a supervised learning algorithm, while the weights inside the reservoir of the ESN are randomly determined and remain unchanged during the training. In this paper, we investigate the influence of neural plasticity applied to the weights inside the reservoir on the learning performance of the ESN. We examine the influence of two plasticity rules, anti-Oja's learning rule and the Bienenstock–Cooper–Munro (BCM) learning rule on the prediction and classification performance when either offline or online supervised learning algorithms are employed for training the read-out connections. Empirical studies are conducted on two widely used classification tasks and two time series prediction problems. Our experimental results demonstrate that neural plasticity can more effectively enhance the learning performance when offline learning is applied. The results also indicate that the BCM rule outperforms the anti-Oja rule in improving the learning performance of the ENS in the offline learning mode.},
  langid = {english}
}

@article{Zhang2018,
  title = {A Neural Network Model for the Orbitofrontal Cortex and Task Space Acquisition during Reinforcement Learning},
  author = {Zhang, Zhewei and Cheng, Zhenbo and Lin, Zhongqiao and Nie, Chechang and Yang, Tianming},
  editor = {Gershman, Samuel J.},
  date = {2018-01},
  journaltitle = {PLOS Computational Biology},
  volume = {14},
  number = {1},
  pages = {e1005925-e1005925},
  doi = {10.1371/journal.pcbi.1005925},
  url = {http://dx.plos.org/10.1371/journal.pcbi.1005925},
  file = {/Users/vitay/Documents/Zotero/storage/2QEZBC7L/Zhang et al_2018_A neural network model for the orbitofrontal cortex and task space acquisition.pdf}
}

@online{Zhang2023,
  title = {A {{Survey}} on {{Reservoir Computing}} and Its {{Interdisciplinary Applications Beyond Traditional Machine Learning}}},
  author = {Zhang, Heng and Vargas, Danilo Vasconcellos},
  date = {2023-07-27},
  eprint = {2307.15092},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.1109/ACCESS.2023.3299296},
  url = {http://arxiv.org/abs/2307.15092},
  urldate = {2024-05-28},
  abstract = {Reservoir computing (RC), first applied to temporal signal processing, is a recurrent neural network in which neurons are randomly connected. Once initialized, the connection strengths remain unchanged. Such a simple structure turns RC into a non-linear dynamical system that maps low-dimensional inputs into a high-dimensional space. The model's rich dynamics, linear separability, and memory capacity then enable a simple linear readout to generate adequate responses for various applications. RC spans areas far beyond machine learning, since it has been shown that the complex dynamics can be realized in various physical hardware implementations and biological devices. This yields greater flexibility and shorter computation time. Moreover, the neuronal responses triggered by the model's dynamics shed light on understanding brain mechanisms that also exploit similar dynamical processes. While the literature on RC is vast and fragmented, here we conduct a unified review of RC's recent developments from machine learning to physics, biology, and neuroscience. We first review the early RC models, and then survey the state-of-the-art models and their applications. We further introduce studies on modeling the brain's mechanisms by RC. Finally, we offer new perspectives on RC development, including reservoir design, coding frameworks unification, physical RC implementations, and interaction between RC, cognitive neuroscience and evolution.},
  pubstate = {prepublished},
  file = {/Users/vitay/Documents/Zotero/storage/IGRDR3VQ/Zhang and Vargas - 2023 - A Survey on Reservoir Computing and its Interdisciplinary Applications Beyond Traditional Machine Le.pdf}
}

@report{Zuge2021,
  type = {preprint},
  title = {Weight {{Perturbation Learning Performs Similarly}} or {{Better}} than {{Node Perturbation}} on {{Broad Classes}} of {{Temporally Extended Tasks}}},
  author = {Züge, Paul and Klos, Christian and Memmesheimer, Raoul-Martin},
  date = {2021-10-05},
  institution = {Neuroscience},
  doi = {10.1101/2021.10.04.463055},
  url = {http://biorxiv.org/lookup/doi/10.1101/2021.10.04.463055},
  urldate = {2022-10-14},
  abstract = {Biological constraints often impose restrictions for plausible plasticity rules such as locality and reward-based rather than supervised learning. Two learning rules that comply with these restrictions are weight (WP) and node (NP) perturbation. NP is often used in learning studies, in particular as a benchmark; it is considered to be superior to WP and more likely neurobiologically realized, as the number of weights and therefore their perturbation dimension typically massively exceeds the number of nodes. Here we show that this conclusion no longer holds when we take two biologically relevant properties into account: First, tasks extend in time. This increases the perturbation dimension of NP but not WP. Second, tasks are low dimensional, with many weight configurations providing solutions. We analytically delineate regimes where these properties let WP perform as well as or better than NP. Furthermore we find that the changes in weight space directions that are irrelevant for the task differ qualitatively between WP and NP and that only in WP gathering batches of subtasks in a trial decreases the number of trials required. This may allow to experimentally distinguish which of the two rules underlies a learning process. Our insights suggest new learning rules, which combine for specific task types the advantages of WP and NP. If the inputs are similarly correlated, temporally correlated perturbations improve NP. Using numerical simulations, we generalize the results to networks with various architectures solving biologically relevant and standard network learning tasks. Our findings, together with WP’s practicability suggest WP as a useful benchmark and plausible model for learning in the brain.},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/54DSV582/Züge_et_al_2021_Weight_Perturbation_Learning_Performs_Similarly_or_Better_than_Node.pdf}
}

@article{Zuge2023,
  title = {Weight versus {{Node Perturbation Learning}} in {{Temporally Extended Tasks}}: {{Weight Perturbation Often Performs Similarly}} or {{Better}}},
  shorttitle = {Weight versus {{Node Perturbation Learning}} in {{Temporally Extended Tasks}}},
  author = {Züge, Paul and Klos, Christian and Memmesheimer, Raoul-Martin},
  date = {2023-04-11},
  journaltitle = {Physical Review X},
  shortjournal = {Phys. Rev. X},
  volume = {13},
  number = {2},
  pages = {021006},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevX.13.021006},
  url = {https://link.aps.org/doi/10.1103/PhysRevX.13.021006},
  urldate = {2023-09-26},
  abstract = {Biological constraints often impose restrictions on plasticity rules such as locality and reward-based rather than supervised learning. Two learning rules that comply with these restrictions are weight (WP) and node (NP) perturbation. NP is often used in learning studies, in particular, as a benchmark; it is considered to be superior to WP and more likely neurobiologically realized, as the number of weights and, therefore, their perturbation dimension typically massively exceed the number of nodes. Here, we show that this conclusion no longer holds when we take two properties into account that are relevant for biological and artificial neural network learning: First, tasks extend in time and/or are trained in batches. This increases the perturbation dimension of NP but not WP. Second, tasks are (comparably) low dimensional, with many weight configurations providing solutions. We analytically delineate regimes where these properties let WP perform as well as or better than NP. Furthermore, we find that the changes in weight space directions that are irrelevant for the task differ qualitatively between WP and NP and that only in WP gathering batches of subtasks in a trial decreases the number of trials required. This may allow one to experimentally distinguish which of the two rules underlies a learning process. Our insights suggest new learning rules which combine for specific task types the advantages of WP and NP. If the inputs are similarly correlated, temporally correlated perturbations improve NP. Using numerical simulations, we generalize the results to networks with various architectures solving biologically relevant and standard network learning tasks. Our findings, together with WP’s practicability, suggest WP as a useful benchmark and plausible model for learning in the brain.},
  file = {/Users/vitay/Documents/Zotero/storage/T24GY889/Züge_et_al_2023_Weight_versus_Node_Perturbation_Learning_in_Temporally_Extended_Tasks.pdf}
}

@online{Zuo2023,
  title = {Self-{{Evolutionary Reservoir Computer Based}} on {{Kuramoto Model}}},
  author = {Zuo, Zhihao and Gan, Zhongxue and Fan, Yuchuan and Bobrovs, Vjaceslavs and Pang, Xiaodan and Ozolins, Oskars},
  date = {2023-01-25},
  eprint = {2301.10654},
  eprinttype = {arXiv},
  eprintclass = {nlin},
  doi = {10.48550/arXiv.2301.10654},
  url = {http://arxiv.org/abs/2301.10654},
  urldate = {2024-06-20},
  abstract = {The human brain's synapses have remarkable activity-dependent plasticity, where the connectivity patterns of neurons change dramatically, relying on neuronal activities. As a biologically inspired neural network, reservoir computing (RC) has unique advantages in processing spatiotemporal information. However, typical reservoir architectures only take static random networks into account or consider the dynamics of neurons and connectivity separately. In this paper, we propose a structural autonomous development reservoir computing model (sad-RC), which structure can adapt to the specific problem at hand without any human expert knowledge. Specifically, we implement the reservoir by adaptive networks of phase oscillators, a commonly used model for synaptic plasticity in biological neural networks. In this co-evolving dynamic system, the dynamics of nodes and coupling weights in the reservoir constantly interact and evolve together when disturbed by external inputs.},
  pubstate = {prepublished},
  file = {/Users/vitay/Documents/Zotero/storage/VJMJSSZI/Zuo et al. - 2023 - Self-Evolutionary Reservoir Computer Based on Kuramoto Model.pdf}
}
