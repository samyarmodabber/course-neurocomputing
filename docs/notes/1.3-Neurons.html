<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.175">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Neurocomputing - 3&nbsp; Neurons</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../notes/2.1-Optimization.html" rel="next">
<link href="../notes/1.2-Math.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Neurons</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Neurocomputing</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">Course description</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">Introduction</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/1.1-Introduction.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/1.2-Math.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Math basics (optional)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/1.3-Neurons.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Neurons</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">Linear algorithms</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.1-Optimization.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Optimization</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.2-LinearRegression.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Linear regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.3-LinearClassification.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Linear classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.4-Multiclassification.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Multi-class classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.5-LearningTheory.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Learning theory</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">Neural networks</span>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">Convolutional neural networks</span>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">Unsupervised learning and generative modeling</span>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">Recurrent neural networks</span>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">Self-supervised learning</span>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">Outlook</span>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">Exercises</span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#biological-neurons" id="toc-biological-neurons" class="nav-link active" data-scroll-target="#biological-neurons"><span class="toc-section-number">3.1</span>  Biological neurons</a></li>
  <li><a href="#spiking-neurons" id="toc-spiking-neurons" class="nav-link" data-scroll-target="#spiking-neurons"><span class="toc-section-number">3.2</span>  Spiking neurons</a></li>
  <li><a href="#artificial-neurons" id="toc-artificial-neurons" class="nav-link" data-scroll-target="#artificial-neurons"><span class="toc-section-number">3.3</span>  Artificial neurons</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Neurons</span></h1>
</div>



<div class="quarto-title-meta">

    
    
  </div>
  

</header>

<p>Slides: <a href="https://www.tu-chemnitz.de/informatik/KI/edu/neurocomputing/lectures/pdf/1.3-Neurons.pdf">pdf</a></p>
<section id="biological-neurons" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="biological-neurons"><span class="header-section-number">3.1</span> Biological neurons</h2>
<div class="embed-container">
<iframe src="https://www.youtube.com/embed/ALBRu-AK53I" frameborder="0" allowfullscreen="">
</iframe>
</div>
<p>The human brain is composed of 100 billion <strong>neurons</strong>. A biological neuron is a cell, composed of a cell body (<strong>soma</strong>), multiple <strong>dendrites</strong> and an <strong>axon</strong>. The axon of a neuron can contact the dendrites of another through <strong>synapses</strong> to transmit information. There are hundreds of different types of neurons, each with different properties.</p>
<table class="table">
<thead>
<tr class="header">
<th>```{figure} ../img/biologicalneuron.png</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>width: 100%</td>
</tr>
</tbody>
</table>
<p>Biological neuron. Source: <a href="https://en.wikipedia.org/wiki/Neuron" class="uri">https://en.wikipedia.org/wiki/Neuron</a></p>
<pre><code>
Neurons are negatively charged: they have a resting potential at around -70 mV. When a neuron receives enough input currents, its **membrane potential** can exceed a threshold and the neuron emits an **action potential** (or **spike**) along its axon. 


```{figure} ../img/actionpotential.gif
---
width: 100%
---
Propagation of an action potential along the axon. Source: &lt;https://en.wikipedia.org/wiki/Action_potential&gt;</code></pre>
<p>A spike has a very small duration (1 or 2 ms) and its amplitude is rather constant. It is followed by a <strong>refractory period</strong> where the neuron is hyperpolarized, limiting the number of spikes per second to 200.</p>
<table class="table">
<thead>
<tr class="header">
<th>```{figure} ../img/actionpotential.png</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>width: 50%</td>
</tr>
</tbody>
</table>
<p>Action potential or spike. Source: <a href="https://en.wikipedia.org/wiki/Action_potential" class="uri">https://en.wikipedia.org/wiki/Action_potential</a></p>
<pre><code>
The action potential arrives at the synapses and releases **neurotransmitters** in the synaptic cleft: glutamate (AMPA, NMDA), GABA, dopamine, serotonin, nicotin, etc...
Neurotransmitters can enter the receiving neuron through **receptors** and change its potential: the neuron may emit a spike too. Synaptic currents change the membrane potential of the post.synaptic neuron. The change depends on the strength of the synapse called the **synaptic efficiency** or **weight**. Some synapses are stronger than others, and have a larger influence on the post-synaptic cell.

```{figure} ../img/chemicalsynapse.jpg
---
width: 60%
---
Neurotransmitter release at the synapse. Source: &lt;https://en.wikipedia.org/wiki/Neuron&gt;</code></pre>
<style>.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }</style>
<div class="embed-container">
<iframe src="https://www.youtube.com/embed/WCqNn9PEELw" frameborder="0" allowfullscreen="">
</iframe>
</div>
<p>The two important dimensions of the information exchanged by neurons are:</p>
<ul>
<li><p>The instantaneous <strong>frequency</strong> or <strong>firing rate</strong>: number of spikes per second (Hz).</p></li>
<li><p>The precise <strong>timing</strong> of the spike trains.</p></li>
</ul>
<table class="table">
<thead>
<tr class="header">
<th>```{figure} ../img/oscillations.png</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>width: 80%</td>
</tr>
</tbody>
</table>
<p>Neurons emit spikes at varying frequencies (firing rate) and variable timings. Source: <a href="https://en.wikipedia.org/wiki/Neural_oscillation" class="uri">https://en.wikipedia.org/wiki/Neural_oscillation</a></p>
<pre><code>
The shape of the spike (amplitude, duration) does not matter much for synaptic transission: spikes can be considered as binary signals (0 or 1) occuring at precise moments of time.

Some neuron models called **rate-coded models** only represent the firing rate of a neuron and ignore spike timing at all. Other models called **spiking models** represent explicitly the spiking behavior.


## Hodgkin-Huxley neurons

&lt;div class='embed-container'&gt;&lt;iframe src='https://www.youtube.com/embed/WAfOUZW4rq8' frameborder='0' allowfullscreen&gt;&lt;/iframe&gt;&lt;/div&gt;


Alan Hodgkin and Andrew Huxley (Nobel prize 1963) were the first to propose a detailed mathematical model of the giant squid neuron. The membrane potential $V$ of the neuron is governed by an electrical circuit, including sodium and potassium channels. The membrane has a **capacitance** $C$ that models the dynamics of the membrane (time constant). The **conductance** $g_L$ allows the membrane potential to relax back to its resting potential $E_L$ in the absence of external currents.  External currents (synaptic inputs) perturb the membrane potential and can bring the neuron to fire an action potential. 

Their neuron model include:

* An ordinary differential equation (ODE) for the membrane potential $v$.

* Three ODEs for $n$, $m$ and $h$ representing potassium channel activation, sodium channel activation, and sodium channel inactivation.

* Several parameters determined experimentally.

$$
\begin{aligned}
    a_n &amp;= 0.01 \, (v + 60) / (1.0 - \exp(-0.1\, (v + 60) ) ) \\
    a_m &amp;= 0.1 \, (v + 45) / (1.0 - \exp (- 0.1 \, ( v + 45 ))) \\
    a_h &amp;= 0.07 \, \exp(- 0.05 \, ( v + 70 )) \\
    b_n &amp;= 0.125 \, \exp (- 0.0125 \, (v + 70)) \\
    b_m &amp;= 4 \,  \exp (- (v + 70) / 80) \\
    b_h &amp;= 1/(1 + \exp (- 0.1 \, ( v + 40 )) ) \\
    &amp; \\
    \frac{dn}{dt} &amp;= a_n \, (1 - n) - b_n \, n  \\
    \frac{dm}{dt} &amp;= a_m \, (1 - m) - b_m \, m  \\
    \frac{dh}{dt} &amp;= a_h \, (1 - h) - b_h \, h  \\
\end{aligned}
$$

$$
\begin{aligned}
    C \, \frac{dv}{dt} = g_L \, (V_L - v) &amp;+ g_K \, n^4 \, (V_K - v) \\
        &amp; + g_\text{Na} \, m^3 \, h \, (V_\text{Na} - v) + I \\
\end{aligned}
$$


These equations allow to describe very precisely how an action potential is created from external currents.


```{figure} ../img/hodgkinhuxley-data.png
---
width: 100%
---
Action potential for a Hodgkin-Huxley neuron.</code></pre>
</section>
<section id="spiking-neurons" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="spiking-neurons"><span class="header-section-number">3.2</span> Spiking neurons</h2>
<p>As action potentials are stereotypical, it is a waste of computational resources to model their generation precisely. What actually matters are the <strong>sub-threshold dynamics</strong>, i.e.&nbsp;what happens before the spike is emitted.</p>
<p>The <strong>leaky integrate-and-fire</strong> (LIF; Lapicque, 1907) neuron integrates its input current and emits a spike if the membrane potential exceeds a threshold.</p>
<p><span class="math display">\[
    C \, \frac{dv}{dt} = - g_L \, (v - V_L) + I
\]</span></p>
<p><span class="math display">\[
    \text{if} \; v &gt; V_T \; \text{emit a spike and reset.}
\]</span></p>
<table class="table">
<thead>
<tr class="header">
<th>```{figure} ../img/LIF-data.png</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>width: 70%</td>
</tr>
</tbody>
</table>
<p>Spike emission for a LIF neuron.</p>
<pre><code>
Other well-known spiking neuron models include:

* Izhikevich quadratic IF {cite}`Izhikevich2003`, using a quadratic function of the membrane potential and an adaptation variable $u$.

$$
    \frac{dv}{dt} = 0.04 \, v^2 + 5 \, v + 140 - u + I 
$$
$$
    \frac{du}{dt} = a \, (b \, v - u)
$$

* Adaptive exponential IF (AdEx, {cite}`Brette2005`), using an exponential function.

$$
\begin{aligned}
    C \, \frac{dv}{dt} = -g_L \ (v - E_L) + &amp; g_L \, \Delta_T \, \exp(\frac{v - v_T}{\Delta_T}) \\
                                            &amp; + I - w
\end{aligned}
$$
$$
    \tau_w \, \frac{dw}{dt} = a \, (v - E_L) - w
$$

```{figure} ../img/LIF-Izhi-AdEx.png
---
width: 100%
---
Different subthreshold dynamics between the LIF, Izhikevich and AdEx neuron models.</code></pre>
<p>Contrary to the simple LIF model, these realistic neuron models can reproduce a variety of dynamics, as biological neurons do not all respond the same to an input current. Some fire regularly, some slow down with time, while others emit bursts of spikes. Modern spiking neuron models allow to recreate these variety of dynamics by changing a few parameters.</p>
<table class="table">
<thead>
<tr class="header">
<th>```{figure} ../img/adex.png</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>width: 100%</td>
</tr>
</tbody>
</table>
<p>Different parameters of the AdEx neuron model produce different spiking patterns.</p>
<pre><code>

## Rate-coded neurons

&lt;div class='embed-container'&gt;&lt;iframe src='https://www.youtube.com/embed/AFzYj1VUnCg' frameborder='0' allowfullscreen&gt;&lt;/iframe&gt;&lt;/div&gt;


At the population level, interconnected networks of spiking neurons tend to fire synchronously (code redundancy). What if the important information was not the precise spike timings, but the **firing rate** of a small population? The instantaneous firing rate is defined in Hz (number of spikes per second). It can be estimated by an histogram of the spikes emitted by a network of similar neurons, or by repeating the same experiment multiple times for a single neuron. One can also build neural models that directly model the **firing rate** of (a population of) neuron(s): the **rate-coded** neuron.

```{figure} ../img/ratecoded-izhikevich.png
---
width: 60%
---
The spiking pattern (raster plot) of a population of interconnected neurons can be approximated by its mean firing rate.</code></pre>
<p>A rate-coded neuron is represented by two time-dependent variables:</p>
<ul>
<li>The <strong>“membrane potential”</strong> <span class="math inline">\(v(t)\)</span> which evolves over time using an ODE.</li>
</ul>
<p><span class="math display">\[
    \tau \, \frac{d v(t)}{dt} + v(t) = \sum_{i=1}^d w_{i, j} \, r_i(t) + b
\]</span></p>
<ul>
<li>The <strong>firing rate</strong> <span class="math inline">\(r(t)\)</span> which transforms the membrane potential into a single continuous value using a <strong>transfer function</strong> or <strong>activation function</strong>.</li>
</ul>
<p><span class="math display">\[
    r(t) = f(v(t))
\]</span></p>
<table class="table">
<thead>
<tr class="header">
<th>```{figure} ../img/ratecoded-neuron.svg</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>width: 70%</td>
</tr>
</tbody>
</table>
<p>Rate-coded neuron.</p>
<pre><code>
The membrane potential uses a weighted sum of inputs (the firing rates $r_i(t)$ of other neurons) by multiplying each rate with a **weight** $w_i$ and adds a constant value $b$ (the **bias**). The activation function can be any non-linear function, usually making sure that the firing rate is positive.

```{figure} ../img/ratecoded-simple.png
---
width: 70%
---
Firing rate of a rate-coded neuron for a step input.</code></pre>
<p><strong>Remarks on ODEs</strong></p>
<p>Let’s consider a simple rate-coded neuron taking a step signal <span class="math inline">\(I(t)\)</span> as input:</p>
<p><span class="math display">\[
    \tau \, \frac{d v(t)}{dt} + v(t) = I(t)
\]</span></p>
<p><span class="math display">\[
    r(t) = (v(t))^+
\]</span></p>
<p>The “speed” of <span class="math inline">\(v(t)\)</span> is given by its temporal derivative:</p>
<p><span class="math display">\[
    \frac{d v(t)}{dt} = \frac{I(t) - v(t)}{\tau}
\]</span></p>
<p>When <span class="math inline">\(v(t)\)</span> is quite different from <span class="math inline">\(I(t)\)</span>, the membrane potential “accelerates” to reduce the difference. When <span class="math inline">\(v(t)\)</span> is similar to <span class="math inline">\(I(t)\)</span>, the membrane potential stays constant.</p>
<table class="table">
<thead>
<tr class="header">
<th>```{figure} ../img/ratecoded-simple-multiple.png</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>width: 70%</td>
</tr>
</tbody>
</table>
<p>The time constant <span class="math inline">\(\tau\)</span> of a rate-coded neuron influences the speed at which it reacts to inputs.</p>
<pre><code>
The membrane potential follows an exponential function which tries to "match" its input with a speed determined by the **time constant** $\tau$. The time constant $\tau$ determines how fast the rate-coded neuron matches its inputs. Biological neurons have time constants between 5 and 30 ms depending on the cell type.

There exists a significant number of transfer functions that can be used:

```{figure} ../img/ratecoded-transferfunctions.png
---
width: 70%
---
Typical transfer functions used in neural networks include the rectifier (ReLU), piece-wise linear, sigmoid (or logistic) and tanh functions..</code></pre>
<p>When using the rectifier activation function (ReLU):</p>
<p><span class="math display">\[
    f(x) = \max(0, x)
\]</span></p>
<p>the membrane potential <span class="math inline">\(v(t)\)</span> can take any value, but the firing rate <span class="math inline">\(r(t)\)</span> is only positive.</p>
<table class="table">
<thead>
<tr class="header">
<th>```{figure} ../img/ratecoded-simple2.png</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>width: 80%</td>
</tr>
</tbody>
</table>
<p>The rectifier function only keeps the positive part of the membrane potential.</p>
<pre><code>
When using the logistic (or sigmoid) activation function:

$$
    f(x) = \frac{1}{1 + \exp(-x)}
$$

the firing rate $r(t)$ is bounded between 0 and 1, but responds for negative membrane potentials.

```{figure} ../img/ratecoded-simple3.png
---
width: 80%
---
The sigmoid/logistic function bounds the firing rate between 0 and 1, even if the membrane potential is negative.</code></pre>
</section>
<section id="artificial-neurons" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="artificial-neurons"><span class="header-section-number">3.3</span> Artificial neurons</h2>
<div class="embed-container">
<iframe src="https://www.youtube.com/embed/NVj17dve8B8" frameborder="0" allowfullscreen="">
</iframe>
</div>
<p>By omitting the dynamics of the rate-coded neuron, one obtains the very simple <strong>artificial neuron</strong> (McCulloch and Pitts, 1943):</p>
<table class="table">
<thead>
<tr class="header">
<th>```{figure} ../img/artificialneuron.svg</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>width: 70%</td>
</tr>
</tbody>
</table>
<p>Artificial neuron.</p>
<pre><code>
An artificial neuron sums its inputs $x_1, \ldots, x_d$ by multiplying them with weights $w_1, \ldots, w_d$, adds a bias $b$ and transforms the result into an output $y$ using an activation function $f$.

$$
    y = f( \sum_{i=1}^d w_i \, x_i + b)
$$


The output $y$ directly reflects the input, without temporal integration. The weighted sum of inputs + bias $\sum_{i=1}^d w_i \, x_i + b$ is called the **net activation**. 

This overly simplified neuron model is the basic unit of the **artificial neural networks** (ANN) used in machine learning / deep learning.

**Artificial neurons and hyperplanes**

Let's consider an artificial neuron with only two inputs $x_1$ and $x_2$.

The net activation $w_1 \, x_1 + w_2 \, x_2 + b$ is the equation of a line in the space $(x_1, x_2)$. 

$$
    w_1 \, x_1 + w_2 \, x_2 + b = 0 \Leftrightarrow x_2 = - \frac{w_1}{w_2} \, x_1 - \frac{b}{w_2}
$$



```{figure} ../img/artificialneuron-simple.png
---
width: 50%
---
The net activation represents an hyperplane in 2D.</code></pre>
<p>The net activation is a line in 2D, a plane in 3D, etc. Generally, the net activation describes an <strong>hyperplane</strong> in the input space with <span class="math inline">\(d\)</span> dimensions <span class="math inline">\((x_1, x_2, \ldots, x_d)\)</span>. An hyperplane has one dimension less than the space.</p>
<table class="table">
<thead>
<tr class="header">
<th>```{figure} ../img/hyperplane.gif</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>width: 50%</td>
</tr>
</tbody>
</table>
<p>Hyperplane in 3D. Source: <a href="https://newvitruvian.com/explore/vector-planes/#gal_post_7186_nonzero-vector.gif" class="uri">https://newvitruvian.com/explore/vector-planes/#gal_post_7186_nonzero-vector.gif</a></p>
<pre><code>

We can write the net activation using a **weight vector** $\mathbf{w}$ and a **bias** $b$:

$$
    \sum_{i=1}^d w_i \, x_i + b  = \langle\mathbf{w} \cdot \mathbf{x} \rangle + b
$$

with:

$$
    \mathbf{w} = \begin{bmatrix} w_1 \\ w_2 \\ \ldots \\ w_d \end{bmatrix} \qquad \mathbf{x} = \begin{bmatrix} x_1 \\ x_2 \\ \ldots \\ x_d \end{bmatrix}
$$

$\langle \cdot \rangle$ is the **dot product** (aka inner product, scalar product) between the **input vector** $\mathbf{x}$ and the weight vector $\mathbf{w}$.

The weight vector is orthogonal to the hyperplane $(\mathbf{w}, b)$ and defines its orientation. $b$ is the "distance" between the hyperplane and the origin. The hyperplane separates the input space into two parts:

* $\langle\mathbf{w} \cdot \mathbf{x} \rangle + b &gt; 0$ for all points $\mathbf{x}$ **above** the hyperplane.

* $\langle\mathbf{w} \cdot \mathbf{x} \rangle + b &lt; 0$ for all points $\mathbf{x}$ **below** the hyperplane.

By looking at the **sign** of the net activation, we can separate the input space into two classes. This will be the main principle of **linear classification**.


```{figure} ../img/projection.svg
---
width: 80%
---
The sign of the projection of an input $\mathbf{x}$ on the hyperplane tells whether the input is above or below the hyperplane.</code></pre>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../notes/1.2-Math.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Math basics (optional)</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../notes/2.1-Optimization.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Optimization</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>