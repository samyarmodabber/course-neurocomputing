<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.175">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Neurocomputing - 4&nbsp; Optimization</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../notes/2.2-LinearRegression.html" rel="next">
<link href="../notes/1.3-Neurons.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Optimization</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Neurocomputing</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">Course description</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">Introduction</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/1.1-Introduction.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/1.2-Math.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Math basics (optional)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/1.3-Neurons.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Neurons</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">Linear algorithms</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.1-Optimization.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Optimization</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.2-LinearRegression.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Linear regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.3-LinearClassification.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Linear classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.4-Multiclassification.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Multi-class classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.5-LearningTheory.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Learning theory</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">Neural networks</span>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">Convolutional neural networks</span>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">Unsupervised learning and generative modeling</span>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">Recurrent neural networks</span>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">Self-supervised learning</span>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">Outlook</span>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">Exercises</span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#analytic-optimization" id="toc-analytic-optimization" class="nav-link active" data-scroll-target="#analytic-optimization"><span class="toc-section-number">4.1</span>  Analytic optimization</a></li>
  <li><a href="#gradient-descent" id="toc-gradient-descent" class="nav-link" data-scroll-target="#gradient-descent"><span class="toc-section-number">4.2</span>  Gradient descent</a>
  <ul class="collapse">
  <li><a href="#l1---regularization" id="toc-l1---regularization" class="nav-link" data-scroll-target="#l1---regularization"><span class="toc-section-number">4.2.1</span>  L1 - Regularization</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Optimization</span></h1>
</div>



<div class="quarto-title-meta">

    
    
  </div>
  

</header>

<p>Slides: <a href="https://www.tu-chemnitz.de/informatik/KI/edu/neurocomputing/lectures/pdf/2.1-Optimization.pdf">pdf</a></p>
<section id="analytic-optimization" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="analytic-optimization"><span class="header-section-number">4.1</span> Analytic optimization</h2>
<div class="embed-container">
<iframe src="https://www.youtube.com/embed/1_sPEA6nnIA" frameborder="0" allowfullscreen="">
</iframe>
</div>
<p>Machine learning is all about optimization:</p>
<ul>
<li>Supervised learning minimizes the error between the prediction and the data.</li>
<li>Unsupervised learning maximizes the fit between the model and the data</li>
<li>Reinforcement learning maximizes the collection of rewards.</li>
</ul>
<p>The function to be optimized is called the <strong>objective function</strong>, <strong>cost function</strong> or <strong>loss function</strong>. ML searches for the value of <strong>free parameters</strong> which optimize the objective function on the data set. The simplest optimization method is the <strong>gradient descent</strong> (or ascent) method.</p>
<p>The easiest method to find the optima of a function <span class="math inline">\(f(x)\)</span> is to look where its first-order derivative is equal to 0:</p>
<p><span class="math display">\[
    x^* = \min_x f(x) \Leftrightarrow f'(x^*) = 0 \; \text{and} \; f''(x^*) &gt; 0
\]</span></p>
<p><span class="math display">\[
    x^* = \max_x f(x) \Leftrightarrow f'(x^*) = 0 \; \text{and} \; f''(x^*) &lt; 0
\]</span></p>
<p>The sign of the second order derivative tells us whether it is a maximum or minimum. There can be multiple minima or maxima (or none) depending on the function. The “best” minimum (with the lowest value among all minima) is called the <strong>global minimum</strong>. The others are called <strong>local minima</strong>.</p>
<table class="table">
<thead>
<tr class="header">
<th>```{figure} ../img/localminimum.png</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>width: 60%</td>
</tr>
</tbody>
</table>
<p>Functions (may) have one global minimum but several local minima.</p>
<pre><code>

**Multivariate functions**

A multivariate function is a function of more than one variable, e.g.  $f(x, y)$. A point $(x^*, y^*)$ is an optimum of $f$ if all partial derivatives are zero:

$$
    \begin{cases}
        \dfrac{\partial f(x^*, y^*)}{\partial x} = 0 \\
        \dfrac{\partial f(x^*, y^*)}{\partial y} = 0 \\
    \end{cases}
$$

The vector of partial derivatives is called the **gradient of the function**:

$$
    \nabla_{x, y} \, f(x^*, y^*) = \begin{bmatrix} \dfrac{\partial f(x^*, y^*)}{\partial x} \\ \dfrac{\partial f(x^*, y^*)}{\partial y} \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \end{bmatrix}
$$

```{figure} ../img/optimization-example-multivariate.png
---
width: 80%
---
Multivariate optimization of $f(x, y) = (x - 1)^2 + y^2 + 1$. The minimum is in $(1, 0)$.</code></pre>
</section>
<section id="gradient-descent" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="gradient-descent"><span class="header-section-number">4.2</span> Gradient descent</h2>
<div class="embed-container">
<iframe src="https://www.youtube.com/embed/UpJL2M6GKog" frameborder="0" allowfullscreen="">
</iframe>
</div>
<p>In machine learning, we generally do not have access to the analytical form of the objective function. We can not therefore get its derivative and search where it is 0. However, we have access to its value (and derivative) for certain values, for example:</p>
<p><span class="math display">\[
    f(0, 1) = 2 \qquad f'(0, 1) = -1.5
\]</span></p>
<p>We can “ask” the model for as many values as we want, but we never get its analytical form. For most useful problems, the function would be too complex to differentiate anyway.</p>
<table class="table">
<thead>
<tr class="header">
<th>```{figure} ../img/derivative-approx.png</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>width: 60%</td>
</tr>
</tbody>
</table>
<p>Euler method: the derivative of a function is the slope of its tangent.</p>
<pre><code>
Let's remember the definition of the derivative of a function. The derivative $f'(x)$ is defined by the slope of the tangent of the function:

$$
    f'(x) = \lim_{h \to 0} \frac{f(x + h) - f(x)}{x + h - x} = \lim_{h \to 0} \frac{f(x + h) - f(x)}{h}
$$

If we take $h$ small enough, we have the following approximation:

$$
    f(x + h) - f(x) \approx h \, f'(x)
$$

If we want $x+h$ to be closer to the minimum than $x$, we want:

$$
    f(x + h) &lt; f(x)
$$

or:

$$
    f(x + h) - f(x) &lt; 0
$$

We therefore want that:

$$
    h \, f'(x) &lt; 0
$$

The **change** $h$ in the value of $x$ must have the opposite sign of $f'(x)$ in order to get closer to the minimum. If the function is increasing in $x$, the minimum is smaller (to the left) than $x$. If the function is decreasing in $x$, the minimum is bigger than $x$ (to the right).


**Gradient descent** (GD) is a first-order method to iteratively find the minimum of a function $f(x)$. It starts with a random estimate $x_0$ and iteratively changes its value so that it becomes closer to the minimum.

```{figure} ../img/gradient.png
---
width: 70%
---
Gradient descent iteratively modifies the estimate $x_n$ in the opposite direction of the derivative.</code></pre>
<p>It creates a series of estimates <span class="math inline">\([x_0, x_1, x_2, \ldots]\)</span> that converge to a local minimum of <span class="math inline">\(f\)</span>. Each element of the series is calculated based on the previous element and the derivative of the function in that element:</p>
<p><span class="math display">\[
    x_{n+1} = x_n + \Delta x =  x_n - \eta \, f'(x_n)
\]</span></p>
<p>If the function is locally increasing (resp. decreasing), the new estimate should be smaller (resp. bigger) than the previous one. <span class="math inline">\(\eta\)</span> is a small parameter between 0 and 1 called the <strong>learning rate</strong> that controls the speed of convergence (more on that later).</p>
<p><strong>Gradient descent algorithm</strong>:</p>
<ul>
<li><p>We start with an initially wrong estimate of <span class="math inline">\(x\)</span>: <span class="math inline">\(x_0\)</span></p></li>
<li><p>for <span class="math inline">\(n \in [0, \infty]\)</span>:</p>
<ul>
<li><p>We compute or estimate the derivative of the loss function in <span class="math inline">\(x_{n}\)</span>: <span class="math inline">\(f'(x_{n})\)</span></p></li>
<li><p>We compute a new value <span class="math inline">\(x_{n+1}\)</span> for the estimate using the <strong>gradient descent update rule</strong>:</p></li>
</ul>
<p><span class="math display">\[
      \Delta x = x_{n+1} - x_n =  - \eta \, f'(x_n)
  \]</span></p></li>
</ul>
<p>There is theoretically no end to the GD algorithm: we iterate forever and always get closer to the minimum. The algorithm can be stopped when the change <span class="math inline">\(\Delta x\)</span> is below a threshold.</p>
<table class="table">
<thead>
<tr class="header">
<th>```{figure} ../img/gradient-descent-animation.gif</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>width: 80%</td>
</tr>
</tbody>
</table>
<p>Visualization of Gradient Descent on a quadratic function. Notice how the speed of convergence slows down when approaching the minimum.</p>
<pre><code>

Gradient descent can be applied to multivariate functions:

$$
    \min_{x, y, z} \qquad f(x, y, z)
$$

Each variable is updated independently using partial derivatives:

$$
    \Delta x = x_{n+1} - x_{n} = - \eta \, \frac{\partial f(x_n, y_n, z_n)}{\partial x}
$$
$$
    \Delta y = y_{n+1} - y_{n} = - \eta \, \frac{\partial f(x_n, y_n, z_n)}{\partial y}
$$
$$
    \Delta z = z_{n+1} - z_{n} = - \eta \, \frac{\partial f(x_n, y_n, z_n)}{\partial z}
$$

We can also use the vector notation to use the **gradient operator**:

$$
    \mathbf{x}_n = \begin{bmatrix} x_n \\ y_n \\ z_n \end{bmatrix} \quad \text{and} \quad \nabla_\mathbf{x} f(\mathbf{x}) = \begin{bmatrix} \frac{\partial f(x, y, z)}{\partial x} \\ \frac{\partial f(x, y, z)}{\partial y} \\ \frac{\partial f(x, y, z)}{\partial z} \end{bmatrix}
    \qquad \rightarrow \qquad \Delta \mathbf{x} = - \eta \, \nabla_\mathbf{x} f(\mathbf{x}_n)
$$

The change in the estimation is in the **opposite direction of the gradient**, hence the name **gradient descent**.


```{figure} ../img/gradient-descent-animation-multivariate.gif
---
width: 100%
---
Visualization of Gradient Descent on a multivariate function in 2 dimensions.</code></pre>
<p>The choice of the learning rate <span class="math inline">\(\eta\)</span> is critical:</p>
<ul>
<li>If it is too small, the algorithm will need a lot of iterations to converge.</li>
<li>If it is too big, the algorithm can oscillate around the desired values without ever converging.</li>
</ul>
<table class="table">
<thead>
<tr class="header">
<th>```{figure} ../img/gradient-descent-learningrate.gif</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>width: 100%</td>
</tr>
</tbody>
</table>
<p>Influence of the learning on convergence: too small (red) and it takes forever, too high (green) and convergence is unstable. Finding its optimal value (blue) is hard as it depends on the function itself.</p>
<pre><code>
Gradient descent is not optimal: it always finds a local minimum, but there is no guarantee that it is the global minimum. The found solution depends on the initial choice of $x_0$. If you initialize the parameters near to the global minimum, you are lucky. But how? This will be a big issue in neural networks.


## Regularization

&lt;div class='embed-container'&gt;&lt;iframe src='https://www.youtube.com/embed/LI5ExC4d9Js' frameborder='0' allowfullscreen&gt;&lt;/iframe&gt;&lt;/div&gt;


### L2 - Regularization

Most of the time, there are many minima to a function, if not an infinity. As GD only converges to the "closest" local minimum, you are never sure that you get a good solution. Consider the following function:

$$
    f(x, y) = (x -1)^2
$$

As it does not depend on $y$, whatever initial value $y_0$ will be considered as a solution. As we will see later, this is something we do not want.


```{figure} ../img/gradient-descent-animation-regularization1.gif
---
width: 100%
---
Function with an infinity of minima: as long as $x=1$, each point on the vertical line is a minimum.</code></pre>
<p>To obtain a single solution, we may want to put the additional <strong>constraint</strong> that both <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> should be as small as possible. One possibility is to also minimize the <strong>Euclidian norm</strong> (or <strong>L2-norm</strong>) of the vector <span class="math inline">\(\mathbf{x} = [x, y]\)</span>.</p>
<p><span class="math display">\[
    \min_{x, y} ||\mathbf{x}||^2 = x^2 + y^2
\]</span></p>
<p>Note that this objective is in contradiction with the original objective: <span class="math inline">\((0, 0)\)</span> minimizes the norm, but not the function <span class="math inline">\(f(x, y)\)</span>. We construct a new function as the sum of <span class="math inline">\(f(x, y)\)</span> and the norm of <span class="math inline">\(\mathbf{x}\)</span>, weighted by the <strong>regularization parameter</strong> <span class="math inline">\(\lambda\)</span>:</p>
<p><span class="math display">\[
    \mathcal{L}(x, y) = f(x, y) + \lambda \, (x^2 + y^2)
\]</span></p>
<p>For a fixed value of <span class="math inline">\(\lambda\)</span> (for example 0.1), we now minimize using gradient descent this new loss function. To do that, we just need to compute its gradient:</p>
<p><span class="math display">\[
    \nabla_{x, y} \, \mathcal{L}(x, y) = \begin{bmatrix} \frac{\partial f(x, y)}{\partial x} + 2\, \lambda \, x \\ \frac{\partial f(x, y)}{\partial y} + 2\, \lambda \, y \end{bmatrix}
\]</span></p>
<p>and apply gradient descent iteratively:</p>
<p><span class="math display">\[
    \Delta \begin{bmatrix} x \\ y \end{bmatrix} = - \eta \, \nabla_{x, y} \, \mathcal{L}(x, y) = - \eta \, \begin{bmatrix} \frac{\partial f(x, y)}{\partial x} + 2\, \lambda \, x \\ \frac{\partial f(x, y)}{\partial y} + 2\, \lambda \, y \end{bmatrix}
\]</span></p>
<table class="table">
<thead>
<tr class="header">
<th>```{figure} ../img/gradient-descent-animation-regularization2.gif</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>width: 100%</td>
</tr>
</tbody>
</table>
<p>Gradient descent with L2 regularization, using <span class="math inline">\(\lambda = 0.1\)</span>.</p>
<pre><code>
You may notice that the result of the optimization is a bit off, it is not exactly $(1, 0)$. This is because we do not optimize $f(x, y)$ directly, but $\mathcal{L}(x, y)$. Let's have a look at the landscape of the loss function:

```{figure} ../img/gradient-descent-animation-regularization3.gif
---
width: 100%
---
Landscape of the loss function $\mathcal{L}(x, y) = f(x, y) + \lambda \, (x^2 + y^2)$ with $\lambda = 0.1$.</code></pre>
<p>The optimization with GD indeed works, it is just that the function is different. The constraint on the Euclidian norm “attracts” or “distorts” the function towards <span class="math inline">\((0, 0)\)</span>. This may seem counter-intuitive, but we will see with deep networks that we can live with it. Let’s now look at what happens when we increase <span class="math inline">\(\lambda\)</span> to 5:</p>
<table class="table">
<thead>
<tr class="header">
<th>```{figure} ../img/gradient-descent-animation-regularization4.gif</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>width: 100%</td>
</tr>
</tbody>
</table>
<p>Gradient descent with L2 regularization, using <span class="math inline">\(\lambda = 5\)</span>.</p>
<pre><code>
```{figure} ../img/gradient-descent-animation-regularization5.gif
---
width: 100%
---
Landscape of the loss function $\mathcal{L}(x, y) = f(x, y) + \lambda \, (x^2 + y^2)$ with $\lambda = 5$.</code></pre>
<p>Now the result of the optimization is totally wrong: the constraint on the norm completely dominates the optimization process.</p>
<p><span class="math display">\[
    \mathcal{L}(x, y) = f(x, y) + \lambda \, (x^2 + y^2)
\]</span></p>
<p><span class="math inline">\(\lambda\)</span> controls which of the two objectives, <span class="math inline">\(f(x, y)\)</span> or <span class="math inline">\(x^2 + y^2\)</span>, has the priority:</p>
<ul>
<li><p>When <span class="math inline">\(\lambda\)</span> is small, <span class="math inline">\(f(x, y)\)</span> dominates and the norm of <span class="math inline">\(\mathbf{x}\)</span> can be anything.</p></li>
<li><p>When <span class="math inline">\(\lambda\)</span> is big, <span class="math inline">\(x^2 + y^2\)</span> dominates, the result will be very small but <span class="math inline">\(f(x, y)\)</span> will have any value.</p></li>
</ul>
<p>The right value for <span class="math inline">\(\lambda\)</span> is hard to find. We will see later methods to experimentally find its most adequate value.</p>
<pre class="{note}"><code>Regularization is a form of **constrained optimization**. What we actually want to solve is the constrained optimization problem:

$$
    \min_{x, y} \qquad f(x, y) \\
    \text{so that} \qquad x^2 + y^2 &lt; \delta
$$

i.e. minimize $f(x, y)$ while keeping the norm of $[x, y]$ below a threshold $\delta$. **Lagrange optimization** (technically KKT optimization; see the course Introduction to AI) allows to solve that problem by searching the minimum of the generalized Lagrange function:

$$
    \mathcal{L}(x, y, \lambda) = f(x, y) + \lambda \, (x^2 + y^2 - \delta)
$$

Regularization is a special case of Lagrange optimization, as it considers $\lambda$ to be fixed, while it is an additional variable in Lagrange optimization. When differentiating this function, $\delta$ disappears anyway, so it is equivalent to our regularized loss function.</code></pre>
<section id="l1---regularization" class="level3" data-number="4.2.1">
<h3 data-number="4.2.1" class="anchored" data-anchor-id="l1---regularization"><span class="header-section-number">4.2.1</span> L1 - Regularization</h3>
<p>Another form of regularization is <strong>L1 - regularization</strong> using the L1-norm (absolute values):</p>
<p><span class="math display">\[
    \mathcal{L}(x, y) = f(x, y) + \lambda \, (|x| + |y|)
\]</span></p>
<p>Its gradient only depend on the sign of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>:</p>
<p><span class="math display">\[
    \nabla_{x, y} \, \mathcal{L}(x, y) = \begin{bmatrix} \frac{\partial f(x, y)}{\partial x} + \lambda \, \text{sign}(x) \\ \frac{\partial f(x, y)}{\partial y} + \lambda \, \text{sign}(y) \end{bmatrix}
\]</span></p>
<p>It tends to lead to <strong>sparser</strong> value of <span class="math inline">\((x, y)\)</span>, i.e.&nbsp;either <span class="math inline">\(x\)</span> or <span class="math inline">\(y\)</span> will be close or equal to 0.</p>
<table class="table">
<thead>
<tr class="header">
<th>```{figure} ../img/gradient-descent-animation-regularization6.gif</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>width: 100%</td>
</tr>
</tbody>
</table>
<p>Gradient descent with L1 regularization, using <span class="math inline">\(\lambda = 0.1\)</span>. ```</p>
<p>Both L1 and L2 regularization can be used in neural networks depending on the desired effect.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../notes/1.3-Neurons.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Neurons</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../notes/2.2-LinearRegression.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Linear regression</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>