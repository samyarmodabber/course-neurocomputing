<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.269">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Neurocomputing - 5&nbsp; Linear regression</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../notes/2.3-LinearClassification.html" rel="next">
<link href="../notes/2.1-Optimization.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-title">Linear regression</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="../index.html" class="sidebar-logo-link">
      <img src="../notes/img/tuc-new.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Neurocomputing</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">Overview</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true"><strong>Introduction</strong></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/1.1-Introduction.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/1.2-Math.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Math basics (optional)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/1.3-Neurons.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Neurons</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true"><strong>Linear algorithms</strong></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.1-Optimization.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Optimization</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.2-LinearRegression.html" class="sidebar-item-text sidebar-link active"><span class="chapter-title">Linear regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.3-LinearClassification.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Linear classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.4-LearningTheory.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Learning theory</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true"><strong>Neural networks</strong></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.1-NeuralNetworks.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Multi-layer perceptron</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.2-DNN.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Modern neural networks</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true"><strong>Computer Vision</strong></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/4.1-CNN.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Convolutional neural networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/4.2-ObjectDetection.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Object detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/4.3-SemanticSegmentation.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Semantic segmentation</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true"><strong>Generative modeling</strong></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/5.1-Autoencoders.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Autoencoders</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/5.2-RBM.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Restricted Boltzmann machines (optional)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/5.3-GAN.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Generative adversarial networks</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true"><strong>Recurrent neural networks</strong></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/6.1-RNN.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Recurrent neural networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/6.2-NLP.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Natural Language Processing and attention</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="true"><strong>Self-supervised learning</strong></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/7.1-Transformers.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Transformers</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/7.2-ContrastiveLearning.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Contrastive Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/7.3-VisionTransformer.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Vision Transformers</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/7.4-DiffusionModels.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Diffusion Probabilistic Models</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" aria-expanded="true"><strong>Outlook</strong></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/8.1-Limits.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Limits of deep learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/8.2-Beyond.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Beyond deep Learning</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" aria-expanded="true"><strong>Exercises</strong></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/Content.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">List of exercises</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/Installation.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Python installation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/1-Python-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Introduction To Python</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/2-Numpy-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Numpy and Matplotlib</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/3-LinearRegression-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Linear regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/4-MLR-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Multiple linear regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/5-Crossvalidation-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Cross-validation and polynomial regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/6-LinearClassification-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Linear classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/7-SoftmaxClassifier-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Softmax classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/8-MLP-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Multi-layer Perceptron</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/9-MNIST-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">MNIST classification using keras</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/10-CNN-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Convolutional neural networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/11-TransferLearning-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Transfer learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/12-VAE-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Variational autoencoder</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/13-RNN-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Recurrent neural networks</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#linear-regression" id="toc-linear-regression" class="nav-link active" data-scroll-target="#linear-regression">Linear regression</a>
  <ul class="collapse">
  <li><a href="#least-mean-squares" id="toc-least-mean-squares" class="nav-link" data-scroll-target="#least-mean-squares">Least Mean Squares</a></li>
  <li><a href="#delta-learning-rule" id="toc-delta-learning-rule" class="nav-link" data-scroll-target="#delta-learning-rule">Delta learning rule</a></li>
  </ul></li>
  <li><a href="#multiple-linear-regression" id="toc-multiple-linear-regression" class="nav-link" data-scroll-target="#multiple-linear-regression">Multiple linear regression</a></li>
  <li><a href="#logistic-regression" id="toc-logistic-regression" class="nav-link" data-scroll-target="#logistic-regression">Logistic regression</a></li>
  <li><a href="#polynomial-regression" id="toc-polynomial-regression" class="nav-link" data-scroll-target="#polynomial-regression">Polynomial regression</a></li>
  <li><a href="#a-bit-of-learning-theory" id="toc-a-bit-of-learning-theory" class="nav-link" data-scroll-target="#a-bit-of-learning-theory">A bit of learning theory</a>
  <ul class="collapse">
  <li><a href="#sensibility-to-outliers" id="toc-sensibility-to-outliers" class="nav-link" data-scroll-target="#sensibility-to-outliers">Sensibility to outliers</a></li>
  <li><a href="#cross-validation" id="toc-cross-validation" class="nav-link" data-scroll-target="#cross-validation">Cross-validation</a></li>
  <li><a href="#underfitting---overfitting" id="toc-underfitting---overfitting" class="nav-link" data-scroll-target="#underfitting---overfitting">Underfitting - overfitting</a></li>
  </ul></li>
  <li><a href="#regularized-regression" id="toc-regularized-regression" class="nav-link" data-scroll-target="#regularized-regression">Regularized regression</a>
  <ul class="collapse">
  <li><a href="#l2-regularization---ridge-regression" id="toc-l2-regularization---ridge-regression" class="nav-link" data-scroll-target="#l2-regularization---ridge-regression">L2 regularization - Ridge regression</a></li>
  <li><a href="#l1-regularization---lasso-regression" id="toc-l1-regularization---lasso-regression" class="nav-link" data-scroll-target="#l1-regularization---lasso-regression">L1 regularization - LASSO regression</a></li>
  <li><a href="#l1l2-regularization---elasticnet" id="toc-l1l2-regularization---elasticnet" class="nav-link" data-scroll-target="#l1l2-regularization---elasticnet">L1+L2 regularization - ElasticNet</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-body" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-title">Linear regression</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>Slides: <a href="../slides/2.2-LinearRegression.html" target="_blank">html</a> <a href="../slides/pdf/2.2-LinearRegression.pdf" target="_blank">pdf</a></p>
<section id="linear-regression" class="level2">
<h2 class="anchored" data-anchor-id="linear-regression">Linear regression</h2>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/lEILkDvT0gI" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/regression-animation2.png" class="img-fluid figure-img" style="width:70.0%"></p>
<p></p><figcaption class="figure-caption">Simple linear regression. <span class="math inline">x</span> is the input, <span class="math inline">y</span> the output. The data is represented by blue dots, the model by the black line.</figcaption><p></p>
</figure>
</div>
<p>Let’s consider a training set of N examples <span class="math inline">\mathcal{D} = (x_i, t_i)_{i=1..N}</span>. In <strong>linear regression</strong>, we want to learn a linear model (hypothesis) <span class="math inline">y</span> that is linearly dependent on the input <span class="math inline">x</span>:</p>
<p><span class="math display">
    y = f_{w, b}(x) = w \, x + b
</span></p>
<p>The <strong>free parameters</strong> of the model are the slope <span class="math inline">w</span> and the intercept <span class="math inline">b</span>. This model corresponds to a single <strong>artificial neuron</strong> with output <span class="math inline">y</span>, having one input <span class="math inline">x</span>, one weight <span class="math inline">w</span>, one bias <span class="math inline">b</span> and a <strong>linear</strong> activation function <span class="math inline">f(x) = x</span>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/artificialneuron.svg" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Artificial neuron with multiple inputs.</figcaption><p></p>
</figure>
</div>
<p>The goal of the linear regression (or least mean squares - LMS) is to minimize the <strong>mean square error</strong> (mse) between the targets and the predictions. This loss function is defined as the mathematical expectation of the quadratic error over the training set:</p>
<p><span class="math display">
    \mathcal{L}(w, b) =  \mathbb{E}_{x_i, t_i \in \mathcal{D}} [ (t_i - y_i )^2 ]
</span></p>
<p>As the training set is finite and the samples i.i.d (independent and identically distributed), we can simply replace the expectation by a sampling average over the training set:</p>
<p><span class="math display">
    \mathcal{L}(w, b) = \frac{1}{N} \, \sum_{i=1}^{N} (t_i - y_i )^2
</span></p>
<p>The minimum of the mse is achieved when the <strong>prediction</strong> <span class="math inline">y_i = f_{w, b}(x_i)</span> is equal to the <strong>true value</strong> <span class="math inline">t_i</span> for all training examples. In other words, we want to minimize the <strong>residual error</strong> of the model on the data. It is not always possible to obtain the global minimum (0) as the data may be noisy, but the closer, the better.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/regression-animation-mse-dual.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">A good fit to the data is when the prediction <span class="math inline">y_i</span> (on the line) is close to the data <span class="math inline">t_i</span> for all training examples.</figcaption><p></p>
</figure>
</div>
<section id="least-mean-squares" class="level3">
<h3 class="anchored" data-anchor-id="least-mean-squares">Least Mean Squares</h3>
<p>We search for <span class="math inline">w</span> and <span class="math inline">b</span> which minimize the mean square error:</p>
<p><span class="math display">
    \mathcal{L}(w, b) = \frac{1}{N} \, \sum_{i=1}^{N} (t_i - y_i )^2
</span></p>
<p>We will apply <strong>gradient descent</strong> to iteratively modify estimates of <span class="math inline">w</span> and <span class="math inline">b</span>:</p>
<p><span class="math display">
    \Delta w = - \eta \, \frac{\partial \mathcal{L}(w, b)}{\partial w}
</span> <span class="math display">
    \Delta b = - \eta \, \frac{\partial \mathcal{L}(w, b)}{\partial b}
</span></p>
<p>Let’s search for the partial derivative of the mean square error with respect to <span class="math inline">w</span>:</p>
<p><span class="math display">
    \frac{\partial \mathcal{L}(w, b)}{\partial w} = \frac{\partial}{\partial w} [\frac{1}{N} \, \sum_{i=1}^{N} (t_i - y_i )^2]
</span></p>
<p>Partial derivatives are linear, so the derivative of a sum is the sum of the derivatives:</p>
<p><span class="math display">
    \frac{\partial \mathcal{L}(w, b)}{\partial w} = \frac{1}{N} \, \sum_{i=1}^{N} \frac{\partial}{\partial w} (t_i - y_i )^2
</span></p>
<p>This means we can compute a gradient for each training example instead of for the whole training set (see later the distinction batch/online):</p>
<p><span class="math display">
    \frac{\partial \mathcal{L}(w, b)}{\partial w} = \frac{1}{N} \, \sum_{i=1}^{N} \frac{\partial}{\partial w} \mathcal{l}_i(w, b)
    \qquad \text{with} \qquad \mathcal{l}_i(w, b) = (t_i - y_i )^2
</span></p>
<p>The individual loss <span class="math inline">\mathcal{l}_i(w, b) = (t_i - y_i )^2</span> is the composition of two functions:</p>
<ul>
<li><p>a square error function <span class="math inline">g_i(y_i) = (t_i - y_i)^2</span>.</p></li>
<li><p>the prediction <span class="math inline">y_i = f_{w, b}(x_i) = w \, x_i + b</span>.</p></li>
</ul>
<p>The <strong>chain rule</strong> tells us how to derive such composite functions:</p>
<p><span class="math display">
    \frac{ d f(g(x))}{dx} = \frac{ d f(g(x))}{d g(x)} \times \frac{ d g(x)}{dx} = \frac{ d f(y)}{dy} \times \frac{ d g(x)}{dx}
</span></p>
<p>The first derivative considers <span class="math inline">g(x)</span> to be a single variable. Applied to our problem, this gives:</p>
<p><span class="math display">
     \frac{\partial}{\partial w} \mathcal{l}_i(w, b) =  \frac{\partial g_i(y_i)}{\partial y_i} \times  \frac{\partial y_i}{\partial w}
</span></p>
<p>The square error function <span class="math inline">g_i(y) = (t_i - y)^2</span> is easy to differentiate w.r.t <span class="math inline">y</span>:</p>
<p><span class="math display">
    \frac{\partial g_i(y_i)}{\partial y_i} = - 2 \, (t_i - y_i)
</span></p>
<p>The prediction <span class="math inline">y_i = w \, x_i + b</span> also w.r.t <span class="math inline">w</span> and <span class="math inline">b</span>:</p>
<p><span class="math display">
   \frac{\partial  y_i}{\partial w} = x_i
</span></p>
<p><span class="math display">
   \frac{\partial  y_i}{\partial b} = 1
</span></p>
<p>The partial derivative of the individual loss is:</p>
<p><span class="math display">
    \frac{\partial \mathcal{l}_i(w, b)}{\partial w} = - 2 \, (t_i - y_i) \, x_i
</span></p>
<p><span class="math display">
    \frac{\partial \mathcal{l}_i(w, b)}{\partial b} = - 2 \, (t_i - y_i)
</span></p>
<p>This gives us:</p>
<p><span class="math display">
    \frac{\partial \mathcal{L}(w, b)}{\partial w} = - \frac{2}{N} \sum_{i=1}^{N} (t_i - y_i) \, x_i
</span></p>
<p><span class="math display">
    \frac{\partial \mathcal{L}(w, b)}{\partial b} = - \frac{2}{N} \sum_{i=1}^{N} (t_i - y_i)
</span></p>
<p>Gradient descent is then defined by the learning rules (absorbing the 2 in <span class="math inline">\eta</span>):</p>
<p><span class="math display">
    \Delta w = \eta \, \frac{1}{N} \sum_{i=1}^{N} (t_i - y_i) \, x_i
</span></p>
<p><span class="math display">
    \Delta b = \eta \, \frac{1}{N} \sum_{i=1}^{N} (t_i - y_i)
</span></p>
<p><strong>Least Mean Squares</strong> (LMS) or Ordinary Least Squares (OLS) is a <strong>batch</strong> algorithm: the parameter changes are computed over the whole dataset.</p>
<p><span class="math display">
    \Delta w = \eta \, \frac{1}{N} \sum_{i=1}^{N} (t_i - y_i) \, x_i
</span> <span class="math display">
    \Delta b = \eta \, \frac{1}{N} \sum_{i=1}^{N} (t_i - y_i)
</span></p>
<p>The parameter changes have to be applied multiple times (<strong>epochs</strong>) in order for the parameters to converge. One can stop when the parameters do not change much, or after a fixed number of epochs.</p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
LMS algorithm
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p><span class="math inline">w=0 \quad;\quad b=0</span></p></li>
<li><p><strong>for</strong> M epochs:</p>
<ul>
<li><p><span class="math inline">dw=0 \quad;\quad db=0</span></p></li>
<li><p><strong>for</strong> each sample <span class="math inline">(x_i, t_i)</span>:</p>
<ul>
<li><p><span class="math inline">y_i = w \, x_i + b</span></p></li>
<li><p><span class="math inline">dw = dw + (t_i - y_i) \, x_i</span></p></li>
<li><p><span class="math inline">db = db + (t_i - y_i)</span></p></li>
</ul></li>
<li><p><span class="math inline">\Delta w = \eta \, \frac{1}{N} dw</span></p></li>
<li><p><span class="math inline">\Delta b = \eta \, \frac{1}{N} db</span></p></li>
</ul></li>
</ul>
</div>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/regression-animation.gif" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Visualization of least mean squares applied to a simple regression problem with <span class="math inline">\eta=0.1</span>. Each step of the animation corresponds to one epoch (iteration over the training set).</figcaption><p></p>
</figure>
</div>
<p>During learning, the <strong>mean square error</strong> (mse) decreases with the number of epochs but does not reach zero because of the noise in the data.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/regression-animation-loss.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Evolution of the loss function during training.</figcaption><p></p>
</figure>
</div>
</section>
<section id="delta-learning-rule" class="level3">
<h3 class="anchored" data-anchor-id="delta-learning-rule">Delta learning rule</h3>
<p>LMS is very slow, because it changes the weights only after the whole training set has been evaluated. It is also possible to update the weights immediately after each example using the <strong>delta learning rule</strong>, which is the <strong>online</strong> version of LMS:</p>
<p><span class="math display">\Delta w = \eta \, (t_i - y_i) \, x_i</span></p>
<p><span class="math display">\Delta b = \eta \, (t_i - y_i)</span></p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Delta learning rule
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p><span class="math inline">w=0 \quad;\quad b=0</span></p></li>
<li><p><strong>for</strong> M epochs:</p>
<ul>
<li><p><strong>for</strong> each sample <span class="math inline">(x_i, t_i)</span>:</p>
<ul>
<li><p><span class="math inline">y_i = w \, x_i + b</span></p></li>
<li><p><span class="math inline">\Delta w = \eta \, (t_i - y_i ) \, x_i</span></p></li>
<li><p><span class="math inline">\Delta b = \eta \, (t_i - y_i)</span></p></li>
</ul></li>
</ul></li>
</ul>
</div>
</div>
<p>The batch version is more stable, but the online version is faster: the weights have already learned something when arriving at the end of the first epoch. Note that the loss function is slightly higher at the end of learning (see Exercise 3 for a deeper discussion).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/regression-animation-online.gif" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Visualization of the delta learning rule applied to a simple regression problem with <span class="math inline">\eta = 0.1</span>. Each step of the animation corresponds to one epoch (iteration over the training set).</figcaption><p></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/regression-animation-online-loss.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Evolution of the loss function during training. With the same learning rate, the delta learning rule converges much faster but reaches a poorer minimum. Lowering the learning rate slows down learning but reaches a better minimum.</figcaption><p></p>
</figure>
</div>
</section>
</section>
<section id="multiple-linear-regression" class="level2">
<h2 class="anchored" data-anchor-id="multiple-linear-regression">Multiple linear regression</h2>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/BOJFFy0nA6I" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
<p>The key idea of linear regression (one input <span class="math inline">x</span>, one output <span class="math inline">y</span>) can be generalized to multiple inputs and outputs.</p>
<p><strong>Multiple Linear Regression</strong> (MLR) predicts several output variables based on several explanatory variables:</p>
<p><span class="math display">
\begin{cases}
y_1 = w_1 \, x_1 + w_2 \, x_2 + b_1\\
\\
y_2 = w_3 \, x_1 + w_3 \, x_2 + b_2\\
\end{cases}
</span></p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Example: fuel consumption and CO2 emissions
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let’s suppose you have 13971 measurements in some Excel file, linking engine size, number of cylinders, fuel consumption and CO2 emissions of various cars. You want to predict fuel consumption and CO2 emissions when you know the engine size and the number of cylinders.</p>
<table class="table">
<thead>
<tr class="header">
<th>Engine size</th>
<th>Cylinders</th>
<th>Fuel consumption</th>
<th>CO2 emissions</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>2</td>
<td>&nbsp;4</td>
<td>8.5</td>
<td>196</td>
</tr>
<tr class="even">
<td>2.4</td>
<td>4</td>
<td>9.6</td>
<td>221</td>
</tr>
<tr class="odd">
<td>1.5</td>
<td>4</td>
<td>5.9</td>
<td>136</td>
</tr>
<tr class="even">
<td>3.5</td>
<td>6</td>
<td>11</td>
<td>255</td>
</tr>
<tr class="odd">
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
</tr>
</tbody>
</table>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/MLR-example-data.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">CO2 emissions and fuel consumption depend almost linearly on the engine size and number of cylinders.</figcaption><p></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/MLR-example-data-3d.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">CO2 emissions and fuel consumption depend almost linearly on the engine size and number of cylinders.</figcaption><p></p>
</figure>
</div>
<p>We can notice that the output variables seem to linearly depend on the inputs. Noting the input variables <span class="math inline">x_1</span>, <span class="math inline">x_2</span> and the output ones <span class="math inline">y_1</span>, <span class="math inline">y_2</span>, we can define our problem as a multiple linear regression:</p>
<p><span class="math display">
\begin{cases}
y_1 = w_1 \, x_1 + w_2 \, x_2 + b_1\\
\\
y_2 = w_3 \, x_1 + w_3 \, x_2 + b_2\\
\end{cases}
</span></p>
<p>and solve it using the least mean squares method by minimizing the mse between the model and the data.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/MLR-example-fit-3d.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">The result of MLR is a plane in the input space.</figcaption><p></p>
</figure>
</div>
<p>Using the Python library <code>scikit-learn</code> (<a href="https://scikit-learn.org" class="uri">https://scikit-learn.org</a>), this is done in two lines of code:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>reg <span class="op">=</span> LinearRegression().fit(X, y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
<p>The system of equations:</p>
<p><span class="math display">
\begin{cases}
y_1 = w_1 \, x_1 + w_2 \, x_2 + b_1\\
\\
y_2 = w_3 \, x_1 + w_4 \, x_2 + b_2\\
\end{cases}
</span></p>
<p>can be put in a matrix-vector form:</p>
<p><span class="math display">
    \begin{bmatrix} y_1 \\ y_2 \\\end{bmatrix} = \begin{bmatrix} w_1 &amp; w_2 \\ w_3 &amp; w_4 \\\end{bmatrix} \times \begin{bmatrix} x_1 \\ x_2 \\\end{bmatrix} + \begin{bmatrix} b_1 \\ b_2 \\\end{bmatrix}
</span></p>
<p>We simply create the corresponding vectors and matrices:</p>
<p><span class="math display">
    \mathbf{x} = \begin{bmatrix} x_1 \\ x_2 \\\end{bmatrix} \qquad \mathbf{y} = \begin{bmatrix} y_1 \\ y_2 \\\end{bmatrix} \qquad \mathbf{t} = \begin{bmatrix} t_1 \\ t_2 \\\end{bmatrix} \qquad \mathbf{b} = \begin{bmatrix} b_1 \\ b_2 \\\end{bmatrix} \qquad W = \begin{bmatrix} w_1 &amp; w_2 \\ w_3 &amp; w_4 \\\end{bmatrix}
</span></p>
<p><span class="math inline">\mathbf{x}</span> is the input vector, <span class="math inline">\mathbf{y}</span> is the output vector, <span class="math inline">\mathbf{t}</span> is the target vector. <span class="math inline">W</span> is called the <strong>weight matrix</strong> and <span class="math inline">\mathbf{b}</span> the <strong>bias vector</strong>.</p>
<p>The model is now defined by:</p>
<p><span class="math display">
    \mathbf{y} = f_{W, \mathbf{b}}(\mathbf{x}) = W \times \mathbf{x} + \mathbf{b}
</span></p>
<p>The problem is exactly the same as before, except that we use vectors and matrices instead of scalars: <span class="math inline">\mathbf{x}</span> and <span class="math inline">\mathbf{y}</span> can have any number of dimensions, the same procedure will apply. This corresponds to a <strong>linear neural network</strong> (or linear perceptron), with one <strong>output neuron</strong> per predicted value <span class="math inline">y_i</span> using the linear activation function.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/linearperceptron.svg" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">A linear perceptron is a single layer of artificial neurons. The output vector <span class="math inline">\mathbf{y}</span> is compared to the ground truth vector <span class="math inline">\mathbf{t}</span> using the mse loss.</figcaption><p></p>
</figure>
</div>
<p>The mean square error still needs to be a scalar in order to be minimized. We can define it as the squared norm of the error <strong>vector</strong>:</p>
<p><span class="math display">
    \min_{W, \mathbf{b}} \, \mathcal{L}(W, \mathbf{b}) = \mathbb{E}_\mathcal{D} [ ||\mathbf{t} - \mathbf{y}||^2 ] = \mathbb{E}_\mathcal{D} [ ((t_1 - y_1)^2 + (t_2 - y_2)^2) ]
</span></p>
<p>In order to apply gradient descent, one needs to calculate partial derivatives w.r.t the weight matrix <span class="math inline">W</span> and the bias vector <span class="math inline">\mathbf{b}</span>, i.e.&nbsp;<strong>gradients</strong>:</p>
<p><span class="math display">
    \begin{cases}
    \Delta W = - \eta \, \nabla_W \, \mathcal{L}(W, \mathbf{b}) \\
    \\
    \Delta \mathbf{b} = - \eta \, \nabla_\mathbf{b} \, \mathcal{L}(W, \mathbf{b}) \\
    \end{cases}
</span></p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Some more advanced linear algebra becomes important to know how to compute these gradients:</p>
<p><a href="https://web.stanford.edu/class/cs224n/readings/gradient-notes.pdf" class="uri">https://web.stanford.edu/class/cs224n/readings/gradient-notes.pdf</a></p>
</div>
</div>
<p>We search the minimum of the mse loss function:</p>
<p><span class="math display">
    \min_{W, \mathbf{b}} \, \mathcal{L}(W, \mathbf{b}) = \mathbb{E}_\mathcal{D} [ ||\mathbf{t} - \mathbf{y}||^2 ] \approx \frac{1}{N} \, \sum_{i=1}^N ||\mathbf{t}_i - \mathbf{y}_i||^2 = \frac{1}{N} \, \sum_{i=1}^N \mathcal{l}_i(W, \mathbf{b})
</span></p>
<p>The individual loss function <span class="math inline">\mathcal{l}_i(W, \mathbf{b})</span> is the squared <span class="math inline">\mathcal{L}^2</span>-norm of the error vector, what can be expressed as a dot product or a vector multiplication:</p>
<p><span class="math display">
    \mathcal{l}_i(W, \mathbf{b}) = ||\mathbf{t}_i - \mathbf{y}_i||^2 = \langle \mathbf{t}_i - \mathbf{y}_i \cdot \mathbf{t}_i - \mathbf{y}_i \rangle = (\mathbf{t}_i - \mathbf{y}_i)^T \times (\mathbf{t}_i - \mathbf{y}_i)
</span></p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Remember:</p>
<p><span class="math display">\mathbf{x}^T \times \mathbf{x} = \begin{bmatrix} x_1 &amp; x_2 &amp; \ldots &amp; x_n \end{bmatrix} \times \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix} = x_1 \, x_1 + x_2 \, x_2 + \ldots + x_n \, x_n = \langle \mathbf{x} \cdot \mathbf{x} \rangle = ||\mathbf{x}||^2_2</span></p>
</div>
</div>
<p>The chain rule tells us in principle that:</p>
<p><span class="math display">\nabla_{W} \, \mathcal{l}_i(W, \mathbf{b}) = \nabla_{\mathbf{y}_i} \, \mathcal{l}_i(W, \mathbf{b}) \times \nabla_{W} \, \mathbf{y}_i</span></p>
<p>The gradient w.r.t the output vector <span class="math inline">\mathbf{y}_i</span> is quite easy to obtain, as it a quadratic function of <span class="math inline">\mathbf{t}_i - \mathbf{y}_i</span>:</p>
<p><span class="math display">\nabla_{\mathbf{y}_i} \, \mathcal{l}_i(W, \mathbf{b}) = \nabla_{\mathbf{y}_i} \, (\mathbf{t}_i - \mathbf{y}_i)^T \times (\mathbf{t}_i - \mathbf{y}_i)</span></p>
<p>The proof relies on product differentiation <span class="math inline">(f\times g)' = f' \, g + f \, g'</span>:</p>
<p><span class="math display">\begin{aligned}
    \nabla_{\mathbf{y}_i} \, (\mathbf{t}_i - \mathbf{y}_i)^T \times (\mathbf{t}_i - \mathbf{y}_i) &amp; = ( \nabla_{\mathbf{y}_i} \, (\mathbf{t}_i - \mathbf{y}_i) ) \times (\mathbf{t}_i - \mathbf{y}_i) + (\mathbf{t}_i - \mathbf{y}_i) \times \nabla_{\mathbf{y}_i} \, (\mathbf{t}_i - \mathbf{y}_i)  \\
    &amp;\\
    &amp;= - (\mathbf{t}_i - \mathbf{y}_i) - (\mathbf{t}_i - \mathbf{y}_i) \\
    &amp;\\
    &amp;= - 2 \, (\mathbf{t}_i - \mathbf{y}_i) \\
\end{aligned}
</span></p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>We use the properties <span class="math inline">\nabla_{\mathbf{x}}\, \mathbf{x}^T \times \mathbf{z} = \mathbf{z}</span> and <span class="math inline">\nabla_{\mathbf{z}} \, \mathbf{x}^T \times \mathbf{z} = \mathbf{x}</span> to get rid of the transpose.</p>
</div>
</div>
<p>The “problem” is when computing <span class="math inline">\nabla_{W} \, \mathbf{y}_i = \nabla_{W} \, (W \times \mathbf{x}_i + \mathbf{b})</span>: * <span class="math inline">\mathbf{y}_i</span> is a vector and <span class="math inline">W</span> a matrix. * <span class="math inline">\nabla_{W} \, \mathbf{y}_i</span> is then a Jacobian (matrix), not a gradient (vector).</p>
<p>Intuitively, differentiating <span class="math inline">W \times \mathbf{x}_i + \mathbf{b}</span> w.r.t <span class="math inline">W</span> should return <span class="math inline">\mathbf{x}_i</span>, but it is a vector, not a matrix…</p>
<p>Actually, only the gradient (or Jacobian) of <span class="math inline">\mathcal{l}_i(W, \mathbf{b})</span> w.r.t <span class="math inline">W</span> should be a matrix of the same size as <span class="math inline">W</span> so that we can apply gradient descent:</p>
<p><span class="math display">\Delta W = - \eta \, \nabla_W \, \mathcal{L}(W, \mathbf{b})</span></p>
<p>We already know that:</p>
<p><span class="math display">\nabla_{W} \, \mathcal{l}_i(W, \mathbf{b}) = - 2\, (\mathbf{t}_i - \mathbf{y}_i) \times \nabla_{W} \, \mathbf{y}_i</span></p>
<p>If <span class="math inline">\mathbf{x}_i</span> has <span class="math inline">n</span> elements and <span class="math inline">\mathbf{y}_i</span> <span class="math inline">m</span> elements, <span class="math inline">W</span> is a <span class="math inline">m \times n</span> matrix.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Remember the outer product between two vectors:</p>
<p><span class="math display">
\mathbf{u} \times \mathbf{v}^\textsf{T} =
  \begin{bmatrix}u_1 \\ u_2 \\ u_3 \\ u_4\end{bmatrix}
    \begin{bmatrix}v_1 &amp; v_2 &amp; v_3\end{bmatrix} =
  \begin{bmatrix}
    u_1v_1 &amp; u_1v_2 &amp; u_1v_3 \\
    u_2v_1 &amp; u_2v_2 &amp; u_2v_3 \\
    u_3v_1 &amp; u_3v_2 &amp; u_3v_3 \\
    u_4v_1 &amp; u_4v_2 &amp; u_4v_3
  \end{bmatrix}.
</span></p>
</div>
</div>
<p>It is easy to see that the outer product between <span class="math inline">(\mathbf{t}_i - \mathbf{y}_i)</span> and <span class="math inline">\mathbf{x}_i</span> gives a <span class="math inline">m \times n</span> matrix:</p>
<p><span class="math display">
    \nabla_W \, \mathcal{l}_i(W, \mathbf{b}) = - 2 \, (\mathbf{t}_i - \mathbf{y}_i) \times \mathbf{x}_i^T\\
</span></p>
<p>Let’s prove it element per element on a small matrix:</p>
<p><span class="math display">
    \mathbf{y} = W \times \mathbf{x} + \mathbf{b}
</span></p>
<p><span class="math display">
    \begin{bmatrix} y_1 \\ y_2 \\\end{bmatrix} = \begin{bmatrix} w_1 &amp; w_2 \\ w_3 &amp; w_4 \\\end{bmatrix} \times \begin{bmatrix} x_1 \\ x_2 \\\end{bmatrix} + \begin{bmatrix} b_1 \\ b_2 \\\end{bmatrix}
</span></p>
<p><span class="math display">
\mathcal{l}(W, \mathbf{b}) = (\mathbf{t} - \mathbf{y})^T \times (\mathbf{t} - \mathbf{y}) = \begin{bmatrix} t_1 - y_1 &amp; t_2 - y_2 \\\end{bmatrix} \times \begin{bmatrix} t_1 - y_1 \\ t_2 - y_2 \\\end{bmatrix} = (t_1 - y_1)^2 + (t_2 - y_2)^2
</span></p>
<p>The Jacobian w.r.t <span class="math inline">W</span> can be explicitly formed using partial derivatives:</p>
<p><span class="math display">
\nabla_W \, \mathcal{l}(W, \mathbf{b}) = \begin{bmatrix}
\dfrac{\partial \mathcal{l}(W, \mathbf{b})}{\partial w_1} &amp; \dfrac{\partial \mathcal{l}(W, \mathbf{b})}{\partial w_2} \\ \dfrac{\partial \mathcal{l}(W, \mathbf{b})}{\partial w_3} &amp; \dfrac{\partial \mathcal{l}(W, \mathbf{b})}{\partial w_4} \\
\end{bmatrix}
= \begin{bmatrix}
-2 \, (t_1 - y_1) \, x_1 &amp; -2 \, (t_1 - y_1) \, x_2 \\ -2 \, (t_2 - y_2) \, x_1 &amp; -2 \, (t_2 - y_2) \, x_2 \\
\end{bmatrix}
</span></p>
<p>We can rearrange this matrix as an outer product:</p>
<p><span class="math display">
\nabla_W \, \mathcal{l}(W, \mathbf{b}) = -2 \, \begin{bmatrix}
t_1 - y_1  \\  t_2 - y_2 \\
\end{bmatrix} \times \begin{bmatrix}
x_1 &amp; x_2 \\
\end{bmatrix}
= - 2 \, (\mathbf{t} - \mathbf{y}) \times \mathbf{x}^T
</span></p>
<p><strong>Multiple linear regression</strong></p>
<ul>
<li>Batch version (<strong>least mean squares</strong>):</li>
</ul>
<p><span class="math display">\begin{cases}
    \Delta W = \eta \, \dfrac{1}{N} \sum_{i=1}^N \, (\mathbf{t}_i - \mathbf{y}_i ) \times \mathbf{x}_i^T \\
    \\
    \Delta \mathbf{b} = \eta \, \dfrac{1}{N} \sum_{i=1}^N \, (\mathbf{t}_i - \mathbf{y}_i) \\
\end{cases}
</span></p>
<ul>
<li>Online version (<strong>delta learning rule</strong>):</li>
</ul>
<p><span class="math display">\begin{cases}
    \Delta W = \eta \, (\mathbf{t}_i - \mathbf{y}_i ) \times \mathbf{x}_i^T \\
    \\
    \Delta \mathbf{b} = \eta \, (\mathbf{t}_i - \mathbf{y}_i) \\
\end{cases}
</span></p>
<p>The matrix-vector notation is completely equivalent to having one learning rule per parameter:</p>
<p><span class="math display">
\begin{cases}
    \Delta w_1 = \eta \, (t_1 - y_1) \, x_1 \\
    \Delta w_2 = \eta \, (t_1 - y_1) \, x_2 \\
    \Delta w_3 = \eta \, (t_2 - y_2) \, x_1 \\
    \Delta w_4 = \eta \, (t_2 - y_2) \, x_2 \\
\end{cases}
\qquad
\begin{cases}
    \Delta b_1 = \eta \, (t_1 - y_1) \\
    \Delta b_2 = \eta \, (t_2 - y_2) \\
\end{cases}
</span></p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The delta learning rule is always of the form: <span class="math inline">\Delta w</span> = eta * error * input. Biases have an input of 1.</p>
</div>
</div>
</section>
<section id="logistic-regression" class="level2">
<h2 class="anchored" data-anchor-id="logistic-regression">Logistic regression</h2>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/fRuzoyV036Y" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
<p>Let’s suppose we want to perform a regression, but where the outputs <span class="math inline">t_i</span> are bounded between 0 and 1. We could use a logistic (or sigmoid) function instead of a linear function in order to transform the input into an output:</p>
<p><span class="math display">
    y = \sigma(w \, x + b )  = \displaystyle\frac{1}{1+\exp(-w \, x - b )}
</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/sigmoid.png" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Logistic or sigmoid function <span class="math inline">\sigma(x)=\displaystyle\frac{1}{1+\exp(-x)}</span>.</figcaption><p></p>
</figure>
</div>
<p>By definition of the logistic function, the prediction <span class="math inline">y</span> will be bounded between 0 and 1, what matches the targets <span class="math inline">t</span>. Let’s now apply gradient descent on the <strong>mse</strong> loss using this new model. The individual loss will be:</p>
<p><span class="math display">l_i(w, b) = (t_i - \sigma(w \, x_i + b) )^2 </span></p>
<p>The partial derivative of the individual loss is easy to find using the chain rule:</p>
<p><span class="math display">
\begin{aligned}
    \displaystyle\frac{\partial l_i(w, b)}{\partial w}
        &amp;= 2 \, (t_i - y_i)  \, \frac{\partial}{\partial w}  (t_i - \sigma(w \, x_i + b ))\\
        &amp;\\
        &amp;= - 2 \, (t_i - y_i) \, \sigma'(w \, x_i + b ) \,  x_i \\
\end{aligned}
</span></p>
<p>The non-linear transfer function <span class="math inline">\sigma(x)</span> therefore adds its derivative into the gradient:</p>
<p><span class="math display">
    \Delta w = \eta \, (t_i - y_i) \, \sigma'(w \, x_i + b ) \, x_i
</span></p>
<p>The logistic function <span class="math inline">\sigma(x)=\frac{1}{1+\exp(-x)}</span> has the nice property that its derivative can be expressed easily:</p>
<p><span class="math display">
    \sigma'(x) = \sigma(x) \, (1 - \sigma(x) )
</span></p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Here is the proof using the fact that the derivative of <span class="math inline">\displaystyle\frac{1}{f(x)}</span> is <span class="math inline">\displaystyle\frac{- f'(x)}{f^2(x)}</span> :</p>
<p><span class="math display">\begin{aligned}
    \sigma'(x) &amp; = \displaystyle\frac{-1}{(1+\exp(-x))^2} \, (- \exp(-x)) \\
    &amp;\\
    &amp;= \frac{1}{1+\exp(-x)} \times \frac{\exp(-x)}{1+\exp(-x)}\\
    &amp;\\
    &amp;= \frac{1}{1+\exp(-x)} \times \frac{1 + \exp(-x) - 1}{1+\exp(-x)}\\
    &amp;\\
    &amp;= \frac{1}{1+\exp(-x)} \times (1 - \frac{1}{1+\exp(-x)})\\
    &amp;\\
    &amp;= \sigma(x) \, (1 - \sigma(x) )\\
\end{aligned}
</span></p>
</div>
</div>
<p>The delta learning rule for the logistic regression model is therefore easy to obtain:</p>
<p><span class="math display">
\begin{cases}
    \Delta w = \eta \, (t_i - y_i) \, y_i \, ( 1 - y_i ) \, x_i \\
\\
    \Delta b = \eta \, (t_i - y_i) \, y_i \, ( 1 - y_i ) \\
\end{cases}
</span></p>
<p><strong>Generalized form of the delta learning rule</strong></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/artificialneuron.svg" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Artificial neuron with multiple inputs.</figcaption><p></p>
</figure>
</div>
<p>For a linear perceptron with parameters <span class="math inline">W</span> and <span class="math inline">\mathbf{b}</span> and any activation function <span class="math inline">f</span>:</p>
<p><span class="math display">
    \mathbf{y} = f(W \times \mathbf{x} + \mathbf{b} )  
</span></p>
<p>and the <strong>mse</strong> loss function:</p>
<p><span class="math display">
    \mathcal{L}(W, \mathbf{b}) = \mathbb{E}_{\mathcal{D}}[||\mathbf{t} - \mathbf{y}||^2]
</span></p>
<p>the <strong>delta learning rule</strong> has the form:</p>
<p><span class="math display">
\begin{cases}
    \Delta W = \eta \, [(\mathbf{t} - \mathbf{y}) \odot f'(W \times \mathbf{x} + \mathbf{b}) ] \times \mathbf{x}^T \\
\\
    \Delta \mathbf{b} = \eta \, (\mathbf{t} - \mathbf{y}) \odot f'(W \times \mathbf{x} + \mathbf{b}) \\
\end{cases}
</span></p>
<p><span class="math inline">\odot</span> denotes element-wise multiplication, i.e.&nbsp;<span class="math inline">(\mathbf{t} - \mathbf{y}) \odot f'(W \times \mathbf{x} + \mathbf{b})</span> is also a vector.</p>
<p>In the linear case, <span class="math inline">f'(x) = 1</span>. One can use any non-linear function, e.g hyperbolic tangent tanh(), ReLU, etc. Transfer functions are chosen for neural networks so that we can compute their derivative easily.</p>
</section>
<section id="polynomial-regression" class="level2">
<h2 class="anchored" data-anchor-id="polynomial-regression">Polynomial regression</h2>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/a6sQgJovhzU" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/polynomialregression.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>The functions underlying real data are rarely linear plus some noise around the ideal value. In the figure above, the input/output function is better modeled by a second-order polynomial:</p>
<p><span class="math display">y = f_{\mathbf{w}, b}(x) = w_1 \, x + w_2 \, x^2 +b</span></p>
<p>We can transform the input into a vector of coordinates:</p>
<p><span class="math display">\mathbf{x} = \begin{bmatrix} x \\ x^2 \\ \end{bmatrix} \qquad \mathbf{w} = \begin{bmatrix} w_1 \\ w_2 \\ \end{bmatrix}</span></p>
<p>The problem becomes:</p>
<p><span class="math display">y = \langle \mathbf{w} . \mathbf{x} \rangle + b = \sum_j w_j \, x_j + b</span></p>
<p>We can simply apply multiple linear regression (MLR) to find <span class="math inline">\mathbf{w}</span> and b:</p>
<p><span class="math display">\begin{cases}
\Delta \mathbf{w} =  \eta \, (t - y) \, \mathbf{x}\\
\\
\Delta b =  \eta \, (t - y)\\
\end{cases}</span></p>
<p>This generalizes to polynomials of any order <span class="math inline">p</span>:</p>
<p><span class="math display">y = f_{\mathbf{w}, b}(x) = w_1 \, x + w_2 \, x^2 + \ldots + w_p \, x^p + b</span></p>
<p>We create a vector of powers of <span class="math inline">x</span>:</p>
<p><span class="math display">\mathbf{x} = \begin{bmatrix} x \\ x^2 \\ \ldots \\ x^p \end{bmatrix} \qquad \mathbf{w} = \begin{bmatrix} w_1 \\ w_2 \\ \ldots \\ w_p \end{bmatrix}</span></p>
<p>ad apply multiple linear regression (MLR) to find <span class="math inline">\mathbf{w}</span> and b:</p>
<p><span class="math display">\begin{cases}
\Delta \mathbf{w} =  \eta \, (t - y) \, \mathbf{x}\\
\\
\Delta b =  \eta \, (t - y)\\
\end{cases}
</span></p>
<p>Non-linear problem solved! The only unknown is which order for the polynomial matches best the data. One can perform regression with any kind of parameterized function using gradient descent.</p>
</section>
<section id="a-bit-of-learning-theory" class="level2">
<h2 class="anchored" data-anchor-id="a-bit-of-learning-theory">A bit of learning theory</h2>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/QbvCJNfeXbE" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
<p>Before going further, let’s think about what we have been doing so far. We had a bunch of data samples <span class="math inline">\mathcal{D} = (\mathbf{x}_i, t_i)_{i=1..N}</span> (the <strong>training set</strong>). We decided to apply a (linear) model on it:</p>
<p><span class="math display">y_i = \langle \mathbf{w} . \mathbf{x}_i \rangle + b</span></p>
<p>We then minimized the mean square error (mse) on that training set using gradient descent:</p>
<p><span class="math display">
    \mathcal{L}(w, b) = \mathbb{E}_{\mathbf{x}, t \in \mathcal{D}} [(t_i - y_i )^2]
</span></p>
<p>At the end of learning, we can measure the <strong>residual error</strong> of the model on the data:</p>
<p><span class="math display">
    \epsilon_\mathcal{D} = \frac{1}{N} \, \sum_{i=1}^{N} (t_i - y_i )^2
</span></p>
<p>We get a number, for example 0.04567. Is that good?</p>
<p>The <strong>mean square error</strong> mse is not very informative, as its value depends on how the outputs are scaled: multiply the targets and prediction by 10 and the mse is 100 times higher.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/regression-animation-mse-dual.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">The residual error measures the quality of the fit, but it is sensible to the scaling of the outputs.</figcaption><p></p>
</figure>
</div>
<p>The <strong>coefficient of determination</strong> <span class="math inline">R^2</span> is a rescaled variant of the mse comparing the variance of the residuals to the variance of the data around its mean <span class="math inline">\hat{t}</span>:</p>
<p><span class="math display">
    R^2 = 1 - \frac{\text{Var}(\text{residuals})}{\text{Var}(\text{data})} = 1 - \frac{\sum_{i=1}^N (t_i- y_i)^2}{\sum_{i=1}^N (t_i - \hat{t})^2}
</span></p>
<p><span class="math inline">R^2</span> should be as close from 1 as possible. For example, if <span class="math inline">R^2 = 0.8</span>, we can say that the <strong>model explains 80% of the variance of the data</strong>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/r2.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">The coefficient of determination compares the variance of the residuals to the variance of the data. Source: <a href="https://towardsdatascience.com/introduction-to-linear-regression-in-python-c12a072bedf0" class="uri">https://towardsdatascience.com/introduction-to-linear-regression-in-python-c12a072bedf0</a></figcaption><p></p>
</figure>
</div>
<section id="sensibility-to-outliers" class="level3">
<h3 class="anchored" data-anchor-id="sensibility-to-outliers">Sensibility to outliers</h3>
<p>Suppose we have a training set with one <strong>outlier</strong> (bad measurement, bad luck, etc).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/regression-outlier.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Linear data with one outlier.</figcaption><p></p>
</figure>
</div>
<p>LMS would find the minimum of the mse, but it is clearly a bad fit for most points.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/regression-outlier-fit.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">LMS is attracted by the outlier, leading to a bad prediction for all points.</figcaption><p></p>
</figure>
</div>
<p>This model feels much better, but its residual mse is actually higher…</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/regression-outlier-fit-corrected.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">By ignoring the outlier, the prediction would be correct for most points.</figcaption><p></p>
</figure>
</div>
<p>Let’s visualize polynomial regression with various orders of the polynomial on a small dataset.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/polynomialregression-animation.gif" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Polynomial regression with various orders.</figcaption><p></p>
</figure>
</div>
<p>When only looking at the residual mse on the training data, one could think that the higher the order of the polynomial, the better. But it is obvious that the interpolation quickly becomes very bad when the order is too high. A <strong>complex</strong> model (with a lot of parameters) is useless for predicting new values. We actually do <strong>not</strong> care about the error on the training set, but about <strong>generalization</strong>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/polynomialregression-mse.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Residual mse of polynomial regression depending on the order of the polynomial.</figcaption><p></p>
</figure>
</div>
</section>
<section id="cross-validation" class="level3">
<h3 class="anchored" data-anchor-id="cross-validation">Cross-validation</h3>
<p>Let’s suppose we dispose of <span class="math inline">m</span> models <span class="math inline">\mathcal{M} = \{ M_1, ..., M_m\}</span> that could be used to fit (or classify) some data <span class="math inline">\mathcal{D} = \{\mathbf{x}_i, t_i\}_{i=1}^N</span>. Such a class could be the ensemble of polynomes with different orders, different algorithms (NN, SVM) or the same algorithm with different values for the hyperparameters (learning rate, regularization parameters…).</p>
<p>The naive and <strong>wrong</strong> method to find the best hypothesis would be:</p>
<div class="callout-warning callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Do not do this!
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>For all models <span class="math inline">M_i</span>:</p>
<ul>
<li><p>Train <span class="math inline">M_i</span> on <span class="math inline">\mathcal{D}</span> to obtain an hypothesis <span class="math inline">h_i</span>.</p></li>
<li><p>Compute the training error <span class="math inline">\epsilon_\mathcal{D}(h_i)</span> of <span class="math inline">h_i</span> on <span class="math inline">\mathcal{D}</span> :</p></li>
</ul>
<p><span class="math display">
      \epsilon_\mathcal{D}(h_i) =  \mathbb{E}_{(\mathbf{x}, t) \in \mathcal{D}} [(h_i(\mathbf{x}) - t)^2]
  </span></p></li>
<li><p>Select the hypothesis <span class="math inline">h_{i}^*</span> with the minimal training error : <span class="math inline">h_{i}^* = \text{argmin}_{h_i \in \mathcal{M}} \quad \epsilon_\mathcal{D}(h_i)</span></p></li>
</ul>
</div>
</div>
<p>This method leads to <strong>overfitting</strong>, as only the training error is used.</p>
<p>The solution is randomly take some samples out of the training set to form the <strong>test set</strong>. Typical values are 20 or 30 % of the samples in the test set.</p>
<ol type="1">
<li>Train the model on the training set (70% of the data).</li>
<li>Test the performance of the model on the test set (30% of the data).</li>
</ol>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/polynomialregression-traintest.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Polynomial data split in a training set and a test set.</figcaption><p></p>
</figure>
</div>
<p>The test performance will better measure how well the model generalizes to new examples.</p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Simple hold-out cross-validation
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Split the training data <span class="math inline">\mathcal{D}</span> into <span class="math inline">\mathcal{S}_{\text{train}}</span> and <span class="math inline">\mathcal{S}_{\text{test}}</span>.</p></li>
<li><p>For all models <span class="math inline">M_i</span>:</p>
<ul>
<li><p>Train <span class="math inline">M_i</span> on <span class="math inline">\mathcal{S}_{\text{train}}</span> to obtain an hypothesis <span class="math inline">h_i</span>.</p></li>
<li><p>Compute the empirical error <span class="math inline">\epsilon_{\text{test}}(h_i)</span> of <span class="math inline">h_i</span> on <span class="math inline">\mathcal{S}_{\text{test}}</span> :</p></li>
</ul>
<p><span class="math display">\epsilon_{\text{test}}(h_i) = \mathbb{E}_{(\mathbf{x}, t) \in  \mathcal{S}_{\text{test}}} [(h_i(\mathbf{x}) - t)^2]</span></p></li>
<li><p>Select the hypothesis <span class="math inline">h_{i}^*</span> with the minimal empirical error : <span class="math inline">h_{i}^* = \text{argmin}_{h_i \in \mathcal{M}} \quad \epsilon_{\text{test}}(h_i)</span></p></li>
</ul>
</div>
</div>
<p>The disadvantage of <strong>simple hold-out cross-validation</strong> is that 20 or 30% of the data is wasted and not used for learning. It may be a problem when data is rare or expensive.</p>
<p><strong>k-fold cross-validation</strong> allows a more efficient use os the available data and a better measure of the generalization error. The idea is to build several different training/test sets with the same data, train and test each model repeatedly on each partition and choose the hypothesis that works best on average.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/kfold.jpg" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">k-fold cross-validation. Source <a href="https://upload.wikimedia.org/wikipedia/commons/1/1c/K-fold_cross_validation_EN.jpg" class="uri">https://upload.wikimedia.org/wikipedia/commons/1/1c/K-fold_cross_validation_EN.jpg</a></figcaption><p></p>
</figure>
</div>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
k-fold cross-validation
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Randomly split the data <span class="math inline">\mathcal{D}</span> into <span class="math inline">k</span> subsets of <span class="math inline">\frac{N}{k}</span> examples <span class="math inline">\{ \mathcal{S}_{1}, \dots , \mathcal{S}_{k}\}</span></p></li>
<li><p>For all models <span class="math inline">M_i</span>:</p>
<ul>
<li><p>For all <span class="math inline">k</span> subsets <span class="math inline">\mathcal{S}_j</span>:</p>
<ul>
<li><p>Train <span class="math inline">M_i</span> on <span class="math inline">\mathcal{D} - \mathcal{S}_j</span> to obtain an hypothesis <span class="math inline">h_{ij}</span></p></li>
<li><p>Compute the empirical error <span class="math inline">\epsilon_{\mathcal{S}_j}(h_{ij})</span> of <span class="math inline">h_{ij}</span> on <span class="math inline">\mathcal{S}_j</span></p></li>
</ul></li>
<li><p>The empirical error of the model <span class="math inline">M_i</span> on <span class="math inline">\mathcal{D}</span> is the average of empirical errors made on <span class="math inline">(\mathcal{S}_j)_{j=1}^{k}</span></p>
<p><span class="math display">
      \epsilon_{\mathcal{D}} (M_i) = \frac{1}{k} \cdot \sum_{j=1}^{k} \epsilon_{\mathcal{S}_j}(h_{ij})
  </span></p></li>
</ul></li>
<li><p>Select the model <span class="math inline">M_{i}^*</span> with the minimal empirical error on <span class="math inline">\mathcal{D}</span>.</p></li>
</ul>
</div>
</div>
<p>In general, you can take <span class="math inline">k=10</span> partitions. The extreme case is to take <span class="math inline">k=N</span> partition, i.e.&nbsp;the test set has only one sample each time: <strong>leave-one-out cross-validation</strong>. k-fold cross-validation works well, but needs a lot of repeated learning.</p>
</section>
<section id="underfitting---overfitting" class="level3">
<h3 class="anchored" data-anchor-id="underfitting---overfitting">Underfitting - overfitting</h3>
<p>While the training mse always decrease with more complex models, the test mse increases after a while. This is called <strong>overfitting</strong>: learning by heart the data without caring about generalization. The two curves suggest that we should chose a polynomial order between 2 and 9.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/polynomialregression-mse-traintest.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Training and test mse of polynomial regression.</figcaption><p></p>
</figure>
</div>
<p>A model not complex enough for the data will <strong>underfit</strong>: its training error is high. A model too complex for the data will <strong>overfit</strong>: its test error is high. In between, there is the right complexity for the model: it learns the data correctly but does not overfit.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/underfitting-overfitting.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Underfitting and overfitting.</figcaption><p></p>
</figure>
</div>
<p>What does complexity mean? In polynomial regression, the complexity is related to the order of the polynomial, i.e.&nbsp;the number of coefficients to estimate:</p>
<p><span class="math display">y = f_{\mathbf{w}, b}(x) = \sum_{k=1}^p w_k \, x^k + b</span></p>
<p><span class="math display">\mathbf{x} = \begin{bmatrix} x \\ x^2 \\ \ldots \\ x^p \end{bmatrix} \qquad \mathbf{w} = \begin{bmatrix} w_1 \\ w_2 \\ \ldots \\ w_p \end{bmatrix}</span></p>
<p>A polynomial of order <span class="math inline">p</span> has <span class="math inline">p+1</span> unknown parameters (<strong>free parameters</strong>): the <span class="math inline">p</span> weights and the bias. Generally, the <strong>complexity of a model</strong> relates to its <strong>number of free parameters</strong>:</p>
<blockquote class="blockquote">
<p><strong>The more free parameters, the more complex the model is, the more likely it will overfit.</strong></p>
</blockquote>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Bias-variance trade-off
</div>
</div>
<div class="callout-body-container callout-body">
<p>Under-/Over-fitting relates to the statistical concept of <strong>bias-variance trade-off</strong>. The <strong>bias</strong> is the training error that the hypothesis would make if the training set was infinite (accuracy, flexibility of the model): a model with high bias is underfitting. The <strong>variance</strong> is the error that will be made by the hypothesis on new examples taken from the same distribution (spread, the model is correct on average, but not for individual samples): a model with high variance is overfitting.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/biasvariance3.png" class="img-fluid figure-img" style="width:70.0%"></p>
<p></p><figcaption class="figure-caption">Bias and variance of an estimator. Source: <a href="http://scott.fortmann-roe.com/docs/BiasVariance.html" class="uri">http://scott.fortmann-roe.com/docs/BiasVariance.html</a></figcaption><p></p>
</figure>
</div>
<p>The bias decreases when the model becomes complex; the variance increases when the model becomes complex. The <strong>generalization error</strong> is a combination of the bias and variance:</p>
<p><span class="math display">
    \text{generalization error} = \text{bias}^2 + \text{variance}
</span></p>
<p>We search for the model with the <strong>optimum complexity</strong> realizing the trade-off between bias and variance. It is better to have a model with a slightly higher bias (training error) but with a smaller variance (generalization error).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/biasvariance2.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">The optimal complexity of an algorithm is a trade-off between bias and variance. Source: <a href="http://scott.fortmann-roe.com/docs/BiasVariance.html" class="uri">http://scott.fortmann-roe.com/docs/BiasVariance.html</a></figcaption><p></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="regularized-regression" class="level2">
<h2 class="anchored" data-anchor-id="regularized-regression">Regularized regression</h2>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/6R46KLgfw5s" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
<p>Linear regression can either underfit or overfit depending on the data.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/underfitting-overfitting-linear.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Linear regression underfits non-linear data.</figcaption><p></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/regression-outlier-fit.png" class="img-fluid figure-img" style="width:80.0%"></p>
<p></p><figcaption class="figure-caption">Linear regression overfits outliers.</figcaption><p></p>
</figure>
</div>
<p>When linear regression <strong>underfits</strong> (both training and test errors are high), the data is not linear: we need to use a <strong>neural network</strong>. When linear regression <strong>overfits</strong> (the test error is higher than the training error), we would like to <strong>decrease its complexity</strong>.</p>
<p>The problem is that the number of free parameters in linear regression only depends on the number of inputs (dimensions of the input space).</p>
<p><span class="math display">
    y = \sum_{i=1}^d w_i \, x_i + b
</span></p>
<p>For <span class="math inline">d</span> inputs, there are <span class="math inline">d+1</span> free parameters: the <span class="math inline">d</span> weights and the bias.</p>
<p>We must find a way to reduce the complexity of the linear regression without changing the number of parameters, which is impossible. The solution is to <strong>constrain</strong> the values that the parameters can take: <strong>regularization</strong>. Regularization reduces the variance at the cost of increasing the bias.</p>
<section id="l2-regularization---ridge-regression" class="level3">
<h3 class="anchored" data-anchor-id="l2-regularization---ridge-regression">L2 regularization - Ridge regression</h3>
<p>Using <strong>L2 regularization</strong> for linear regression leads to the <strong>Ridge regression</strong> algorithm. The individual loss function is defined as:</p>
<p><span class="math display">
    \mathcal{l}_i(\mathbf{w}, b) = (t_i - y_i)^2 + \lambda \, ||\mathbf{w}||^2
</span></p>
<p>The first part of the loss function is the classical <strong>mse</strong> on the training set: its role is to reduce the <strong>bias</strong>. The second part minimizes the L2 norm of the weight vector (or matrix), reducing the variance:</p>
<p><span class="math display">
    ||\mathbf{w}||^2 = \sum_{i=1}^d w_i^2
</span></p>
<p>Deriving the regularized delta learning rule is straightforward:</p>
<p><span class="math display">
    \Delta w_i = \eta \, ((t_i - y_i) \ x_i - \lambda \, w_i)
</span></p>
<p>Ridge regression is also called <strong>weight decay</strong>: even if there is no error, all weights will decay to 0.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/ridge-effect.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Ridge regression finds the smallest value for the weights that minimize the mse. Source: <a href="https://www.mlalgorithms.org/articles/l1-l2-regression/" class="uri">https://www.mlalgorithms.org/articles/l1-l2-regression/</a></figcaption><p></p>
</figure>
</div>
</section>
<section id="l1-regularization---lasso-regression" class="level3">
<h3 class="anchored" data-anchor-id="l1-regularization---lasso-regression">L1 regularization - LASSO regression</h3>
<p>Using <strong>L1 regularization</strong> for linear regression leads to the <strong>LASSO regression</strong> algorithm (least absolute shrinkage and selection operator). The individual loss function is defined as:</p>
<p><span class="math display">
    \mathcal{l}_i(\mathbf{w}, b) =  (t_i - y_i)^2 + \lambda \, |\mathbf{w}|
</span></p>
<p>The second part minimizes this time the L1 norm of the weight vector, i.e.&nbsp;its absolute value:</p>
<p><span class="math display">
    |\mathbf{w}| = \sum_{i=1}^d |w_i|
</span></p>
<p>Regularized delta learning rule with LASSO:</p>
<p><span class="math display">
    \Delta w_i = \eta \, ((t_i - y_i) \ x_i - \lambda \, \text{sign}(w_i))
</span></p>
<p><strong>Weight decay</strong> does not depend on the value of the weight, only its sign. Weights can decay very fast to 0.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/lasso-effect.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">LASSO regression tries to set as many weight to 0 as possible (sparse code). Source: <a href="https://www.mlalgorithms.org/articles/l1-l2-regression/" class="uri">https://www.mlalgorithms.org/articles/l1-l2-regression/</a></figcaption><p></p>
</figure>
</div>
<p>Both methods depend on the <strong>regularization parameter</strong> <span class="math inline">\lambda</span>. Its value determines how important the regularization term should. Regularization introduce a <strong>bias</strong>, as the solution found is <strong>not</strong> the minimum of the mse, but reduces the variance of the estimation, as small weights are less sensible to noise.</p>
<p>LASSO allows <strong>feature selection</strong>: features with a zero weight can be removed from the training set.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/linearregression-withoutregularization.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Linear regression tends to assign values to all weights. Source: <a href="https://www.analyticsvidhya.com/blog/2017/06/a-comprehensive-guide-for-linear-ridge-and-lasso-regression/" class="uri">https://www.analyticsvidhya.com/blog/2017/06/a-comprehensive-guide-for-linear-ridge-and-lasso-regression/</a></figcaption><p></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/linearregression-withregularization.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">LASSO regression tries to set as many weights to 0 as possible (sparse code). Source: <a href="https://www.analyticsvidhya.com/blog/2017/06/a-comprehensive-guide-for-linear-ridge-and-lasso-regression/" class="uri">https://www.analyticsvidhya.com/blog/2017/06/a-comprehensive-guide-for-linear-ridge-and-lasso-regression/</a></figcaption><p></p>
</figure>
</div>
</section>
<section id="l1l2-regularization---elasticnet" class="level3">
<h3 class="anchored" data-anchor-id="l1l2-regularization---elasticnet">L1+L2 regularization - ElasticNet</h3>
<p>An <strong>ElasticNet</strong> is a linear regression using both L1 and L2 regression:</p>
<p><span class="math display">
    \mathcal{l}_i(\mathbf{w}, b) =  (t_i - y_i)^2 + \lambda_1 \, |\mathbf{w}| + \lambda_2 \, ||\mathbf{w}||^2
</span></p>
<p>It combines the advantages of Ridge and LASSO, at the cost of having now two regularization parameters to determine.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation column-body">
  <div class="nav-page nav-page-previous">
      <a href="../notes/2.1-Optimization.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-title">Optimization</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../notes/2.3-LinearClassification.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-title">Linear classification</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
      <div class="nav-footer-center">Copyright 2022, Julien Vitay - <a href="mailto:julien.vitay@informatik.tu-chemnitz.de" class="email">julien.vitay@informatik.tu-chemnitz.de</a></div>
  </div>
</footer>



<script src="../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>