<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.251">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Neurocomputing - 6&nbsp; Linear classification</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../notes/2.4-LearningTheory.html" rel="next">
<link href="../notes/2.2-LinearRegression.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-title">Linear classification</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="../" class="sidebar-logo-link">
      <img src="../../slides/img/tuc-new.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Neurocomputing</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">Overview</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">Introduction</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/1.1-Introduction.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/1.2-Math.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Math basics (optional)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/1.3-Neurons.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Neurons</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">Linear algorithms</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.1-Optimization.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Optimization</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.2-LinearRegression.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Linear regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.3-LinearClassification.html" class="sidebar-item-text sidebar-link active"><span class="chapter-title">Linear classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.4-LearningTheory.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Learning theory</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">Neural networks</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.1-NeuralNetworks.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Multi-layer perceptron</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.2-DNN.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Modern neural networks</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">Computer Vision</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/4.1-CNN.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Convolutional neural networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/4.2-ObjectDetection.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Object detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/4.3-SemanticSegmentation.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Semantic segmentation</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">Generative modeling</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/5.1-Autoencoders.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Autoencoders</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/5.2-RBM.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Restricted Boltzmann machines (optional)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/5.3-GAN.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Generative adversarial networks</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true">Recurrent neural networks</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/6.1-RNN.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Recurrent neural networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/6.2-NLP.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Natural Language Processing</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/6.3-Attention.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Attentional neural networks</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="true">Self-supervised learning</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/7.1-Transformers.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Transformers</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/7.2-ContrastiveLearning.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Contrastive Learning</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" aria-expanded="true">Outlook</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/8.1-Limits.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Limits of deep learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/8.2-Beyond.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Beyond deep Learning</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" aria-expanded="true">Exercises</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/Content.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">List of exercises</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/1-Python-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Introduction To Python</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/2-Numpy-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Numpy and Matplotlib</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/3-LinearRegression-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Linear regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/4-MLR-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Multiple linear regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/5-Crossvalidation-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Cross-validation and polynomial regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/6-LinearClassification-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Linear classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/7-SoftmaxClassifier-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Softmax classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/8-MLP-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Multi-layer Perceptron</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/9-MNIST-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">MNIST classification using keras</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/10-CNN-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Convolutional neural networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/11-TransferLearning-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Transfer learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/12-VAE-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Variational autoencoder</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/13-RNN-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Recurrent neural networks</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#hard-linear-classification" id="toc-hard-linear-classification" class="nav-link active" data-scroll-target="#hard-linear-classification">Hard linear classification</a>
  <ul class="collapse">
  <li><a href="#perceptron-algorithm" id="toc-perceptron-algorithm" class="nav-link" data-scroll-target="#perceptron-algorithm">Perceptron algorithm</a></li>
  <li><a href="#stochastic-gradient-descent" id="toc-stochastic-gradient-descent" class="nav-link" data-scroll-target="#stochastic-gradient-descent">Stochastic Gradient descent</a></li>
  </ul></li>
  <li><a href="#maximum-likelihood-estimation" id="toc-maximum-likelihood-estimation" class="nav-link" data-scroll-target="#maximum-likelihood-estimation">Maximum Likelihood Estimation</a></li>
  <li><a href="#soft-linear-classification-logistic-regression" id="toc-soft-linear-classification-logistic-regression" class="nav-link" data-scroll-target="#soft-linear-classification-logistic-regression">Soft linear classification : Logistic regression</a></li>
  <li><a href="#multi-class-classification" id="toc-multi-class-classification" class="nav-link" data-scroll-target="#multi-class-classification">Multi-class classification</a>
  <ul class="collapse">
  <li><a href="#softmax-linear-classifier" id="toc-softmax-linear-classifier" class="nav-link" data-scroll-target="#softmax-linear-classifier">Softmax linear classifier</a></li>
  <li><a href="#comparison-of-linear-classification-and-regression" id="toc-comparison-of-linear-classification-and-regression" class="nav-link" data-scroll-target="#comparison-of-linear-classification-and-regression">Comparison of linear classification and regression</a></li>
  <li><a href="#multi-label-classification" id="toc-multi-label-classification" class="nav-link" data-scroll-target="#multi-label-classification">Multi-label classification</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-body" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-title">Linear classification</span></h1>
</div>



<div class="quarto-title-meta">

    
    
  </div>
  

</header>

<p>Slides: <a href="../slides/2.3-LinearClassification.html" target="_blank">html</a> <a href="../slides/pdf/2.3-LinearClassification.pdf" target="_blank">pdf</a></p>
<section id="hard-linear-classification" class="level2">
<h2 class="anchored" data-anchor-id="hard-linear-classification">Hard linear classification</h2>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/jZoJHIHi6Qw" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
<p>The training data <span class="math inline">\mathcal{D}</span> is composed of <span class="math inline">N</span> examples <span class="math inline">(\mathbf{x}_i, t_i)_{i=1..N}</span> , with a d-dimensional input vector <span class="math inline">\mathbf{x}_i \in \Re^d</span> and a binary output <span class="math inline">t_i \in \{-1, +1\}</span>. The data points where <span class="math inline">t = + 1</span> are called the <strong>positive class</strong>, the other the <strong>negative class</strong>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/classification-animation1.png" class="img-fluid figure-img" style="width:80.0%"></p>
<p></p><figcaption class="figure-caption">Binary linear classification of 2D data.</figcaption><p></p>
</figure>
</div>
<p>For example, the inputs <span class="math inline">\mathbf{x}_i</span> can be images (one dimension per pixel) and the positive class corresponds to cats (<span class="math inline">t_i = +1</span>), the negative class to dogs (<span class="math inline">t_i = -1</span>).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/cats-dogs.jpg" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Binary linear classification of cats vs.&nbsp;dogs images. Source: <a href="http://adilmoujahid.com/posts/2016/06/introduction-deep-learning-python-caffe" class="uri">http://adilmoujahid.com/posts/2016/06/introduction-deep-learning-python-caffe</a></figcaption><p></p>
</figure>
</div>
<p>We want to find the hyperplane <span class="math inline">(\mathbf{w}, b)</span> of <span class="math inline">\Re^d</span> that correctly separates the two classes.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/classification-animation2.png" class="img-fluid figure-img" style="width:80.0%"></p>
<p></p><figcaption class="figure-caption">The hyperplane separates the input space into two regions.</figcaption><p></p>
</figure>
</div>
<p>For a point <span class="math inline">\mathbf{x} \in \mathcal{D}</span>, <span class="math inline">\langle \mathbf{w} \cdot \mathbf{x} \rangle +b</span> is the projection of <span class="math inline">\mathbf{x}</span> onto the hyperplane <span class="math inline">(\mathbf{w}, b)</span>.</p>
<ul>
<li><p>If <span class="math inline">\langle \mathbf{w} \cdot \mathbf{x} \rangle +b &gt; 0</span>, the point is above the hyperplane.</p></li>
<li><p>If <span class="math inline">\langle \mathbf{w} \cdot \mathbf{x} \rangle +b &lt; 0</span>, the point is below the hyperplane.</p></li>
<li><p>If <span class="math inline">\langle \mathbf{w} \cdot \mathbf{x} \rangle +b = 0</span>, the point is on the hyperplane.</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/projection.svg" class="img-fluid figure-img" style="width:80.0%"></p>
<p></p><figcaption class="figure-caption">Projection on an hyperplane.</figcaption><p></p>
</figure>
</div>
<p>By looking at the <strong>sign</strong> of <span class="math inline">\langle \mathbf{w} \cdot \mathbf{x} \rangle +b</span>, we can predict the class of the input.</p>
<p><span class="math display">\text{sign}(\langle \mathbf{w} \cdot \mathbf{x} \rangle +b) = \begin{cases} +1 \; \text{if} \; \langle \mathbf{w} \cdot \mathbf{x} \rangle +b \geq 0 \\ -1 \; \text{if} \; \langle \mathbf{w} \cdot \mathbf{x} \rangle +b &lt; 0 \\ \end{cases}</span></p>
<p>Binary linear classification can therefore be made by a single <strong>artificial neuron</strong> using the sign transfer function.</p>
<p><span class="math display">
y = f_{\mathbf{w}, b} (\mathbf{x}) = \text{sign} ( \langle \mathbf{w} \cdot \mathbf{x} \rangle +b )  = \text{sign} ( \sum_{j=1}^d w_j \, x_j +b )
</span></p>
<p><span class="math inline">\mathbf{w}</span> is the weight vector and <span class="math inline">b</span> is the bias.</p>
<p>Linear classification is the process of finding an hyperplane <span class="math inline">(\mathbf{w}, b)</span> that correctly separates the two classes. If such an hyperplane can be found, the training set is said <strong>linearly separable</strong>. Otherwise, the problem is <strong>non-linearly separable</strong> and other methods have to be applied (MLP, SVM…).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/linearlyseparable.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Linearly and non-linearly separable datasets.</figcaption><p></p>
</figure>
</div>
<section id="perceptron-algorithm" class="level3">
<h3 class="anchored" data-anchor-id="perceptron-algorithm">Perceptron algorithm</h3>
<p>The Perceptron algorithm tries to find the weights and biases minimizing the <strong>mean square error</strong> (<em>mse</em>) or <strong>quadratic loss</strong>:</p>
<p><span class="math display">\mathcal{L}(\mathbf{w}, b) = \mathbb{E}_\mathcal{D} [(t_i - y_i)^2] \approx \frac{1}{N} \, \sum_{i=1}^{N} (t_i - y_i)^2</span></p>
<p>When the prediction <span class="math inline">y_i</span> is the same as the data <span class="math inline">t_i</span> for all examples in the training set (perfect classification), the mse is minimal and equal to 0. We can apply gradient descent to find this minimum.</p>
<p><span class="math display">
\begin{cases}
    \Delta \mathbf{w} = - \eta \, \nabla_\mathbf{w} \, \mathcal{L}(\mathbf{w}, b)\\
    \\
    \Delta b = - \eta \, \nabla_b \, \mathcal{L}(\mathbf{w}, b)\\
\end{cases}
</span></p>
<p>Let’s search for the partial derivative of the quadratic error function with respect to the weight vector:</p>
<p><span class="math display">
    \nabla_\mathbf{w} \, \mathcal{L}(\mathbf{w}, b) = \nabla_\mathbf{w} \,  \frac{1}{N} \, \sum_{i=1}^{N} (t_i - y_i )^2 = \frac{1}{N} \, \sum_{i=1}^{N} \nabla_\mathbf{w} \,  (t_i - y_i )^2 = \frac{1}{N} \, \sum_{i=1}^{N} \nabla_\mathbf{w} \,  \mathcal{l}_i (\mathbf{w}, b)
</span></p>
<p>Everything is similar to linear regression until we get:</p>
<p><span class="math display">
    \nabla_\mathbf{w} \,  \mathcal{l}_i (\mathbf{w}, b) = - 2 \, (t_i - y_i) \, \nabla_\mathbf{w} \, \text{sign}( \langle \mathbf{w} \cdot \mathbf{x}_i \rangle +b)
</span></p>
<p>In order to continue with the chain rule, we would need to differentiate <span class="math inline">\text{sign}(x)</span>.</p>
<p><span class="math display">
    \nabla_\mathbf{w} \,  \mathcal{l}_i (\mathbf{w}, b) = - 2 \, (t_i - y_i) \, \text{sign}'( \langle \mathbf{w} \cdot \mathbf{x}_i \rangle +b) \,  \mathbf{x}_i
</span></p>
<p>But the sign function is <strong>not</strong> differentiable… We will simply pretend that the sign() function is linear, with a derivative of 1:</p>
<p><span class="math display">
    \nabla_\mathbf{w} \,  \mathcal{l}_i (\mathbf{w}, b) = - 2 \, (t_i - y_i) \,   \mathbf{x}_i
</span></p>
<p>The update rule for the weight vector <span class="math inline">\mathbf{w}</span> and the bias <span class="math inline">b</span> is therefore the same as in linear regression:</p>
<p><span class="math display">
    \Delta \mathbf{w} =  \eta \, \frac{1}{N} \, \sum_{i=1}^{N} (t_i - y_i) \, \mathbf{x}_i
</span></p>
<p><span class="math display">
    \Delta b = \eta \, \frac{1}{N} \, \sum_{i=1}^{N} (t_i - y_i )
</span></p>
<p>By applying gradient descent on the quadratic error function, one obtains the following algorithm:</p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Batch perceptron
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p><strong>for</strong> <span class="math inline">M</span> epochs:</p>
<ul>
<li><p><span class="math inline">\mathbf{dw} = 0 \qquad db = 0</span></p></li>
<li><p><strong>for</strong> each sample <span class="math inline">(\mathbf{x}_i, t_i)</span>:</p>
<ul>
<li><p><span class="math inline">y_i = \text{sign}( \langle \mathbf{w} \cdot \mathbf{x}_i \rangle + b)</span></p></li>
<li><p><span class="math inline">\mathbf{dw} = \mathbf{dw} + (t_i - y_i) \, \mathbf{x}_i</span></p></li>
<li><p><span class="math inline">db = db + (t_i - y_i)</span></p></li>
</ul></li>
<li><p><span class="math inline">\Delta \mathbf{w} = \eta \, \frac{1}{N} \, \mathbf{dw}</span></p></li>
<li><p><span class="math inline">\Delta b = \eta \, \frac{1}{N} \, db</span></p></li>
</ul></li>
</ul>
</div>
</div>
<p>This is called the <strong>batch</strong> version of the Perceptron algorithm. If the data is linearly separable and <span class="math inline">\eta</span> is well chosen, it converges to the minimum of the mean square error.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/classification-animation.gif" class="img-fluid figure-img" style="width:80.0%"></p>
<p></p><figcaption class="figure-caption">Batch perceptron algorithm.</figcaption><p></p>
</figure>
</div>
<p>The <strong>Perceptron algorithm</strong> was invented by the psychologist Frank Rosenblatt in 1958. It was the first algorithmic neural network able to learn linear classification.</p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Online perceptron algorithm
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p><strong>for</strong> <span class="math inline">M</span> epochs:</p>
<ul>
<li><p><strong>for</strong> each sample <span class="math inline">(\mathbf{x}_i, t_i)</span>:</p>
<ul>
<li><p><span class="math inline">y_i = \text{sign}( \langle \mathbf{w} \cdot \mathbf{x}_i \rangle + b)</span></p></li>
<li><p><span class="math inline">\Delta \mathbf{w} = \eta \, (t_i - y_i) \, \mathbf{x}_i</span></p></li>
<li><p><span class="math inline">\Delta b = \eta \, (t_i - y_i)</span></p></li>
</ul></li>
</ul></li>
</ul>
</div>
</div>
<p>This algorithm iterates over all examples of the training set and applies the <strong>delta learning rule</strong> to each of them immediately, not at the end on the whole training set. One could check whether there are still classification errors on the training set at the end of each epoch and stop the algorithm. The delta learning rule depends as always on the learning rate <span class="math inline">\eta</span>, the error made by the prediction (<span class="math inline">t_i - y_i</span>) and the input <span class="math inline">\mathbf{x}_i</span>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/classification-animation-online.gif" class="img-fluid figure-img" style="width:80.0%"></p>
<p></p><figcaption class="figure-caption">Online perceptron algorithm.</figcaption><p></p>
</figure>
</div>
</section>
<section id="stochastic-gradient-descent" class="level3">
<h3 class="anchored" data-anchor-id="stochastic-gradient-descent">Stochastic Gradient descent</h3>
<p>The mean square error is defined as the <strong>expectation</strong> over the data:</p>
<p><span class="math display">\mathcal{L}(\mathbf{w}, b) = \mathbb{E}_\mathcal{D} [(t_i - y_i)^2]</span></p>
<p><strong>Batch learning</strong> uses the whole training set as samples to estimate the mse:</p>
<p><span class="math display">\mathcal{L}(\mathbf{w}, b) \approx \frac{1}{N} \, \sum_{i=1}^{N} (t_i - y_i)^2</span></p>
<p><span class="math display">
    \Delta \mathbf{w} = \eta \, \frac{1}{N} \sum_{i=1}^{N} (t_i - y_i ) \, \mathbf{x_i}
</span></p>
<p><strong>Online learning</strong> uses a single sample to estimate the mse:</p>
<p><span class="math display">\mathcal{L}(\mathbf{w}, b) \approx (t_i - y_i)^2</span></p>
<p><span class="math display">
    \Delta \mathbf{w} = \eta \, (t_i - y_i) \, \mathbf{x_i}
</span></p>
<p>Batch learning has less bias (central limit theorem) and is less sensible to noise in the data, but is very slow. Online learning converges faster, but can be instable and overfits (high variance).</p>
<p>In practice, we use a trade-off between batch and online learning called <strong>Stochastic Gradient Descent (SGD)</strong> or <strong>Minibatch Gradient Descent</strong>.</p>
<p>The training set is randomly split at each epoch into small chunks of data (a <strong>minibatch</strong>, usually 32 or 64 examples) and the batch learning rule is applied on each chunk.</p>
<p><span class="math display">
    \Delta \mathbf{w} = \eta \, \frac{1}{K} \sum_{i=1}^{K} (t_i - y_i) \, \mathbf{x_i}
</span></p>
<p>If the <strong>batch size</strong> is well chosen, SGD is as stable as batch learning and as fast as online learning. The minibatches are randomly selected at each epoch (i.i.d).</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Online learning is a stochastic gradient descent with a batch size of 1.</p>
</div>
</div>
</section>
</section>
<section id="maximum-likelihood-estimation" class="level2">
<h2 class="anchored" data-anchor-id="maximum-likelihood-estimation">Maximum Likelihood Estimation</h2>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/9Hw6nLMiPiI" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
<p>Let’s consider <span class="math inline">N</span> <strong>samples</strong> <span class="math inline">\{x_i\}_{i=1}^N</span> independently taken from a <strong>normal distribution</strong> <span class="math inline">X</span>. The probability density function (pdf) of a normal distribution is:</p>
<p><span class="math display">
    f(x ; \mu, \sigma) =  \frac{1}{\sqrt{2\pi \sigma^2}} \, \exp{- \frac{(x - \mu)^2}{2\sigma^2}}
</span></p>
<p>where <span class="math inline">\mu</span> is the mean of the distribution and <span class="math inline">\sigma</span> its standard deviation.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/MLE2.png" class="img-fluid figure-img" style="width:80.0%"></p>
<p></p><figcaption class="figure-caption">Normal distributions with different parameters <span class="math inline">\mu</span> and <span class="math inline">\sigma</span> explain the data with different likelihoods.</figcaption><p></p>
</figure>
</div>
<p>The problem is to find the values of <span class="math inline">\mu</span> and <span class="math inline">\sigma</span> which explain best the observations <span class="math inline">\{x_i\}_{i=1}^N</span>.</p>
<p>The idea of MLE is to maximize the joint density function for all observations. This function is expressed by the <strong>likelihood function</strong>:</p>
<p><span class="math display">
    L(\mu, \sigma) = P( x ; \mu , \sigma  )  = \prod_{i=1}^{N} f(x_i ; \mu, \sigma )
</span></p>
<p>When the pdf takes high values for all samples, it is quite likely that the samples come from this particular distribution. The likelihood function reflects the probability that the parameters <span class="math inline">\mu</span> and <span class="math inline">\sigma</span> explain the observations <span class="math inline">\{x_i\}_{i=1}^N</span>.</p>
<p>We therefore search for the values <span class="math inline">\mu</span> and <span class="math inline">\sigma</span> which <strong>maximize</strong> the likelihood function.</p>
<p><span class="math display">
    \text{max}_{\mu, \sigma} \quad L(\mu, \sigma) = \prod_{i=1}^{N} f(x_i ; \mu, \sigma )
</span></p>
<p>For the normal distribution, the likelihood function is:</p>
<p><span class="math display">
\begin{aligned}
    L(\mu, \sigma) &amp; = \prod_{i=1}^{N} f(x_i ; \mu, \sigma ) \\
                   &amp; = \prod_{i=1}^{N} \frac{1}{\sqrt{2\pi \sigma^2}} \, \exp{- \frac{(x_i - \mu)^2}{2\sigma^2}}\\
                   &amp; =  (\frac{1}{\sqrt{2\pi \sigma^2}})^N \, \prod_{i=1}^{N} \exp{- \frac{(x_i - \mu)^2}{2\sigma^2}}\\
                   &amp; =  (\frac{1}{\sqrt{2\pi \sigma^2}})^N \, \exp{- \frac{\sum_{i=1}^{N}(x_i - \mu)^2}{2\sigma^2}}\\
\end{aligned}
</span></p>
<p>To find the maximum of <span class="math inline">L(\mu, \sigma)</span>, we need to search where the gradient is equal to zero:</p>
<p><span class="math display">
\begin{cases}
    \dfrac{\partial L(\mu, \sigma)}{\partial \mu} = 0 \\
    \dfrac{\partial L(\mu, \sigma)}{\partial \sigma} = 0 \\
\end{cases}
</span></p>
<p>The likelihood function is complex to differentiate, so we consider its logarithm <span class="math inline">l(\mu, \sigma) = \log(L(\mu, \sigma))</span> which has a maximum for the same value of <span class="math inline">(\mu, \sigma)</span> as the log function is monotonic.</p>
<p><span class="math display">
\begin{aligned}
    l(\mu, \sigma) &amp; = \log(L(\mu, \sigma)) \\
                   &amp; =  \log \left((\frac{1}{\sqrt{2\pi \sigma^2}})^N \, \exp{- \frac{\sum_{i=1}^{N}(x_i - \mu)^2}{2\sigma^2}} \right)\\
                   &amp; =  - \frac{N}{2} \log (2\pi \sigma^2) - \frac{\sum_{i=1}^{N}(x_i - \mu)^2}{2\sigma^2}\\
\end{aligned}
</span></p>
<p><span class="math inline">l(\mu, \sigma)</span> is called the <strong>log-likelihood</strong> function. The maximum of the log-likelihood function respects:</p>
<p><span class="math display">
\begin{aligned}
    \frac{\partial l(\mu, \sigma)}{\partial \mu} &amp; = \frac{\sum_{i=1}^{N}(x_i - \mu)}{\sigma^2} = 0 \\
    \frac{\partial l(\mu, \sigma)}{\partial \sigma} &amp; = - \frac{N}{2} \frac{4 \pi \sigma}{2 \pi \sigma^2} + \frac{\sum_{i=1}^{N}(x_i - \mu)^2}{\sigma^3} \\
                                                    &amp; = - \frac{N}{\sigma} + \frac{\sum_{i=1}^{N}(x_i - \mu)^2}{\sigma^3} = 0\\
\end{aligned}
</span></p>
<p>We obtain:</p>
<p><span class="math display">
    \mu = \frac{1}{N} \sum_{i=1}^{N} x_i  \qquad\qquad    \sigma^2 = \frac{1}{N} \sum_{i=1}^{N}(x_i - \mu)^2
</span></p>
<p>Unsurprisingly, the mean and variance of the normal distribution which best explains the data are the mean and variance of the data…</p>
<p>The same principle can be applied to estimate the parameters of any distribution: normal, exponential, Bernouilli, Poisson, etc… When a machine learning method has an probabilistic interpretation (i.e.&nbsp;it outputs probabilities), MLE can be used to find its parameters. One can use global optimization like here, or gradient descent to estimate the parameters iteratively.</p>
</section>
<section id="soft-linear-classification-logistic-regression" class="level2">
<h2 class="anchored" data-anchor-id="soft-linear-classification-logistic-regression">Soft linear classification : Logistic regression</h2>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/_Zc-k9pXVvE" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
<p>In logistic regression, we want to perform a regression, but where the targets <span class="math inline">t_i</span> are bounded betwen 0 and 1. We can use a logistic function instead of a linear function in order to transform the net activation into an output:</p>
<p><span class="math display">
\begin{aligned}
    y = \sigma(w \, x + b )  = \frac{1}{1+\exp(-w \, x - b )}
\end{aligned}
</span></p>
<p>Logistic regression can be used in binary classification if we consider <span class="math inline">y = \sigma(w \, x + b )</span> as the probability that the example belongs to the positive class (<span class="math inline">t=1</span>).</p>
<p><span class="math display">
    P(t = 1 | x; w, b) = y ; \qquad P(t = 0 | x; w, b) = 1 - y
</span></p>
<p>The output <span class="math inline">t</span> therefore comes from a Bernouilli distribution <span class="math inline">\mathcal{B}</span> of parameter <span class="math inline">p = y = f_{w, b}(x)</span>. The probability density function (pdf) is:</p>
<p><span class="math display">f(t | x; w, b) = y^t \, (1- y)^{1-t}</span></p>
<p>If we consider our training samples <span class="math inline">(x_i, t_i)</span> as independently taken from this distribution, our task is to find the parameterized distribution that best explains the data, which means to find the parameters <span class="math inline">w</span> and <span class="math inline">b</span> maximizing the <strong>likelihood</strong> that the samples <span class="math inline">t</span> come from a Bernouilli distribution when <span class="math inline">x</span>, <span class="math inline">w</span> and <span class="math inline">b</span> are given. We only need to apply <strong>Maximum Likelihood Estimation</strong> (MLE) on this Bernouilli distribution!</p>
<p>The likelihood function for logistic regression is :</p>
<p><span class="math display">
\begin{aligned}
    L( w, b) &amp;= P( t | x; w,  b )  = \prod_{i=1}^{N} f(t_i | x_i;  w,  b ) \\
    &amp;= \prod_{i=1}^{N}  y_i^{t_i} \, (1- y_i)^{1-t_i}
\end{aligned}
</span></p>
<p>The likelihood function is quite hard to differentiate, so we take the <strong>log-likelihood</strong> function:</p>
<p><span class="math display">
\begin{aligned}
    l( w, b) &amp;= \log L( w, b) \\
    &amp;=  \sum_{i=1}^{N} [t_i \, \log y_i + (1 - t_i) \, \log( 1- y_i)]\\
\end{aligned}
</span></p>
<p>or even better: the <strong>negative log-likelihood</strong> which will be minimized using gradient descent:</p>
<p><span class="math display">
    \mathcal{L}( w, b) =  - \sum_{i=1}^{N} [t_i \, \log y_i + (1 - t_i) \, \log( 1- y_i)]
</span></p>
<p>We then search for the minimum of the negative log-likelihood function by computing its gradient (here for a single sample):</p>
<p><span class="math display">
\begin{aligned}
    \frac{\partial \mathcal{l}_i(w, b)}{\partial w}
        &amp;= -\frac{\partial}{\partial w} [ t_i \, \log y_i + (1 - t_i) \, \log( 1- y_i) ] \\
        &amp;= - t_i \, \frac{\partial}{\partial w} \log y_i - (1 - t_i) \, \frac{\partial}{\partial w}\log( 1- y_i) \\
        &amp;= - t_i \, \frac{\frac{\partial}{\partial w} y_i}{y_i} - (1 - t_i) \, \frac{\frac{\partial}{\partial w}( 1- y_i)}{1- y_i} \\
        &amp;= - t_i \, \frac{y_i \, (1 - y_i) \, x_i}{y_i} + (1 - t_i) \, \frac{y_i \, (1-y_i) \, x_i}{1 - y_i}\\
        &amp;= - ( t_i - y_i ) \, x_i\\
\end{aligned}
</span></p>
<p>We obtain the same gradient as the linear perceptron, but with a non-linear output function! Logistic regression is therefore a regression method used for classification. It uses a non-linear transfer function <span class="math inline">\sigma(x)=\frac{1}{1+\exp(-x)}</span> applied on the net activation:</p>
<p><span class="math display">
    y_i = \sigma(\langle \mathbf{w} \cdot \mathbf{x}_i \rangle + b )
</span></p>
<p>The continuous output <span class="math inline">y</span> is interpreted as the probability of belonging to the positive class.</p>
<p><span class="math display">
   P(t_i = 1 | \mathbf{x}_i; \mathbf{w}, b) = y_i ; \qquad P(t_i = 0 | \mathbf{x}_i; \mathbf{w}, b) = 1 - y_i
</span></p>
<p>We minimize the <strong>negative log-likelihood</strong> loss function:</p>
<p><span class="math display">
    \mathcal{L}(\mathbf{w}, b) =  - \sum_{i=1}^{N} [t_i \, \log y_i + (1 - t_i) \, \log( 1- y_i)]
</span></p>
<p>Gradient descent leads to the delta learning rule, using the class as a target and the probability as a prediction:</p>
<p><span class="math display">
    \begin{cases}
    \Delta \mathbf{w} = \eta \, ( t_i - y_i ) \, \mathbf{x}_i \\
    \\
    \Delta b = \eta \, ( t_i - y_i ) \\
    \end{cases}
</span></p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Logistic regression
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p><span class="math inline">\mathbf{w} = 0 \qquad b = 0</span></p></li>
<li><p><strong>for</strong> <span class="math inline">M</span> epochs:</p>
<ul>
<li><p><strong>for</strong> each sample <span class="math inline">(\mathbf{x}_i, t_i)</span>:</p>
<ul>
<li><p><span class="math inline">y_i = \sigma( \langle \mathbf{w} \cdot \mathbf{x}_i \rangle + b)</span></p></li>
<li><p><span class="math inline">\Delta \mathbf{w} = \eta \, (t_i - y_i) \, \mathbf{x}_i</span></p></li>
<li><p><span class="math inline">\Delta b = \eta \, (t_i - y_i)</span></p></li>
</ul></li>
</ul></li>
</ul>
</div>
</div>
<p>Logistic regression works just like linear classification, except in the way the prediction is done. To know to which class <span class="math inline">\mathbf{x}_i</span> belongs, simply draw a random number between 0 and 1:</p>
<ul>
<li>if it is smaller than <span class="math inline">y_i</span> (probability <span class="math inline">y_i</span>), it belongs to the positive class.</li>
<li>if it is bigger than <span class="math inline">y_i</span> (probability <span class="math inline">1-y_i</span>), it belongs to the negative class.</li>
</ul>
<p>Alternatively, you can put a <strong>hard limit</strong> at 0.5:</p>
<ul>
<li>if <span class="math inline">y_i &gt; 0.5</span> then the class is positive.</li>
<li>if <span class="math inline">y_i &lt; 0.5</span> then the class is negative.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/logisticregression-animation.gif" class="img-fluid figure-img" style="width:80.0%"></p>
<p></p><figcaption class="figure-caption">Logistic regression for soft classification. The confidence scores tells how certain the classification is.</figcaption><p></p>
</figure>
</div>
<p>Logistic regression also provides a <strong>confidence score</strong>: the closer <span class="math inline">y</span> is from 0 or 1, the more confident we can be that the classification is correct. This is particularly important in <strong>safety critical</strong> applications: if you detect the positive class but with a confidence of 0.51, you should perhaps not trust the prediction. If the confidence score is 0.99, you can probably trust the prediction.</p>
</section>
<section id="multi-class-classification" class="level2">
<h2 class="anchored" data-anchor-id="multi-class-classification">Multi-class classification</h2>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/24GpIHcaTxA" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
<p>Can we perform multi-class classification using the previous methods when <span class="math inline">t \in \{A, B, C\}</span> instead of <span class="math inline">t = +1</span> or <span class="math inline">-1</span>? There are two main solutions:</p>
<ul>
<li><p><strong>One-vs-All</strong> (or One-vs-the-rest): one trains simultaneously a binary (linear) classifier for each class. The examples belonging to this class form the positive class, all others are the negative class:</p>
<ul>
<li>A vs.&nbsp;B and C</li>
<li>B vs.&nbsp;A and C</li>
<li>C vs.&nbsp;A and B</li>
</ul></li>
</ul>
<p>If multiple classes are predicted for a single example, ones needs a confidence level for each classifier saying how sure it is of its prediction.</p>
<ul>
<li><p><strong>One-vs-One</strong>: one trains a classifier for each pair of class:</p>
<ul>
<li>A vs.&nbsp;B</li>
<li>B vs.&nbsp;C</li>
<li>C vs.&nbsp;A</li>
</ul></li>
</ul>
<p>A majority vote is then performed to find the correct class.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/pixelspace.jpg" class="img-fluid figure-img" style="width:80.0%"></p>
<p></p><figcaption class="figure-caption">Example of <strong>One-vs-All</strong> classification: one binary classifier per class. Source <a href="http://cs231n.github.io/linear-classify" class="uri">http://cs231n.github.io/linear-classify</a></figcaption><p></p>
</figure>
</div>
<section id="softmax-linear-classifier" class="level3">
<h3 class="anchored" data-anchor-id="softmax-linear-classifier">Softmax linear classifier</h3>
<p>Suppose we have <span class="math inline">C</span> classes (dog vs.&nbsp;cat vs.&nbsp;ship vs…). The One-vs-All scheme involves <span class="math inline">C</span> binary classifiers <span class="math inline">(\mathbf{w}_i, b_i)</span>, each with a weight vector and a bias, working on the same input <span class="math inline">\mathbf{x}</span>.</p>
<p><span class="math display">y_i = f(\langle \mathbf{w}_i \cdot \mathbf{x} \rangle + b_i)</span></p>
<p>Putting all neurons together, we obtain a <strong>linear perceptron</strong> similar to multiple linear regression:</p>
<p><span class="math display">
    \mathbf{y} = f(W \times \mathbf{x} + \mathbf{b})
</span></p>
<p>The <span class="math inline">C</span> weight vectors form a <span class="math inline">d\times C</span> <strong>weight matrix</strong> <span class="math inline">W</span>, the biases form a vector <span class="math inline">\mathbf{b}</span>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/imagemap.jpg" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Linear perceptron for images. The output is the logit score. Source <a href="http://cs231n.github.io/linear-classify" class="uri">http://cs231n.github.io/linear-classify</a></figcaption><p></p>
</figure>
</div>
<p>The net activations form a vector <span class="math inline">\mathbf{z}</span>:</p>
<p><span class="math display">
    \mathbf{z} = f_{W, \mathbf{b}}(\mathbf{x}) = W \times \mathbf{x} + \mathbf{b}
</span></p>
<p>Each element <span class="math inline">z_j</span> of the vector <span class="math inline">\mathbf{z}</span> is called the <strong>logit score</strong> of the class: the higher the score, the more likely the input belongs to this class. The logit scores are not probabilities, as they can be negative and do not sum to 1.</p>
<section id="one-hot-encoding" class="level4">
<h4 class="anchored" data-anchor-id="one-hot-encoding">One-hot encoding</h4>
<p>How do we represent the ground truth <span class="math inline">\mathbf{t}</span> for each neuron? The target vector <span class="math inline">\mathbf{t}</span> is represented using <strong>one-hot encoding</strong>. The binary vector has one element per class: only one element is 1, the others are 0. Example:</p>
<p><span class="math display">
    \mathbf{t} = [\text{cat}, \text{dog}, \text{ship}, \text{house}, \text{car}] = [0, 1, 0, 0, 0]
</span></p>
<p>The labels can be seen as a <strong>probability distribution</strong> over the training set, in this case a <strong>multinomial</strong> distribution (a dice with <span class="math inline">C</span> sides). For a given image <span class="math inline">\mathbf{x}</span> (e.g.&nbsp;a picture of a dog), the conditional pmf is defined by the one-hot encoded vector <span class="math inline">\mathbf{t}</span>:</p>
<p><span class="math display">P(\mathbf{t} | \mathbf{x}) = [P(\text{cat}| \mathbf{x}), P(\text{dog}| \mathbf{x}), P(\text{ship}| \mathbf{x}), P(\text{house}| \mathbf{x}), P(\text{car}| \mathbf{x})] = [0, 1, 0, 0, 0]</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/softmax-transformation.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">The logit scores <span class="math inline">\mathbf{z}</span> cannot be compared to the targets <span class="math inline">\mathbf{t}</span>: we need to transform them into a probability distribution <span class="math inline">\mathbf{y}</span>.</figcaption><p></p>
</figure>
</div>
<p>We need to transform the logit score <span class="math inline">\mathbf{z}</span> into a <strong>probability distribution</strong> <span class="math inline">P(\mathbf{y} | \mathbf{x})</span> that should be as close as possible from <span class="math inline">P(\mathbf{t} | \mathbf{x})</span>.</p>
</section>
<section id="softmax-activation" class="level4">
<h4 class="anchored" data-anchor-id="softmax-activation">Softmax activation</h4>
<p>The <strong>softmax</strong> operator makes sure that the sum of the outputs <span class="math inline">\mathbf{y} = \{y_i\}</span> over all classes is 1.</p>
<p><span class="math display">
    y_j = P(\text{class = j} | \mathbf{x}) = \mathcal{S}(z_j) = \frac{\exp(z_j)}{\sum_k \exp(z_k)}
</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/softmax-comp.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Softmax activation transforms the logit score into a probability distribution. Source <a href="http://cs231n.github.io/linear-classify" class="uri">http://cs231n.github.io/linear-classify</a></figcaption><p></p>
</figure>
</div>
<p>The higher <span class="math inline">z_j</span>, the higher the probability that the example belongs to class <span class="math inline">j</span>. This is very similar to logistic regression for soft classification, except that we have multiple classes.</p>
</section>
<section id="cross-entropy-loss-function" class="level4">
<h4 class="anchored" data-anchor-id="cross-entropy-loss-function">Cross-entropy loss function</h4>
<p>We cannot use the mse as a loss function, as the softmax function would be hard to differentiate:</p>
<p><span class="math display">
    \text{mse}(W, \mathbf{b}) = \sum_j (t_{j} - \frac{\exp(z_j)}{\sum_k \exp(z_k)})^2
</span></p>
<p>We actually want to minimize the statistical distance netween two distributions:</p>
<ul>
<li>The model outputs a multinomial probability distribution <span class="math inline">\mathbf{y}</span> for an input <span class="math inline">\mathbf{x}</span>: <span class="math inline">P(\mathbf{y} | \mathbf{x}; W, \mathbf{b})</span>.</li>
<li>The one-hot encoded classes also come from a multinomial probability distribution <span class="math inline">P(\mathbf{t} | \mathbf{x})</span>.</li>
</ul>
<p>We search which parameters <span class="math inline">(W, \mathbf{b})</span> make the two distributions <span class="math inline">P(\mathbf{y} | \mathbf{x}; W, \mathbf{b})</span> and <span class="math inline">P(\mathbf{t} | \mathbf{x})</span> close. The training data <span class="math inline">\{\mathbf{x}_i, \mathbf{t}_i\}</span> represents samples from <span class="math inline">P(\mathbf{t} | \mathbf{x})</span>. <span class="math inline">P(\mathbf{y} | \mathbf{x}; W, \mathbf{b})</span> is a good model of the data when the two distributions are close, i.e.&nbsp;when the <strong>negative log-likelihood</strong> of each sample under the model is small.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/crossentropy.svg" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Cross-entropy between two distributions <span class="math inline">X</span> and <span class="math inline">Y</span>: are samples of <span class="math inline">X</span> likely under <span class="math inline">Y</span>?</figcaption><p></p>
</figure>
</div>
<p>For an input <span class="math inline">\mathbf{x}</span>, we minimize the <strong>cross-entropy</strong> between the target distribution and the predicted outputs:</p>
<p><span class="math display">
    \mathcal{l}(W, \mathbf{b}) = \mathcal{H}(\mathbf{t} | \mathbf{x}, \mathbf{y} | \mathbf{x}) =  \mathbb{E}_{t \sim P(\mathbf{t} | \mathbf{x})} [ - \log P(\mathbf{y} = t | \mathbf{x})]
</span></p>
<p>The cross-entropy samples from <span class="math inline">\mathbf{t} | \mathbf{x}</span>:</p>
<p><span class="math display">
    \mathcal{l}(W, \mathbf{b}) = \mathcal{H}(\mathbf{t} | \mathbf{x}, \mathbf{y} | \mathbf{x}) =  \mathbb{E}_{t \sim P(\mathbf{t} | \mathbf{x})} [ - \log P(\mathbf{y} = t | \mathbf{x})]
</span></p>
<p>For a given input <span class="math inline">\mathbf{x}</span>, <span class="math inline">\mathbf{t}</span> is non-zero only for the correct class <span class="math inline">t^*</span>, as <span class="math inline">\mathbf{t}</span> is a one-hot encoded vector <span class="math inline">[0, 1, 0, 0, 0]</span>:</p>
<p><span class="math display">
    \mathcal{l}(W, \mathbf{b}) =  - \log P(\mathbf{y} = t^* | \mathbf{x})
</span></p>
<p>If we note <span class="math inline">j^*</span> the index of the correct class <span class="math inline">t^*</span>, the cross entropy is simply:</p>
<p><span class="math display">
    \mathcal{l}(W, \mathbf{b}) =  - \log y_{j^*}
</span></p>
<p>As only one element of <span class="math inline">\mathbf{t}</span> is non-zero, the cross-entropy is the same as the <strong>negative log-likelihood</strong> of the prediction for the true label:</p>
<p><span class="math display">
    \mathcal{l}(W, \mathbf{b}) =  - \log y_{j^*}
</span></p>
<p>The minimum of <span class="math inline">- \log y</span> is obtained when <span class="math inline">y =1</span>: We want to classifier to output a probability 1 for the true label. Because of the softmax activation function, the probability for the other classes should become closer from 0.</p>
<p><span class="math display">
    y_j = P(\text{class = j}) = \frac{\exp(z_j)}{\sum_k \exp(z_k)}
</span></p>
<p>Minimizing the cross-entropy / negative log-likelihood pushes the output distribution <span class="math inline">\mathbf{y} | \mathbf{x}</span> to be as close as possible to the target distribution <span class="math inline">\mathbf{t} | \mathbf{x}</span>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/crossentropy-animation.gif" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Minimizing the cross-entropy between <span class="math inline">\mathbf{t} | \mathbf{x}</span> and <span class="math inline">\mathbf{y} | \mathbf{x}</span> makes them similar.</figcaption><p></p>
</figure>
</div>
<p>As <span class="math inline">\mathbf{t}</span> is a binary vector <span class="math inline">[0, 1, 0, 0, 0]</span>, the cross-entropy / negative log-likelihood can also be noted as the dot product between <span class="math inline">\mathbf{t}</span> and <span class="math inline">\log \mathbf{y}</span>:</p>
<p><span class="math display">
    \mathcal{l}(W, \mathbf{b}) = - \langle \mathbf{t} \cdot \log \mathbf{y} \rangle = - \sum_{j=1}^C t_j \, \log y_j =  - \log y_{j^*}
</span></p>
<p>The <strong>cross-entropy loss function</strong> is then the expectation over the training set of the individual cross-entropies:</p>
<p><span class="math display">
    \mathcal{L}(W, \mathbf{b}) = \mathbb{E}_{\mathbf{x}, \mathbf{t} \sim \mathcal{D}} [- \langle \mathbf{t} \cdot \log \mathbf{y} \rangle ] \approx \frac{1}{N} \sum_{i=1}^N - \langle \mathbf{t}_i \cdot \log \mathbf{y}_i \rangle
</span></p>
<p>The nice thing with the <strong>cross-entropy</strong> loss function, when used on a softmax activation function, is that the partial derivative w.r.t the logit score <span class="math inline">\mathbf{z}</span> is simple:</p>
<p><span class="math display">
\begin{split}
    \frac{\partial {l}(W, \mathbf{b})}{\partial z_i} &amp; = - \sum_j \frac{\partial}{\partial z_i}  t_j \log(y_j)=
- \sum_j t_j \frac{\partial \log(y_j)}{\partial z_i} = - \sum_j t_j \frac{1}{y_j} \frac{\partial y_j}{\partial z_i} \\
&amp; = - \frac{t_i}{y_i} \frac{\partial y_i}{\partial z_i} - \sum_{j \neq i}^C \frac{t_j}{y_j} \frac{\partial y_j}{\partial z_i}
= - \frac{t_i}{y_i} y_i (1-y_i) - \sum_{j \neq i}^C \frac{t_j}{y_i} (-y_j \, y_i) \\
&amp; = - t_i + t_i \, y_i + \sum_{j \neq i}^C t_j \, y_i = - t_i + \sum_{j = 1}^C t_j y_i
= -t_i + y_i \sum_{j = 1}^C t_j \\
&amp; = - (t_i - y_i)
\end{split}
</span></p>
<p>i.e.&nbsp;the same as with the mse in linear regression! Refer <a href="https://peterroelants.github.io/posts/cross-entropy-softmax/" class="uri">https://peterroelants.github.io/posts/cross-entropy-softmax/</a> for more explanations on the proof.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>When differentiating a softmax probability <span class="math inline">y_j = \dfrac{\exp(z_j)}{\sum_k \exp(z_k)}</span> w.r.t a logit score <span class="math inline">z_i</span>, i.e.&nbsp;<span class="math inline">\dfrac{\partial y_j}{\partial z_i}</span>, we need to consider two cases:</p>
<ul>
<li>If <span class="math inline">i=j</span>, <span class="math inline">\exp(z_i)</span> appears both at the numerator and denominator of <span class="math inline">\frac{\exp(z_i)}{\sum_k \exp(z_k)}</span>. The product rule <span class="math inline">(f\times g)' = f'\, g + f \, g'</span> gives us:</li>
</ul>
<p><span class="math display">\begin{aligned}
\dfrac{\partial \log(y_i)}{\partial z_i} &amp;= \dfrac{\exp(z_i)}{\sum_k \exp(z_k)} + \exp(z_i) \, \dfrac{- \exp(z_i)}{(\sum_k \exp(z_k))^2} \\
&amp;= \dfrac{\exp(z_i)  \, \sum_k \exp(z_k) - \exp(z_i)^2}{(\sum_k \exp(z_k))^2} \\
&amp;= \dfrac{\exp(z_i)}{\sum_k \exp(z_k)} \, (1- \dfrac{\exp(z_i)}{\sum_k \exp(z_k)})\\
&amp;= y_i \, (1 - y_i)\\
\end{aligned}
</span></p>
<p>This is similar to the derivative of the logistic function.</p>
<ul>
<li>If <span class="math inline">i \neq j</span>, <span class="math inline">z_i</span> only appears at the denominator, so we only need the chain rule:</li>
</ul>
<p><span class="math display">\begin{aligned}
\dfrac{\partial \log(y_j)}{\partial z_i} &amp;= - \exp(z_j) \, \dfrac{\exp(z_i)}{(\sum_k \exp(z_k))^2} \\
&amp;= - \dfrac{\exp(z_i)}{\sum_k \exp(z_k)} \, \dfrac{\exp(z_j)}{\sum_k \exp(z_k)} \\
&amp;= - y_i \, y_j \\
\end{aligned}
</span></p>
</div>
</div>
<p>Using the vector notation, we get:</p>
<p><span class="math display">
    \frac{\partial \mathcal{l}(W, \mathbf{b})}{\partial \mathbf{z}} =  -  (\mathbf{t} - \mathbf{y} )
</span></p>
<p>As:</p>
<p><span class="math display">
    \mathbf{z} = W \times \mathbf{x} + \mathbf{b}
</span></p>
<p>we can obtain the partial derivatives:</p>
<p><span class="math display">
\begin{cases}
    \dfrac{\partial \mathcal{l}(W, \mathbf{b})}{\partial W} = \dfrac{\partial \mathcal{l}(W, \mathbf{b})}{\partial \mathbf{z}} \times \dfrac{\partial \mathbf{z}}{\partial W} = - (\mathbf{t} - \mathbf{y} ) \times \mathbf{x}^T \\
    \\
    \dfrac{\partial \mathcal{l}(W, \mathbf{b})}{\partial \mathbf{b}} = \dfrac{\partial \mathcal{l}(W, \mathbf{b})}{\partial \mathbf{z}} \times \dfrac{\partial \mathbf{z}}{\partial \mathbf{b}} = - (\mathbf{t} - \mathbf{y} ) \\
\end{cases}
</span></p>
<p>So gradient descent leads to the <strong>delta learning rule</strong>:</p>
<p><span class="math display">
\begin{cases}
    \Delta W = \eta \,  (\mathbf{t} - \mathbf{y} ) \times \mathbf{x}^T \\
    \\
    \Delta \mathbf{b} = \eta \,  (\mathbf{t} - \mathbf{y} ) \\
\end{cases}
</span></p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Softmax linear classifier
</div>
</div>
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/softmaxclassifier.svg" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Softmax linear classifier</figcaption><p></p>
</figure>
</div>
<ul>
<li>We first compute the <strong>logit scores</strong> <span class="math inline">\mathbf{z}</span> using a linear layer:</li>
</ul>
<p><span class="math display">
    \mathbf{z} = W \times \mathbf{x} + \mathbf{b}
</span></p>
<ul>
<li>We turn them into probabilities <span class="math inline">\mathbf{y}</span> using the <strong>softmax activation function</strong>:</li>
</ul>
<p><span class="math display">
    y_j = \frac{\exp(z_j)}{\sum_k \exp(z_k)}
</span></p>
<ul>
<li>We minimize the <strong>cross-entropy</strong> / <strong>negative log-likelihood</strong> on the training set:</li>
</ul>
<p><span class="math display">
    \mathcal{L}(W, \mathbf{b}) = \mathbb{E}_{\mathbf{x}, \mathbf{t} \sim \mathcal{D}} [ - \langle \mathbf{t} \cdot \log \mathbf{y} \rangle]
</span></p>
<p>which simplifies into the <strong>delta learning rule</strong>:</p>
<p><span class="math display">
\begin{cases}
    \Delta W = \eta \,  (\mathbf{t} - \mathbf{y} ) \times \mathbf{x}^T \\
    \\
    \Delta \mathbf{b} = \eta \,  (\mathbf{t} - \mathbf{y} ) \\
\end{cases}
</span></p>
</div>
</div>
</section>
</section>
<section id="comparison-of-linear-classification-and-regression" class="level3">
<h3 class="anchored" data-anchor-id="comparison-of-linear-classification-and-regression">Comparison of linear classification and regression</h3>
<p>Classification and regression differ in the nature of their outputs: in classification they are discrete, in regression they are continuous values. However, when trying to minimize the mismatch between a model <span class="math inline">\mathbf{y}</span> and the real data <span class="math inline">\mathbf{t}</span>, we have found the same <strong>delta learning rule</strong>:</p>
<p><span class="math display">
\begin{cases}
    \Delta W = \eta \,  (\mathbf{t} - \mathbf{y} ) \times \mathbf{x}^T \\
    \\
    \Delta \mathbf{b} = \eta \,  (\mathbf{t} - \mathbf{y} ) \\
\end{cases}
</span></p>
<p>Regression and classification are in the end the same problem for us. The only things that needs to be adapted is the activation function of the output and the <strong>loss function</strong>:</p>
<ul>
<li><p>For regression, we use regular activation functions and the <strong>mean square error</strong> (mse):</p>
<p><span class="math display">
  \mathcal{L}(W, \mathbf{b}) = \mathbb{E}_{\mathbf{x}, \mathbf{t} \in \mathcal{D}} [ ||\mathbf{t} - \mathbf{y}||^2 ]
  </span></p></li>
<li><p>For classification, we use the softmax activation function and the <strong>cross-entropy</strong> (negative log-likelihood) loss function:</p>
<p><span class="math display">\mathcal{L}(W, \mathbf{b}) = \mathbb{E}_{\mathbf{x}, \mathbf{t} \sim \mathcal{D}} [ - \langle \mathbf{t} \cdot \log \mathbf{y} \rangle]</span></p></li>
</ul>
</section>
<section id="multi-label-classification" class="level3">
<h3 class="anchored" data-anchor-id="multi-label-classification">Multi-label classification</h3>
<p>What if there is more than one label on the image? The target vector <span class="math inline">\mathbf{t}</span> does not represent a probability distribution anymore:</p>
<p><span class="math display">
    \mathbf{t} = [\text{cat}, \text{dog}, \text{ship}, \text{house}, \text{car}] = [1, 1, 0, 0, 0]
</span></p>
<p>Normalizing the vector does not help: it is not a dog <strong>or</strong> a cat, it is a dog <strong>and</strong> a cat.</p>
<p><span class="math display">
    \mathbf{t} = [\text{cat}, \text{dog}, \text{ship}, \text{house}, \text{car}] = [0.5, 0.5, 0, 0, 0]
</span></p>
<p>For multi-label classification, we can simply use the <strong>logistic</strong> activation function for the output neurons:</p>
<p><span class="math display">
    \mathbf{y} = \sigma(W \times \mathbf{x} + \mathbf{b})
</span></p>
<p>The outputs are between 0 and 1, but they do not sum to one. Each output neuron performs a <strong>logistic regression for soft classification</strong> on their class:</p>
<p><span class="math display">y_j = P(\text{class} = j | \mathbf{x})</span></p>
<p>Each output neuron <span class="math inline">y_j</span> has a binary target <span class="math inline">t_j</span> (one-vs-the-rest) and has to minimize the negative log-likelihood:</p>
<p><span class="math display">
\mathcal{l}_j(W, \mathbf{b}) =  - t_j \, \log y_j + (1 - t_j) \, \log( 1- y_j)
</span></p>
<p>The <strong>binary cross-entropy</strong> loss for the whole network is the sum of the negative log-likelihood for each class:</p>
<p><span class="math display">
\mathcal{L}(W, \mathbf{b}) =  \mathbb{E}_{\mathcal{D}} [- \sum_{j=1}^C t_j \, \log y_j + (1 - t_j) \, \log( 1- y_j)]
</span></p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
<nav class="page-navigation column-body">
  <div class="nav-page nav-page-previous">
      <a href="../notes/2.2-LinearRegression.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-title">Linear regression</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../notes/2.4-LearningTheory.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-title">Learning theory</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
      <div class="nav-footer-center">Copyright 2022, Julien Vitay - <a href="mailto:julien.vitay@informatik.tu-chemnitz.de" class="email">julien.vitay@informatik.tu-chemnitz.de</a></div>
  </div>
</footer>



<script src="../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>