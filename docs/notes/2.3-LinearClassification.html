<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.175">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Neurocomputing - 6&nbsp; Linear classification</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../notes/2.4-Multiclassification.html" rel="next">
<link href="../notes/2.2-LinearRegression.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Linear classification</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Neurocomputing</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">Course description</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">Introduction</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/1.1-Introduction.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/1.2-Math.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Math basics (optional)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/1.3-Neurons.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Neurons</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">Linear algorithms</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.1-Optimization.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Optimization</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.2-LinearRegression.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Linear regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.3-LinearClassification.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Linear classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.4-Multiclassification.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Multi-class classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.5-LearningTheory.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Learning theory</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">Neural networks</span>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">Convolutional neural networks</span>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">Unsupervised learning and generative modeling</span>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">Recurrent neural networks</span>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">Self-supervised learning</span>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">Outlook</span>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">Exercises</span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#hard-linear-classification" id="toc-hard-linear-classification" class="nav-link active" data-scroll-target="#hard-linear-classification"><span class="toc-section-number">6.1</span>  Hard linear classification</a></li>
  <li><a href="#maximum-likelihood-estimation" id="toc-maximum-likelihood-estimation" class="nav-link" data-scroll-target="#maximum-likelihood-estimation"><span class="toc-section-number">6.2</span>  Maximum Likelihood Estimation</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Linear classification</span></h1>
</div>



<div class="quarto-title-meta">

    
    
  </div>
  

</header>

<p>Slides: <a href="https://www.tu-chemnitz.de/informatik/KI/edu/neurocomputing/lectures/pdf/2.4-LinearClassification.pdf">pdf</a></p>
<section id="hard-linear-classification" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="hard-linear-classification"><span class="header-section-number">6.1</span> Hard linear classification</h2>
<div class="embed-container">
<iframe src="https://www.youtube.com/embed/jZoJHIHi6Qw" frameborder="0" allowfullscreen="">
</iframe>
</div>
<p>The training data <span class="math inline">\(\mathcal{D}\)</span> is composed of <span class="math inline">\(N\)</span> examples <span class="math inline">\((\mathbf{x}_i, t_i)_{i=1..N}\)</span> , with a d-dimensional input vector <span class="math inline">\(\mathbf{x}_i \in \Re^d\)</span> and a binary output <span class="math inline">\(t_i \in \{-1, +1\}\)</span>. The data points where <span class="math inline">\(t = + 1\)</span> are called the <strong>positive class</strong>, the other the <strong>negative class</strong>.</p>
<table class="table">
<thead>
<tr class="header">
<th>```{figure} ../img/classification-animation1.png</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>width: 80%</td>
</tr>
</tbody>
</table>
<p>Binary linear classification of 2D data.</p>
<pre><code>
For example, the inputs $\mathbf{x}_i$ can be images (one dimension per pixel) and the positive class corresponds to cats ($t_i = +1$), the negative class to dogs ($t_i = -1$).

```{figure} ../img/cats-dogs.jpg
---
width: 100%
---
Binary linear classification of cats vs. dogs images. Source: &lt;http://adilmoujahid.com/posts/2016/06/introduction-deep-learning-python-caffe&gt;</code></pre>
<p>We want to find the hyperplane <span class="math inline">\((\mathbf{w}, b)\)</span> of <span class="math inline">\(\Re^d\)</span> that correctly separates the two classes.</p>
<table class="table">
<thead>
<tr class="header">
<th>```{figure} ../img/classification-animation2.png</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>width: 80%</td>
</tr>
</tbody>
</table>
<p>The hyperplane separates the input space into two regions.</p>
<pre><code>
For a point $\mathbf{x} \in \mathcal{D}$, $\langle \mathbf{w} \cdot \mathbf{x} \rangle +b$  is the projection of $\mathbf{x}$  onto the hyperplane $(\mathbf{w}, b)$.

* If $\langle \mathbf{w} \cdot \mathbf{x} \rangle +b &gt; 0$, the point is
    above the hyperplane.

* If $\langle \mathbf{w} \cdot \mathbf{x} \rangle +b &lt; 0$, the point is
    below the hyperplane.

* If $\langle \mathbf{w} \cdot \mathbf{x} \rangle +b = 0$, the point is
    on the hyperplane.

```{figure} ../img/projection.svg
---
width: 80%
---
Projection on an hyperplane.</code></pre>
<p>By looking at the <strong>sign</strong> of <span class="math inline">\(\langle \mathbf{w} \cdot \mathbf{x} \rangle +b\)</span>, we can predict the class.</p>
<p><span class="math display">\[\text{sign}(x) = \begin{cases} +1 \; \text{if} \; x \geq 0 \\ -1 \; \text{if} \; x &lt; 0 \\ \end{cases}\]</span></p>
<p>Binary linear classification can therefore be made by a single <strong>artificial neuron</strong> using the sign transfer function.</p>
<p><span class="math display">\[
y = f_{\mathbf{w}, b} (\mathbf{x}) = \text{sign} ( \langle \mathbf{w} \cdot \mathbf{x} \rangle +b )  = \text{sign} ( \sum_{j=1}^d w_j \, x_j +b )
\]</span></p>
<p><span class="math inline">\(\mathbf{w}\)</span> is the weight vector and <span class="math inline">\(b\)</span> is the bias.</p>
<p>Linear classification is the process of finding an hyperplane <span class="math inline">\((\mathbf{w}, b)\)</span> that correctly separates the two classes. If such an hyperplane can be found, the training set is said <strong>linearly separable</strong>. Otherwise, the problem is <strong>non-linearly separable</strong> and other methods have to be applied (MLP, SVM…).</p>
<table class="table">
<thead>
<tr class="header">
<th>```{figure} ../img/linearlyseparable.png</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>width: 100%</td>
</tr>
</tbody>
</table>
<p>Linearly and non-linearly spearable datasets.</p>
<pre><code>
### Perceptron algorithm

The Perceptron algorithm tries to find the weights and biases minimizing the **mean square error** (*mse*) or **quadratic loss**:

$$\mathcal{L}(\mathbf{w}, b) = \mathbb{E}_\mathcal{D} [(t_i - y_i)^2] \approx \frac{1}{N} \, \sum_{i=1}^{N} (t_i - y_i)^2$$

When the prediction $y_i$ is the same as the data $t_i$ for all examples in the training set (perfect classification), the mse is minimal and equal to 0. We can apply gradient descent to find this minimum.

$$
    \Delta \mathbf{w} = - \eta \, \nabla_\mathbf{w} \, \mathcal{L}(\mathbf{w}, b)
$$

$$
    \Delta b = - \eta \, \nabla_b \, \mathcal{L}(\mathbf{w}, b)
$$

Let's search for the partial derivative of the quadratic error function with respect to the weight vector:

$$
    \nabla_\mathbf{w} \, \mathcal{L}(\mathbf{w}, b) = \nabla_\mathbf{w} \,  \frac{1}{N} \, \sum_{i=1}^{N} (t_i - y_i )^2 = \frac{1}{N} \, \sum_{i=1}^{N} \nabla_\mathbf{w} \,  (t_i - y_i )^2 = \frac{1}{N} \, \sum_{i=1}^{N} \nabla_\mathbf{w} \,  \mathcal{l}_i (\mathbf{w}, b)
$$


Everything is similar to linear regression until we get:

$$
    \nabla_\mathbf{w} \,  \mathcal{l}_i (\mathbf{w}, b) = - 2 \, (t_i - y_i) \, \nabla_\mathbf{w} \, \text{sign}( \langle \mathbf{w} \cdot \mathbf{x}_i \rangle +b)
$$

In order to continue with the chain rule, we would need to differentiate $\text{sign}(x)$.

$$
    \nabla_\mathbf{w} \,  \mathcal{l}_i (\mathbf{w}, b) = - 2 \, (t_i - y_i) \, \text{sign}'( \langle \mathbf{w} \cdot \mathbf{x}_i \rangle +b) \,  \mathbf{x}_i 
$$

But the sign function is **not** differentiable... We will simply pretend that the sign() function is linear, with a derivative of 1:

$$
    \nabla_\mathbf{w} \,  \mathcal{l}_i (\mathbf{w}, b) = - 2 \, (t_i - y_i) \,   \mathbf{x}_i 
$$

The update rule for the weight vector $\mathbf{w}$ and the bias $b$ is therefore the same as in linear regression:

$$
    \Delta \mathbf{w} =  \eta \, \frac{1}{N} \, \sum_{i=1}^{N} (t_i - y_i) \, \mathbf{x}_i
$$

$$
    \Delta b = \eta \, \frac{1}{N} \, \sum_{i=1}^{N} (t_i - y_i )
$$

By applying gradient descent on the quadratic error function, one obtains the following algorithm:

```{admonition} Batch perceptron

* **for** $M$ epochs:

    * $\mathbf{dw} = 0 \qquad db = 0$

    * **for** each sample $(\mathbf{x}_i, t_i)$:

        * $y_i =  \text{sign}( \langle \mathbf{w} \cdot \mathbf{x}_i \rangle + b)$

        * $\mathbf{dw} = \mathbf{dw} + (t_i - y_i) \, \mathbf{x}_i$

        * $db = db + (t_i - y_i)$

    * $\Delta \mathbf{w} = \eta \, \frac{1}{N} \, \mathbf{dw}$

    * $\Delta b = \eta \, \frac{1}{N} \, db$</code></pre>
<p>This is called the <strong>batch</strong> version of the Perceptron algorithm. If the data is linearly separable and <span class="math inline">\(\eta\)</span> is well chosen, it converges to the minimum of the mean square error.</p>
<table class="table">
<thead>
<tr class="header">
<th>```{figure} ../img/classification-animation.gif</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>width: 80%</td>
</tr>
</tbody>
</table>
<p>Batch perceptron algorithm.</p>
<pre><code>
The **Perceptron algorithm** was invented by the psychologist Frank Rosenblatt in 1958. It was the first algorithmic neural network able to learn linear classification.

```{admonition} Online perceptron algorithm

* **for** $M$ epochs:

    * **for** each sample $(\mathbf{x}_i, t_i)$:

        * $y_i =  \text{sign}( \langle \mathbf{w} \cdot \mathbf{x}_i \rangle + b)$

        * $\Delta \mathbf{w} = \eta \, (t_i - y_i) \, \mathbf{x}_i$

        * $\Delta b = \eta \, (t_i - y_i)$</code></pre>
<p>This algorithm iterates over all examples of the training set and applies the <strong>delta learning rule</strong> to each of them immediately, not at the end on the whole training set. One could check whether there are still classification errors on the training set at the end of each epoch and stop the algorithm. The delta learning rule depends as always on the learning rate <span class="math inline">\(\eta\)</span>, the error made by the prediction (<span class="math inline">\(t_i - y_i\)</span>) and the input <span class="math inline">\(\mathbf{x}_i\)</span>.</p>
<table class="table">
<thead>
<tr class="header">
<th>```{figure} ../img/classification-animation-online.gif</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>width: 80%</td>
</tr>
</tbody>
</table>
<p>Online perceptron algorithm.</p>
<pre><code>
### Stochastic Gradient descent

The mean square error is defined as the **expectation** over the data:

$$\mathcal{L}(\mathbf{w}, b) = \mathbb{E}_\mathcal{D} [(t_i - y_i)^2]$$

**Batch learning** uses the whole training set as samples to estimate the mse:

$$\mathcal{L}(\mathbf{w}, b) \approx \frac{1}{N} \, \sum_{i=1}^{N} (t_i - y_i)^2$$


$$
    \Delta \mathbf{w} = \eta \, \frac{1}{N} \sum_{i=1}^{N} (t_i - y_i ) \, \mathbf{x_i}
$$

**Online learning** uses a single sample to estimate the mse:

$$\mathcal{L}(\mathbf{w}, b) \approx (t_i - y_i)^2$$


$$
    \Delta \mathbf{w} = \eta \, (t_i - y_i) \, \mathbf{x_i}
$$

Batch learning has less bias (central limit theorem) and is less sensible to noise in the data, but is very slow. Online learning converges faster, but can be instable and overfits (high variance). 

In practice, we use a trade-off between batch and online learning called **Stochastic Gradient Descent (SGD)** or **Minibatch Gradient Descent**.

The training set is randomly split at each epoch into small chunks of data (a **minibatch**, usually 32 or 64 examples) and the batch learning rule is applied on each chunk.

$$
    \Delta \mathbf{w} = \eta \, \frac{1}{K} \sum_{i=1}^{K} (t_i - y_i) \, \mathbf{x_i}
$$

If the **batch size** is well chosen, SGD is as stable as batch learning and as fast as online learning. The minibatches are randomly selected at each epoch (i.i.d).



```{note}
Online learning is a stochastic gradient descent with a batch size of 1.</code></pre>
</section>
<section id="maximum-likelihood-estimation" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="maximum-likelihood-estimation"><span class="header-section-number">6.2</span> Maximum Likelihood Estimation</h2>
<div class="embed-container">
<iframe src="https://www.youtube.com/embed/9Hw6nLMiPiI" frameborder="0" allowfullscreen="">
</iframe>
</div>
<p>Let’s consider <span class="math inline">\(N\)</span> <strong>samples</strong> <span class="math inline">\(\{x_i\}_{i=1}^N\)</span> independently taken from a <strong>normal distribution</strong> <span class="math inline">\(X\)</span>. The probability density function (pdf) of a normal distribution is:</p>
<p><span class="math display">\[
    f(x ; \mu, \sigma) =  \frac{1}{\sqrt{2\pi \sigma^2}} \, \exp{- \frac{(x - \mu)^2}{2\sigma^2}}
\]</span></p>
<p>where <span class="math inline">\(\mu\)</span> is the mean of the distribution and <span class="math inline">\(\sigma\)</span> its standard deviation.</p>
<table class="table">
<thead>
<tr class="header">
<th>```{figure} ../img/MLE2.png</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>width: 80%</td>
</tr>
</tbody>
</table>
<p>Normal distributions with different parameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> explain the data with different likelihoods.</p>
<pre><code>
The problem is to find the values of $\mu$ and $\sigma$ which explain best the observations $\{x_i\}_{i=1}^N$.

The idea of MLE is to maximize the joint density function for all observations. This function is expressed by the **likelihood function**:

$$
    L(\mu, \sigma) = P( x ; \mu , \sigma  )  = \prod_{i=1}^{N} f(x_i ; \mu, \sigma )
$$

When the pdf takes high values for all samples, it is quite likely that the samples come from this particular distribution. The likelihood function reflects the probability that the parameters $\mu$ and $\sigma$ explain the observations $\{x_i\}_{i=1}^N$.

We therefore search for the values $\mu$ and $\sigma$ which **maximize** the likelihood function.

$$
    \text{max}_{\mu, \sigma} \quad L(\mu, \sigma) = \prod_{i=1}^{N} f(x_i ; \mu, \sigma )
$$


For the normal distribution, the likelihood function is:

$$
\begin{aligned}
    L(\mu, \sigma) &amp; = \prod_{i=1}^{N} f(x_i ; \mu, \sigma ) \\
                   &amp; = \prod_{i=1}^{N} \frac{1}{\sqrt{2\pi \sigma^2}} \, \exp{- \frac{(x_i - \mu)^2}{2\sigma^2}}\\
                   &amp; =  (\frac{1}{\sqrt{2\pi \sigma^2}})^N \, \prod_{i=1}^{N} \exp{- \frac{(x_i - \mu)^2}{2\sigma^2}}\\
                   &amp; =  (\frac{1}{\sqrt{2\pi \sigma^2}})^N \, \exp{- \frac{\sum_{i=1}^{N}(x_i - \mu)^2}{2\sigma^2}}\\
\end{aligned}
$$

To find the maximum of $L(\mu, \sigma)$, we need to search where the gradient is equal to zero:

$$
\begin{cases}
    \dfrac{\partial L(\mu, \sigma)}{\partial \mu} = 0 \\
    \dfrac{\partial L(\mu, \sigma)}{\partial \sigma} = 0 \\
\end{cases}
$$

The likelihood function is complex to differentiate, so we consider its logarithm $l(\mu, \sigma) = \log(L(\mu, \sigma))$ which has a maximum for the same value of $(\mu, \sigma)$ as the log function is monotonic.

$$
\begin{aligned}
    l(\mu, \sigma) &amp; = \log(L(\mu, \sigma)) \\
                   &amp; =  \log \left((\frac{1}{\sqrt{2\pi \sigma^2}})^N \, \exp{- \frac{\sum_{i=1}^{N}(x_i - \mu)^2}{2\sigma^2}} \right)\\
                   &amp; =  - \frac{N}{2} \log (2\pi \sigma^2) - \frac{\sum_{i=1}^{N}(x_i - \mu)^2}{2\sigma^2}\\
\end{aligned}
$$

$l(\mu, \sigma)$ is called the **log-likelihood** function. The maximum of the log-likelihood function respects:

$$
\begin{aligned}
    \frac{\partial l(\mu, \sigma)}{\partial \mu} &amp; = \frac{\sum_{i=1}^{N}(x_i - \mu)}{\sigma^2} = 0 \\
    \frac{\partial l(\mu, \sigma)}{\partial \sigma} &amp; = - \frac{N}{2} \frac{4 \pi \sigma}{2 \pi \sigma^2} + \frac{\sum_{i=1}^{N}(x_i - \mu)^2}{\sigma^3} \\
                                                    &amp; = - \frac{N}{\sigma} + \frac{\sum_{i=1}^{N}(x_i - \mu)^2}{\sigma^3} = 0\\
\end{aligned}
$$

We obtain:

$$
    \mu = \frac{1}{N} \sum_{i=1}^{N} x_i  \qquad\qquad    \sigma^2 = \frac{1}{N} \sum_{i=1}^{N}(x_i - \mu)^2
$$

Unsurprisingly, the mean and variance of the normal distribution which best explains the data are the mean and variance of the data...

The same principle can be applied to estimate the parameters of any distribution: normal, exponential, Bernouilli, Poisson, etc... When a machine learning method has an probabilistic interpretation (i.e. it outputs probabilities), MLE can be used to find its parameters. One can use global optimization like here, or gradient descent to estimate the parameters iteratively.


## Soft linear classification : Logistic regression

&lt;div class='embed-container'&gt;&lt;iframe src='https://www.youtube.com/embed/_Zc-k9pXVvE' frameborder='0' allowfullscreen&gt;&lt;/iframe&gt;&lt;/div&gt;

In logistic regression, we want to perform a regression, but where the targets $t_i$ are bounded betwen 0 and 1. We can use a logistic function instead of a linear function in order to transform the net activation into an output:

$$
\begin{aligned}
    y = \sigma(w \, x + b )  = \frac{1}{1+\exp(-w \, x - b )}
\end{aligned}
$$

Logistic regression can be used in binary classification if we consider $y = \sigma(w \, x + b )$ as the probability that the example belongs to the positive class ($t=1$).

$$
    P(t = 1 | x; w, b) = y ; \qquad P(t = 0 | x; w, b) = 1 - y
$$

The output $t$ therefore comes from a Bernouilli distribution $\mathcal{B}$ of parameter $p = y = f_{w, b}(x)$. The probability density function (pdf) is:

$$f(t | x; w, b) = y^t \, (1- y)^{1-t}$$


If we consider our training samples $(x_i, t_i)$ as independently taken from this distribution, our task is to find the parameterized distribution that best explains the data, which means to find the parameters $w$ and $b$ maximizing the **likelihood** that the samples $t$ come from a Bernouilli distribution when $x$, $w$  and $b$ are given. We only need to apply **Maximum Likelihood Estimation** (MLE) on this Bernouilli distribution!

The likelihood function for logistic regression is :

$$
\begin{aligned}
    L( w, b) &amp;= P( t | x; w,  b )  = \prod_{i=1}^{N} f(t_i | x_i;  w,  b ) \\
    &amp;= \prod_{i=1}^{N}  y_i^{t_i} \, (1- y_i)^{1-t_i}
\end{aligned}
$$

The likelihood function is quite hard to differentiate, so we take the **log-likelihood** function:

$$
\begin{aligned}
    l( w, b) &amp;= \log L( w, b) \\
    &amp;=  \sum_{i=1}^{N} [t_i \, \log y_i + (1 - t_i) \, \log( 1- y_i)]\\
\end{aligned}
$$

or even better: the **negative log-likelihood** which will be minimized using gradient descent:

$$
    \mathcal{L}( w, b) =  - \sum_{i=1}^{N} [t_i \, \log y_i + (1 - t_i) \, \log( 1- y_i)]
$$

We then search for the minimum of the negative log-likelihood function by computing its gradient (here for a single sample):

$$
\begin{aligned}
    \frac{\partial \mathcal{l}_i(w, b)}{\partial w}
        &amp;= -\frac{\partial}{\partial w} [ t_i \, \log y_i + (1 - t_i) \, \log( 1- y_i) ] \\
        &amp;= - t_i \, \frac{\partial}{\partial w} \log y_i - (1 - t_i) \, \frac{\partial}{\partial w}\log( 1- y_i) \\
        &amp;= - t_i \, \frac{\frac{\partial}{\partial w} y_i}{y_i} - (1 - t_i) \, \frac{\frac{\partial}{\partial w}( 1- y_i)}{1- y_i} \\
        &amp;= - t_i \, \frac{y_i \, (1 - y_i) \, x_i}{y_i} + (1 - t_i) \, \frac{y_i \, (1-y_i) \, x_i}{1 - y_i}\\
        &amp;= - ( t_i - y_i ) \, x_i\\
\end{aligned}
$$

We obtain the same gradient as the linear perceptron, but with a non-linear output function! Logistic regression is therefore a regression method used for classification. It uses a non-linear transfer function $\sigma(x)=\frac{1}{1+\exp(-x)}$ applied on the net activation:

$$
    y_i = \sigma(\langle \mathbf{w} \cdot \mathbf{x}_i \rangle + b )
$$

The continuous output $y$ is interpreted as the probability of belonging to the positive class.

$$
   P(t_i = 1 | \mathbf{x}_i; \mathbf{w}, b) = y_i ; \qquad P(t_i = 0 | \mathbf{x}_i; \mathbf{w}, b) = 1 - y_i
$$

We minimize the **negative log-likelihood** loss function:

$$
    \mathcal{L}(\mathbf{w}, b) =  - \sum_{i=1}^{N} [t_i \, \log y_i + (1 - t_i) \, \log( 1- y_i)]
$$

Gradient descent leads to the delta learning rule, using the class as a target and the probability as a prediction:

$$
    \begin{cases}
    \Delta \mathbf{w} = \eta \, ( t_i - y_i ) \, \mathbf{x}_i \\
    \\
    \Delta b = \eta \, ( t_i - y_i ) \\
    \end{cases}
$$

```{admonition} Logistic regression

* $\mathbf{w} = 0 \qquad b = 0$

* **for** $M$ epochs:

    * **for** each sample $(\mathbf{x}_i, t_i)$:

        * $y_i =  \sigma( \langle \mathbf{w} \cdot \mathbf{x}_i \rangle  + b)$

        * $\Delta \mathbf{w} = \eta \, (t_i - y_i) \, \mathbf{x}_i$

        * $\Delta b = \eta \, (t_i - y_i)$</code></pre>
<p>Logistic regression works just like linear classification, except in the way the prediction is done. To know to which class <span class="math inline">\(\mathbf{x}_i\)</span> belongs, simply draw a random number between 0 and 1:</p>
<ul>
<li>if it is smaller than <span class="math inline">\(y_i\)</span> (probability <span class="math inline">\(y_i\)</span>), it belongs to the positive class.</li>
<li>if it is bigger than <span class="math inline">\(y_i\)</span> (probability <span class="math inline">\(1-y_i\)</span>), it belongs to the negative class.</li>
</ul>
<p>Alternatively, you can put a <strong>hard limit</strong> at 0.5:</p>
<ul>
<li>if <span class="math inline">\(y_i &gt; 0.5\)</span> then the class is positive.</li>
<li>if <span class="math inline">\(y_i &lt; 0.5\)</span> then the class is negative.</li>
</ul>
<table class="table">
<thead>
<tr class="header">
<th>```{figure} ../img/logisticregression-animation.gif</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>width: 80%</td>
</tr>
</tbody>
</table>
<p>Logistic regression for soft classification. The confidence scores tells how certain the classification is. ```</p>
<p>Logistic regression also provides a <strong>confidence score</strong>: the closer <span class="math inline">\(y\)</span> is from 0 or 1, the more confident we can be that the classification is correct. This is particularly important in <strong>safety critical</strong> applications: if you detect the positive class but with a confidence of 0.51, you should perhaps not trust the prediction. If the confidence score is 0.99, you can probably trust the prediction.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../notes/2.2-LinearRegression.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Linear regression</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../notes/2.4-Multiclassification.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Multi-class classification</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>