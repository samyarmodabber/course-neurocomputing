<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.269">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Neurocomputing - 8&nbsp; Multi-layer perceptron</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../notes/3.2-DNN.html" rel="next">
<link href="../notes/2.4-LearningTheory.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-title">Multi-layer perceptron</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="../index.html" class="sidebar-logo-link">
      <img src="../notes/img/tuc-new.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Neurocomputing</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">Overview</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true"><strong>Introduction</strong></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/1.1-Introduction.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/1.2-Math.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Math basics (optional)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/1.3-Neurons.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Neurons</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true"><strong>Linear algorithms</strong></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.1-Optimization.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Optimization</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.2-LinearRegression.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Linear regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.3-LinearClassification.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Linear classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.4-LearningTheory.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Learning theory</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true"><strong>Neural networks</strong></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.1-NeuralNetworks.html" class="sidebar-item-text sidebar-link active"><span class="chapter-title">Multi-layer perceptron</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.2-DNN.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Modern neural networks</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true"><strong>Computer Vision</strong></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/4.1-CNN.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Convolutional neural networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/4.2-ObjectDetection.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Object detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/4.3-SemanticSegmentation.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Semantic segmentation</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true"><strong>Generative modeling</strong></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/5.1-Autoencoders.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Autoencoders</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/5.2-RBM.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Restricted Boltzmann machines (optional)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/5.3-GAN.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Generative adversarial networks</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true"><strong>Recurrent neural networks</strong></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/6.1-RNN.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Recurrent neural networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/6.2-NLP.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Natural Language Processing and attention</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="true"><strong>Self-supervised learning</strong></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/7.1-Transformers.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Transformers</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/7.2-ContrastiveLearning.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Contrastive Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/7.3-VisionTransformer.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Vision Transformers</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" aria-expanded="true"><strong>Outlook</strong></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/8.1-Limits.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Limits of deep learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/8.2-Beyond.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Beyond deep Learning</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" aria-expanded="true"><strong>Exercises</strong></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/Content.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">List of exercises</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/Installation.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Python installation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/1-Python-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Introduction To Python</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/2-Numpy-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Numpy and Matplotlib</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/3-LinearRegression-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Linear regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/4-MLR-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Multiple linear regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/5-Crossvalidation-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Cross-validation and polynomial regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/6-LinearClassification-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Linear classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/7-SoftmaxClassifier-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Softmax classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/8-MLP-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Multi-layer Perceptron</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/9-MNIST-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">MNIST classification using keras</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/10-CNN-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Convolutional neural networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/11-TransferLearning-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Transfer learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/12-VAE-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Variational autoencoder</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/13-RNN-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Recurrent neural networks</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#multi-layer-perceptron" id="toc-multi-layer-perceptron" class="nav-link active" data-scroll-target="#multi-layer-perceptron">Multi-layer perceptron</a>
  <ul class="collapse">
  <li><a href="#fully-connected-layer" id="toc-fully-connected-layer" class="nav-link" data-scroll-target="#fully-connected-layer">Fully-connected layer</a></li>
  <li><a href="#activation-functions" id="toc-activation-functions" class="nav-link" data-scroll-target="#activation-functions">Activation functions</a></li>
  <li><a href="#loss-function" id="toc-loss-function" class="nav-link" data-scroll-target="#loss-function">Loss function</a></li>
  <li><a href="#optimizer" id="toc-optimizer" class="nav-link" data-scroll-target="#optimizer">Optimizer</a></li>
  </ul></li>
  <li><a href="#backpropagation" id="toc-backpropagation" class="nav-link" data-scroll-target="#backpropagation">Backpropagation</a>
  <ul class="collapse">
  <li><a href="#backpropagation-on-a-shallow-network" id="toc-backpropagation-on-a-shallow-network" class="nav-link" data-scroll-target="#backpropagation-on-a-shallow-network">Backpropagation on a shallow network</a></li>
  <li><a href="#backpropagation-at-the-neuron-level" id="toc-backpropagation-at-the-neuron-level" class="nav-link" data-scroll-target="#backpropagation-at-the-neuron-level">Backpropagation at the neuron level</a></li>
  <li><a href="#universal-approximation-theorem" id="toc-universal-approximation-theorem" class="nav-link" data-scroll-target="#universal-approximation-theorem">Universal approximation theorem</a></li>
  <li><a href="#backpropagation-on-a-deep-neural-network" id="toc-backpropagation-on-a-deep-neural-network" class="nav-link" data-scroll-target="#backpropagation-on-a-deep-neural-network">Backpropagation on a deep neural network</a></li>
  </ul></li>
  <li><a href="#example-of-a-shallow-network" id="toc-example-of-a-shallow-network" class="nav-link" data-scroll-target="#example-of-a-shallow-network">Example of a shallow network</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-body" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-title">Multi-layer perceptron</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>Slides: <a href="../slides/3.1-NeuralNetworks.html" target="_blank">html</a> <a href="../slides/pdf/3.1-NeuralNetworks.pdf" target="_blank">pdf</a></p>
<section id="multi-layer-perceptron" class="level2">
<h2 class="anchored" data-anchor-id="multi-layer-perceptron">Multi-layer perceptron</h2>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/M5GwvFpzrjE" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
<p>A <strong>Multi-Layer Perceptron</strong> (MLP) or <strong>feedforward neural network</strong> is composed of:</p>
<ul>
<li>an input layer for the input vector <span class="math inline">\mathbf{x}</span></li>
<li>one or several hidden layers allowing to project non-linearly the input into a space of higher dimensions <span class="math inline">\mathbf{h}_1, \mathbf{h}_2, \mathbf{h}_3, \ldots</span>.</li>
<li>an output layer for the output <span class="math inline">\mathbf{y}</span>.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/mlp.svg" class="img-fluid figure-img" style="width:70.0%"></p>
<p></p><figcaption class="figure-caption">Multi-layer perceptron with one hidden layer.</figcaption><p></p>
</figure>
</div>
<p>If there is a single hidden layer <span class="math inline">\mathbf{h}</span> (shallow network), it corresponds to the feature space. Each layer takes inputs from the previous layer. If the hidden layer is adequately chosen, the output neurons can learn to replicate the desired output <span class="math inline">\mathbf{t}</span>.</p>
<section id="fully-connected-layer" class="level3">
<h3 class="anchored" data-anchor-id="fully-connected-layer">Fully-connected layer</h3>
<p>The operation performed by each layer can be written in the form of a <strong>matrix-vector</strong> multiplication:</p>
<p><span class="math display">
    \mathbf{h} = f(\textbf{net}_\mathbf{h}) = f(W^1 \, \mathbf{x} + \mathbf{b}^1)
</span> <span class="math display">
    \mathbf{y} = f(\textbf{net}_\mathbf{y}) = f(W^2 \, \mathbf{h} + \mathbf{b}^2)
</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/matrixvector.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Fully-connected layer.</figcaption><p></p>
</figure>
</div>
<p><strong>Fully-connected layers</strong> (FC) transform an input vector <span class="math inline">\mathbf{x}</span> into a new vector <span class="math inline">\mathbf{h}</span> by multiplying it by a <strong>weight matrix</strong> <span class="math inline">W</span> and adding a <strong>bias vector</strong> <span class="math inline">\mathbf{b}</span>. A non-linear <strong>activation function</strong> transforms each element of the net activation.</p>
</section>
<section id="activation-functions" class="level3">
<h3 class="anchored" data-anchor-id="activation-functions">Activation functions</h3>
<p>Here are some of the most useful activation functions in a MLP.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/ratecoded-transferfunctions.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Some classical activation functions in MLPs.</figcaption><p></p>
</figure>
</div>
<ul>
<li>Threshold function (output is binary)</li>
</ul>
<p><span class="math display">
    f(x) = \begin{cases} 1 \; \text{if} \; x \geq 0 \\ 0 \; \text{otherwise.} \end{cases}
</span></p>
<ul>
<li>Logistic / sigmoid function (output is continuous and bounded between 0 and 1)</li>
</ul>
<p><span class="math display">
    f(x) = \dfrac{1}{1 + \exp -x}
</span></p>
<ul>
<li>Hyperbolic function (output is continuous and bounded between -1 and 1)</li>
</ul>
<p><span class="math display">
    f(x) = \text{tanh}(x)
</span></p>
<ul>
<li>Rectified linear function - ReLU (output is continuous and positive).</li>
</ul>
<p><span class="math display">
    f(x) = \max(0, x) = \begin{cases} x \quad \text{if} \quad x \geq 0 \\ 0 \quad \text{otherwise.} \end{cases}
</span></p>
<ul>
<li>Parametric Rectifier Linear Unit - PReLU (output is continuous and unbounded).</li>
</ul>
<p><span class="math display">
    f(x) = \begin{cases} x \quad \text{if} \quad x \geq 0 \\ \alpha \, x  \quad \text{otherwise.}\end{cases}
</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/prelu.ppm" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">ReLU vs.&nbsp;PReLU activation functions.</figcaption><p></p>
</figure>
</div>
<p>For classification problems, the <strong>softmax</strong> activation function can be used in the output layer to make sure that the sum of the outputs <span class="math inline">\mathbf{y} = \{y_j\}</span> over all output neurons is one.</p>
<p><span class="math display">
    y_j = P(\text{class = j}) = \frac{\exp(\text{net}_j)}{\sum_k \exp(\text{net}_k)}
</span></p>
<p>The higher the net activation <span class="math inline">\text{net}_j</span>, the higher the probability that the example belongs to class <span class="math inline">j</span>. Softmax is not <em>per se</em> a transfer function (not local to each neuron), but the idea is similar.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Why not use the linear function <span class="math inline">f(x) = x</span> in the hidden layer?</p>
<p><span class="math display">
    \mathbf{h} = W^1 \, \mathbf{x} + \mathbf{b}^1
</span> <span class="math display">
    \mathbf{y} = W^2 \, \mathbf{h} + \mathbf{b}^2
</span></p>
<p>We would have:</p>
<p><span class="math display">
\begin{align*}
    \mathbf{y} &amp;= W^2 \, (W^1 \, \mathbf{x} + \mathbf{b}^1) + \mathbf{b}^2 \\
               &amp;= (W^2 \, W^1) \, \mathbf{x} + (W^2 \, \mathbf{b}^1 + \mathbf{b}^2) \\
               &amp;= W \, \mathbf{x} + \mathbf{b} \\
\end{align*}
</span></p>
<p>so the equivalent function would be linear…</p>
<p>Remember Cover’s theorem:</p>
<blockquote class="blockquote">
<p>A complex pattern-classification problem, cast in a high dimensional space <strong>non-linearly</strong>, is more likely to be linearly separable than in a low-dimensional space, provided that the space is not densely populated.</p>
</blockquote>
<p>In practice it does not matter how non-linear the function is (e.g PReLU is almost linear), but there must be at least one non-linearity.</p>
</div>
</div>
</section>
<section id="loss-function" class="level3">
<h3 class="anchored" data-anchor-id="loss-function">Loss function</h3>
<p>We have a training set composed of N input/output pairs <span class="math inline">(\mathbf{x}_i, \mathbf{t}_i)_{i=1..N}</span>. What are the free parameters <span class="math inline">\theta</span> (weights <span class="math inline">W^1, W^2</span> and biases <span class="math inline">\textbf{b}^1, \textbf{b}^2</span>) making the prediction <span class="math inline">\mathbf{y}</span> as close as possible from the desired output <span class="math inline">\mathbf{t}</span>?</p>
<p>We define a <strong>loss function</strong> <span class="math inline">\mathcal{L}(\theta)</span> of the free parameters which should be minimized:</p>
<ul>
<li>For <strong>regression</strong> problems, we take the <strong>mean square error</strong> (mse):</li>
</ul>
<p><span class="math display">
    \mathcal{L}_\text{reg}(\theta) = \mathbb{E}_{\mathbf{x}, \mathbf{t} \in \mathcal{D}} [ ||\mathbf{t} - \mathbf{y}||^2 ]
</span></p>
<ul>
<li>For <strong>classification</strong> problems, we take the <strong>cross-entropy</strong> or <strong>negative log-likelihood</strong> on a softmax output layer:</li>
</ul>
<p><span class="math display">\mathcal{L}_\text{class}(\theta) = \mathbb{E}_{\mathbf{x}, \mathbf{t} \sim \mathcal{D}} [ - \langle \mathbf{t} \cdot \log \mathbf{y} \rangle]</span></p>
</section>
<section id="optimizer" class="level3">
<h3 class="anchored" data-anchor-id="optimizer">Optimizer</h3>
<p>To minimize the chosen loss function, we are going to use <strong>stochastic gradient descent</strong> iteratively until the network converges:</p>
<p><span class="math display">\begin{cases}
    \Delta W^1 = - \eta \, \nabla_{W^1} \, \mathcal{L}(\theta) \\
    \\
    \Delta \mathbf{b}^1 = - \eta \, \nabla_{\mathbf{b}^1} \, \mathcal{L}(\theta) \\
    \\
    \Delta W^2 = - \eta \, \nabla_{W^2} \, \mathcal{L}(\theta) \\
    \\
    \Delta \mathbf{b}^2 = - \eta \, \nabla_{\mathbf{b}^2} \, \mathcal{L}(\theta)\\
\end{cases}
</span></p>
<p>We will see later that other optimizers than SGD can be used. The question is now how to compute efficiently these <strong>gradients</strong> w.r.t all the weights and biases. The algorithm to achieve this is called <strong>backpropagation</strong> <span class="citation" data-cites="Rumelhart1986a">(<a href="../references.html#ref-Rumelhart1986a" role="doc-biblioref">Rumelhart et al., 1986</a>)</span>, which is simply a smart implementation of the chain rule.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p><span class="citation" data-cites="Rumelhart1986a">(<a href="../references.html#ref-Rumelhart1986a" role="doc-biblioref">Rumelhart et al., 1986</a>)</span> did not actually invent backpropagation but merely popularized it in an article in Nature. Seppo Linnainmaa published the <strong>reverse mode of automatic differentiation</strong> in his Master thesis back in 1970 <span class="citation" data-cites="Linnainmaa1970">(<a href="../references.html#ref-Linnainmaa1970" role="doc-biblioref">Linnainmaa, 1970</a>)</span> and Paul Werbos applied it to deep neural networks in 1982 <span class="citation" data-cites="Werbos1982">(<a href="../references.html#ref-Werbos1982" role="doc-biblioref">Werbos, 1982</a>)</span>.</p>
<p>For a controversy about the origins of backpropagation and who should get credit for deep learning, see this strongly-worded post by Jürgen Schmidhuber:</p>
<p><a href="https://people.idsia.ch/~juergen/scientific-integrity-turing-award-deep-learning.html" class="uri">https://people.idsia.ch/~juergen/scientific-integrity-turing-award-deep-learning.html</a></p>
</div>
</div>
</section>
</section>
<section id="backpropagation" class="level2">
<h2 class="anchored" data-anchor-id="backpropagation">Backpropagation</h2>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/eq3uVwtzWmA" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
<section id="backpropagation-on-a-shallow-network" class="level3">
<h3 class="anchored" data-anchor-id="backpropagation-on-a-shallow-network">Backpropagation on a shallow network</h3>
<p>A shallow MLP computes:</p>
<p><span class="math display">
    \mathbf{h} = f(\textbf{net}_\mathbf{h}) = f(W^1 \, \mathbf{x} + \mathbf{b}^1)
</span> <span class="math display">
    \mathbf{y} = f(\textbf{net}_\mathbf{y}) = f(W^2 \, \mathbf{h} + \mathbf{b}^2)
</span></p>
<p>The chain rule gives us for the parameters of the output layer:</p>
<p><span class="math display">
    \frac{\partial \mathcal{L}(\theta)}{\partial W^2} = \frac{\partial \mathcal{L}(\theta)}{\partial \mathbf{y}} \times \frac{\partial \mathbf{y}}{\partial \textbf{net}_\mathbf{y}} \times \frac{\partial \textbf{net}_\mathbf{y}}{\partial W^2}
</span></p>
<p><span class="math display">
    \frac{\partial \mathcal{L}(\theta)}{\partial \mathbf{b}^2} = \frac{\partial \mathcal{L}(\theta)}{\partial \mathbf{y}} \times \frac{\partial \mathbf{y}}{\partial \textbf{net}_\mathbf{y}} \times \frac{\partial \textbf{net}_\mathbf{y}}{\partial \mathbf{b}^2}
</span></p>
<p>and for the hidden layer:</p>
<p><span class="math display">
    \frac{\partial \mathcal{L}(\theta)}{\partial W^1} = \frac{\partial \mathcal{L}(\theta)}{\partial \mathbf{y}} \times \frac{\partial \mathbf{y}}{\partial \textbf{net}_\mathbf{y}} \times \frac{\partial \textbf{net}_\mathbf{y}}{\partial \mathbf{h}} \times \frac{\partial \mathbf{h}}{\partial \textbf{net}_\mathbf{h}} \times \frac{\partial \textbf{net}_\mathbf{h}}{\partial W^1}
</span></p>
<p><span class="math display">
    \frac{\partial \mathcal{L}(\theta)}{\partial \mathbf{b}^1} = \frac{\partial \mathcal{L}(\theta)}{\partial \mathbf{y}} \times \frac{\partial \mathbf{y}}{\partial \textbf{net}_\mathbf{y}} \times \frac{\partial \textbf{net}_\mathbf{y}}{\partial \mathbf{h}} \times \frac{\partial \mathbf{h}}{\partial \textbf{net}_\mathbf{h}} \times \frac{\partial \textbf{net}_\mathbf{h}}{\partial \mathbf{b}^1}
</span></p>
<p>If we can compute all these partial derivatives / gradients individually, the problem is solved.</p>
<p>We have already seen for the linear algorithms that the derivative of the loss function w.r.t the net activation of the output <span class="math inline">\textbf{net}_\mathbf{y}</span> is proportional to the <strong>prediction error</strong> <span class="math inline">\mathbf{t} - \mathbf{y}</span>:</p>
<ul>
<li>mse for regression:</li>
</ul>
<p><span class="math display">
    \mathbf{\delta_y} = - \frac{\partial \mathcal{l}_\text{reg}(\theta)}{\partial \textbf{net}_\mathbf{y}} = - \frac{\partial \mathcal{l}_\text{reg}(\theta)}{\partial \mathbf{y}} \times \frac{\partial \mathbf{y}}{\partial \textbf{net}_\mathbf{y}} = 2 \, (\mathbf{t} - \mathbf{y}) \, f'(\textbf{net}_\mathbf{y})
</span></p>
<ul>
<li>cross-entropy using a softmax output layer:</li>
</ul>
<p><span class="math display">
    \mathbf{\delta_y} = - \frac{\partial \mathcal{l}_\text{class}(\theta)}{\partial \textbf{net}_\mathbf{y}} = (\mathbf{t} - \mathbf{y})
</span></p>
<p><span class="math inline">\mathbf{\delta_y} = - \dfrac{\partial \mathcal{l}(\theta)}{\partial \textbf{net}_\mathbf{y}}</span> is called the output error. The output error is going to appear in all partial derivatives, i.e.&nbsp;in all learning rules. The backpropagation algorithm is sometimes called <strong>backpropagation of the error</strong>.</p>
<p>We now have everything we need to train the output layer:</p>
<p><span class="math display">
    \frac{\partial \mathcal{l}(\theta)}{\partial W^2} = \frac{\partial \mathcal{l}(\theta)}{\partial \textbf{net}_\mathbf{y}}  \times \frac{\partial \textbf{net}_\mathbf{y}}{\partial W^2} = - \mathbf{\delta_y}  \times \frac{\partial \textbf{net}_\mathbf{y}}{\partial W^2}
</span></p>
<p><span class="math display">
    \frac{\partial \mathcal{l}(\theta)}{\partial \mathbf{b}^2} = \frac{\partial \mathcal{l}(\theta)}{\partial \textbf{net}_\mathbf{y}}  \times \frac{\partial \textbf{net}_\mathbf{y}}{\partial \mathbf{b}^2} = - \mathbf{\delta_y} \times \frac{\partial \textbf{net}_\mathbf{y}}{\partial \mathbf{b}^2}
</span></p>
<p>As <span class="math inline">\textbf{net}_\mathbf{y} = W^2 \, \mathbf{h} + \mathbf{b}^2</span>, we get for the cross-entropy loss:</p>
<p><span class="math display">
    \frac{\partial \mathcal{l}(\theta)}{\partial W^2} = - \mathbf{\delta_y} \times \mathbf{h}^T
</span></p>
<p><span class="math display">
    \frac{\partial \mathcal{l}(\theta)}{\partial \mathbf{b}^2} = - \mathbf{\delta_y}
</span></p>
<p>i.e.&nbsp;exactly the same delta learning rule as a softmax linear classifier or multiple linear regression using the vector <span class="math inline">\mathbf{h}</span> as an input.</p>
<p><span class="math display">
\begin{cases}
    \Delta W^2 = \eta \, \mathbf{\delta_y} \times \mathbf{h}^T = \eta \,  (\mathbf{t} - \mathbf{y} ) \times \mathbf{h}^T \\
    \\
    \Delta \mathbf{b}^2 = \eta \,  \mathbf{\delta_y} = \eta \,  (\mathbf{t} - \mathbf{y} ) \\
\end{cases}
</span></p>
<p>Let’s now note <span class="math inline">\mathbf{\delta_h}</span> the <strong>hidden error</strong>, i.e.&nbsp;minus the gradient of the loss function w.r.t the net activation of the hidden layer:</p>
<p><span class="math display">
    \mathbf{\delta_h} = - \frac{\partial \mathcal{l}(\theta)}{\partial \textbf{net}_\mathbf{h}} = - \frac{\partial \mathcal{l}(\theta)}{\partial \textbf{net}_\mathbf{y}} \times \frac{\partial \textbf{net}_\mathbf{y}}{\partial \mathbf{h}} \times \frac{\partial \mathbf{h}}{\partial \textbf{net}_\mathbf{h}}  = \mathbf{\delta_y} \times \frac{\partial \textbf{net}_\mathbf{y}}{\partial \mathbf{h}} \times \frac{\partial \mathbf{h}}{\partial \textbf{net}_\mathbf{h}}
</span></p>
<p>Using this hidden error, we can compute the gradients w.r.t <span class="math inline">W^1</span> and <span class="math inline">\mathbf{b}^1</span>:</p>
<p><span class="math display">
    \frac{\partial \mathcal{l}(\theta)}{\partial W^1} = \frac{\partial \mathcal{l}(\theta)}{\partial \textbf{net}_\mathbf{h}} \times \frac{\partial \textbf{net}_\mathbf{h}}{\partial W^1} = - \mathbf{\delta_h} \times \frac{\partial \textbf{net}_\mathbf{h}}{\partial W^1}
</span></p>
<p><span class="math display">
    \frac{\partial \mathcal{l}(\theta)}{\partial \mathbf{b}^1} = \frac{\partial \mathcal{l}(\theta)}{\partial \textbf{net}_\mathbf{h}} \times \frac{\partial \textbf{net}_\mathbf{h}}{\partial \mathbf{b}^1} = - \mathbf{\delta_h} \times \frac{\partial \textbf{net}_\mathbf{h}}{\partial \mathbf{b}^1}
</span></p>
<p>As <span class="math inline">\textbf{net}_\mathbf{h} = W^1 \, \mathbf{x} + \mathbf{b}^1</span>, we get:</p>
<p><span class="math display">
    \frac{\partial \mathcal{l}(\theta)}{\partial W^1} =  - \mathbf{\delta_h} \times \mathbf{x}^T
</span></p>
<p><span class="math display">
    \frac{\partial \mathcal{l}(\theta)}{\partial \mathbf{b}^1} =  - \mathbf{\delta_h}
</span></p>
<p>If we know the <strong>hidden error</strong> <span class="math inline">\mathbf{\delta_h}</span>, the update rules for the input weights <span class="math inline">W^1</span> and <span class="math inline">\mathbf{b}^1</span> also take the form of the delta learning rule:</p>
<p><span class="math display">
\begin{cases}
    \Delta W^1 = \eta \,   \mathbf{\delta_h} \times \mathbf{x}^T \\
    \\
    \Delta \mathbf{b}^1 = \eta \,  \mathbf{\delta_h} \\
\end{cases}
</span></p>
<p>This is the classical form eta * error * input. All we need to know is the <strong>backpropagated error</strong> <span class="math inline">\mathbf{\delta_h}</span> and we can apply the delta learning rule!</p>
<p>The backpropagated error <span class="math inline">\mathbf{\delta_h}</span> is a vector assigning an error to each of the hidden neurons:</p>
<p><span class="math display">
    \mathbf{\delta_h} = - \frac{\partial \mathcal{l}(\theta)}{\partial \textbf{net}_\mathbf{h}}  = \mathbf{\delta_y} \times \frac{\partial \textbf{net}_\mathbf{y}}{\partial \mathbf{h}} \times \frac{\partial \mathbf{h}}{\partial \textbf{net}_\mathbf{h}}
</span></p>
<p>As :</p>
<p><span class="math display">\textbf{net}_\mathbf{y} = W^2 \, \mathbf{h} + \mathbf{b}^2</span></p>
<p><span class="math display">\mathbf{h} = f(\textbf{net}_\mathbf{h})</span></p>
<p>we obtain:</p>
<p><span class="math display">
    \mathbf{\delta_h} = f'(\textbf{net}_\mathbf{h}) \, (W^2)^T \times \mathbf{\delta_y}
</span></p>
<p>If <span class="math inline">\mathbf{h}</span> and <span class="math inline">\mathbf{\delta_h}</span> have <span class="math inline">K</span> elements and <span class="math inline">\mathbf{y}</span> and <span class="math inline">\mathbf{\delta_y}</span> have <span class="math inline">C</span> elements, the matrix <span class="math inline">W^2</span> is <span class="math inline">C \times K</span> as <span class="math inline">W^2 \times \mathbf{h}</span> must be a vector with <span class="math inline">C</span> elements. <span class="math inline">(W^2)^T \times \mathbf{\delta_y}</span> is therefore a vector with <span class="math inline">K</span> elements, which is then multiplied element-wise with the derivative of the transfer function to obtain <span class="math inline">\mathbf{\delta_h}</span>.</p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Backpropagation for a shallow MLP
</div>
</div>
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/mlp.svg" class="img-fluid figure-img" style="width:70.0%"></p>
</figure>
</div>
<p>For a shallow MLP with one hidden layer:</p>
<p><span class="math display">
    \mathbf{h} = f(\textbf{net}_\mathbf{h}) = f(W^1 \, \mathbf{x} + \mathbf{b}^1)
</span> <span class="math display">
    \mathbf{y} = f(\textbf{net}_\mathbf{y}) = f(W^2 \, \mathbf{h} + \mathbf{b}^2)
</span></p>
<p>the output error:</p>
<p><span class="math display">
        \mathbf{\delta_y} = - \frac{\partial \mathcal{l}(\theta)}{\partial \textbf{net}_\mathbf{y}} = (\mathbf{t} - \mathbf{y})
    </span></p>
<p>is <strong>backpropagated</strong> to the hidden layer:</p>
<p><span class="math display">
    \mathbf{\delta_h} = f'(\textbf{net}_\mathbf{h}) \, (W^2)^T \times \mathbf{\delta_y}
</span></p>
<p>what allows to apply the delta learning rule to all parameters:</p>
<p><span class="math display">
\begin{cases}
    \Delta W^2 = \eta \, \mathbf{\delta_y} \times \mathbf{h}^T \\
    \Delta \mathbf{b}^2 = \eta \,  \mathbf{\delta_y} \\
    \Delta W^1 = \eta \, \mathbf{\delta_h}  \times  \mathbf{x}^T \\
    \Delta \mathbf{b}^1 = \eta \, \mathbf{\delta_h}  \\
\end{cases}
</span></p>
</div>
</div>
<p>The usual transfer functions are easy to derive (that is why they are chosen…).</p>
<ul>
<li>Threshold and sign functions are not differentiable, we simply consider the derivative is 1.</li>
</ul>
<p><span class="math display">
f(x) = \begin{cases} 1 \quad \text{if} \quad x \geq 0 \\ 0 \text{ or } 1 \quad \text{otherwise.} \end{cases} \qquad \rightarrow \qquad f'(x) = 1
</span></p>
<ul>
<li>The logistic or sigmoid function has the nice property that its derivative can be expressed as a function of itself:</li>
</ul>
<p><span class="math display">
f(x) = \frac{1}{1+\exp(-x)} \qquad \rightarrow \qquad f'(x) = f(x) \, (1 - f(x))
</span></p>
<ul>
<li>The hyperbolic tangent function too:</li>
</ul>
<p><span class="math display">
f(x) = \tanh(x) \qquad \rightarrow \qquad f'(x) = 1 - f(x)^2
</span></p>
<ul>
<li>ReLU is even simpler:</li>
</ul>
<p><span class="math display">
f(x) = \max(0, x) = \begin{cases} x \quad \text{if} \quad x \geq 0 \\ 0 \quad \text{otherwise.} \end{cases} \qquad \rightarrow \qquad f'(x) = \begin{cases} 1 \quad \text{if} \quad x \geq 0 \\ 0 \quad \text{otherwise.}\end{cases}
</span></p>
</section>
<section id="backpropagation-at-the-neuron-level" class="level3">
<h3 class="anchored" data-anchor-id="backpropagation-at-the-neuron-level">Backpropagation at the neuron level</h3>
<p>Let’s have a closer look at what is backpropagated using single neurons and weights. The output neuron <span class="math inline">y_k</span> computes:</p>
<p><span class="math display">y_k = f(\sum_{j=1}^K W^2_{jk} \, h_j + b^2_k)</span></p>
<p>All output weights <span class="math inline">W^2_{jk}</span> are updated proportionally to the output error of the neuron <span class="math inline">y_k</span>:</p>
<p><span class="math display">\Delta W^2_{jk} = \eta \, \delta_{{y}_k} \, h_j = \eta \, (t_k - y_k) \, h_j</span></p>
<p>This is possible because we know the output error directly from the data <span class="math inline">t_k</span>.</p>
<p>The hidden neuron <span class="math inline">h_j</span> computes:</p>
<p><span class="math display">h_j = f(\sum_{i=1}^d W^1_{ij} \, x_i + b^1_j)</span></p>
<p>We want to learn the hidden weights <span class="math inline">W^1_{ij}</span> using the delta learning rule:</p>
<p><span class="math display">\Delta W^1_{ij} = \eta \, \delta_{{h}_j} \, x_i</span></p>
<p>but we do not know the ground truth of the hidden neuron in the data:</p>
<p><span class="math display">\delta_{{h}_j} = (? - h_j)</span></p>
<p>We need to <strong>estimate</strong> the backpropagated error using the output error. If we omit the derivative of the transfer function, the backpropagated error for the hidden neuron <span class="math inline">h_j</span> is:</p>
<p><span class="math display">\delta_{{h}_j} = \sum_{k=1}^C W^2_{jk} \, \delta_{{y}_k}</span></p>
<p>The backpropagated error is an <strong>average</strong> of the output errors <span class="math inline">\delta_{{y}_k}</span>, weighted by the output weights between the hidden neuron <span class="math inline">h_j</span> and the output neurons <span class="math inline">y_k</span>.</p>
<p>The backpropagated error is the <strong>contribution</strong> of each hidden neuron <span class="math inline">h_j</span> to the output error:</p>
<ul>
<li>If there is no output error, there is no hidden error.</li>
<li>If a hidden neuron sends <strong>strong weights</strong> <span class="math inline">|W^2_{jk}|</span> to an output neuron <span class="math inline">y_k</span> with a strong prediction error <span class="math inline">\delta_{{y}_k}</span>, this means that it participates strongly to the output error and should learn from it.</li>
<li>If the weight <span class="math inline">|W^2_{jk}|</span> is small, it means that the hidden neuron does not take part in the output error.</li>
</ul>
</section>
<section id="universal-approximation-theorem" class="level3">
<h3 class="anchored" data-anchor-id="universal-approximation-theorem">Universal approximation theorem</h3>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Universal approximation theorem
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Cybenko, 1989</strong></p>
<p>Let <span class="math inline">\varphi()</span> be a nonconstant, bounded, and monotonically-increasing continuous function. Let <span class="math inline">I_{m_0}</span> denote the <span class="math inline">m_0</span>-dimensional unit hypercube <span class="math inline">[0,1]^{m_0}</span>. The space of continuous functions on <span class="math inline">I_{m_0}</span> is denoted by <span class="math inline">C(I_{m_0})</span>. Then, given any function <span class="math inline">f \in C(I_{m_0})</span> and <span class="math inline">\epsilon &gt; 0</span>, there exists an integer <span class="math inline">m_1</span> and sets of real constants <span class="math inline">\alpha_i, b_i</span> and <span class="math inline">w_{ij} \in \Re</span>, where <span class="math inline">i = 1, ..., m_1</span> and <span class="math inline">j = 1, ..., m_0</span> such that we may define:</p>
<p><span class="math display">
    F(\mathbf{x}) = \sum_{i=1}^{m_1} \alpha_i \cdot \varphi \left( \sum_{j=1}^{m_0} w_{ij} \cdot x_j + b_i \right)
</span></p>
<p>as an approximate realization of the function f; that is,</p>
<p><span class="math display"> | F(\mathbf{x}) - f(\mathbf{x})| &lt; \epsilon</span></p>
<p>for all <span class="math inline">x \in I_m</span>.</p>
</div>
</div>
<p>This theorem shows that for <strong>any</strong> input/output mapping function <span class="math inline">f</span> in supervised learning, there exists a MLP with <span class="math inline">m_1</span> neurons in the hidden layer which is able to approximate it with a desired precision!</p>
<p>The universal approximation theorem only proves the existence of a shallow MLP with <span class="math inline">m_1</span> neurons in the hidden layer that can approximate any function, but it does not tell how to find this number. A rule of thumb to find this number is that the generalization error is empirically close to:</p>
<p><span class="math display"> \epsilon = \frac{\text{VC}_{\text{dim}}(\text{MLP})}{N}
</span></p>
<p>where <span class="math inline">\text{VC}_{\text{dim}}(\text{MLP})</span> is the total number of weights and biases in the model, and <span class="math inline">N</span> the number of training samples.</p>
<p>The more neurons in the hidden layer, the better the training error, but the worse the generalization error (overfitting). The optimal number should be found with cross-validation methods. For most functions, the optimal number <span class="math inline">m_1</span> is high and becomes quickly computationally untractable. We need to go deep!</p>
</section>
<section id="backpropagation-on-a-deep-neural-network" class="level3">
<h3 class="anchored" data-anchor-id="backpropagation-on-a-deep-neural-network">Backpropagation on a deep neural network</h3>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/2Ew8Sz0Giw0" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
<p>A MLP with more than one hidden layer is a <strong>deep neural network</strong>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/deeplearning.png" class="img-fluid figure-img" style="width:80.0%"></p>
<p></p><figcaption class="figure-caption">Deep network extract progressively more complex features.</figcaption><p></p>
</figure>
</div>
<p>Backpropagation still works if we have many hidden layers <span class="math inline">\mathbf{h}_1, \ldots, \mathbf{h}_n</span>:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/deepforward.png" class="img-fluid figure-img" style="width:80.0%"></p>
<p></p><figcaption class="figure-caption">Forward pass. Source: David Silver <a href="https://icml.cc/2016/tutorials/deep_rl_tutorial.pdf" class="uri">https://icml.cc/2016/tutorials/deep_rl_tutorial.pdf</a></figcaption><p></p>
</figure>
</div>
<p>If each layer is differentiable, i.e.&nbsp;one can compute its gradient <span class="math inline">\frac{\partial \mathbf{h}_{k}}{\partial \mathbf{h}_{k-1}}</span>, we can chain <strong>backwards</strong> each partial derivatives to know how to update each layer.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/deepbackward.png" class="img-fluid figure-img" style="width:90.0%"></p>
<p></p><figcaption class="figure-caption">Backpropagation pass. Source: David Silver <a href="https://icml.cc/2016/tutorials/deep_rl_tutorial.pdf" class="uri">https://icml.cc/2016/tutorials/deep_rl_tutorial.pdf</a></figcaption><p></p>
</figure>
</div>
<p><strong>Backpropagation</strong> is simply an efficient implementation of the chain rule: the partial derivatives are iteratively reused in the backwards phase.</p>
<p>A fully connected layer transforms an input vector <span class="math inline">\mathbf{h}_{k-1}</span> into an output vector <span class="math inline">\mathbf{h}_{k}</span> using a weight matrix <span class="math inline">W^k</span>, a bias vector <span class="math inline">\mathbf{b}^k</span> and a non-linear activation function <span class="math inline">f</span>:</p>
<p><span class="math display">
    \mathbf{h}_{k} = f(\textbf{net}_{\mathbf{h}^k}) = f(W^k \, \mathbf{h}_{k-1} + \mathbf{b}^k)
</span></p>
<p>The gradient of its output w.r.t the input <span class="math inline">\mathbf{h}_{k-1}</span> is (using the chain rule):</p>
<p><span class="math display">
\frac{\partial \mathbf{h}_{k}}{\partial \mathbf{h}_{k-1}} = f'(\textbf{net}_{\mathbf{h}^k}) \, W^k
</span></p>
<p>The gradients of its output w.r.t the free parameters <span class="math inline">W^k</span> and <span class="math inline">\mathbf{b}_{k}</span> are:</p>
<p><span class="math display">
\frac{\partial \mathbf{h}_{k}}{\partial W^{k}} =  f'(\textbf{net}_{\mathbf{h}^k}) \, \mathbf{h}_{k-1}
</span></p>
<p><span class="math display">
\frac{\partial \mathbf{h}_{k}}{\partial \mathbf{b}_{k}} =  f'(\textbf{net}_{\mathbf{h}^k})
</span></p>
<p>A fully connected layer <span class="math inline">\mathbf{h}_{k} = f(W^k \, \mathbf{h}_{k-1} + \mathbf{b}^k)</span> receives the gradient of the loss function w.r.t. its output <span class="math inline">\mathbf{h}_{k}</span> from the layer above:</p>
<p><span class="math display">
    \frac{\partial \mathcal{L}(\theta)}{\partial \mathbf{h}_{k}}
</span></p>
<p>It adds to this gradient its own contribution and transmits it to the previous layer:</p>
<p><span class="math display">
    \frac{\partial \mathcal{L}(\theta)}{\partial \mathbf{h}_{k-1}} = \frac{\partial \mathcal{L}(\theta)}{\partial \mathbf{h}_{k}} \times \frac{\partial \mathbf{h}_{k}}{\partial \mathbf{h}_{k-1}} = f'(\textbf{net}_{\mathbf{h}^k}) \, (W^k)^T \times \frac{\partial \mathcal{L}(\theta)}{\partial \mathbf{h}_{k}}
</span></p>
<p>It then updates its parameters <span class="math inline">W^k</span> and <span class="math inline">\mathbf{b}_{k}</span> with:</p>
<p><span class="math display">\begin{cases}
\dfrac{\partial \mathcal{L}(\theta)}{\partial W^{k}} = \dfrac{\partial \mathcal{L}(\theta)}{\partial \mathbf{h}_{k}}  \times \dfrac{\partial \mathbf{h}_{k}}{\partial W^{k}} =  f'(\textbf{net}_{\mathbf{h}^k}) \, \dfrac{\partial \mathcal{L}(\theta)}{\partial \mathbf{h}_{k}}  \times \mathbf{h}_{k-1}^T \\
\\
\dfrac{\partial \mathcal{L}(\theta)}{\partial \mathbf{b}_{k}}  = \dfrac{\partial \mathcal{L}(\theta)}{\partial \mathbf{h}_{k}}  \times \dfrac{\partial \mathbf{h}_{k}}{\partial \mathbf{b}_{k}} =  f'(\textbf{net}_{\mathbf{h}^k}) \, \dfrac{\partial \mathcal{L}(\theta)}{\partial \mathbf{h}_{k}} \\
\end{cases}
</span></p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Training a deep neural network with backpropagation
</div>
</div>
<div class="callout-body-container callout-body">
<p>A <strong>feedforward</strong> neural network is an acyclic graph of differentiable and parameterized layers.</p>
<p><span class="math display">
    \mathbf{x} \rightarrow \mathbf{h}_1 \rightarrow \mathbf{h}_2 \rightarrow \ldots \rightarrow  \mathbf{h}_n \rightarrow \mathbf{y}
</span></p>
<p>The <strong>backpropagation</strong> algorithm is used to assign the gradient of the loss function <span class="math inline">\mathcal{L}(\theta)</span> to each layer using backward chaining:</p>
<p><span class="math display">
    \frac{\partial \mathcal{L}(\theta)}{\partial \mathbf{h}_{k-1}} = \frac{\partial \mathcal{L}(\theta)}{\partial \mathbf{h}_{k}} \times \frac{\partial \mathbf{h}_{k}}{\partial \mathbf{h}_{k-1}}
</span></p>
<p><strong>Stochastic gradient descent</strong> is then used to update the parameters of each layer:</p>
<p><span class="math display">
    \Delta W^k = - \eta \, \frac{\partial \mathcal{L}(\theta)}{\partial W^{k}} = - \eta \, \frac{\partial \mathcal{L}(\theta)}{\partial \mathbf{h}_{k}} \times \frac{\partial \mathbf{h}_{k}}{\partial W^{k}}
</span></p>
</div>
</div>
</section>
</section>
<section id="example-of-a-shallow-network" class="level2">
<h2 class="anchored" data-anchor-id="example-of-a-shallow-network">Example of a shallow network</h2>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/8B7RcBC11Lo" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
<p>Let’s try to solve this <strong>non-linear</strong> binary classification problem:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/mlp-data.png" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Non-linearly separable dataset.</figcaption><p></p>
</figure>
</div>
<p>We can create a shallow MLP with:</p>
<ul>
<li><p>Two input neurons <span class="math inline">x_1, x_2</span> for the two input variables.</p></li>
<li><p>Enough hidden neurons (e.g.&nbsp;20), with a sigmoid or ReLU activation function.</p></li>
<li><p>One output neuron with the logistic activation function.</p></li>
<li><p>The cross-entropy (negative log-likelihood) loss function.</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/mlp-example.svg" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Simple MLP for the non-linearly separable dataset.</figcaption><p></p>
</figure>
</div>
<p>We train it on the input data using the backpropagation algorithm and the SGD optimizer.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/mlp-animation.gif" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Prediction after each epoch of training.</figcaption><p></p>
</figure>
</div>
<p>Experiment with this network live on <a href="https://playground.tensorflow.org/" class="uri">https://playground.tensorflow.org/</a>! You will implement this MLP with numpy during exercise 8.</p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Automatic differentiation Deep Learning frameworks
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Current:</strong></p>
<ul>
<li><p><strong>Tensorflow</strong> <a href="https://www.tensorflow.org/" class="uri">https://www.tensorflow.org/</a> released by Google in 2015 is one of the two standard DL frameworks.</p></li>
<li><p><strong>Keras</strong> <a href="https://keras.io/" class="uri">https://keras.io/</a> is a high-level Python API over tensorflow (but also theano, CNTK and MxNet) written by Francois Chollet.</p></li>
<li><p><strong>PyTorch</strong> <a href="http://pytorch.org" class="uri">http://pytorch.org</a> by Facebook is the other standard framework.</p></li>
</ul>
<p><strong>Historical:</strong></p>
<ul>
<li><p><strong>Theano</strong> <a href="http://deeplearning.net/software/theano/" class="uri">http://deeplearning.net/software/theano/</a> released by U Toronto in 2010 is the predecessor of tensorflow. Now abandoned.</p></li>
<li><p><strong>Caffe</strong> <a href="http://caffe.berkeleyvision.org/" class="uri">http://caffe.berkeleyvision.org/</a> by U Berkeley was long the standard library for convolutional networks.</p></li>
<li><p><strong>CNTK</strong> <a href="https://github.com/Microsoft/CNTK" class="uri">https://github.com/Microsoft/CNTK</a> (Microsoft Cognitive Toolkit) is a <strong>free</strong> library by Microsoft!</p></li>
<li><p><strong>MxNet</strong> <a href="https://github.com/apache/incubator-mxnet" class="uri">https://github.com/apache/incubator-mxnet</a> from Apache became the DL framework at Amazon.</p></li>
</ul>
</div>
</div>
<p>Let’s implement the previous MLP using keras. We first need to generate the data using <code>scikit-learn</code>:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sklearn.datasets</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>X, t <span class="op">=</span> sklearn.datasets.make_circles(n_samples<span class="op">=</span><span class="dv">100</span>, shuffle<span class="op">=</span><span class="va">True</span>, noise<span class="op">=</span><span class="fl">0.15</span>, factor<span class="op">=</span><span class="fl">0.3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>We then import <code>tensorflow</code>:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The neural network is called a <code>Sequential</code> model in keras:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> tf.keras.Sequential()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Creating a NN is simply <strong>stacking</strong> layers in the model. The input layer is just a placeholder for the data:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>model.add( tf.keras.layers.Input(shape<span class="op">=</span>(<span class="dv">2</span>, )) )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The hidden layer has 20 neurons, the ReLU activation and takes input from the previous layer:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>model.add(</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    tf.keras.layers.Dense(</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>        <span class="dv">20</span>, <span class="co"># Number of hidden neurons</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>        activation<span class="op">=</span><span class="st">'relu'</span> <span class="co"># Activation function</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The output layer has 1 neuron with the logistic/sigmoid activation function:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>model.add(</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    tf.keras.layers.Dense(</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>        <span class="dv">1</span>, <span class="co"># Number of output neurons</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>        activation<span class="op">=</span><span class="st">'sigmoid'</span> <span class="co"># Soft classification</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>We now choose an optimizer (SGD) with a learning rate <span class="math inline">\eta = 0.001</span>:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> tf.keras.optimizers.SGD(lr<span class="op">=</span><span class="fl">0.001</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>We choose a loss function (binary cross-entropy, aka negative log-likelihood):</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> tf.keras.losses.binary_crossentropy</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>We compile the model (important!) and tell it to track the accuracy of the model:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>    loss<span class="op">=</span>loss,</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>    optimizer<span class="op">=</span>optimizer, </span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>    metrics<span class="op">=</span>tf.keras.metrics.categorical_accuracy</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Et voilà! The network has been created.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model.summary())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense (Dense)                (None, 20)                60        
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 21        
=================================================================
Total params: 81
Trainable params: 81
Non-trainable params: 0
_________________________________________________________________
None</code></pre>
<p>We now train the model on the data for 100 epochs using a batch size of 10 and wait for it to finish:</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>model.fit(X, t, batch_size<span class="op">=</span><span class="dv">10</span>, nb_epoch<span class="op">=</span><span class="dv">100</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>With keras (and the other automatic differentiation frameworks), you only need to define the structure of the network. The rest (backpropagation, SGD) is done automatically. To make predictions on new data, just do:</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>model.predict(X_test)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>


<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography" style="display: none">
<div id="ref-Linnainmaa1970" class="csl-entry" role="doc-biblioentry">
Linnainmaa, S. (1970). The representation of the cumulative rounding error of an algorithm as a <span>Taylor</span> expansion of the local rounding errors.
</div>
<div id="ref-Rumelhart1986a" class="csl-entry" role="doc-biblioentry">
Rumelhart, D. E., Hinton, G. E., and Williams, R. J. (1986). Learning representations by back-propagating errors. <em>Nature</em> 323, 533–536. doi:<a href="https://doi.org/10.1038/323533a0">10.1038/323533a0</a>.
</div>
<div id="ref-Werbos1982" class="csl-entry" role="doc-biblioentry">
Werbos, P. J. (1982). Applications of advances in nonlinear sensitivity analysis. in <em>System <span>Modeling</span> and <span>Optimization</span>: <span>Proc</span>. <span>IFIP</span></em> (<span>Springer</span>).
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation column-body">
  <div class="nav-page nav-page-previous">
      <a href="../notes/2.4-LearningTheory.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-title">Learning theory</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../notes/3.2-DNN.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-title">Modern neural networks</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
      <div class="nav-footer-center">Copyright 2022, Julien Vitay - <a href="mailto:julien.vitay@informatik.tu-chemnitz.de" class="email">julien.vitay@informatik.tu-chemnitz.de</a></div>
  </div>
</footer>



<script src="../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>