<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.251">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Neurocomputing - 9&nbsp; Modern neural networks</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../notes/4.1-CNN.html" rel="next">
<link href="../notes/3.1-NeuralNetworks.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-title">Modern neural networks</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="../" class="sidebar-logo-link">
      <img src="../notes/img/tuc-new.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Neurocomputing</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">Overview</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true"><strong>Introduction</strong></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/1.1-Introduction.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/1.2-Math.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Math basics (optional)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/1.3-Neurons.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Neurons</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true"><strong>Linear algorithms</strong></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.1-Optimization.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Optimization</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.2-LinearRegression.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Linear regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.3-LinearClassification.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Linear classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.4-LearningTheory.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Learning theory</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true"><strong>Neural networks</strong></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.1-NeuralNetworks.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Multi-layer perceptron</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.2-DNN.html" class="sidebar-item-text sidebar-link active"><span class="chapter-title">Modern neural networks</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true"><strong>Computer Vision</strong></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/4.1-CNN.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Convolutional neural networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/4.2-ObjectDetection.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Object detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/4.3-SemanticSegmentation.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Semantic segmentation</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true"><strong>Generative modeling</strong></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/5.1-Autoencoders.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Autoencoders</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/5.2-RBM.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Restricted Boltzmann machines (optional)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/5.3-GAN.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Generative adversarial networks</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true"><strong>Recurrent neural networks</strong></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/6.1-RNN.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Recurrent neural networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/6.2-NLP.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Natural Language Processing</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/6.3-Attention.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Attentional neural networks</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="true"><strong>Self-supervised learning</strong></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/7.1-Transformers.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Transformers</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/7.2-ContrastiveLearning.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Contrastive Learning</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" aria-expanded="true"><strong>Outlook</strong></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/8.1-Limits.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Limits of deep learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/8.2-Beyond.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Beyond deep Learning</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" aria-expanded="true"><strong>Exercises</strong></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/Content.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">List of exercises</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/Installation.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Python installation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/1-Python-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Introduction To Python</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/2-Numpy-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Numpy and Matplotlib</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/3-LinearRegression-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Linear regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/4-MLR-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Multiple linear regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/5-Crossvalidation-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Cross-validation and polynomial regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/6-LinearClassification-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Linear classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/7-SoftmaxClassifier-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Softmax classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/8-MLP-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Multi-layer Perceptron</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/9-MNIST-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">MNIST classification using keras</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/10-CNN-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Convolutional neural networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/11-TransferLearning-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Transfer learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/12-VAE-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Variational autoencoder</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/13-RNN-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Recurrent neural networks</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#why-deep-neural-networks" id="toc-why-deep-neural-networks" class="nav-link active" data-scroll-target="#why-deep-neural-networks">Why deep neural networks?</a></li>
  <li><a href="#bad-convergence" id="toc-bad-convergence" class="nav-link" data-scroll-target="#bad-convergence">Bad convergence</a>
  <ul class="collapse">
  <li><a href="#optimizers" id="toc-optimizers" class="nav-link" data-scroll-target="#optimizers">Optimizers</a></li>
  <li><a href="#hyperparameters-annealing" id="toc-hyperparameters-annealing" class="nav-link" data-scroll-target="#hyperparameters-annealing">Hyperparameters annealing</a></li>
  <li><a href="#hyperparameter-search" id="toc-hyperparameter-search" class="nav-link" data-scroll-target="#hyperparameter-search">Hyperparameter search</a></li>
  </ul></li>
  <li><a href="#long-training-times" id="toc-long-training-times" class="nav-link" data-scroll-target="#long-training-times">Long training times</a>
  <ul class="collapse">
  <li><a href="#importance-of-normalization" id="toc-importance-of-normalization" class="nav-link" data-scroll-target="#importance-of-normalization">Importance of normalization</a></li>
  <li><a href="#batch-normalization" id="toc-batch-normalization" class="nav-link" data-scroll-target="#batch-normalization">Batch normalization</a></li>
  <li><a href="#weight-initialization" id="toc-weight-initialization" class="nav-link" data-scroll-target="#weight-initialization">Weight initialization</a></li>
  </ul></li>
  <li><a href="#overfitting" id="toc-overfitting" class="nav-link" data-scroll-target="#overfitting">Overfitting</a>
  <ul class="collapse">
  <li><a href="#l2-and-l1-regularization" id="toc-l2-and-l1-regularization" class="nav-link" data-scroll-target="#l2-and-l1-regularization">L2 and L1 Regularization</a></li>
  <li><a href="#dropout" id="toc-dropout" class="nav-link" data-scroll-target="#dropout">Dropout</a></li>
  <li><a href="#data-augmentation" id="toc-data-augmentation" class="nav-link" data-scroll-target="#data-augmentation">Data augmentation</a></li>
  <li><a href="#early-stopping" id="toc-early-stopping" class="nav-link" data-scroll-target="#early-stopping">Early-Stopping</a></li>
  </ul></li>
  <li><a href="#vanishing-gradient" id="toc-vanishing-gradient" class="nav-link" data-scroll-target="#vanishing-gradient">Vanishing gradient</a>
  <ul class="collapse">
  <li><a href="#principle" id="toc-principle" class="nav-link" data-scroll-target="#principle">Principle</a></li>
  <li><a href="#derivative-of-the-activation-function" id="toc-derivative-of-the-activation-function" class="nav-link" data-scroll-target="#derivative-of-the-activation-function">Derivative of the activation function</a></li>
  </ul></li>
  <li><a href="#deep-neural-networks-in-practice" id="toc-deep-neural-networks-in-practice" class="nav-link" data-scroll-target="#deep-neural-networks-in-practice">Deep neural networks in practice</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-body" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-title">Modern neural networks</span></h1>
</div>



<div class="quarto-title-meta">

    
    
  </div>
  

</header>

<p>Slides: <a href="../slides/3.2-DeepNN.html" target="_blank">html</a> <a href="../slides/pdf/3.2-DeepNN.pdf" target="_blank">pdf</a></p>
<section id="why-deep-neural-networks" class="level2">
<h2 class="anchored" data-anchor-id="why-deep-neural-networks">Why deep neural networks?</h2>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/TWGcYiykAn4" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
<p>The <strong>universal approximation theorem</strong> (Cybenko, 1989) states that a <strong>shallow</strong> network can approximate any mapping function between inputs and outputs. However, if the mapping function is too complex, a shallow network may need too many hidden neurons.</p>
<p>The hidden neurons extract <strong>features</strong> in the input space: typical characteristics of the input which, when combined by the output neurons, allow to solve the classification task. Problem: the features are not hierarchically organized and cannot become complex enough.</p>
<p>Shallow networks can not work directly with raw images: noise, translation, rotation, scaling… One needs first to extract complex and useful features from the input images in order to classify them correctly.</p>
<p>A MLP with more than one hidden layer is a <strong>deep neural network</strong>. The different layers extract increasingly complex features.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/deeplearning.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>In practice, training a deep network is not as easy as the theory would suggest. Four main problems have to be solved:</p>
<ol type="1">
<li><p><strong>Bad convergence</strong>: the loss function has many local minima.</p>
<ul>
<li>Momentum, adaptive optimizers, annealing…</li>
</ul></li>
<li><p><strong>Long training time</strong>: deep networks use gradient descent-like optimizers, an iterative method whose speed depends on initialization.</p>
<ul>
<li>Normalized initialization, batch normalization…</li>
</ul></li>
<li><p><strong>Overfitting</strong>: deep networks have a lot of free parameters, so they tend to learn by heart the training set.</p>
<ul>
<li>Regularisation, dropout, data augmentation, early-stopping…</li>
</ul></li>
<li><p><strong>Vanishing gradient</strong>: the first layers may not receive sufficient gradients early in training.</p>
<ul>
<li>ReLU activation function, unsupervised pre-training, residual networks…</li>
</ul></li>
</ol>
</section>
<section id="bad-convergence" class="level2">
<h2 class="anchored" data-anchor-id="bad-convergence">Bad convergence</h2>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/UdcmfiDZrVU" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
<p>The loss function <span class="math inline">\mathcal{L}(\theta)</span> of a deep neural network has usually not a single global minimum, but many local minima: irregular <strong>loss landscape</strong>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/losslandscape.png" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Visualizing the loss landscape of neural nets <span class="citation" data-cites="Li2018">(<a href="../references.html#ref-Li2018" role="doc-biblioref">Li et al., 2018</a>)</span>.</figcaption><p></p>
</figure>
</div>
<p>Gradient descent gets stuck in local minima by design. One could perform different weight initializations, in order to find per chance an initial position close enough from the global minimum, but this is <strong>inefficient</strong>.</p>
<section id="optimizers" class="level3">
<h3 class="anchored" data-anchor-id="optimizers">Optimizers</h3>
<section id="stochastic-gradient-descent" class="level4">
<h4 class="anchored" data-anchor-id="stochastic-gradient-descent">Stochastic gradient descent</h4>
<p>What we actually want to minimize is the <strong>mathematical expectation</strong> of the square error (or any other loss) on the distribution of the data.</p>
<p><span class="math display">
    \mathcal{L}(\theta) = \mathbb{E}_\mathcal{D} (||\textbf{t} - \textbf{y}||^2)
</span></p>
<p>We do not have access to the true distribution of the data, so we have to estimate it through sampling.</p>
<ul>
<li><strong>Batch gradient descent</strong> estimates the loss function by sampling the whole training set:</li>
</ul>
<p><span class="math display">
    \mathcal{L}(\theta) \approx \frac{1}{N} \sum_{i=1}^N ||\textbf{t}_i - \textbf{y}_i||^2
</span></p>
<p>The estimated gradient is then unbiased (exact) and has no variance. Batch GD gets stuck in local minima.</p>
<ul>
<li><strong>Online gradient descent</strong> estimates the loss function by sampling a single example:</li>
</ul>
<p><span class="math display">
    \mathcal{L}(\theta) \approx ||\textbf{t}_i - \textbf{y}_i||^2
</span></p>
<p>The estimated gradient has a high variance (never right) but is unbiased on average. Online GD avoids local minima, but also global minima (unstable)…</p>
<ul>
<li><strong>Stochastic gradient descent</strong> samples <strong>minibatches</strong> of <span class="math inline">K</span> ~ 100 examples to approximate the mathematical expectation.</li>
</ul>
<p><span class="math display">
    \mathcal{L}(\theta) = E_\mathcal{D} (||\textbf{t} - \textbf{y}||^2) \approx \frac{1}{K} \sum_{i=1}^K ||\textbf{t}_i - \textbf{y}_i||^2
</span></p>
<p><span class="math display">
    \Delta \theta = - \eta \, \nabla_\theta  \, \mathcal{L}(\theta)
</span></p>
<p>This sampled loss has a high <strong>variance</strong>: take another minibatch and the gradient of the loss function will likely be very different. If the <strong>batch size</strong> is big enough, the estimated gradient is wrong, but usable on average (unbiased). The <strong>high variance</strong> of the estimated gradient helps getting out of local minimum: because our estimation of the gradient is often <strong>wrong</strong>, we get out of the local minima although we should have stayed in it. The <em>true</em> gradient is 0 for a local minimum, but its sampled value may not, so the parameters will be updated and hopefully get out of the local minimum. Which <strong>batch size</strong> works the best for your data? You need to use <strong>cross-validation</strong>, but beware that big batch sizes increase memory consumption, what can be a problem on GPUs.</p>
<p>Another issue with stochastic gradient descent is that it uses the same learning rate for all parameters. In <strong>ravines</strong> (which are common around minima), some parameters (or directions) have a higher influence on the loss function than others.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/ravine.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Ravine in the loss function. Source: <a href="https://distill.pub/2017/momentum/" class="uri">https://distill.pub/2017/momentum/</a>.</figcaption><p></p>
</figure>
</div>
<p>In the example above, you may want to go faster in the “horizontal” direction than in the “vertical” one, although the gradient is very small in the horizontal direction. With a fixed high learning rate for all parameters, SGD would start oscillating for the steep parameters, while being still very slow for the flat ones. The high variance of the sampled gradient is detrimental to performance as it can lead to oscillations. Most modern optimizers have a <strong>parameter-dependent adaptive learning rate</strong>.</p>
</section>
<section id="sgd-with-momentum" class="level4">
<h4 class="anchored" data-anchor-id="sgd-with-momentum">SGD with momentum</h4>
<p>One solution is to <strong>smooth</strong> the gradients over time (i.e.&nbsp;between minibatches), in order to avoid that one parameter is increased by one minibatch and decreased by the next one. The momentum method uses a <strong>moving average</strong> of the gradient (momentum step) to update the parameters:</p>
<p><span class="math display">
    v(\theta) = \alpha \, v(\theta) - (1 - \alpha)  \, \nabla_\theta  \, \mathcal{L}(\theta)
</span></p>
<p><span class="math display">
    \Delta \theta = \eta \,  v(\theta)
</span></p>
<p><span class="math inline">0 \leq \alpha &lt; 1</span> controls how much of the gradient we use for the parameter update (usually around 0.9). <span class="math inline">\alpha=0</span> is the vanilla SGD.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/momentum-sgd.gif" class="img-fluid figure-img"></p>
</figure>
</div>
<p>When the gradient for a single parameter has always the same direction between successive examples, gradient descent accelerates (bigger steps). When its sign changes, the weight changes continue in the same direction for while, allowing to “jump” over small local minima if the speed is sufficient. If the gradient keeps being in the opposite direction, the weight changes will finally reverse their direction. SGD with momentum uses an <strong>adaptive learning rate</strong>: the learning is implictly higher when the gradient does not reverse its sign (the estimate “accelerates”).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/momentum-goh.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">The momentum dampens oscillations around ravines. Source: <a href="https://distill.pub/2017/momentum/" class="uri">https://distill.pub/2017/momentum/</a>.</figcaption><p></p>
</figure>
</div>
<p>With momentum, the flat parameters keep increasing their update speed, while the steep ones slow down. SGD with momentum gets rid of oscillations at higher learning rates. The momentum method benefits a lot from the variance of SGD: noisy gradients are used to escape local minima but are averaged around the global minimum.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Check the great visualization by Gabriel Goh on <a href="https://distill.pub/2017/momentum/" class="uri">https://distill.pub/2017/momentum/</a>.</p>
</div>
</div>
</section>
<section id="sgd-with-nesterov-momentum" class="level4">
<h4 class="anchored" data-anchor-id="sgd-with-nesterov-momentum">SGD with Nesterov momentum</h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/momentum-vs-nesterov-momentum.gif" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">The momentum method tends to oscillate around the global minimum. Source: <a href="https://ikocabiyik.com/blog/en/visualizing-ml-optimizers/" class="uri">https://ikocabiyik.com/blog/en/visualizing-ml-optimizers/</a>.</figcaption><p></p>
</figure>
</div>
<p>SGD with momentum tends to oscillate around the minimum. The Nesterov momentum <strong>corrects</strong> these oscillations by estimating the gradient <strong>after</strong> the momentum update:</p>
<p><span class="math display">
    v(\theta) = \alpha \, v(\theta) - (1 - \alpha) \, \nabla_\theta  \, \mathcal{L}(\theta \color{red}{+ \alpha \, v(\theta)})
</span></p>
<p><span class="math display">
    \Delta \theta = \eta \,  v(\theta)
</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/nesterov.jpeg" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Difference between the momentum and Nesterov momentum. Source: <a href="https://cs231n.github.io/neural-networks-3/" class="uri">https://cs231n.github.io/neural-networks-3/</a></figcaption><p></p>
</figure>
</div>
</section>
<section id="rmsprop" class="level4">
<h4 class="anchored" data-anchor-id="rmsprop">RMSprop</h4>
<p>Instead of smoothing the gradient, what destroys information, one could adapt the learning rate to the <strong>curvature</strong> of the loss function:</p>
<ul>
<li>put the <strong>brakes</strong> on when the function is steep (high gradient).</li>
<li><strong>accelerate</strong> when the loss function is flat (plateau).</li>
</ul>
<p>RMSprop (Root Mean Square Propagation, proposed by Geoffrey Hinton in his lecture <a href="http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf" class="uri">http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf</a>) scales the learning rate by a running average of the squared gradient (second moment <span class="math inline">\approx</span> variance).</p>
<p><span class="math display">
    v(\theta) = \alpha \, v(\theta) + (1 - \alpha) \, (\nabla_\theta  \, \mathcal{L}(\theta))^2
</span></p>
<p><span class="math display">
    \Delta \theta = - \frac{\eta}{\epsilon + \sqrt{v(\theta)}} \, \nabla_\theta  \, \mathcal{L}(\theta)
</span></p>
<p>If the gradients vary a lot between two minibatches, the learning rate is reduced. If the gradients do not vary much, the learning rate is increased.</p>
</section>
<section id="adam" class="level4">
<h4 class="anchored" data-anchor-id="adam">Adam</h4>
<p>Adam (Adaptive Moment Estimation, <span class="citation" data-cites="Kingma2014">(<a href="../references.html#ref-Kingma2014" role="doc-biblioref">Kingma and Ba, 2014</a>)</span>) builds on the idea of RMSprop, but uses also a moving average of the gradient.</p>
<p><span class="math display">
    m(\theta) = \beta_1 m(\theta) + (1 - \beta_1) \, \nabla_\theta  \, \mathcal{L}(\theta)
</span> <span class="math display">
    v(\theta) = \beta_2 v(\theta) + (1 - \beta_2) \, \nabla_\theta  \, \mathcal{L}(\theta)^2
</span> <span class="math display">
    \Delta \theta = - \eta \, \frac{m(\theta)}{\epsilon + \sqrt{v(\theta)}}
</span></p>
<p>In short: Adam = RMSprop + momentum. Other possible optimizers: Adagrad, Adadelta, AdaMax, Nadam…</p>
</section>
<section id="comparison-of-modern-optimizers" class="level4">
<h4 class="anchored" data-anchor-id="comparison-of-modern-optimizers">Comparison of modern optimizers</h4>
<p>The different optimizers build on the idea of gradient descent and try to fix the main issues. They have different convergence properties, which can be seen in the figures below.</p>
<p>In practice, SGD with momentum allows to find better solutions (global minimum), but the meta-parameters are harder to find (need for cross-validation). Adam finds slightly poorer solutions, but the parameters <span class="math inline">\beta_1</span>, <span class="math inline">\beta_2</span> and <span class="math inline">\epsilon</span> can usually be kept at default, so it is a good idea to start with it, find the NN architecture that solves the problem and then replace it with SGD+momentum to fine-tune the performance.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/contours_evaluation_optimizers.gif" class="img-fluid figure-img" style="width:80.0%"></p>
<p></p><figcaption class="figure-caption">Source: Alec Radford <a href="https://imgur.com/a/Hqolp" class="uri">https://imgur.com/a/Hqolp</a></figcaption><p></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/saddle_point_evaluation_optimizers.gif" class="img-fluid figure-img" style="width:80.0%"></p>
<p></p><figcaption class="figure-caption">Source: Alec Radford <a href="https://imgur.com/a/Hqolp" class="uri">https://imgur.com/a/Hqolp</a>.</figcaption><p></p>
</figure>
</div>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The different optimizers are available in keras, see <a href="https://keras.io/api/optimizers" class="uri">https://keras.io/api/optimizers</a>.</p>
<ul>
<li>SGD:</li>
</ul>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> tf.keras.optimizers.SGD(learning_rate<span class="op">=</span><span class="fl">0.01</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li>SGD with Nesterov momentum:</li>
</ul>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> tf.keras.optimizers.SGD(</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    learning_rate<span class="op">=</span><span class="fl">0.01</span>, momentum<span class="op">=</span><span class="fl">0.9</span>, </span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    nesterov<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li>RMSprop:</li>
</ul>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> tf.keras.optimizers.RMSprop(</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    learning_rate<span class="op">=</span><span class="fl">0.001</span>, rho<span class="op">=</span><span class="fl">0.9</span>, </span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    momentum<span class="op">=</span><span class="fl">0.0</span>, epsilon<span class="op">=</span><span class="fl">1e-07</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li>Adam:</li>
</ul>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> tf.keras.optimizers.Adam(</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    learning_rate<span class="op">=</span><span class="fl">0.001</span>, beta_1<span class="op">=</span><span class="fl">0.9</span>, </span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    beta_2<span class="op">=</span><span class="fl">0.999</span>, epsilon<span class="op">=</span><span class="fl">1e-07</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</section>
</section>
<section id="hyperparameters-annealing" class="level3">
<h3 class="anchored" data-anchor-id="hyperparameters-annealing">Hyperparameters annealing</h3>
<p>Finding the optimal value for the hyperparameters (or metaparameters) of the network is not easy: learning rate <span class="math inline">\eta</span>, momentum <span class="math inline">\alpha</span>, etc.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/learningrates.jpeg" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Finding the optimal learning rate is difficult. Source: <a href="https://cs231n.github.io/neural-networks-3/" class="uri">https://cs231n.github.io/neural-networks-3/</a></figcaption><p></p>
</figure>
</div>
<p>For example, choosing <span class="math inline">\eta</span> too small leads to very slow learning. Choosing it too big can lead to oscillations and prevent convergence. A better strategy is to start with a big learning rate to “roughly” find the position of the global minimum and progressively decrease its value for a better convergence:</p>
<p><span class="math display">
    \eta \leftarrow (1 - \beta) \, \eta
</span></p>
<p>The decrease can be applied after each epoch. <span class="math inline">\beta</span> is called the <strong>decay rate</strong>, usually very small (<span class="math inline">10^{-6}</span>). The method is called <strong>annealing</strong> or <strong>scheduling</strong>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/learningratescheduler.jpeg" class="img-fluid figure-img" style="width:70.0%"></p>
<p></p><figcaption class="figure-caption">Exponentially-decaying learning rate. Source: <a href="https://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1" class="uri">https://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1</a></figcaption><p></p>
</figure>
</div>
<p>A simple trick to find a good estimate of the learning rate (or its start/stop value) is to increase its value exponentially <strong>for each minibatch</strong> at the beginning of learning. The “good” region for the learning rate is the one where the validation loss decreases, but does not oscillate.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/learningrate-selection.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Estimating the optimal range for the learning rate by increasing its value at the beginning of learning. Source <a href="https://towardsdatascience.com/advanced-topics-in-neural-networks-f27fbcc638ae" class="uri">https://towardsdatascience.com/advanced-topics-in-neural-networks-f27fbcc638ae</a>.</figcaption><p></p>
</figure>
</div>
</section>
<section id="hyperparameter-search" class="level3">
<h3 class="anchored" data-anchor-id="hyperparameter-search">Hyperparameter search</h3>
<p>Even with annealing, it is tricky to find the optimal value of the hyperparameters. The only option is to perform <strong>cross-validation</strong>, by varying the hyperparameters systematically and initializing the weights randomly every time. There are two basic strategies:</p>
<ul>
<li><strong>Grid search</strong>: different values of each parameter are chosen linearly (<span class="math inline">[0.1, 0.2, \ldots, 0.9]</span>) or logarithmically (<span class="math inline">[10^{-6}, 10^{-5}, \ldots, 10^{-1}]</span>).</li>
<li><strong>Random search</strong>: the value are randomly chosen each time from some distribution (uniform, normal, lognormal).</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/gridsearchbad.jpg" class="img-fluid figure-img" style="width:70.0%"></p>
<p></p><figcaption class="figure-caption">Grid-search vs.&nbsp;random search. Source: <a href="http://cs231n.github.io/neural-networks-3/" class="uri">http://cs231n.github.io/neural-networks-3/</a></figcaption><p></p>
</figure>
</div>
<p>The advantage of random search is that you can stop it anytime if you can not wait any longer. Grid search is very time-consuming, but easy to perform in parallel if you have clusters of CPUs or GPUs (<strong>data-parallel</strong>).</p>
<p>A more advanced and efficient technique is <strong>Bayesian hyperparameter optimization</strong>, for example the <strong>Tree Parzen Estimator</strong> (TPE) algorithm. The idea is to build a probability model of the objective function and use it to select the most promising hyperparameters to evaluate in the true objective function. Roughly speaking, it focuses parameter sampling on the interesting regions.</p>
<p>The <code>hyperopt</code> Python library <a href="https://github.com/hyperopt/hyperopt" class="uri">https://github.com/hyperopt/hyperopt</a> is extremely simple to use:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> hyperopt <span class="im">import</span> fmin, tpe, hp, STATUS_OK</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> objective(eta):</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Train model with:</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> tf.keras.optimizers.SGD(eta)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> {<span class="st">'loss'</span>: test_loss, <span class="st">'status'</span>: STATUS_OK }</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>best <span class="op">=</span> fmin(objective,</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    space<span class="op">=</span>hp.loguniform(<span class="st">'eta'</span>, <span class="op">-</span><span class="dv">6</span>, <span class="op">-</span><span class="dv">1</span>),</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    algo<span class="op">=</span>tpe.suggest,</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    max_evals<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> best</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="long-training-times" class="level2">
<h2 class="anchored" data-anchor-id="long-training-times">Long training times</h2>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/nnf253n2HCY" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
<section id="importance-of-normalization" class="level3">
<h3 class="anchored" data-anchor-id="importance-of-normalization">Importance of normalization</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/meanremoval.gif" class="img-fluid figure-img"></p>
</figure>
</div>
<p>If the data is not centered in the input space, the hyperplane (i.e.&nbsp;each neuron) may need a lot of iterations to “move” to the correct position using gradient descent. The initialization of the weights will matter a lot: if you start too far away from the solution, you will need many iterations.</p>
<p>If the data is normalized (zero mean, unit variance), the bias can be initialized to 0 and will converge much faster. Only the <strong>direction</strong> of the weight vector matters, not its norm, so it will be able to classify the data much faster.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/datanormalization.jpg" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Input data normalization. Source: <a href="http://cs231n.github.io/neural-networks-2/" class="uri">http://cs231n.github.io/neural-networks-2/</a></figcaption><p></p>
</figure>
</div>
<p>In practice, the input data <span class="math inline">X</span> <strong>must</strong> be normalized before training, in order to improve the training time:</p>
<ul>
<li><strong>Mean removal</strong> or <strong>zero-centering</strong>:</li>
</ul>
<p><span class="math display">
    X' = X - \mathbb{E}(X)
</span></p>
<ul>
<li><strong>Normalization</strong> : mean removal + <strong>unit variance</strong>:</li>
</ul>
<p><span class="math display">
    X' = \frac{X - \mathbb{E}(X)}{\text{Std}(X)}
</span></p>
<p><strong>Whitening</strong> goes one step further by first decorrelating the input dimensions (using <strong>Principal Component Analysis</strong> - PCA) and then scaling them so that the data lies in the unit sphere. It is better method than simple data normalization, but computationally expensive. When predicting on new data, do not forget to normalize/whiten them too!</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/datawhitening.jpg" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Input data whitening. Source: <a href="http://cs231n.github.io/neural-networks-2/" class="uri">http://cs231n.github.io/neural-networks-2/</a></figcaption><p></p>
</figure>
</div>
</section>
<section id="batch-normalization" class="level3">
<h3 class="anchored" data-anchor-id="batch-normalization">Batch normalization</h3>
<p>A single layer can learn very fast if its inputs are normalized with zero mean and unit variance. This is easy to do for the first layer, as one only need to preprocess the inputs <span class="math inline">\mathbf{x}</span>, but not the others. The outputs of the first layer are not normalized anymore, so learning in the second layer will be slow.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/batchnorm/batchnorm.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p><strong>Batch normalization</strong> <span class="citation" data-cites="Ioffe2015">(<a href="../references.html#ref-Ioffe2015" role="doc-biblioref">Ioffe and Szegedy, 2015</a>)</span> allows each layer to normalize its inputs on a <strong>single minibatch</strong>:</p>
<p><span class="math display">
    X_\mathcal{B}' = \frac{X_\mathcal{B} - E(X_\mathcal{B})}{\text{Std}(X_\mathcal{B})}
</span></p>
<p>The mean and variance will vary from one minibatch to another, but it does not matter. At the end of learning, the mean and variance over the whole training set is computed and stored. BN allows to more easily initialize the weights relative to the input strength and to use higher learning rates.</p>
<p>The <strong>Batch Normalization</strong> layer is usually placed between the FC layer and the activation function.It is differentiable w.r.t the input layer and the parameters, so backpropagation still works.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/batch_normalization.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">A batch normalization layer should be introduced between the linear net activation and the activation function. Source: <a href="http://heimingx.cn/2016/08/18/cs231n-neural-networks-part-2-setting-up-the-Data-and-the-loss/" class="uri">http://heimingx.cn/2016/08/18/cs231n-neural-networks-part-2-setting-up-the-Data-and-the-loss/</a></figcaption><p></p>
</figure>
</div>
</section>
<section id="weight-initialization" class="level3">
<h3 class="anchored" data-anchor-id="weight-initialization">Weight initialization</h3>
<p>Weight matrices are initialized randomly, but how they are initialized impacts performance a lot There are empirical rules to initialize the weights between two layers with <span class="math inline">N_{\text{in}}</span> and <span class="math inline">N_{\text{out}}</span> neurons.</p>
<ul>
<li><strong>Xavier</strong>: Uniform initialization (when using logistic or tanh, <span class="citation" data-cites="Glorot2010">(<a href="../references.html#ref-Glorot2010" role="doc-biblioref">Glorot and Bengio, 2010</a>)</span>):</li>
</ul>
<p><span class="math display">
    W \in \mathcal{U}( - \sqrt{\frac{6}{N_{\text{in}}+N_{\text{out}}}} , \sqrt{\frac{6}{N_{\text{in}}+N_{\text{out}}}}  )
</span></p>
<ul>
<li><strong>He</strong>: Gaussian initialization (when using ReLU or PReLU, <span class="citation" data-cites="He2015a">(<a href="../references.html#ref-He2015a" role="doc-biblioref">He et al., 2015</a>)</span>):</li>
</ul>
<p><span class="math display">
    W \in \mathcal{N}( 0 , \sqrt{\frac{2}{N_{\text{in}} }} )
</span></p>
<ul>
<li><p>When using BN, the bias <span class="math inline">b</span> can be initialized to 0.</p></li>
<li><p>Most frameworks (tensorflow, pytorch) initialize the weights correctly for you, but you can also control it.</p></li>
</ul>
</section>
</section>
<section id="overfitting" class="level2">
<h2 class="anchored" data-anchor-id="overfitting">Overfitting</h2>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/AflXkfzAzoo" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
<p>The main problem with deep NN is <strong>overfitting</strong>. With increasing depth, the network has too many weights = free parameters, so its VC dimension is high.</p>
<p><span class="math display">
    \epsilon = \frac{\text{VC}_\text{dim}}{N}
</span></p>
<p>The training error will be very small, but the generalization error high. <strong>The network learns the data, not the underlying function.</strong></p>
<p>We need to put constraints on the weights to reduce the VC dimension.</p>
<ul>
<li>If the weights move freely (i.e.&nbsp;can take any value), the VC dimension is equal to the number of free parameters.</li>
<li>If the weights cannot take any value they like, this implicitely reduces the VC dimension.</li>
</ul>
<p>In linear classification, the weights were unconstrained: the norm of the weight vector can take any value, as only its direction is important.</p>
<p><strong>Intuition:</strong> The norm of the weight vector influences the speed of learning in linear classification. A weight update on a strong weight has less influence than on a weak weight:</p>
<p><span class="math display">
    W \leftarrow W + \Delta W = W - \eta \, \frac{\partial \mathcal{l}(\theta)}{\partial W}
</span></p>
<p>as the gradient <span class="math inline">\frac{\partial \mathcal{l}(\theta)}{\partial W}</span> does not depend on the norm of the weights, only the output error.</p>
<section id="l2-and-l1-regularization" class="level3">
<h3 class="anchored" data-anchor-id="l2-and-l1-regularization">L2 and L1 Regularization</h3>
<p><strong><span class="math inline">\mathcal{L}_2</span> regularization</strong> keeps the <span class="math inline">\mathcal{L}_2</span> norm of the free parameters <span class="math inline">||\theta||</span> as small as possible during learning.</p>
<p><span class="math display">
    ||\theta||^2 = w_1^2 + w_2^2 + \dots + w_M^2
</span></p>
<p>Each neuron will use all its inputs with small weights, instead on specializing on a small part with high weights. Two things have to be minimized at the same time: the training loss and a <strong>penalty term</strong> representing the norm of the weights:</p>
<p><span class="math display">
    \mathcal{L}(\theta) = \mathbb{E}_\mathcal{D}( ||\mathbf{t} - \mathbf{y}||^2) + \lambda \, ||\theta||^2
</span></p>
<p>The <strong>regularization parameter</strong> <span class="math inline">\lambda</span> controls the strength of regularization:</p>
<ul>
<li>if <span class="math inline">\lambda</span> is small, there is only a small regularization, the weights can increase.</li>
<li>if <span class="math inline">\lambda</span> is high, the weights will be kept very small, but they may not minimize the training loss.</li>
</ul>
<p>Example of the mse loss with <span class="math inline">\mathcal{L}_2</span> regularization penalty term:</p>
<p><span class="math display">
    \mathcal{L}(\theta) = \mathbb{E}_\mathcal{D} [||\mathbf{t} - \mathbf{y}||^2] + \lambda \, ||\theta||^2
</span></p>
<p>The gradient of the new loss function is easy to find:</p>
<p><span class="math display">
    \nabla_\theta \mathcal{L}(\theta) = - 2 \, (\mathbf{t} - \mathbf{y}) \nabla_\theta \mathbf{y} + 2 \, \lambda \, \theta
</span></p>
<p>The parameter updates become:</p>
<p><span class="math display">
    \Delta \theta = \eta \, (\mathbf{t} - \mathbf{y}) \nabla_\theta \mathbf{y} - \eta \, \lambda \, \theta
</span></p>
<p><span class="math inline">\mathcal{L}_2</span> regularization leads to <strong>weight decay</strong>: even if there is no output error, the weight will converge to 0. This forces the weight to constantly learn: it can not specialize on a particular example anymore (overfitting) and is forced to generalize.</p>
<p><strong><span class="math inline">\mathcal{L}_1</span> regularization</strong> penalizes the absolute value of the weights instead of their Euclidian norm:</p>
<p><span class="math display">\mathcal{L}(\theta) = \mathbb{E}_\mathcal{D} [||\mathbf{t} - \mathbf{y}||^2] + \lambda \, |\theta|</span></p>
<p>It leads to very sparse representations: a lot of neurons will be inactive, and only a few will represent the input.</p>
</section>
<section id="dropout" class="level3">
<h3 class="anchored" data-anchor-id="dropout">Dropout</h3>
<p>Randomly dropping (inactivating) some neurons with a <strong>probability</strong> <span class="math inline">p</span> between two input presentations reduces the number of free parameters available for each learning phase. Multiple smaller networks (smaller VC dimension) are in fact learned in parallel on different data, but they share some parameters. This <strong>dropout</strong> method forces the network to generalize <span class="citation" data-cites="Srivastava2014">(<a href="../references.html#ref-Srivastava2014" role="doc-biblioref">Srivastava et al., 2014</a>)</span>. It is a form of regularization (mathematically equivalent to L2), now preferred in deep networks. <em>p</em> is usually around 0.5.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/dropout.gif" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Each new input <span class="math inline">\mathbf{x}</span> (or minibatch of inputs) is learned by a different neural network, but <strong>on average</strong>, the big neural network has learned the whole dataset without overfitting. Source: <a href="https://towardsdatascience.com/preventing-deep-neural-network-from-overfitting-953458db800a" class="uri">https://towardsdatascience.com/preventing-deep-neural-network-from-overfitting-953458db800a</a></figcaption><p></p>
</figure>
</div>
</section>
<section id="data-augmentation" class="level3">
<h3 class="anchored" data-anchor-id="data-augmentation">Data augmentation</h3>
<p>The best way to avoid overfitting is to use more data (with variability), but this is not always possible. A simple trick to have more data is <strong>data augmentation</strong>, i.e.&nbsp;modifying the inputs while keeping the output constant. For object recognition, it consists of applying various affine transformations (translation, rotation, scaling) on each input image, while keeping the label constant. This allows virtually infinite training sets.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/data_augmentation2.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Data augmentation. Source : <a href="https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html" class="uri">https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html</a></figcaption><p></p>
</figure>
</div>
</section>
<section id="early-stopping" class="level3">
<h3 class="anchored" data-anchor-id="early-stopping">Early-Stopping</h3>
<p>Early-stopping fights overfitting by monitoring the model’s performance on a validation set. A <strong>validation set</strong> is a set of examples that we never use for gradient descent, but which is also not a part of the test set. If the model’s performance ceases to improve sufficiently on the validation set, or even degrades with further optimization, we can either stop learning or modify some meta-parameters (learning rate, momentum, regularization…). The validation loss is usually lower than the training loss at the beginning of learning (underfitting), but becomes higher when the network overfits.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/earlystopping.png" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Early-stopping by checking the validation loss during training.</figcaption><p></p>
</figure>
</div>
</section>
</section>
<section id="vanishing-gradient" class="level2">
<h2 class="anchored" data-anchor-id="vanishing-gradient">Vanishing gradient</h2>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/NPUqQb9U-ww" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
<section id="principle" class="level3">
<h3 class="anchored" data-anchor-id="principle">Principle</h3>
<p>Contrary to what we could think, adding more layers to a DNN does not necessarily lead to a better performance, both on the training and test set. Here is the performance of neural networks with 20 or 56 layers on <strong>CIFAR-10</strong>:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/vanishinggradient-cifar.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Adding more layers does not necessarily increase the training or test accuracy on CIFAR-10. Source: <a href="https://towardsdatascience.com/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8" class="uri">https://towardsdatascience.com/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8</a></figcaption><p></p>
</figure>
</div>
<p>The main reason behind this is the <strong>vanishing gradient problem</strong>. The gradient of the loss function is repeatedly multiplied by a weight matrix <span class="math inline">W</span> as it travels backwards in a deep network.</p>
<p><span class="math display">
    \frac{\partial \mathbf{h}_{k}}{\partial \mathbf{h}_{k-1}} = f' (W^k \, \mathbf{h}_{k-1} + \mathbf{b}^k) \, W^k
</span></p>
<p>When it arrives in the first FC layer, the contribution of the weight matrices is comprised between:</p>
<p><span class="math display">
    (W_\text{min})^d \quad \text{and} \quad (W_\text{max})^d
</span></p>
<p>where <span class="math inline">W_\text{max}</span> (resp. <span class="math inline">W_\text{min}</span>) is the weight matrix with the highest (resp. lowest) norm, and <span class="math inline">d</span> is the depth of the network.</p>
<ul>
<li>If <span class="math inline">|W_\text{max}| &lt; 1</span>, then <span class="math inline">(W_\text{max})^d</span> is very small for high values of <span class="math inline">d</span> : <strong>the gradient vanishes</strong>.</li>
<li>If <span class="math inline">|W_\text{min}| &gt; 1</span>, then <span class="math inline">(W_\text{min})^d</span> is very high for high values of <span class="math inline">d</span> : <strong>the gradient explodes</strong>.</li>
</ul>
<p><strong>Exploding gradients</strong> can be solved by <strong>gradient clipping</strong>, i.e.&nbsp;normalizing the backpropagated gradient if its norm exceeds a threshold.</p>
<p><span class="math display">
    || \frac{\partial \mathcal{L}(\theta)}{\partial W^k}|| \gets \min(||\frac{\partial \mathcal{L}(\theta)}{\partial W^k}||, K)
</span></p>
<p><strong>Vanishing gradients</strong> are still the current limitation of deep networks. The solutions include: ReLU activation functions, unsupervised pre-training, batch normalization, <strong>residual networks</strong>…</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/vanishinggradient.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Vanishing gradient problem. Source: <a href="https://smartstuartkim.wordpress.com/2019/02/09/vanishing-gradient-problem/" class="uri">https://smartstuartkim.wordpress.com/2019/02/09/vanishing-gradient-problem/</a></figcaption><p></p>
</figure>
</div>
</section>
<section id="derivative-of-the-activation-function" class="level3">
<h3 class="anchored" data-anchor-id="derivative-of-the-activation-function">Derivative of the activation function</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/derivative_sigmoid.png" class="img-fluid figure-img" style="width:70.0%"></p>
<p></p><figcaption class="figure-caption">The derivative of the logistic function is 0 for extreme values, what blocks the backpropagation of the gradient.</figcaption><p></p>
</figure>
</div>
<p>Old-school MLP used logistic or tanh transfer functions for the hidden neurons, but their gradient is zero for very high or low net activations. If a neuron is saturated, it won’t transmit the gradient backwards, so the vanishing gradient is even worse. Deep networks now typically use the ReLU <span class="citation" data-cites="Maas2013">(<a href="../references.html#ref-Maas2013" role="doc-biblioref">Maas et al., 2013</a>)</span> or PReLU activation functions to improve convergence.</p>
<p><span class="math display">
    f'(x) = \begin{cases} 1 \qquad \text{if } x &gt; 0 \\
                          \alpha \qquad \text{if } x \leq 0 \\
            \end{cases}
</span></p>
<p>PReLU always backpropagates the gradient, so it helps fighting against vanishing gradient.</p>
</section>
</section>
<section id="deep-neural-networks-in-practice" class="level2">
<h2 class="anchored" data-anchor-id="deep-neural-networks-in-practice">Deep neural networks in practice</h2>
<p>The definition of a deep NN in keras can be as simple as:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tf.keras.models <span class="im">import</span> Sequential</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tf.keras.layers <span class="im">import</span> Input, Dense, Dropout, Activation, BatchNormalization</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tf.keras.optimizers <span class="im">import</span> Adam</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Sequential()</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>model.add(Input(<span class="dv">784</span>,))</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>model.add(Dense(<span class="dv">200</span>))</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>model.add(BatchNormalization())</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>model.add(Activation(<span class="st">'relu'</span>))</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>model.add(Dropout(<span class="fl">0.5</span>))</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>model.add(Dense(<span class="dv">100</span>)</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>model.add(BatchNormalization())</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>model.add(Activation(<span class="st">'relu'</span>))</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>model.add(Dropout(<span class="fl">0.5</span>))</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>model.add(Dense(units<span class="op">=</span><span class="dv">10</span>, activation<span class="op">=</span><span class="st">'softmax'</span>))</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">'categorical_crossentropy'</span>,</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>    optimizer<span class="op">=</span>Adam(lr<span class="op">=</span><span class="fl">0.01</span>, decay<span class="op">=</span><span class="fl">1e-6</span>), </span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>    metrics<span class="op">=</span>[<span class="st">'accuracy'</span>] </span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>If you want to successfully train a deep neural network, you should:</p>
<ul>
<li>Use as much data as possible, with <strong>data augmentation</strong> if needed.</li>
<li><strong>Normalize</strong> the inputs.</li>
<li>Use <strong>batch normalization</strong> in every layer and at least <strong>ReLU</strong>.</li>
<li>Use a good <strong>optimizer</strong> (SGD with momentum, Adam).</li>
<li><strong>Regularize</strong> learning (L2, L1, dropout).</li>
<li>Track overfitting on the validation set and use <strong>early-stopping</strong>.</li>
<li>Search for the best <strong>hyperparameters</strong> using grid search or hyperopt:
<ul>
<li>Learning rate, schedule, momentum, dropout level, number of layers/neurons, transfer functions, etc.</li>
</ul></li>
</ul>


<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography" style="display: none">
<div id="ref-Glorot2010" class="csl-entry" role="doc-biblioentry">
Glorot, X., and Bengio, Y. (2010). Understanding the difficulty of training deep feedforward neural networks. in <em><span>AISTATS</span></em>, 8.
</div>
<div id="ref-He2015a" class="csl-entry" role="doc-biblioentry">
He, K., Zhang, X., Ren, S., and Sun, J. (2015). Delving <span>Deep</span> into <span>Rectifiers</span>: <span>Surpassing Human-Level Performance</span> on <span>ImageNet Classification</span>. <a href="http://arxiv.org/abs/1502.01852">http://arxiv.org/abs/1502.01852</a>.
</div>
<div id="ref-Ioffe2015" class="csl-entry" role="doc-biblioentry">
Ioffe, S., and Szegedy, C. (2015). Batch <span>Normalization</span>: <span>Accelerating Deep Network Training</span> by <span>Reducing Internal Covariate Shift</span>. <a href="http://arxiv.org/abs/1502.03167">http://arxiv.org/abs/1502.03167</a>.
</div>
<div id="ref-Kingma2014" class="csl-entry" role="doc-biblioentry">
Kingma, D., and Ba, J. (2014). Adam: <span>A Method</span> for <span>Stochastic Optimization</span>. in <em>Proc. <span>ICLR</span></em>, 1–13. doi:<a href="https://doi.org/10.1145/1830483.1830503">10.1145/1830483.1830503</a>.
</div>
<div id="ref-Li2018" class="csl-entry" role="doc-biblioentry">
Li, H., Xu, Z., Taylor, G., Studer, C., and Goldstein, T. (2018). Visualizing the <span>Loss Landscape</span> of <span>Neural Nets</span>. <a href="http://arxiv.org/abs/1712.09913">http://arxiv.org/abs/1712.09913</a>.
</div>
<div id="ref-Maas2013" class="csl-entry" role="doc-biblioentry">
Maas, A. L., Hannun, A. Y., and Ng, A. Y. (2013). Rectifier <span>Nonlinearities Improve Neural Network Acoustic Models</span>. in <em><span>ICML</span></em>, 6.
</div>
<div id="ref-Srivastava2014" class="csl-entry" role="doc-biblioentry">
Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. (2014). Dropout: <span>A Simple Way</span> to <span>Prevent Neural Networks</span> from <span>Overfitting</span>. <em>Journal of Machine Learning Research</em> 15, 1929–1958. <a href="http://jmlr.org/papers/v15/srivastava14a.html">http://jmlr.org/papers/v15/srivastava14a.html</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
<nav class="page-navigation column-body">
  <div class="nav-page nav-page-previous">
      <a href="../notes/3.1-NeuralNetworks.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-title">Multi-layer perceptron</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../notes/4.1-CNN.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-title">Convolutional neural networks</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
      <div class="nav-footer-center">Copyright 2022, Julien Vitay - <a href="mailto:julien.vitay@informatik.tu-chemnitz.de" class="email">julien.vitay@informatik.tu-chemnitz.de</a></div>
  </div>
</footer>



<script src="../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>