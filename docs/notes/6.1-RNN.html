<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.269">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Neurocomputing - 16&nbsp; Recurrent neural networks</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../notes/6.2-NLP.html" rel="next">
<link href="../notes/5.3-GAN.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-title">Recurrent neural networks</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="../index.html" class="sidebar-logo-link">
      <img src="../notes/img/tuc-new.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Neurocomputing</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">Overview</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true"><strong>Introduction</strong></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/1.1-Introduction.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/1.2-Math.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Math basics (optional)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/1.3-Neurons.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Neurons</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true"><strong>Linear algorithms</strong></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.1-Optimization.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Optimization</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.2-LinearRegression.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Linear regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.3-LinearClassification.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Linear classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.4-LearningTheory.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Learning theory</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true"><strong>Neural networks</strong></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.1-NeuralNetworks.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Multi-layer perceptron</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.2-DNN.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Modern neural networks</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true"><strong>Computer Vision</strong></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/4.1-CNN.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Convolutional neural networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/4.2-ObjectDetection.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Object detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/4.3-SemanticSegmentation.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Semantic segmentation</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true"><strong>Generative modeling</strong></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/5.1-Autoencoders.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Autoencoders</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/5.2-RBM.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Restricted Boltzmann machines (optional)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/5.3-GAN.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Generative adversarial networks</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true"><strong>Recurrent neural networks</strong></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/6.1-RNN.html" class="sidebar-item-text sidebar-link active"><span class="chapter-title">Recurrent neural networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/6.2-NLP.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Natural Language Processing</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/6.3-Attention.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Attentional neural networks</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="true"><strong>Self-supervised learning</strong></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/7.1-Transformers.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Transformers</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/7.2-ContrastiveLearning.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Contrastive Learning</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" aria-expanded="true"><strong>Outlook</strong></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/8.1-Limits.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Limits of deep learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/8.2-Beyond.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Beyond deep Learning</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" aria-expanded="true"><strong>Exercises</strong></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/Content.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">List of exercises</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/Installation.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Python installation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/1-Python-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Introduction To Python</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/2-Numpy-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Numpy and Matplotlib</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/3-LinearRegression-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Linear regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/4-MLR-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Multiple linear regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/5-Crossvalidation-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Cross-validation and polynomial regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/6-LinearClassification-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Linear classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/7-SoftmaxClassifier-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Softmax classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/8-MLP-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Multi-layer Perceptron</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/9-MNIST-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">MNIST classification using keras</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/10-CNN-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Convolutional neural networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/11-TransferLearning-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Transfer learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/12-VAE-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Variational autoencoder</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/13-RNN-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Recurrent neural networks</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#recurrent-neural-networks" id="toc-recurrent-neural-networks" class="nav-link active" data-scroll-target="#recurrent-neural-networks">Recurrent neural networks</a>
  <ul class="collapse">
  <li><a href="#problem-with-feedforward-neural-networks" id="toc-problem-with-feedforward-neural-networks" class="nav-link" data-scroll-target="#problem-with-feedforward-neural-networks">Problem with feedforward neural networks</a></li>
  <li><a href="#recurrent-neural-network" id="toc-recurrent-neural-network" class="nav-link" data-scroll-target="#recurrent-neural-network">Recurrent neural network</a></li>
  <li><a href="#bptt-backpropagation-through-time" id="toc-bptt-backpropagation-through-time" class="nav-link" data-scroll-target="#bptt-backpropagation-through-time">BPTT: Backpropagation through time</a></li>
  <li><a href="#vanishing-gradients" id="toc-vanishing-gradients" class="nav-link" data-scroll-target="#vanishing-gradients">Vanishing gradients</a></li>
  </ul></li>
  <li><a href="#long-short-term-memory-networks---lstm" id="toc-long-short-term-memory-networks---lstm" class="nav-link" data-scroll-target="#long-short-term-memory-networks---lstm">Long short-term memory networks - LSTM</a>
  <ul class="collapse">
  <li><a href="#state-conveyor-belt" id="toc-state-conveyor-belt" class="nav-link" data-scroll-target="#state-conveyor-belt">State conveyor belt</a></li>
  <li><a href="#forget-gate" id="toc-forget-gate" class="nav-link" data-scroll-target="#forget-gate">Forget gate</a></li>
  <li><a href="#input-gate" id="toc-input-gate" class="nav-link" data-scroll-target="#input-gate">Input gate</a></li>
  <li><a href="#candidate-state" id="toc-candidate-state" class="nav-link" data-scroll-target="#candidate-state">Candidate state</a></li>
  <li><a href="#output-gate" id="toc-output-gate" class="nav-link" data-scroll-target="#output-gate">Output gate</a></li>
  <li><a href="#lstm-layer" id="toc-lstm-layer" class="nav-link" data-scroll-target="#lstm-layer">LSTM layer</a></li>
  <li><a href="#vanishing-gradients-1" id="toc-vanishing-gradients-1" class="nav-link" data-scroll-target="#vanishing-gradients-1">Vanishing gradients</a></li>
  <li><a href="#peephole-connections" id="toc-peephole-connections" class="nav-link" data-scroll-target="#peephole-connections">Peephole connections</a></li>
  <li><a href="#gru-gated-recurrent-unit" id="toc-gru-gated-recurrent-unit" class="nav-link" data-scroll-target="#gru-gated-recurrent-unit">GRU: Gated Recurrent Unit</a></li>
  <li><a href="#bidirectional-lstm" id="toc-bidirectional-lstm" class="nav-link" data-scroll-target="#bidirectional-lstm">Bidirectional LSTM</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-body" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-title">Recurrent neural networks</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>Slides: <a href="../slides/6.1-RNN.html" target="_blank">html</a> <a href="../slides/pdf/6.1-RNN.pdf" target="_blank">pdf</a></p>
<section id="recurrent-neural-networks" class="level2">
<h2 class="anchored" data-anchor-id="recurrent-neural-networks">Recurrent neural networks</h2>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/IIeWpVMgriI" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
<section id="problem-with-feedforward-neural-networks" class="level3">
<h3 class="anchored" data-anchor-id="problem-with-feedforward-neural-networks">Problem with feedforward neural networks</h3>
<p><strong>Feedforward neural networks</strong> learn to associate an input vector to an output.</p>
<p><span class="math display">\mathbf{y} = F_\theta(\mathbf{x})</span></p>
<p>If you present a sequence of inputs <span class="math inline">\mathbf{x}_0, \mathbf{x}_1, \ldots, \mathbf{x}_t</span> to a feedforward network, the outputs will be independent from each other:</p>
<p><span class="math display">\mathbf{y}_0 = F_\theta(\mathbf{x}_0)</span> <span class="math display">\mathbf{y}_1 = F_\theta(\mathbf{x}_1)</span> <span class="math display">\dots</span> <span class="math display">\mathbf{y}_t = F_\theta(\mathbf{x}_t)</span></p>
<p>Many problems depend on time series, such as predicting the future of a time series by knowing its past values:</p>
<p><span class="math display">x_{t+1} = F_\theta(x_0, x_1, \ldots, x_t)</span></p>
<p>Example: weather prediction, financial prediction, predictive maintenance, natural language processing, video analysis…</p>
<p>A naive solution is to <strong>aggregate</strong> (concatenate) inputs over a sufficiently long window and use it as a new input vector for the feedforward network.</p>
<p><span class="math display">\mathbf{X} = \begin{bmatrix}\mathbf{x}_{t-T} &amp; \mathbf{x}_{t-T+1} &amp; \ldots &amp; \mathbf{x}_t \\ \end{bmatrix}</span></p>
<p><span class="math display">\mathbf{y}_t = F_\theta(\mathbf{X})</span></p>
<ul>
<li><strong>Problem 1:</strong> How long should the window be?</li>
<li><strong>Problem 2:</strong> Having more input dimensions increases dramatically the complexity of the classifier (VC dimension), hence the number of training examples required to avoid overfitting.</li>
</ul>
</section>
<section id="recurrent-neural-network" class="level3">
<h3 class="anchored" data-anchor-id="recurrent-neural-network">Recurrent neural network</h3>
<p>A <strong>recurrent neural network</strong> (RNN) uses it previous output as an additional input (<em>context</em>). All vectors have a time index <span class="math inline">t</span> denoting the time at which this vector was computed.</p>
<p>The input vector at time <span class="math inline">t</span> is <span class="math inline">\mathbf{x}_t</span>, the output vector is <span class="math inline">\mathbf{h}_t</span>:</p>
<p><span class="math display">
    \mathbf{h}_t = \sigma(W_x \times \mathbf{x}_t + W_h \times \mathbf{h}_{t-1} + \mathbf{b})
</span></p>
<p><span class="math inline">\sigma</span> is a transfer function, usually logistic or tanh. The input <span class="math inline">\mathbf{x}_t</span> and previous output <span class="math inline">\mathbf{h}_{t-1}</span> are multiplied by <strong>learnable weights</strong>:</p>
<ul>
<li><span class="math inline">W_x</span> is the input weight matrix.</li>
<li><span class="math inline">W_h</span> is the recurrent weight matrix.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/RNN-rolled.png" class="img-fluid figure-img" style="width:30.0%"></p>
<p></p><figcaption class="figure-caption">Recurrent neural network. Source: <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs" class="uri">http://colah.github.io/posts/2015-08-Understanding-LSTMs</a>.</figcaption><p></p>
</figure>
</div>
<p>One can <strong>unroll</strong> a recurrent network: the output <span class="math inline">\mathbf{h}_t</span> depends on the whole history of inputs from <span class="math inline">\mathbf{x}_0</span> to <span class="math inline">\mathbf{x}_t</span>.</p>
<p><span class="math display">
\begin{aligned}
    \mathbf{h}_t &amp; = \sigma(W_x \times \mathbf{x}_t + W_h \times \mathbf{h}_{t-1} + \mathbf{b}) \\
                 &amp;\\
                 &amp; = \sigma(W_x \times \mathbf{x}_t + W_h \times \sigma(W_x \times \mathbf{x}_{t-1} + W_h \times \mathbf{h}_{t-2} + \mathbf{b})  + \mathbf{b}) \\
                 &amp;\\
                 &amp; = f_{W_x, W_h, \mathbf{b}} (\mathbf{x}_0, \mathbf{x}_1, \dots,\mathbf{x}_t) \\
\end{aligned}
</span></p>
<p>A RNN is considered as part of <strong>deep learning</strong>, as there are many layers of weights between the first input <span class="math inline">\mathbf{x}_0</span> and the output <span class="math inline">\mathbf{h}_t</span>. The only difference with a DNN is that the weights <span class="math inline">W_x</span> and <span class="math inline">W_h</span> are <strong>reused</strong> at each time step.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/RNN-unrolled.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Recurrent neural network unrolled. Source: <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs" class="uri">http://colah.github.io/posts/2015-08-Understanding-LSTMs</a>.</figcaption><p></p>
</figure>
</div>
</section>
<section id="bptt-backpropagation-through-time" class="level3">
<h3 class="anchored" data-anchor-id="bptt-backpropagation-through-time">BPTT: Backpropagation through time</h3>
<p>The function between the history of inputs and the output at time <span class="math inline">t</span> is differentiable: we can simply apply gradient descent to find the weights! This variant of backpropagation is called <strong>Backpropagation Through Time</strong> (BPTT). Once the loss between <span class="math inline">\mathbf{h}_t</span> and its desired value is computed, one applies the <strong>chain rule</strong> to find out how to modify the weights <span class="math inline">W_x</span> and <span class="math inline">W_h</span> using the history <span class="math inline">(\mathbf{x}_0, \mathbf{x}_1, \ldots, \mathbf{x}_t)</span>.</p>
<p>Let’s compute the gradient accumulated between <span class="math inline">\mathbf{h}_{t-1}</span> and <span class="math inline">\mathbf{h}_{t}</span>:</p>
<p><span class="math display">
\begin{aligned}
    \mathbf{h}_{t} &amp; = \sigma(W_x \times \mathbf{x}_{t} + W_h \times \mathbf{h}_{t-1} + \mathbf{b}) \\
\end{aligned}
</span></p>
<p>As for feedforward networks, the gradient of the loss function is decomposed into two parts:</p>
<p><span class="math display">
    \frac{\partial \mathcal{L}(W_x, W_h)}{\partial W_x} =
    \frac{\partial \mathcal{L}(W_x, W_h)}{\partial \mathbf{h}_t} \times
    \frac{\partial \mathbf{h}_t}{\partial W_x}
</span></p>
<p><span class="math display">
    \frac{\partial \mathcal{L}(W_x, W_h)}{\partial W_h} =
    \frac{\partial \mathcal{L}(W_x, W_h)}{\partial \mathbf{h}_t} \times
    \frac{\partial \mathbf{h}_t}{\partial W_h}
</span></p>
<p>The first part only depends on the loss function (mse, cross-entropy):</p>
<p><span class="math display">
    \frac{\partial \mathcal{L}(W_x, W_h)}{\partial \mathbf{h}_t} = - (\mathbf{t}_{t}- \mathbf{h}_{t})
</span></p>
<p>The second part depends on the RNN itself:</p>
<p><span class="math display">
\begin{aligned}
    \mathbf{h}_{t} &amp; = \sigma(W_x \times \mathbf{x}_{t} + W_h \times \mathbf{h}_{t-1} + \mathbf{b}) \\
\end{aligned}
</span></p>
<p>The gradients w.r.t the two weight matrices are given by this <strong>recursive</strong> relationship (product rule):</p>
<p><span class="math display">
\begin{aligned}
    \frac{\partial \mathbf{h}_t}{\partial W_x} &amp; = \mathbf{h'}_{t} \times (\mathbf{x}_t + W_h \times \frac{\partial \mathbf{h}_{t-1}}{\partial W_x})\\
    &amp; \\
    \frac{\partial \mathbf{h}_t}{\partial W_h} &amp; = \mathbf{h'}_{t} \times (\mathbf{h}_{t-1} + W_h \times \frac{\partial \mathbf{h}_{t-1}}{\partial W_h})\\
\end{aligned}
</span></p>
<p>The derivative of the transfer function is noted <span class="math inline">\mathbf{h'}_{t}</span>:</p>
<p><span class="math display">
    \mathbf{h'}_{t} = \begin{cases}
        \mathbf{h}_{t} \, (1 - \mathbf{h}_{t}) \quad \text{ for logistic}\\
        (1 - \mathbf{h}_{t}^2) \quad \text{ for tanh.}\\
    \end{cases}
</span></p>
<p>If we <strong>unroll</strong> the gradient, we obtain:</p>
<p><span class="math display">
\begin{aligned}
    \frac{\partial \mathbf{h}_t}{\partial W_x} &amp; = \mathbf{h'}_{t} \, (\mathbf{x}_t + W_h \times \mathbf{h'}_{t-1} \, (\mathbf{x}_{t-1} + W_h \times \mathbf{h'}_{t-2} \, (\mathbf{x}_{t-2} + W_h \times \ldots (\mathbf{x}_0))))\\
    &amp; \\
    \frac{\partial \mathbf{h}_t}{\partial W_h} &amp; = \mathbf{h'}_{t} \, (\mathbf{h}_{t-1} + W_h \times \mathbf{h'}_{t-1} \, (\mathbf{h}_{t-2} + W_h \times \mathbf{h'}_{t-2} \, \ldots (\mathbf{h}_{0})))\\
\end{aligned}
</span></p>
<p>When updating the weights at time <span class="math inline">t</span>, we need to store in memory:</p>
<ul>
<li>the complete history of inputs <span class="math inline">\mathbf{x}_0</span>, <span class="math inline">\mathbf{x}_1</span>, … <span class="math inline">\mathbf{x}_t</span>.</li>
<li>the complete history of outputs <span class="math inline">\mathbf{h}_0</span>, <span class="math inline">\mathbf{h}_1</span>, … <span class="math inline">\mathbf{h}_t</span>.</li>
<li>the complete history of derivatives <span class="math inline">\mathbf{h'}_0</span>, <span class="math inline">\mathbf{h'}_1</span>, … <span class="math inline">\mathbf{h'}_t</span>.</li>
</ul>
<p>before computing the gradients iteratively, starting from time <span class="math inline">t</span> and accumulating gradients <strong>backwards</strong> in time until <span class="math inline">t=0</span>. Each step backwards in time adds a bit to the gradient used to update the weights.</p>
<p>In practice, going back to <span class="math inline">t=0</span> at each time step requires too many computations, which may not be needed. <strong>Truncated BPTT</strong> only updates the gradients up to <span class="math inline">T</span> steps before: the gradients are computed backwards from <span class="math inline">t</span> to <span class="math inline">t-T</span>. The partial derivative in <span class="math inline">t-T-1</span> is considered 0. This limits the <strong>horizon</strong> of BPTT: dependencies longer than <span class="math inline">T</span> will not be learned, so it has to be chosen carefully for the task. <span class="math inline">T</span> becomes yet another hyperparameter of your algorithm…</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/truncated_backprop.png" class="img-fluid figure-img" style="width:80.0%"></p>
<p></p><figcaption class="figure-caption">Truncated backpropagation through time. Source: <a href="https://r2rt.com/styles-of-truncated-backpropagation.html" class="uri">https://r2rt.com/styles-of-truncated-backpropagation.html</a>.</figcaption><p></p>
</figure>
</div>
</section>
<section id="vanishing-gradients" class="level3">
<h3 class="anchored" data-anchor-id="vanishing-gradients">Vanishing gradients</h3>
<p>BPTT is able to find <strong>short-term dependencies</strong> between inputs and outputs: perceiving the inputs <span class="math inline">\mathbf{x}_0</span> and <span class="math inline">\mathbf{x}_1</span> allows to respond correctly at <span class="math inline">t = 3</span>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/RNN-shorttermdependencies.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">RNN can learn short-term dependencies. Source: <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs" class="uri">http://colah.github.io/posts/2015-08-Understanding-LSTMs</a>.</figcaption><p></p>
</figure>
</div>
<p>But it fails to detect <strong>long-term dependencies</strong> because of:</p>
<ul>
<li>the truncated horizon <span class="math inline">T</span> (for computational reasons).</li>
<li>the <strong>vanishing gradient problem</strong> <span class="citation" data-cites="Hochreiter1991">(<a href="../references.html#ref-Hochreiter1991" role="doc-biblioref">Hochreiter, 1991</a>)</span>.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/RNN-longtermdependencies.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">RNN cannot learn long-term dependencies. Source: <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs" class="uri">http://colah.github.io/posts/2015-08-Understanding-LSTMs</a>.</figcaption><p></p>
</figure>
</div>
<p>Let’s look at the gradient w.r.t to the input weights:</p>
<p><span class="math display">
\begin{aligned}
    \frac{\partial \mathbf{h}_t}{\partial W_x} &amp; = \mathbf{h'}_{t} \, (\mathbf{x}_t + W_h \times \frac{\partial \mathbf{h}_{t-1}}{\partial W_x})\\
    &amp; \\
\end{aligned}
</span></p>
<p>At each iteration backwards in time, the gradients are multiplied by <span class="math inline">W_h</span>. If you search how <span class="math inline">\frac{\partial \mathbf{h}_t}{\partial W_x}</span> depends on <span class="math inline">\mathbf{x}_0</span>, you obtain something like:</p>
<p><span class="math display">
\begin{aligned}
    \frac{\partial \mathbf{h}_t}{\partial W_x} &amp; \approx \prod_{k=0}^t \mathbf{h'}_{k} \, ((W_h)^t \, \mathbf{x}_0 + \dots) \\
\end{aligned}
</span></p>
<p>If <span class="math inline">|W_h| &gt; 1</span>, <span class="math inline">|(W_h)^t|</span> increases exponentially with <span class="math inline">t</span>: the gradient <strong>explodes</strong>. If <span class="math inline">|W_h| &lt; 1</span>, <span class="math inline">|(W_h)^t|</span> decreases exponentially with <span class="math inline">t</span>: the gradient <strong>vanishes</strong>.</p>
<p><strong>Exploding gradients</strong> are relatively easy to deal with: one just clips the norm of the gradient to a maximal value.</p>
<p><span class="math display">
    || \frac{\partial \mathcal{L}(W_x, W_h)}{\partial W_x}|| \gets \min(||\frac{\partial \mathcal{L}(W_x, W_h)}{\partial W_x}||, T)
</span></p>
<p>But there is no solution to the <strong>vanishing gradient problem</strong> for regular RNNs: the gradient fades over time (backwards) and no long-term dependency can be learned. This is the same problem as for feedforward deep networks: a RNN is just a deep network rolled over itself. Its depth (number of layers) corresponds to the maximal number of steps back in time. In order to limit vanishing gradients and learn long-term dependencies, one has to use a more complex structure for the layer. This is the idea behind <strong>long short-term memory</strong> (LSTM) networks.</p>
</section>
</section>
<section id="long-short-term-memory-networks---lstm" class="level2">
<h2 class="anchored" data-anchor-id="long-short-term-memory-networks---lstm">Long short-term memory networks - LSTM</h2>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/jvBOgXSebV0" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>All figures in this section are taken from this great blog post by Christopher Olah, which is worth a read:</p>
<p><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs" class="uri">http://colah.github.io/posts/2015-08-Understanding-LSTMs</a></p>
</div>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/LSTM3-SimpleRNN.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">RNN layer. Source: <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs" class="uri">http://colah.github.io/posts/2015-08-Understanding-LSTMs</a>.</figcaption><p></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/LSTM3-chain.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">LSTM layer. Source: <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs" class="uri">http://colah.github.io/posts/2015-08-Understanding-LSTMs</a>.</figcaption><p></p>
</figure>
</div>
<p>A <strong>LSTM layer</strong> <span class="citation" data-cites="Hochreiter1997">(<a href="../references.html#ref-Hochreiter1997" role="doc-biblioref">Hochreiter and Schmidhuber, 1997</a>)</span> is a RNN layer with the ability to control what it memorizes. In addition to the input <span class="math inline">\mathbf{x}_t</span> and output <span class="math inline">\mathbf{h}_t</span>, it also has a <strong>state</strong> <span class="math inline">\mathbf{C}_t</span> which is maintained over time. The state is the memory of the layer (sometimes called context). It also contains three multiplicative <strong>gates</strong>:</p>
<ul>
<li>The <strong>input gate</strong> controls which inputs should enter the memory.
<ul>
<li><em>are they worth remembering?</em></li>
</ul></li>
<li>The <strong>forget gate</strong> controls which memory should be forgotten.
<ul>
<li><em>do I still need them?</em></li>
</ul></li>
<li>The <strong>output gate</strong> controls which part of the memory should be used to produce the output.
<ul>
<li><em>should I respond now? Do I have enough information?</em></li>
</ul></li>
</ul>
<p>The <strong>state</strong> <span class="math inline">\mathbf{C}_t</span> can be seen as an accumulator integrating inputs (and previous outputs) over time. The gates <strong>learn</strong> to open and close through learnable weights.</p>
<section id="state-conveyor-belt" class="level3">
<h3 class="anchored" data-anchor-id="state-conveyor-belt">State conveyor belt</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/LSTM3-C-line.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">State conveyor belt. Source: <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs" class="uri">http://colah.github.io/posts/2015-08-Understanding-LSTMs</a>.</figcaption><p></p>
</figure>
</div>
<p>By default, the cell state <span class="math inline">\mathbf{C}_t</span> stays the same over time (<em>conveyor belt</em>). It can have the same number of dimensions as the output <span class="math inline">\mathbf{h}_t</span>, but does not have to. Its content can be erased by multiplying it with a vector of 0s, or preserved by multiplying it by a vector of 1s. We can use a <strong>sigmoid</strong> to achieve this:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/LSTM3-gate.png" class="img-fluid figure-img" style="width:30.0%"></p>
<p></p><figcaption class="figure-caption">Element-wise multiplication with a vector using using the logistic/sigmoid function. Source: <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs" class="uri">http://colah.github.io/posts/2015-08-Understanding-LSTMs</a>.</figcaption><p></p>
</figure>
</div>
</section>
<section id="forget-gate" class="level3">
<h3 class="anchored" data-anchor-id="forget-gate">Forget gate</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/LSTM3-focus-f.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Forget gate. Source: <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs" class="uri">http://colah.github.io/posts/2015-08-Understanding-LSTMs</a>.</figcaption><p></p>
</figure>
</div>
<p>Forget weights <span class="math inline">W_f</span> and a sigmoid function are used to decide if the state should be preserved or not.</p>
<p><span class="math display">
    \mathbf{f}_t = \sigma(W_f \times [\mathbf{h}_{t-1}; \mathbf{x}_t] + \mathbf{b}_f)
</span></p>
<p><span class="math inline">[\mathbf{h}_{t-1}; \mathbf{x}_t]</span> is simply the concatenation of the two vectors <span class="math inline">\mathbf{h}_{t-1}</span> and <span class="math inline">\mathbf{x}_t</span>. <span class="math inline">\mathbf{f}_t</span> is a vector of values between 0 and 1, one per dimension of the cell state <span class="math inline">\mathbf{C}_t</span>.</p>
</section>
<section id="input-gate" class="level3">
<h3 class="anchored" data-anchor-id="input-gate">Input gate</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/LSTM3-focus-i.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Input gate. Source: <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs" class="uri">http://colah.github.io/posts/2015-08-Understanding-LSTMs</a>.</figcaption><p></p>
</figure>
</div>
<p>Similarly, the input gate uses a sigmoid function to decide if the state should be updated or not.</p>
<p><span class="math display">
    \mathbf{i}_t = \sigma(W_i \times [\mathbf{h}_{t-1}; \mathbf{x}_t] + \mathbf{b}_i)
</span></p>
<p>As for RNNs, the input <span class="math inline">\mathbf{x}_t</span> and previous output <span class="math inline">\mathbf{h}_{t-1}</span> are combined to produce a <strong>candidate state</strong> <span class="math inline">\tilde{\mathbf{C}}_t</span> using the tanh transfer function.</p>
<p><span class="math display">
    \tilde{\mathbf{C}}_t = \text{tanh}(W_C \times [\mathbf{h}_{t-1}; \mathbf{x}_t] + \mathbf{b}_c)
</span></p>
</section>
<section id="candidate-state" class="level3">
<h3 class="anchored" data-anchor-id="candidate-state">Candidate state</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/LSTM3-focus-C.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Candidate state. Source: <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs" class="uri">http://colah.github.io/posts/2015-08-Understanding-LSTMs</a>.</figcaption><p></p>
</figure>
</div>
<p>The new state <span class="math inline">\mathbf{C}_t</span> is computed as a part of the previous state <span class="math inline">\mathbf{C}_{t-1}</span> (element-wise multiplication with the forget gate <span class="math inline">\mathbf{f}_t</span>) plus a part of the candidate state <span class="math inline">\tilde{\mathbf{C}}_t</span> (element-wise multiplication with the input gate <span class="math inline">\mathbf{i}_t</span>).</p>
<p><span class="math display">
    \mathbf{C}_t = \mathbf{f}_t \odot \mathbf{C}_{t-1} + \mathbf{i}_t \odot \tilde{\mathbf{C}}_t
</span></p>
<p>Depending on the gates, the new state can be equal to the previous state (gates closed), the candidate state (gates opened) or a mixture of both.</p>
</section>
<section id="output-gate" class="level3">
<h3 class="anchored" data-anchor-id="output-gate">Output gate</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/LSTM3-focus-o.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Output gate. Source: <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs" class="uri">http://colah.github.io/posts/2015-08-Understanding-LSTMs</a>.</figcaption><p></p>
</figure>
</div>
<p>The output gate decides which part of the new state will be used for the output.</p>
<p><span class="math display">
    \mathbf{o}_t = \sigma(W_o \times [\mathbf{h}_{t-1}; \mathbf{x}_t] + \mathbf{b}_o)
</span></p>
<p>The output not only influences the decision, but also how the gates will updated at the next step.</p>
<p><span class="math display">
    \mathbf{h}_t = \mathbf{o}_t \odot \text{tanh} (\mathbf{C}_t)
</span></p>
</section>
<section id="lstm-layer" class="level3">
<h3 class="anchored" data-anchor-id="lstm-layer">LSTM layer</h3>
<p>The function between <span class="math inline">\mathbf{x}_t</span> and <span class="math inline">\mathbf{h}_t</span> is quite complicated, with many different weights, but everything is differentiable: BPTT can be applied.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/LSTM-cell2.png" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">LSTM layer. Source: <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs" class="uri">http://colah.github.io/posts/2015-08-Understanding-LSTMs</a>.</figcaption><p></p>
</figure>
</div>
<p><strong>Equations:</strong></p>
<ul>
<li><strong>Forget gate</strong></li>
</ul>
<p><span class="math display">\mathbf{f}_t = \sigma(W_f \times [\mathbf{h}_{t-1}; \mathbf{x}_t] + \mathbf{b}_f)</span></p>
<ul>
<li><strong>Input gate</strong></li>
</ul>
<p><span class="math display">\mathbf{i}_t = \sigma(W_i \times [\mathbf{h}_{t-1}; \mathbf{x}_t] + \mathbf{b}_i)</span></p>
<ul>
<li><strong>Output gate</strong></li>
</ul>
<p><span class="math display">\mathbf{o}_t = \sigma(W_o \times [\mathbf{h}_{t-1}; \mathbf{x}_t] + \mathbf{b}_o)</span></p>
<ul>
<li><strong>Candidate state</strong></li>
</ul>
<p><span class="math display">\tilde{\mathbf{C}}_t = \text{tanh}(W_C \times [\mathbf{h}_{t-1}; \mathbf{x}_t] + \mathbf{b}_c)</span></p>
<ul>
<li><strong>New state</strong></li>
</ul>
<p><span class="math display">\mathbf{C}_t = \mathbf{f}_t \odot \mathbf{C}_{t-1} + \mathbf{i}_t \odot \tilde{\mathbf{C}}_t</span></p>
<ul>
<li><strong>Output</strong></li>
</ul>
<p><span class="math display">\mathbf{h}_t = \mathbf{o}_t \odot \text{tanh} (\mathbf{C}_t)</span></p>
</section>
<section id="vanishing-gradients-1" class="level3">
<h3 class="anchored" data-anchor-id="vanishing-gradients-1">Vanishing gradients</h3>
<p>How do LSTM solve the vanishing gradient problem? Not all inputs are remembered by the LSTM: the input gate controls what comes in. If only <span class="math inline">\mathbf{x}_0</span> and <span class="math inline">\mathbf{x}_1</span> are needed to produce <span class="math inline">\mathbf{h}_{t+1}</span>, they will be the only ones stored in the state, the other inputs are ignored.</p>
<p>If the state stays constant between <span class="math inline">t=1</span> and <span class="math inline">t</span>, the gradient of the error will not vanish when backpropagating from <span class="math inline">t</span> to <span class="math inline">t=1</span>, because nothing happens!</p>
<p><span class="math display">
    \mathbf{C}_t = \mathbf{C}_{t-1} \rightarrow \frac{\partial \mathbf{C}_t}{\partial \mathbf{C}_{t-1}} = 1
</span></p>
<p>The gradient is multiplied by exactly one when the gates are closed.</p>
<p>LSTM are particularly good at learning long-term dependencies, because the gates protect the cell from vanishing gradients. Its problem is how to find out which inputs (e.g.&nbsp;<span class="math inline">\mathbf{x}_0</span> and <span class="math inline">\mathbf{x}_1</span>) should enter or leave the state memory.</p>
<p>Truncated BPTT is used to train all weights: the weights for the candidate state (as for RNN), and the weights of the three gates. LSTM are also subject to overfitting. Regularization (including dropout) can be used. The weights (also for the gates) can be convolutional. The gates also have a bias, which can be fixed (but hard to find). LSTM layers can be stacked to detect dependencies at different scales (deep LSTM network).</p>
</section>
<section id="peephole-connections" class="level3">
<h3 class="anchored" data-anchor-id="peephole-connections">Peephole connections</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/LSTM3-var-peepholes.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Peephole connections. Source: <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs" class="uri">http://colah.github.io/posts/2015-08-Understanding-LSTMs</a>.</figcaption><p></p>
</figure>
</div>
<p>A popular variant of LSTM adds <strong>peephole connections</strong> <span class="citation" data-cites="Gers2000">(<a href="../references.html#ref-Gers2000" role="doc-biblioref">Gers and Schmidhuber, 2000</a>)</span>, where the three gates have additionally access to the state <span class="math inline">\mathbf{C}_{t-1}</span>.</p>
<p><span class="math display">\begin{align}
    \mathbf{f}_t &amp;= \sigma(W_f \times [\mathbf{C}_{t-1}; \mathbf{h}_{t-1}; \mathbf{x}_t] + \mathbf{b}_f) \\
    &amp;\\
    \mathbf{i}_t &amp;= \sigma(W_i \times [\mathbf{C}_{t-1}; \mathbf{h}_{t-1}; \mathbf{x}_t] + \mathbf{b}_i) \\
    &amp;\\
    \mathbf{o}_t &amp;= \sigma(W_o \times [\mathbf{C}_{t}; \mathbf{h}_{t-1}; \mathbf{x}_t] + \mathbf{b}_o) \\
\end{align}</span></p>
<p>It usually works better, but adds more weights.</p>
</section>
<section id="gru-gated-recurrent-unit" class="level3">
<h3 class="anchored" data-anchor-id="gru-gated-recurrent-unit">GRU: Gated Recurrent Unit</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/LSTM3-var-GRU.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Gated recurrent unit (GRU). Source: <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs" class="uri">http://colah.github.io/posts/2015-08-Understanding-LSTMs</a>.</figcaption><p></p>
</figure>
</div>
<p>Another variant is called the <strong>Gated Recurrent Unit</strong> (GRU) <span class="citation" data-cites="Chung2014">(<a href="../references.html#ref-Chung2014" role="doc-biblioref">Chung et al., 2014</a>)</span>. It uses directly the output <span class="math inline">\mathbf{h}_t</span> as a state, and the forget and input gates are merged into a single gate <span class="math inline">\mathbf{r}_t</span>.</p>
<p><span class="math display">\begin{align}
    \mathbf{z}_t &amp;= \sigma(W_z \times [\mathbf{h}_{t-1}; \mathbf{x}_t]) \\
    &amp;\\
    \mathbf{r}_t &amp;= \sigma(W_r \times [\mathbf{h}_{t-1}; \mathbf{x}_t]) \\
    &amp;\\
    \tilde{\mathbf{h}}_t &amp;= \text{tanh} (W_h \times [\mathbf{r}_t \odot \mathbf{h}_{t-1}; \mathbf{x}_t])\\
    &amp; \\
    \mathbf{h}_t &amp;= (1 - \mathbf{z}_t) \odot \mathbf{h}_{t-1} + \mathbf{z}_t \odot \tilde{\mathbf{h}}_t\\
\end{align}</span></p>
<p>It does not even need biases (mostly useless in LSTMs anyway). Much simpler to train as the LSTM, and almost as powerful.</p>
</section>
<section id="bidirectional-lstm" class="level3">
<h3 class="anchored" data-anchor-id="bidirectional-lstm">Bidirectional LSTM</h3>
<p>A <strong>bidirectional LSTM</strong> learns to predict the output in two directions:</p>
<ul>
<li>The <strong>feedforward</strong> line learns using the past context (classical LSTM).</li>
<li>The <strong>backforward</strong> line learns using the future context (inputs are reversed).</li>
</ul>
<p>The two state vectors are then concatenated at each time step to produce the output. Only possible offline, as the future inputs must be known. Works better than LSTM on many problems, but slower.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/bi_lstm.jpg" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Bidirectional LSTM. Source: <a href="http://www.paddlepaddle.org/doc/demo/sentiment_analysis/sentiment_analysis.html" class="uri">http://www.paddlepaddle.org/doc/demo/sentiment_analysis/sentiment_analysis.html</a>.</figcaption><p></p>
</figure>
</div>


<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography" style="display: none">
<div id="ref-Chung2014" class="csl-entry" role="doc-biblioentry">
Chung, J., Gulcehre, C., Cho, K., and Bengio, Y. (2014). Empirical <span>Evaluation</span> of <span>Gated Recurrent Neural Networks</span> on <span>Sequence Modeling</span>. <a href="http://arxiv.org/abs/1412.3555">http://arxiv.org/abs/1412.3555</a>.
</div>
<div id="ref-Gers2000" class="csl-entry" role="doc-biblioentry">
Gers, F. A., and Schmidhuber, J. (2000). Recurrent nets that time and count. in <em>Proceedings of the <span>IEEE-INNS-ENNS International Joint Conference</span> on <span>Neural Networks</span>. <span>IJCNN</span> 2000. <span>Neural Computing</span>: <span>New Challenges</span> and <span>Perspectives</span> for the <span>New Millennium</span></em>, 189–194 vol.3. doi:<a href="https://doi.org/10.1109/IJCNN.2000.861302">10.1109/IJCNN.2000.861302</a>.
</div>
<div id="ref-Hochreiter1991" class="csl-entry" role="doc-biblioentry">
Hochreiter, S. (1991). Untersuchungen zu dynamischen neuronalen <span>Netzen</span>. <a href="http://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf">http://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf</a>.
</div>
<div id="ref-Hochreiter1997" class="csl-entry" role="doc-biblioentry">
Hochreiter, S., and Schmidhuber, J. (1997). Long short-term memory. <em>Neural computation</em> 9, 1735–80. <a href="https://www.ncbi.nlm.nih.gov/pubmed/9377276">https://www.ncbi.nlm.nih.gov/pubmed/9377276</a>.
</div>
</div>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation column-body">
  <div class="nav-page nav-page-previous">
      <a href="../notes/5.3-GAN.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-title">Generative adversarial networks</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../notes/6.2-NLP.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-title">Natural Language Processing</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
      <div class="nav-footer-center">Copyright 2022, Julien Vitay - <a href="mailto:julien.vitay@informatik.tu-chemnitz.de" class="email">julien.vitay@informatik.tu-chemnitz.de</a></div>
  </div>
</footer>



<script src="../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>