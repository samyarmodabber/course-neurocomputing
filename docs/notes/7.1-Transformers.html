<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.251">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Neurocomputing - 19&nbsp; Transformers</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../notes/7.2-ContrastiveLearning.html" rel="next">
<link href="../notes/6.3-Attention.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-title">Transformers</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="../" class="sidebar-logo-link">
      <img src="../notes/img/tuc-new.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Neurocomputing</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">Overview</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">Introduction</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/1.1-Introduction.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/1.2-Math.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Math basics (optional)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/1.3-Neurons.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Neurons</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">Linear algorithms</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.1-Optimization.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Optimization</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.2-LinearRegression.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Linear regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.3-LinearClassification.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Linear classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.4-LearningTheory.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Learning theory</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">Neural networks</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.1-NeuralNetworks.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Multi-layer perceptron</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.2-DNN.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Modern neural networks</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">Computer Vision</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/4.1-CNN.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Convolutional neural networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/4.2-ObjectDetection.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Object detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/4.3-SemanticSegmentation.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Semantic segmentation</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">Generative modeling</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/5.1-Autoencoders.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Autoencoders</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/5.2-RBM.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Restricted Boltzmann machines (optional)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/5.3-GAN.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Generative adversarial networks</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true">Recurrent neural networks</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/6.1-RNN.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Recurrent neural networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/6.2-NLP.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Natural Language Processing</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/6.3-Attention.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Attentional neural networks</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="true">Self-supervised learning</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/7.1-Transformers.html" class="sidebar-item-text sidebar-link active"><span class="chapter-title">Transformers</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/7.2-ContrastiveLearning.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Contrastive Learning</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" aria-expanded="true">Outlook</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/8.1-Limits.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Limits of deep learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/8.2-Beyond.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Beyond deep Learning</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" aria-expanded="true">Exercises</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/Content.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">List of exercises</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/1-Python-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Introduction To Python</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/2-Numpy-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Numpy and Matplotlib</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/3-LinearRegression-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Linear regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/4-MLR-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Multiple linear regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/5-Crossvalidation-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Cross-validation and polynomial regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/6-LinearClassification-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Linear classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/7-SoftmaxClassifier-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Softmax classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/8-MLP-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Multi-layer Perceptron</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/9-MNIST-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">MNIST classification using keras</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/10-CNN-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Convolutional neural networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/11-TransferLearning-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Transfer learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/12-VAE-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Variational autoencoder</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/13-RNN-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Recurrent neural networks</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#transformers" id="toc-transformers" class="nav-link active" data-scroll-target="#transformers">Transformers</a>
  <ul class="collapse">
  <li><a href="#architecture" id="toc-architecture" class="nav-link" data-scroll-target="#architecture">Architecture</a></li>
  <li><a href="#self-attention" id="toc-self-attention" class="nav-link" data-scroll-target="#self-attention">Self-attention</a></li>
  <li><a href="#multi-headed-self-attention" id="toc-multi-headed-self-attention" class="nav-link" data-scroll-target="#multi-headed-self-attention">Multi-headed self-attention</a></li>
  <li><a href="#encoder-layer" id="toc-encoder-layer" class="nav-link" data-scroll-target="#encoder-layer">Encoder layer</a></li>
  <li><a href="#positional-encoding" id="toc-positional-encoding" class="nav-link" data-scroll-target="#positional-encoding">Positional encoding</a></li>
  <li><a href="#layer-normalization" id="toc-layer-normalization" class="nav-link" data-scroll-target="#layer-normalization">Layer normalization</a></li>
  <li><a href="#decoder" id="toc-decoder" class="nav-link" data-scroll-target="#decoder">Decoder</a></li>
  <li><a href="#results" id="toc-results" class="nav-link" data-scroll-target="#results">Results</a></li>
  </ul></li>
  <li><a href="#self-supervised-transformers" id="toc-self-supervised-transformers" class="nav-link" data-scroll-target="#self-supervised-transformers">Self-supervised Transformers</a>
  <ul class="collapse">
  <li><a href="#bert" id="toc-bert" class="nav-link" data-scroll-target="#bert">BERT</a></li>
  <li><a href="#gpt" id="toc-gpt" class="nav-link" data-scroll-target="#gpt">GPT</a></li>
  </ul></li>
  <li><a href="#vision-transformers" id="toc-vision-transformers" class="nav-link" data-scroll-target="#vision-transformers">Vision transformers</a>
  <ul class="collapse">
  <li><a href="#vit" id="toc-vit" class="nav-link" data-scroll-target="#vit">ViT</a></li>
  <li><a href="#sit" id="toc-sit" class="nav-link" data-scroll-target="#sit">SiT</a></li>
  <li><a href="#dino" id="toc-dino" class="nav-link" data-scroll-target="#dino">DINO</a></li>
  </ul></li>
  <li><a href="#time-series-transformers" id="toc-time-series-transformers" class="nav-link" data-scroll-target="#time-series-transformers">Time series transformers</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-body" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-title">Transformers</span></h1>
</div>



<div class="quarto-title-meta">

    
    
  </div>
  

</header>

<p>Slides: <a href="../slides/7.1-Transformers.html" target="_blank">html</a> <a href="../slides/pdf/7.1-Transformers.pdf" target="_blank">pdf</a></p>
<section id="transformers" class="level2">
<h2 class="anchored" data-anchor-id="transformers">Transformers</h2>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/AO-gqXNCEx0" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
<section id="architecture" class="level3">
<h3 class="anchored" data-anchor-id="architecture">Architecture</h3>
<p>Attentional mechanisms are so powerful that recurrent networks are not even needed anymore. <strong>Transformer networks</strong> <span class="citation" data-cites="Vaswani2017">(<a href="../references.html#ref-Vaswani2017" role="doc-biblioref">Vaswani et al., 2017</a>)</span> use <strong>self-attention</strong> in a purely feedforward architecture and outperform recurrent architectures. They are used in Google BERT and OpenAI GPT-2/3 for text understanding (e.g.&nbsp;search engine queries).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/transformer_resideual_layer_norm_3.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Architecture of the Transformer. Source: <a href="http://jalammar.github.io/illustrated-transformer/" class="uri">http://jalammar.github.io/illustrated-transformer/</a></figcaption><p></p>
</figure>
</div>
<p>Transformer networks use an <strong>encoder-decoder</strong> architecture, each with 6 stacked layers.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/transformer1.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Encoder-decoder structure of the Transformer. Source: <a href="http://jalammar.github.io/illustrated-transformer/" class="uri">http://jalammar.github.io/illustrated-transformer/</a></figcaption><p></p>
</figure>
</div>
<p>Each layer of the encoder processes the <span class="math inline">n</span> words of the input sentence <strong>in parallel</strong>. Word embeddings (as in word2vec) of dimension 512 are used as inputs (but learned end-to-end).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/encoder_with_tensors.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Encoder layer. Source: <a href="http://jalammar.github.io/illustrated-transformer/" class="uri">http://jalammar.github.io/illustrated-transformer/</a></figcaption><p></p>
</figure>
</div>
<p>Two operations are performed on each word embedding <span class="math inline">\mathbf{x}_i</span>:</p>
<ul>
<li>self-attention vector <span class="math inline">\mathbf{z}_i</span> depending on the other words.</li>
<li>a regular feedforward layer to obtain a new representation <span class="math inline">\mathbf{r}_i</span> (shared among all words).</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/encoder_with_tensors_2.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Encoder layer. Source: <a href="http://jalammar.github.io/illustrated-transformer/" class="uri">http://jalammar.github.io/illustrated-transformer/</a></figcaption><p></p>
</figure>
</div>
</section>
<section id="self-attention" class="level3">
<h3 class="anchored" data-anchor-id="self-attention">Self-attention</h3>
<p>The first step of self-attention is to compute for each word three vectors of length <span class="math inline">d_k = 64</span> from the embeddings <span class="math inline">\mathbf{x}_i</span> or previous representations <span class="math inline">\mathbf{r}_i</span> (d = 512).</p>
<ul>
<li>The <strong>query</strong> <span class="math inline">\mathbf{q}_i</span> using <span class="math inline">W^Q</span>.</li>
<li>The <strong>key</strong> <span class="math inline">\mathbf{k}_i</span> using <span class="math inline">W^K</span>.</li>
<li>The <strong>value</strong> <span class="math inline">\mathbf{v}_i</span> using <span class="math inline">W^V</span>.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/transformer_self_attention_vectors.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Self-attention. Source: <a href="http://jalammar.github.io/illustrated-transformer/" class="uri">http://jalammar.github.io/illustrated-transformer/</a></figcaption><p></p>
</figure>
</div>
<p>This operation can be done in parallel over all words:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/self-attention-matrix-calculation.png" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Query, key and value matrices. Source: <a href="http://jalammar.github.io/illustrated-transformer/" class="uri">http://jalammar.github.io/illustrated-transformer/</a></figcaption><p></p>
</figure>
</div>
<p>Why query / key / value? This a concept inspired from recommendation systems / databases. A Python dictionary is a set of key / value entries:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>tel <span class="op">=</span> {</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">'jack'</span>: <span class="dv">4098</span>, </span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">'sape'</span>: <span class="dv">4139</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The query would ask the dictionary to iterate over all entries and return the value associated to the key <strong>equal or close to</strong> the query.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>tel[<span class="st">'jacky'</span>] <span class="co"># 4098</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This would be some sort of <strong>fuzzy</strong> dictionary.</p>
<p>In attentional RNNs, the attention scores were used by each word generated by the decoder to decide which <strong>input word</strong> is relevant.</p>
<p>If we apply the same idea to the <strong>same sentence</strong> (self-attention), the attention score tells how much words of the same sentence are related to each other (context).</p>
<blockquote class="blockquote">
<p>The animal didn’t cross the street because it was too tired.</p>
</blockquote>
<p>The goal is to learn a representation for the word <code>it</code> that contains information about <code>the animal</code>, not <code>the street</code>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/transformer_self-attention_visualization.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Source: <a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html" class="uri">https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html</a></figcaption><p></p>
</figure>
</div>
<p>Each word <span class="math inline">\mathbf{x}_i</span> of the sentence generates its query <span class="math inline">\mathbf{q}_i</span>, key <span class="math inline">\mathbf{k}_i</span> and value <span class="math inline">\mathbf{v}_i</span>.</p>
<p>For all other words <span class="math inline">\mathbf{x}_j</span>, we compute the <strong>match</strong> between the query <span class="math inline">\mathbf{q}_i</span> and the keys <span class="math inline">\mathbf{k}_j</span> with a dot product:</p>
<p><span class="math display">e_{i, j} = \mathbf{q}_i^T \, \mathbf{k}_j</span></p>
<p>We normalize the scores by dividing by <span class="math inline">\sqrt{d_k} = 8</span> and apply a softmax:</p>
<p><span class="math display">a_{i, j} = \text{softmax}(\dfrac{\mathbf{q}_i^T \, \mathbf{k}_j}{\sqrt{d_k}})</span></p>
<p>The new representation <span class="math inline">\mathbf{z}_i</span> of the word <span class="math inline">\mathbf{x}_i</span> is a weighted sum of the values of all other words, weighted by the attention score:</p>
<p><span class="math display">\mathbf{z}_i = \sum_{j} a_{i, j} \, \mathbf{v}_j</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/self-attention-output.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Summary of self-attention on two words. Source: <a href="http://jalammar.github.io/illustrated-transformer/" class="uri">http://jalammar.github.io/illustrated-transformer/</a></figcaption><p></p>
</figure>
</div>
<p>If we concatenate the word embeddings into a <span class="math inline">n\times d</span> matrix <span class="math inline">X</span>, self-attention only implies matrix multiplications and a row-based softmax:</p>
<p><span class="math display">
\begin{cases}
    Q = X \times W^Q \\
    K = X \times W^K \\
    V = X \times W^V \\
    Z = \text{softmax}(\dfrac{Q \times K^T}{\sqrt{d_k}}) \times V \\
\end{cases}
</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/self-attention-matrix-calculation-2.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Self-attention. Source: <a href="http://jalammar.github.io/illustrated-transformer/" class="uri">http://jalammar.github.io/illustrated-transformer/</a></figcaption><p></p>
</figure>
</div>
<p>Note 1: everything is differentiable, backpropagation will work.</p>
<p>Note 2: the weight matrices do not depend on the length <span class="math inline">n</span> of the sentence.</p>
</section>
<section id="multi-headed-self-attention" class="level3">
<h3 class="anchored" data-anchor-id="multi-headed-self-attention">Multi-headed self-attention</h3>
<p>In the sentence <em>The animal didn’t cross the street because it was too tired.</em>, the new representation for the word <code>it</code> will hopefully contain features of the word <code>animal</code> after training.</p>
<p>But what if the sentence was <em>The animal didn’t cross the street because it was too <strong>wide</strong>.</em>? The representation of <code>it</code> should be linked to <code>street</code> in that context. This is not possible with a single set of matrices <span class="math inline">W^Q</span>, <span class="math inline">W^K</span> and <span class="math inline">W^V</span>, as they would average every possible context and end up being useless.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/transformer-needforheads.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Source: <a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html" class="uri">https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html</a></figcaption><p></p>
</figure>
</div>
<p>The solution is to use <strong>multiple attention heads</strong> (<span class="math inline">h=8</span>) with their own matrices <span class="math inline">W^Q_k</span>, <span class="math inline">W^K_k</span> and <span class="math inline">W^V_k</span>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/transformer_attention_heads_qkv.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Multiple attention heads. Source: <a href="http://jalammar.github.io/illustrated-transformer/" class="uri">http://jalammar.github.io/illustrated-transformer/</a></figcaption><p></p>
</figure>
</div>
<p>Each <strong>attention head</strong> will output a vector <span class="math inline">\mathbf{z}_i</span> of size <span class="math inline">d=512</span> for each word. How do we combine them?</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/transformer_attention_heads_z.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Multiple attention heads. Source: <a href="http://jalammar.github.io/illustrated-transformer/" class="uri">http://jalammar.github.io/illustrated-transformer/</a></figcaption><p></p>
</figure>
</div>
<p>The proposed solution is based on <strong>ensemble learning</strong> (stacking): let another matrix <span class="math inline">W^O</span> decide which attention head to trust… <span class="math inline">8 \times 512</span> rows, 512 columns.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/transformer_attention_heads_weight_matrix_o.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Multiple attention heads. Source: <a href="http://jalammar.github.io/illustrated-transformer/" class="uri">http://jalammar.github.io/illustrated-transformer/</a></figcaption><p></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/transformer_multi-headed_self-attention-recap.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Summary of self-attention. Source: <a href="http://jalammar.github.io/illustrated-transformer/" class="uri">http://jalammar.github.io/illustrated-transformer/</a></figcaption><p></p>
</figure>
</div>
<p>Each attention head learns a different context:</p>
<ul>
<li><code>it</code> refers to <code>animal</code>.</li>
<li><code>it</code> refers to <code>street</code>.</li>
<li>etc.</li>
</ul>
<p>The original transformer paper in 2017 used 8 attention heads. OpenAI’s GPT-3 uses 96 attention heads…</p>
</section>
<section id="encoder-layer" class="level3">
<h3 class="anchored" data-anchor-id="encoder-layer">Encoder layer</h3>
<p>Multi-headed self-attention produces a vector <span class="math inline">\mathbf{z}_i</span> for each word of the sentence. A regular feedforward MLP transforms it into a new representation <span class="math inline">\mathbf{r}_i</span>.</p>
<ul>
<li>one input layer with 512 neurons.</li>
<li>one hidden layer with 2048 neurons and a ReLU activation function.</li>
<li>one output layer with 512 neurons.</li>
</ul>
<p>The same NN is applied on all words, it does not depend on the length <span class="math inline">n</span> of the sentence.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/encoder_with_tensors_2.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Encoder layer. Source: <a href="http://jalammar.github.io/illustrated-transformer/" class="uri">http://jalammar.github.io/illustrated-transformer/</a></figcaption><p></p>
</figure>
</div>
</section>
<section id="positional-encoding" class="level3">
<h3 class="anchored" data-anchor-id="positional-encoding">Positional encoding</h3>
<p>As each word is processed in parallel, the order of the words in the sentence is lost.</p>
<blockquote class="blockquote">
<p>street was animal tired the the because it cross too didn’t</p>
</blockquote>
<p>We need to explicitly provide that information in the <strong>input</strong> using positional encoding.</p>
<p>A simple method would be to append an index <span class="math inline">i = 1, 2, \ldots, n</span> to the word embeddings, but it is not very robust.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/transformer_positional_encoding_vectors.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Positional encoding. Source: <a href="http://jalammar.github.io/illustrated-transformer/" class="uri">http://jalammar.github.io/illustrated-transformer/</a></figcaption><p></p>
</figure>
</div>
<p>If the elements of the 512-d embeddings are numbers between 0 and 1, concatenating an integer between 1 and <span class="math inline">n</span> will unbalance the dimensions. Normalizing that integer between 0 and 1 requires to know <span class="math inline">n</span> in advance, this introduces a maximal sentence length…</p>
<p>How about we append the binary code of that integer?</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/trasnformer-positionalencoding.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Positional encoding. Source: <a href="https://kazemnejad.com/blog/transformer_architecture_positional_encoding/" class="uri">https://kazemnejad.com/blog/transformer_architecture_positional_encoding/</a></figcaption><p></p>
</figure>
</div>
<p>Sounds good, we have numbers between 0 and 1, and we just need to use enough bits to encode very long sentences. However, representing a binary value (0 or 1) with a 64 bits floating number is a waste of computational resources.</p>
<p>We can notice that the bits of the integers oscillate at various frequencies:</p>
<ul>
<li>the lower bit oscillates every number.</li>
<li>the bit before oscillates every two numbers.</li>
<li>etc.</li>
</ul>
<p>We could also represent the position of a word using sine and cosine functions at different frequencies (Fourier basis). We create a vector, where each element oscillates at increasing frequencies. The “code” for each position in the sentence is unique.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/positional_encoding.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Positional encoding. Source: <a href="https://kazemnejad.com/blog/transformer_architecture_positional_encoding/" class="uri">https://kazemnejad.com/blog/transformer_architecture_positional_encoding/</a></figcaption><p></p>
</figure>
</div>
<p>In practice, a 512-d vector is created using sine and cosine functions.</p>
<p><span class="math display">
    \begin{cases}
        t(\text{pos}, 2i) = \sin(\dfrac{\text{pos}}{10000^{2 i / 512}})\\
        t(\text{pos}, 2i + 1) = \cos(\dfrac{\text{pos}}{10000^{2 i / 512}})\\
    \end{cases}
</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/attention-is-all-you-need-positional-encoding.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Positional encoding. Source: <a href="http://jalammar.github.io/illustrated-transformer/" class="uri">http://jalammar.github.io/illustrated-transformer/</a></figcaption><p></p>
</figure>
</div>
<p>The positional encoding vector is <strong>added</strong> element-wise to the embedding, not concatenated!</p>
<p><span class="math display">\mathbf{x}_{i} = \mathbf{x}^\text{embed}_{i} + \mathbf{t}_i</span></p>
</section>
<section id="layer-normalization" class="level3">
<h3 class="anchored" data-anchor-id="layer-normalization">Layer normalization</h3>
<p>The last tricks of the encoder layers are:</p>
<ul>
<li>skip connections (residual layer)</li>
<li>layer normalization</li>
</ul>
<p>The input <span class="math inline">X</span> is added to the output of the multi-headed self-attention and normalized (zero mean, unit variance).</p>
<p><strong>Layer normalization</strong> <span class="citation" data-cites="Ba2016">(<a href="../references.html#ref-Ba2016" role="doc-biblioref">Ba et al., 2016</a>)</span> is an alternative to batch normalization, where the mean and variance are computed over single vectors, not over a minibatch:</p>
<p><span class="math display">\mathbf{z} \leftarrow \dfrac{\mathbf{z} - \mu}{\sigma}</span></p>
<p>with <span class="math inline">\mu = \dfrac{1}{d} \displaystyle\sum_{i=1}^d z_i</span> and <span class="math inline">\sigma = \dfrac{1}{d} \displaystyle\sum_{i=1}^d (z_i - \mu)^2</span>.</p>
<p>The feedforward network also uses a skip connection and layer normalization.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/transformer_resideual_layer_norm_2.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Encoder layer with layer normalization and skip connections. Source: <a href="http://jalammar.github.io/illustrated-transformer/" class="uri">http://jalammar.github.io/illustrated-transformer/</a></figcaption><p></p>
</figure>
</div>
<p>We can now stack 6 (or more, 96 in GPT-3) of these encoder layers and use the final representation of each word as an input to the decoder.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/transformer_resideual_layer_norm_3.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">The encoder is a stack of encoder layers. Source: <a href="http://jalammar.github.io/illustrated-transformer/" class="uri">http://jalammar.github.io/illustrated-transformer/</a></figcaption><p></p>
</figure>
</div>
</section>
<section id="decoder" class="level3">
<h3 class="anchored" data-anchor-id="decoder">Decoder</h3>
<p>In the first step of decoding, the final representations of the encoder are used as query and value vectors of the decoder to produce the first word. The input to the decoder is a “start of sentence” symbol.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/transformer_decoding_1.gif" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Encoding of a sentence. Source: <a href="http://jalammar.github.io/illustrated-transformer/" class="uri">http://jalammar.github.io/illustrated-transformer/</a></figcaption><p></p>
</figure>
</div>
<p>The decoder is <strong>autoregressive</strong>: it outputs words one at a time, using the previously generated words as an input.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/transformer_decoding_2.gif" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Autoregressive generation of words. Source: <a href="http://jalammar.github.io/illustrated-transformer/" class="uri">http://jalammar.github.io/illustrated-transformer/</a></figcaption><p></p>
</figure>
</div>
<p>Each decoder layer has two multi-head attention sub-layers:</p>
<ul>
<li>A self-attention sub-layer with query/key/values coming from the generated sentence.</li>
<li>An <strong>encoder-decoder</strong> attention sub-layer, with the query coming from the generated sentence and the key/value from the encoder.</li>
</ul>
<p>The encoder-decoder attention is the regular attentional mechanism used in seq2seq architectures. Apart from this additional sub-layer, the same residual connection and layer normalization mechanisms are used.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/transformer-architecture.png" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Transformer architecture. Source <span class="citation" data-cites="Vaswani2017">(<a href="../references.html#ref-Vaswani2017" role="doc-biblioref">Vaswani et al., 2017</a>)</span>.</figcaption><p></p>
</figure>
</div>
<p>When the sentence has been fully generated (up to the <code>&lt;eos&gt;</code> symbol), <strong>masked self-attention</strong> has to applied in order for a word in the middle of the sentence to not “see” the solution in the input when learning. Learning occurs on minibatches of sentences, not on single words.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/self-attention-and-masked-self-attention.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Masked self-attention. Source: <a href="https://jalammar.github.io/illustrated-gpt2/" class="uri">https://jalammar.github.io/illustrated-gpt2/</a></figcaption><p></p>
</figure>
</div>
<p>The output of the decoder is a simple softmax classification layer, predicting the one-hot encoding of the word using a vocabulary (<code>vocab_size=25000</code>).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/transformer_decoder_output_softmax.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Word production from the output. Source: <a href="http://jalammar.github.io/illustrated-transformer/" class="uri">http://jalammar.github.io/illustrated-transformer/</a></figcaption><p></p>
</figure>
</div>
</section>
<section id="results" class="level3">
<h3 class="anchored" data-anchor-id="results">Results</h3>
<p>The transformer is trained on the WMT datasets:</p>
<ul>
<li>English-French: 36M sentences, 32000 unique words.</li>
<li>English-German: 4.5M sentences, 37000 unique words.</li>
</ul>
<p>Cross-entropy loss, Adam optimizer with scheduling, dropout. Training took 3.5 days on 8 P100 GPUs. The sentences can have different lengths, as the decoder is autoregressive. The transformer network beat the state-of-the-art performance in translation with less computations and without any RNN.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/transformer-results.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Performance of the Transformer on NLP tasks. Source <span class="citation" data-cites="Vaswani2017">(<a href="../references.html#ref-Vaswani2017" role="doc-biblioref">Vaswani et al., 2017</a>)</span>.</figcaption><p></p>
</figure>
</div>
</section>
</section>
<section id="self-supervised-transformers" class="level2">
<h2 class="anchored" data-anchor-id="self-supervised-transformers">Self-supervised Transformers</h2>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/jYDEFZWn6co" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
<p>The Transformer is considered as the <strong>AlexNet</strong> moment of natural language processing (NLP). However, it is limited to supervised learning of sentence-based translation.</p>
<p>Two families of architectures have been developed from that idea to perform all NLP tasks using <strong>unsupervised pretraining</strong> or <strong>self-supervised training</strong>:</p>
<ul>
<li>BERT (Bidirectional Encoder Representations from Transformers) from Google <span class="citation" data-cites="Devlin2019">(<a href="../references.html#ref-Devlin2019" role="doc-biblioref">Devlin et al., 2019</a>)</span>.</li>
<li>GPT (Generative Pre-trained Transformer) from OpenAI <a href="https://openai.com/blog/better-language-models/" class="uri">https://openai.com/blog/better-language-models/</a>.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/gpt-2-transformer-xl-bert-3.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Source: <a href="https://jalammar.github.io/illustrated-gpt2/" class="uri">https://jalammar.github.io/illustrated-gpt2/</a></figcaption><p></p>
</figure>
</div>
<section id="bert" class="level3">
<h3 class="anchored" data-anchor-id="bert">BERT</h3>
<p>BERT <span class="citation" data-cites="Devlin2019">(<a href="../references.html#ref-Devlin2019" role="doc-biblioref">Devlin et al., 2019</a>)</span> only uses the encoder of the transformer (12 layers, 12 attention heads, <span class="math inline">d = 768</span>). BERT is pretrained on two different unsupervised tasks before being fine-tuned on supervised tasks.</p>
<ul>
<li>Task 1: Masked language model. Sentences from BooksCorpus and Wikipedia (3.3G words) are presented to BERT during pre-training, with 15% of the words masked. The goal is to predict the masked words from the final representations using a shallow FNN.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/BERT-language-modeling-masked-lm.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Masked language model. Source: <a href="https://jalammar.github.io/illustrated-bert/" class="uri">https://jalammar.github.io/illustrated-bert/</a></figcaption><p></p>
</figure>
</div>
<ul>
<li>Task 2: Next sentence prediction. Two sentences are presented to BERT. The goal is to predict from the first representation whether the second sentence should follow the first.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/bert-next-sentence-prediction.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Next sentence prediction. Source: <a href="https://jalammar.github.io/illustrated-bert/" class="uri">https://jalammar.github.io/illustrated-bert/</a></figcaption><p></p>
</figure>
</div>
<p>Once BERT is pretrained, one can use <strong>transfer learning</strong> with or without fine-tuning from the high-level representations to perform:</p>
<ul>
<li>sentiment analysis / spam detection</li>
<li>question answering</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/bert-classifier.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Sentiment analysis with BERT. Source: <a href="https://jalammar.github.io/illustrated-bert/" class="uri">https://jalammar.github.io/illustrated-bert/</a></figcaption><p></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/bert-transfer-learning.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Transfer learning with BERT. Source: <a href="https://jalammar.github.io/illustrated-bert/" class="uri">https://jalammar.github.io/illustrated-bert/</a></figcaption><p></p>
</figure>
</div>
</section>
<section id="gpt" class="level3">
<h3 class="anchored" data-anchor-id="gpt">GPT</h3>
<p>GPT is an <strong>autoregressive</strong> language model learning to predict the next word using only the transformer’s <strong>decoder</strong>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/transformer-decoder-intro.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">GPT is the decoder of the Transformer. Source: <a href="https://jalammar.github.io/illustrated-gpt2/" class="uri">https://jalammar.github.io/illustrated-gpt2/</a></figcaption><p></p>
</figure>
</div>
<p><strong>Autoregression</strong> mimicks a LSTM that would output words one at a time.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/gpt-2-autoregression-2.gif" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Autoregression. Source: <a href="https://jalammar.github.io/illustrated-gpt2/" class="uri">https://jalammar.github.io/illustrated-gpt2/</a></figcaption><p></p>
</figure>
</div>
<p>GPT-2 comes in various sizes, with increasing performance. GPT-3 is even bigger, with 175 <strong>billion</strong> parameters and a much larger training corpus.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/gpt2-sizes-hyperparameters-3.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">GPT sizes. Source: <a href="https://jalammar.github.io/illustrated-gpt2/" class="uri">https://jalammar.github.io/illustrated-gpt2/</a></figcaption><p></p>
</figure>
</div>
<p>GPT can be fine-tuned (transfer learning) to perform <strong>machine translation</strong>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/decoder-only-transformer-translation.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Machine translation with GPT. Source: <a href="https://jalammar.github.io/illustrated-gpt2/" class="uri">https://jalammar.github.io/illustrated-gpt2/</a></figcaption><p></p>
</figure>
</div>
<p>GPT can be fine-tuned to summarize Wikipedia articles.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/wikipedia-summarization.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Wikipedia summarization with GPT. Source: <a href="https://jalammar.github.io/illustrated-gpt2/" class="uri">https://jalammar.github.io/illustrated-gpt2/</a></figcaption><p></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/decoder-only-summarization.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Wikipedia summarization with GPT. Source: <a href="https://jalammar.github.io/illustrated-gpt2/" class="uri">https://jalammar.github.io/illustrated-gpt2/</a></figcaption><p></p>
</figure>
</div>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Try transformers at <a href="https://huggingface.co/" class="uri">https://huggingface.co/</a></p>
<div class="sourceCode" id="cb3"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install transformers</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
<p>Github and OpenAI trained a GPT-3-like architecture on the available open source code. Copilot is able to “autocomplete” the code based on a simple comment/docstring.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/githubcopliot.gif" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Github Copilot. <a href="https://copilot.github.com/" class="uri">https://copilot.github.com/</a></figcaption><p></p>
</figure>
</div>
<p>All NLP tasks (translation, sentence classification, text generation) are now done using transformer-like architectures (BERT, GPT), <strong>unsupervisedly</strong> pre-trained on huge corpuses. BERT can be used for feature extraction, while GPT is more generative. Transformer architectures seem to <strong>scale</strong>: more parameters = better performance. Is there a limit?</p>
<p>The price to pay is that these models are very expensive to train (training one instance of GPT-3 costs 12M$) and to use (GPT-3 is only accessible with an API). Many attempts have been made to reduce the size of these models while keeping a satisfying performance.</p>
<ul>
<li>DistilBERT, RoBERTa, BART, T5, XLNet…</li>
</ul>
<p>See <a href="https://medium.com/mlearning-ai/recent-language-models-9fcf1b5f17f5" class="uri">https://medium.com/mlearning-ai/recent-language-models-9fcf1b5f17f5</a>.</p>
</section>
</section>
<section id="vision-transformers" class="level2">
<h2 class="anchored" data-anchor-id="vision-transformers">Vision transformers</h2>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/mK9HMXZVSUQ" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
<section id="vit" class="level3">
<h3 class="anchored" data-anchor-id="vit">ViT</h3>
<p>The transformer architecture can also be applied to computer vision, by splitting images into a <strong>sequence</strong> of small patches (16x16). The sequence of vectors can then be classified by the output of the transformer using labels.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/vision-transformer.gif" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Vision Tranformer (ViT). Source: <a href="https://ai.googleblog.com/2020/12/transformers-for-image-recognition-at.html" class="uri">https://ai.googleblog.com/2020/12/transformers-for-image-recognition-at.html</a></figcaption><p></p>
</figure>
</div>
<p>The Vision Transformer (ViT, <span class="citation" data-cites="Dosovitskiy2021">(<a href="../references.html#ref-Dosovitskiy2021" role="doc-biblioref">Dosovitskiy et al., 2021</a>)</span>) outperforms state-of-the-art CNNs while requiring less computations.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/ViTPerformance.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">ViT performance. Source: <span class="citation" data-cites="Dosovitskiy2021">(<a href="../references.html#ref-Dosovitskiy2021" role="doc-biblioref">Dosovitskiy et al., 2021</a>)</span></figcaption><p></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/ViTPerformance2.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">ViT performance. Source: <span class="citation" data-cites="Dosovitskiy2021">(<a href="../references.html#ref-Dosovitskiy2021" role="doc-biblioref">Dosovitskiy et al., 2021</a>)</span></figcaption><p></p>
</figure>
</div>
</section>
<section id="sit" class="level3">
<h3 class="anchored" data-anchor-id="sit">SiT</h3>
<p>ViT only works on big supervised datasets (ImageNet). Can we benefit from self-supervised learning as in BERT or GPT? The Self-supervised Vision Transformer (SiT) <span class="citation" data-cites="Atito2021">(<a href="../references.html#ref-Atito2021" role="doc-biblioref">Atito et al., 2021</a>)</span> has an denoising autoencoder-like structure, reconstructing corrupted patches autoregressively.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/SiT.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">SiT architecture <span class="citation" data-cites="Atito2021">(<a href="../references.html#ref-Atito2021" role="doc-biblioref">Atito et al., 2021</a>)</span>.</figcaption><p></p>
</figure>
</div>
<p>Self-supervised learning is possible through from <strong>data augmentation</strong> techniques. Various corruptions (masking, replacing, color distortion, blurring) are applied to the input image, but SiT must reconstruct the original image (denoising autoencoder).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/SiT-training.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Data augmentation for SiT <span class="citation" data-cites="Atito2021">(<a href="../references.html#ref-Atito2021" role="doc-biblioref">Atito et al., 2021</a>)</span>.</figcaption><p></p>
</figure>
</div>
<p>An auxiliary <strong>rotation loss</strong> forces SiT to predict the orientation of the image (e.g.&nbsp;30°). Another auxiliary <strong>contrastive loss</strong> ensures that high-level representations are different for different images.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/SiT-results.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Performance of SiT <span class="citation" data-cites="Atito2021">(<a href="../references.html#ref-Atito2021" role="doc-biblioref">Atito et al., 2021</a>)</span>.</figcaption><p></p>
</figure>
</div>
</section>
<section id="dino" class="level3">
<h3 class="anchored" data-anchor-id="dino">DINO</h3>
<p>A recent approach for self-supervised learning has been proposed by Facebook AI researchers using <strong>self-distillation</strong> (Self-distillation with no labels - DINO, <span class="citation" data-cites="Caron2021">(<a href="../references.html#ref-Caron2021" role="doc-biblioref">Caron et al., 2021</a>)</span>). The images are split into <strong>global</strong> and <strong>local patches</strong> at different scales. Global patches contain label-related information (whole objects) while local patches contain finer details.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/DINO-images.gif" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Global and local views for DINO. Source: <a href="https://towardsdatascience.com/on-dino-self-distillation-with-no-labels-c29e9365e382" class="uri">https://towardsdatascience.com/on-dino-self-distillation-with-no-labels-c29e9365e382</a></figcaption><p></p>
</figure>
</div>
<p>The idea of <strong>self-distillation</strong> in DINO is to use two similar ViT networks to classify the patches. The <strong>teacher</strong> network gets the global views as an input, while the <strong>student</strong> network get both the local and global ones. Both have a MLP head to predict the softmax probabilities, but do <strong>not</strong> use any labels.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/DINO-distillation.png" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Self-distillation in DINO. <span class="citation" data-cites="Caron2021">(<a href="../references.html#ref-Caron2021" role="doc-biblioref">Caron et al., 2021</a>)</span>.</figcaption><p></p>
</figure>
</div>
<p>The student tries to imitate the output of the teacher, by minimizing the <strong>cross-entropy</strong> (or KL divergence) between the two probability distributions. The teacher slowly integrates the weights of the student (momentum or exponentially moving average ema):</p>
<p><span class="math display">\theta_\text{teacher} \leftarrow \beta \, \theta_\text{teacher} + (1 - \beta) \, \theta_\text{student}</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/DINO-architecture2.gif" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">DINO training. Source: <a href="https://ai.facebook.com/blog/dino-paws-computer-vision-with-self-supervised-transformers-and-10x-more-efficient-training/" class="uri">https://ai.facebook.com/blog/dino-paws-computer-vision-with-self-supervised-transformers-and-10x-more-efficient-training/</a></figcaption><p></p>
</figure>
</div>
<p>The predicted classes do not matter when pre-training, as there is no ground truth. The only thing that matters is the <strong>high-level representation</strong> of an image before the softmax output, which can be used for transfer learning. Self-distillation forces the representations to be meaningful at both the global and local scales, as the teacher gets global views. ImageNet classes are already separated in the high-level representations: a simple kNN (k-nearest neighbour) classifier achieves 74.5% accuracy (vs.&nbsp;79.3% for a supervised ResNet50).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/DINO-tsne.gif" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">t-SNE visualization of the feature representations in DINO. Source: <a href="https://ai.facebook.com/blog/dino-paws-computer-vision-with-self-supervised-transformers-and-10x-more-efficient-training/" class="uri">https://ai.facebook.com/blog/dino-paws-computer-vision-with-self-supervised-transformers-and-10x-more-efficient-training/</a></figcaption><p></p>
</figure>
</div>
<p>More interestingly, by looking at the self-attention layers, one can obtain saliency maps that perform <strong>object segmentation</strong> without ever having been trained to!</p>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/8I1RelnsgMw" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
</section>
</section>
<section id="time-series-transformers" class="level2">
<h2 class="anchored" data-anchor-id="time-series-transformers">Time series transformers</h2>
<p>Transformers can also be used for time-series classification or forecasting instead of RNNs. Example: weather forecasting, market prices, etc.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/transformer-timeseries.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Time series forecasting. <span class="citation" data-cites="Wu2020">(<a href="../references.html#ref-Wu2020" role="doc-biblioref">Wu et al., 2020</a>)</span></figcaption><p></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/transformer-timeseries-architecture.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Time series transformer. <span class="citation" data-cites="Wu2020">(<a href="../references.html#ref-Wu2020" role="doc-biblioref">Wu et al., 2020</a>)</span></figcaption><p></p>
</figure>
</div>


<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography" style="display: none">
<div id="ref-Atito2021" class="csl-entry" role="doc-biblioentry">
Atito, S., Awais, M., and Kittler, J. (2021). <span>SiT</span>: <span class="nocase">Self-supervised vIsion Transformer</span>. Available at: <a href="http://arxiv.org/abs/2104.03602">http://arxiv.org/abs/2104.03602</a> [Accessed December 26, 2021].
</div>
<div id="ref-Ba2016" class="csl-entry" role="doc-biblioentry">
Ba, J. L., Kiros, J. R., and Hinton, G. E. (2016). Layer <span>Normalization</span>. Available at: <a href="http://arxiv.org/abs/1607.06450">http://arxiv.org/abs/1607.06450</a> [Accessed November 23, 2020].
</div>
<div id="ref-Caron2021" class="csl-entry" role="doc-biblioentry">
Caron, M., Touvron, H., Misra, I., Jégou, H., Mairal, J., Bojanowski, P., et al. (2021). Emerging <span>Properties</span> in <span>Self-Supervised Vision Transformers</span>. Available at: <a href="http://arxiv.org/abs/2104.14294">http://arxiv.org/abs/2104.14294</a> [Accessed December 29, 2021].
</div>
<div id="ref-Devlin2019" class="csl-entry" role="doc-biblioentry">
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019). <span>BERT</span>: <span class="nocase">Pre-training</span> of <span>Deep Bidirectional Transformers</span> for <span>Language Understanding</span>. Available at: <a href="http://arxiv.org/abs/1810.04805">http://arxiv.org/abs/1810.04805</a> [Accessed December 5, 2019].
</div>
<div id="ref-Dosovitskiy2021" class="csl-entry" role="doc-biblioentry">
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., et al. (2021). An <span>Image</span> is <span>Worth</span> 16x16 <span>Words</span>: <span>Transformers</span> for <span>Image Recognition</span> at <span>Scale</span>. Available at: <a href="http://arxiv.org/abs/2010.11929">http://arxiv.org/abs/2010.11929</a> [Accessed November 29, 2021].
</div>
<div id="ref-Vaswani2017" class="csl-entry" role="doc-biblioentry">
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., et al. (2017). Attention <span>Is All You Need</span>. Available at: <a href="http://arxiv.org/abs/1706.03762">http://arxiv.org/abs/1706.03762</a> [Accessed August 25, 2019].
</div>
<div id="ref-Wu2020" class="csl-entry" role="doc-biblioentry">
Wu, N., Green, B., Ben, X., and O’Banion, S. (2020). Deep <span>Transformer Models</span> for <span>Time Series Forecasting</span>: <span>The Influenza Prevalence Case</span>. Available at: <a href="http://arxiv.org/abs/2001.08317">http://arxiv.org/abs/2001.08317</a> [Accessed February 16, 2021].
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
<nav class="page-navigation column-body">
  <div class="nav-page nav-page-previous">
      <a href="../notes/6.3-Attention.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-title">Attentional neural networks</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../notes/7.2-ContrastiveLearning.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-title">Contrastive Learning</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
      <div class="nav-footer-center">Copyright 2022, Julien Vitay - <a href="mailto:julien.vitay@informatik.tu-chemnitz.de" class="email">julien.vitay@informatik.tu-chemnitz.de</a></div>
  </div>
</footer>



<script src="../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>