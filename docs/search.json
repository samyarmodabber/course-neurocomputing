[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Neurocomputing",
    "section": "",
    "text": "Overview\nThis website contains the materials for the module Neurocomputing taught by Dr. Julien Vitay at the Technische Universität Chemnitz, Faculty of Computer Science, Professorship for Artificial Intelligence.\nEach section/lecture is accompanied by a set of videos, the slides and a written version of the content. The (slightly outdated) videos are integrated in the lecture notes, but you can also access the complete playlist on Youtube.\nExercises are provided in the form of Jupyter notebooks, allowing to implement in Python at your own pace the algorithms seen in the lectures and to learn to use machine learning libraries such as scikit-learn, keras and tensorflow. A notebook to work on (locally or on Colab) and the solution are available in the Exercises section."
  },
  {
    "objectID": "index.html#syllabus",
    "href": "index.html#syllabus",
    "title": "Neurocomputing",
    "section": "Syllabus",
    "text": "Syllabus\n\nLectures\n\n\n\nIntroduction\n\nIntroduction\nMath basics (optional)\nNeurons\n\nLinear algorithms\n\nOptimization\nLinear regression\nLinear classification\nLearning theory\n\nNeural networks\n\nMulti-layer perceptron\nModern neural networks\n\nComputer Vision\n\nConvolutional neural networks\nObject detection\nSemantic segmentation\n\n\n\n\nGenerative modeling\n\nAutoencoders\nRestricted Boltzmann machines (optional)\nGenerative adversarial networks\n\nRecurrent neural networks\n\nRecurrent neural networks, LSTM\nNatural Language Processing\nAttentional neural networks\n\nSelf-supervised learning\n\nTransformers\nContrastive learning\n\nOutlook\n\nLimits of deep learning\nBeyond deep learning\n\n\n\n\n\n\nExercises\nNotebooks and videos are in the List of Exercises. Below are links to the rendered solutions.\n\n\n\nIntroduction to Python\nNumpy and Matplotlib\nLinear Regression\nMultiple linear regression\nCross-validation\nLinear classification\nSoftmax classification\n\n\n\nMulti-layer perceptron\nMNIST classification using keras\nConvolutional neural networks\nTransfer learning\nVariational autoencoder\nRecurrent neural networks"
  },
  {
    "objectID": "index.html#recommended-readings",
    "href": "index.html#recommended-readings",
    "title": "Neurocomputing",
    "section": "Recommended readings",
    "text": "Recommended readings\n\n(Goodfellow et al., 2016) Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016. http://www.deeplearningbook.org.\n(Haykin, 2009) Simon S. Haykin. Neural Networks and Learning Machines, 3rd Edition. Pearson, 2009. http://dai.fmph.uniba.sk/courses/NN/haykin.neural-networks.3ed.2009.pdf.\n(Chollet, 2017) François Chollet. Deep Learning with Python. Manning publications, 2017. https://www.manning.com/books/deep-learning-with-python.\n\n\n\n\n\nChollet, F. (2017). Deep Learning with Python. Manning publications https://www.manning.com/books/deep-learning-with-python.\n\n\nGoodfellow, I., Bengio, Y., and Courville, A. (2016). Deep Learning. MIT Press http://www.deeplearningbook.org.\n\n\nHaykin, S. S. (2009). Neural Networks and Learning Machines, 3rd Edition. Pearson http://dai.fmph.uniba.sk/courses/NN/haykin.neural-networks.3ed.2009.pdf."
  },
  {
    "objectID": "notes/1.1-Introduction.html#what-is-neurocomputing",
    "href": "notes/1.1-Introduction.html#what-is-neurocomputing",
    "title": "Introduction",
    "section": "What is neurocomputing?",
    "text": "What is neurocomputing?\n\nLet’s first discuss the difference between Artificial Intelligence (AI), Machine Learning (ML), Deep Learning (DL) and Neurocomputing. Nowadays, these terms are used almost interchangeably, but there are historical and methodological differences.\n\n\n\nSource: https://data-science-blog.com/blog/2018/05/14/machine-learning-vs-deep-learning-wo-liegt-der-unterschied\n\n\nThe term Artificial Intelligence was coined by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence in 1956:\n\nThe study is to proceed on the basis of the conjecture that every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it.\n\nGood old-fashion AI (GOFAI) approaches were purely symbolic (logical systems, knowledge-based systems) or using linear neural networks. They were able to play checkers, prove mathematical theorems, make simple conversations (ELIZA), translate languages…\nMachine learning (ML) is a branch of AI that focuses on learning from examples (data-driven AI). It is sometimes also referred to as big data, data science, operational research, pattern recognition… ML algorithms include:\n\nArtificial Neural Networks (multi-layer perceptrons)\nStatistical analysis (Bayesian modeling, PCA)\nClustering algorithms (k-means, GMM, spectral clustering)\nSupport vector machines\nDecision trees, random forests\n\nDeep Learning is a recent re-branding of artificial neural networks. It focuses on learning high-level representations of the data, using highly non-linear neural networks. Many architectures have been developped, including:\n\nDeep neural networks (DNN)\nConvolutional neural networks (CNN)\nRecurrent neural networks (RNN)\nGenerative models (GAN, VAE)\nDeep reinforcement learning (DQN, PPO, AlphaGo)\nTransformers\nGraph neural networks\n\n\n\n\n\n\nNeurocomputing is at the intersection between computational neuroscience and artificial neural networks (deep learning). Computational neuroscience studies the functioning of the brain (human or animal) through biologically detailed models, either at the functional level (e.g. visual attention, decision-making) or cellular level (individual neurons, synapses, neurotransmitters, etc). The goal of computational neuroscience is 1) to provide theoretical explanations to the experimental observations made by neuroscientists and 2) make predictions that can be verified experimentally. Moreover, understanding how the brain solves real-life problems might allow to design better AI algorithms. If you are interested in computational neuroscience, make sure to visit the courses Neurokognition I and II taught by Prof. Dr. Hamker:\nhttps://www.tu-chemnitz.de/informatik/KI/edu/neurokognition/\nNeurocomputing aims at bringing the mechanisms underlying human cognition into artificial intelligence. The first part of this course focuses on deep learning, while the second will discuss how more biologically realistic neural networks could help designing better AI systems."
  },
  {
    "objectID": "notes/1.1-Introduction.html#applications-of-deep-learning",
    "href": "notes/1.1-Introduction.html#applications-of-deep-learning",
    "title": "Introduction",
    "section": "Applications of deep learning",
    "text": "Applications of deep learning\nMachine Learning applications are generally divided into three main branches:\n\nSupervised learning: The program is trained on a pre-defined set of training examples and used to make correct predictions when given new data.\nUnsupervised learning: The program is given a bunch of data and must find patterns and relationships therein.\nReinforcement learning: The program explores its environment by producing actions and receiving rewards.\n\n\n\n\nSource: http://www.isaziconsulting.co.za/machinelearning.html\n\n\nDeep learning has recently revolutionized these types of machine learning, so let’s have a look at some concrete examples for motivation. At the end of the course, if you also perform all exercises, you should be able to reproduce these applications.\n\nSupervised learning\n\n\n\n\nPrinciple of supervised learning. Source: Andrew Ng, Stanford CS229, https://see.stanford.edu/materials/aimlcs229/cs229-notes1.pdf\n\n\nIn a supervised learning, we have a training set (or training data) consisting of N samples (or examples) from which we want to learn the underlying function or distribution. Each sample consists of an input \\mathbf{x}_i and an output (also called ground truth, desired output or target) t_i.\nWhat we want to learn is parameterized model y_i = f_\\theta (\\mathbf{x}_i) which can predict the correct output for the inputs of the training set. The goal of learning (or training) is to find which value of the parameters \\theta allows to reduce (minimize) the prediction error. i.e. the discrepancy between the prediction y_i = f_\\theta (\\mathbf{x}_i) and the desired output t_i.\nDepending on the nature of the outputs t, we have two different supervised problems:\n\nIn regression tasks, the outputs can take an infinity of values (e.g. real numbers). The following figure shows how examples of flat surfaces (input x_i) and prices (output t_i) collected in the neighborhood can be used to predict the price of a new flat. After collecting enough samples, a model is trained to minimize its prediction error. Here, a linear model is used (black line) as we perform linear regression, but any other type of function could be used. The parameters of the line (slope and intercept) are adapted so that the line lies close to the data: the predicted price y_i is never far from the ground truth t_i. Using that line after learning, we can predict that a 60 square meters flat should be rented around 550 euros/month.\n\n\n\n\n\n\n\nIn classification tasks, the outputs are discrete, i.e. take only a finite number of different values (called classes or labels). When there are only two classes, they are called the positive and negative classes and the problem is a binary classification. The two classes can represent yes/no binary values, such as when when a test is positive or negative. When there are more than two classes, they can for example represent different objects (car / bike / dog / cat…) that can be recognized on an image. The following figure depicts a binary classifiation problem, where two input features x_1 and x_2 (temperature and blood pressure) are used to predict the occurence of an illness (yes = ill, no = sane). The linear model is a line that separates the input space into two separate regions: all points above the line are categorized (classified) as ill, all points below as sane, even if they were not in the training data.\n\n\n\n\n\n\nIn practice, when using neural networks, the distinction between classification and regression is not very important, but it can be relevant for other ML techniques (decision trees only work for classification problems, for example).\n\nFeedforward neural networks\nAs we will see later, an artificial neuron is a mathematical model able to perform linear classification or regression using weighted sums of inputs:\ny = f(\\sum_{i=1}^d w_i \\, x_i + b)\n\n\n\nArtificial neuron.\n\n\nBy stacking layers of artificial neurons, we obtain a feedforward neural network able to solve non-linear classification and regression problems.\n\n\n\nFeedforward neural network.\n\n\nFully-connected layers of neurons can be replaced by convolutional layers when dealing with images as inputs, leading to the very successful convolutional neural networks (CNN).\n\n\n\nTypical CNN architecture. Source: Albelwi S, Mahmood A. 2017. A Framework for Designing the Architectures of Deep Convolutional Neural Networks. Entropy 19:242. doi:10.3390/e19060242\n\n\nThe “only” thing to do is to feed these networks with a lot of training data (inputs and desired outputs) and let them adjust their weights to minimize their prediction error using the backpropagation algorithm (Rumelhart et al., 1986) (more on that later). Neural networks (including CNNs) are a very old technology, dating back from the 60’s, with a resurgence in the 80’s thanks to the backpropation algorithm. They had been able to learn small datasets, but their performance was limited by the availability of data and the computing power available at the time. One classical example is the use of a CNN (LeCun et al., 1998) by Yann LeCun in 1998 to automatically classify single digits on ZIP postal codes (what led to the development of the MNIST dataset, the “Hello World!” of machine learning which we will use in the exercises).\n\n\n\nLeNet5 CNN trained on MNIST. Source: http://yann.lecun.com/exdb/lenet/\n\n\nThe revival of artificial neural networks marketed as deep learning at the end of the 2000’s was principally due the availability of massive amounts of training data (thanks to search engines and social networks) and the availability of consumer graphics GPUs able to perform scientific computations, especially using Nvidia’s CUDA programming framework.\nThe first badge of honour obtained by deep learning methods happened during the ImageNet challenge in 2012. The challenge was made for computer vision (CV) scientists to compare their algorithms on a huge dataset of 14 billion annotated images for object recognition (what is on the image?), object detection (which objects are in the image and where?) and object segmentation (which pixels belong to which object?). The object recognition challenge was indeed quite hard, with 1000 different classes (sometimes exotic, such as “ladle” or “porcupine”) with a great variety of backgrounds or lightning conditions. Classical CV methods based on feature extraction and simple classifiers performed reasonably well, with an error rate around 30%.\nHowever, Krizhevsky, Sutskever and Hinton (Krizhevsky et al., 2012) trained a CNN entirely on the images, without any form of preprocessing, and obtained an error rate of 15%, half of the other methods. This achievement marked the beginning of the deep learning era, attracted the attention of the major industrial players (Google, Facebook and soon the rest of the world) who have already invested hundreds of billions on AI research.\nThe whole field of computer vision was taken by storm, and CNNs were able to outperform the state-of-the-art of many vision-related tasks, such as object detection with the YOLO (You Only Look Once) network (Redmon and Farhadi, 2016):\n\nor semantic segmention with SegNet (Badrinarayanan et al., 2016) or its variants such as Mask RCNN (He et al., 2018):\n\nCNNs can even be used to control autonomous cars, by learning to reproduce human commands for a given input image (Bojarski et al., 2016):\n\nCNNs are also gaining an increasing importance in medical applications, for example to help histologists detect cancerous cells:\n\n\n\nRecurrent neural networks\n\nAnother field that was heavily transformed by deep learning is natural language processing (NLP), i.e. the automatic processing of language, be it text understanding, translation, summarization, question answering or even speech recognition and synthesis. In short, everything needed under the hood when you talk to Siri or Alexa.\nThe key neural network involved in this paradigmatic change is the recurrent neural network (RNN), with the most prominent model being the long short-term memory (LSTM) network (Hochreiter and Schmidhuber, 1997).\n\n\n\nLSTM cell. Source: http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n\n\nThe main difference with feedforward neural networks is that RNNs can be applied on sequences (of words, but it could also be video frames or any time-dependent signal). At each step, a RNN produces an output not only depending on its current input, but also on its previous output, implementing a form of memory of past events.\nMore recent advances introduced the concept of attention for processing sequences. This is now at the heart of all translation systems, including the language understanding modules behind Google search/translate and DeepL. The neural architectures may seem complex, but we will break them down in this course.\n\n\n\nGoogle Neural Machine Translation. Source: https://ai.google/research/pubs/pub45610\n\n\nTransformer architectures have revolutionized NLP, allowing to train a massive neural network in a self-supervised manner from raw data, i.e. without annotations, and then fine-tune it on particular tasks such as language translation, text summarization, code generation…\n\n\n\nSource: https://jalammar.github.io/illustrated-bert/\n\n\n\n\n\nUnsupervised learning\n\nIn supervised learning, we use annotated data, i.e. pairs (x_i, t_i) of input/output examples. This requires to know the ground truth for each sample, what can be be very tedious and expensive if humans have to do it.\nIn unsupervised learning, we only have inputs. The goal of the algorithms is to make sense out of the data: extract regularities, model the underlying distribution, group examples into clusters, etc… It may seem much harder than supervised learning, as there is no ground truth to measure performance, but data is very cheap to obtain.\n\nClustering and feature extraction\nClustering is a classical machine technique allowing to group examples in clusters based on their respective distances: close examples should belong to the same cluster. The most well-know algorithms are k-means and Gaussian mixture models (GMM). But the quality of the clustering depends on the space in which the inputs are represented: two images may be similar not because their pixels are similar (e.g. two dark images), but because they contain similar objects (fishes, birds). Neural networks can be used to learn a feature space where distances between inputs are meaningful.\n\n\n\nClustering. Source: https://learn.g2.com/supervised-vs-unsupervised-learning\n\n\n\n\nDimensionality reduction and autoencoders\nData such as images have a lot of dimensions (one per pixel), most of which are redundant. Dimensionality reduction techniques allow to reduce this number of dimensions by projecting the data into a latent space while keeping the information.\nAutoencoders (AE) are neural networks that learn to reproduce their inputs (unsupervised learning, as there are no labels) by compressing information through a bottleneck. The encoder projects the input data onto the latent space, while the decoder recreates the input. The latent space has much less dimensions than the input images, but must contain enough information in order to reconstruct the image.\n\n\n\nAutoencoders. Source: https://hackernoon.com/autoencoders-deep-learning-bits-1-11731e200694g\n\n\nApart from compression, one important application of dimensionality reduction is visualization when the latent space has 2 or 3 dimensions: you can visualize the distribution of your data and estimate how hard the classification/regression will be. Classical ML techniques include PCA (principal component analysis) and t-SNE, but autoencoders can also be used, for example the UMAP (Uniform Manifold Approximation and Projection for Dimension Reduction) architecture (McInnes et al., 2020).\nAnother application of autoencoders is the pretraining (feature extraction) of neural networks on unsupervised data before fine-tuning the resulting classifier on supervised data. This allows self-taught learning or semi-supervised learning, when the annotated data available for supervised learning is scarce, but a lot of unsupervised data from the same domain is available.\n\n\nGenerative models\nThe other major advantage of autoencoders is their decoder: from a low-dimensional latent representation, it is able after training to generate high-dimensional data such as images. By sampling the latent space, one could in principle generate an infinity of new images.\nOne particular form of autoencoder which is very useful for data generation is the variational autoencoder (VAE) (Kingma and Welling, 2013). The main difference with a regular AE is that the latent encodes a probability distribution instead of a single latent vector, what allows to sample new but realistic outputs. For example, a VAE trained to reproduce faces can generate new hybrid faces depending on how the sampling is done:\n\n\n\nSampling the latent space of a VAE trained on faces allows to generate new but realistic faces. Source: https://hackernoon.com/latent-space-visualization-deep-learning-bits-2-bd09a46920df\n\n\nVAE are in particular central to DeepFakes which have widely reached the media because of their impressive possibilities but also ethical issues:\n\nAnother class of generative models are generative adversarial networks (GAN) (Goodfellow et al., 2014) which consist of a generator (decoder) and a discriminator that compete to produce realistic images while trying to discriminate generated from real images.\n\n\n\n\n\nSeveral evolutions of GANs have allowed to produce increasingly realistic images, such as conditional GANs who permit to generate images of a desired class, or CycleGAN which allows to replace an object with another:\n\n\n\nCycleGAN. https://github.com/junyanz/CycleGAN\n\n\n\n\n\nReinforcement learning\n\nReinforcement learning (RL) is not part of this module, as we offer a complete course on it:\nhttps://www.tu-chemnitz.de/informatik/KI/edu/deeprl/\nbut it has recently gained a lot of importance when coupled with deep learning principles. Here we just present a couple of application of deep reinforcement learning to motivate you to also assist to this course.\nRL models the sequential interaction between an agent (algorithm, robot) and its environment. At each time step t, the agent is a state s_t and selects an action a_t according to its policy (or strategy) \\pi. This brings the agent in a new state s_{t+1} and provides a reward r_{t+1}. The reward is the only feedback that the agent receives about its action: when it is positive, it is good; when it is negative, it is bad. The goal of the of the agent is to maximize the sum of rewards that it receives on the long-term. For example in a video game, the states would correspond to each video frame, the actions are joystick movements and the rewards are scores increases and decreases. The goal is to move the joystick correctly so that the final cumulated score is maximal.\n\n\n\nAgent-environment interaction in RL. Source: https://ieeexplore.ieee.org/document/7839568\n\n\nIn deep RL, the policy \\pi is implemented using a deep neural network whose job is to predict which action in a given state is the most likely to provide reward in the long-term. Contrary to supervised learning, we do not know which action should have been performed (ground truth), we only get rewards indicating if this was a good choice or not. This makes the learning problem much harder. But the deep RL methods are quite generic: any problem that can be described in terms of states, actions and rewards (formally, a Markov decision process) can be solved by deep RL techniques, at the cost of quite long training times. Let’s have a look at some applications:\n\nThe first achievement of deep RL was the deep Q-network (DQN) of Deepmind able to solve a multitude of old Atari games from scratch using raw video inputs:\n\n\n\nDeep RL methods have since then been applied to more complex games, such as Starcraft II:\n\n\nor DotA 2:\n\n\nAnother famous achievement of deep RL is when Google Deepmind’s AlphaGo beat Lee Sedol, 19 times world champion, in 2016:\n\n\n\nDeep RL is also a very promising to robotics, be it in simulation:\n\n\nor in reality:\n\n\nIt is also promising for autonomous driving:"
  },
  {
    "objectID": "notes/1.1-Introduction.html#outlook",
    "href": "notes/1.1-Introduction.html#outlook",
    "title": "Introduction",
    "section": "Outlook",
    "text": "Outlook\n\n\n\n\n\nBadrinarayanan, V., Kendall, A., and Cipolla, R. (2016). SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation. http://arxiv.org/abs/1511.00561.\n\n\nBojarski, M., Del Testa, D., Dworakowski, D., Firner, B., Flepp, B., Goyal, P., et al. (2016). End to End Learning for Self-Driving Cars. http://arxiv.org/abs/1604.07316.\n\n\nGoodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., et al. (2014). Generative Adversarial Networks. http://arxiv.org/abs/1406.2661.\n\n\nHe, K., Gkioxari, G., Dollár, P., and Girshick, R. (2018). Mask R-CNN. http://arxiv.org/abs/1703.06870.\n\n\nHochreiter, S., and Schmidhuber, J. (1997). Long short-term memory. Neural computation 9, 1735–80. https://www.ncbi.nlm.nih.gov/pubmed/9377276.\n\n\nKingma, D. P., and Welling, M. (2013). Auto-Encoding Variational Bayes. http://arxiv.org/abs/1312.6114.\n\n\nKrizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). ImageNet Classification with Deep Convolutional Neural Networks. in Advances in Neural Information Processing Systems (NIPS) https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf.\n\n\nLeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. (1998). Gradient Based Learning Applied to Document Recognition. Proceedings of the IEEE 86, 2278–2324. doi:10.1109/5.726791.\n\n\nMcInnes, L., Healy, J., and Melville, J. (2020). UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction. http://arxiv.org/abs/1802.03426.\n\n\nRedmon, J., and Farhadi, A. (2016). YOLO9000: Better, Faster, Stronger. http://arxiv.org/abs/1612.08242.\n\n\nRumelhart, D. E., Hinton, G. E., and Williams, R. J. (1986). Learning representations by back-propagating errors. Nature 323, 533–536. doi:10.1038/323533a0."
  },
  {
    "objectID": "notes/1.2-Math.html#linear-algebra",
    "href": "notes/1.2-Math.html#linear-algebra",
    "title": "Math basics (optional)",
    "section": "Linear algebra",
    "text": "Linear algebra\n\nSeveral mathematical objects are manipulated in linear algebra:\n\nScalars x are 0-dimensional values (single numbers, so to speak). They can either take real values (x \\in \\Re, e.g. x = 1.4573, floats in CS) or natural values (x \\in \\mathbb{N}, e.g. x = 3, integers in CS).\nVectors \\mathbf{x} are 1-dimensional arrays of length d. The bold notation \\mathbf{x} will be used in this course, but you may also be accustomed to the arrow notation \\overrightarrow{x} used on the blackboard. When using real numbers, the vector space with d dimensions is noted \\Re^d, so we can note \\mathbf{x} \\in \\Re^d. Vectors are typically represented vertically to outline their d elements x_1, x_2, \\ldots, x_d:\n\n\\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_d \\end{bmatrix}\n\nMatrices A are 2-dimensional arrays of size (or shape) m \\times n (m rows, n columns, A \\in \\Re^{m \\times n}). They are represented by a capital letter to distinguish them from scalars (classically also in bold \\mathbf{A} but not here). The element a_{ij} of a matrix A is the element on the i-th row and j-th column.\n\nA = \\begin{bmatrix}\na_{11} & a_{12} & \\cdots & a_{1n} \\\\\na_{21} & a_{22} & \\cdots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m1} & a_{m2} & \\cdots & a_{mn} \\\\\n\\end{bmatrix}\n\nTensors \\mathcal{A} are arrays with more than two dimensions. We will not really do math on these objects, but they are useful internally (hence the name of the tensorflow library).\n\n\nVectors\nA vector can be thought of as the coordinates of a point in an Euclidean space (such the 2D space), relative to the origin. A vector space relies on two fundamental operations, which are that:\n\nVectors can be added:\n\n\\mathbf{x} + \\mathbf{y} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_d \\end{bmatrix} + \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_d \\end{bmatrix} = \\begin{bmatrix} x_1 + y_1 \\\\ x_2 + y_2 \\\\ \\vdots \\\\ x_d + y_d \\end{bmatrix}\n\nVectors can be multiplied by a scalar:\n\na \\, \\mathbf{x} = a \\, \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_d \\end{bmatrix} = \\begin{bmatrix} a \\, x_1 \\\\ a \\, x_2 \\\\ \\vdots \\\\ a \\, x_d \\end{bmatrix}\n\n\n\nVector spaces allow additions of vectors. Source: https://mathinsight.org/image/vector_2d_add\n\n\nThese two operations generate a lot of nice properties (see https://en.wikipedia.org/wiki/Vector_space for a full list), including:\n\nassociativity: \\mathbf{x} + (\\mathbf{y} + \\mathbf{z}) = (\\mathbf{x} + \\mathbf{y}) + \\mathbf{z}\ncommutativity: \\mathbf{x} + \\mathbf{y} = \\mathbf{y} + \\mathbf{x}\nthe existence of a zero vector \\mathbf{x} + \\mathbf{0} = \\mathbf{x}\ninversion: \\mathbf{x} + (-\\mathbf{x}) = \\mathbf{0}\ndistributivity: a \\, (\\mathbf{x} + \\mathbf{y}) = a \\, \\mathbf{x} + a \\, \\mathbf{y}\n\nVectors have a norm (or length) ||\\mathbf{x}||. The most intuitive one (if you know the Pythagoras theorem) is the Euclidean norm or L^2-norm, which sums the square of each element:\n||\\mathbf{x}||_2 = \\sqrt{x_1^2 + x_2^2 + \\ldots + x_d^2}\nOther norms exist, distinguished by the subscript. The L^1-norm (also called Taxicab or Manhattan norm) sums the absolute value of each element:\n||\\mathbf{x}||_1 = |x_1| + |x_2| + \\ldots + |x_d|\nThe p-norm generalizes the Euclidean norm to other powers p:\n||\\mathbf{x}||_p = (|x_1|^p + |x_2|^p + \\ldots + |x_d|^p)^{\\frac{1}{p}}\nThe infinity norm (or maximum norm) L^\\infty returns the maximum element of the vector:\n||\\mathbf{x}||_\\infty = \\max(|x_1|, |x_2|, \\ldots, |x_d|)\nOne important operation for vectors is the dot product (also called scalar product or inner product) between two vectors:\n\\langle \\mathbf{x} \\cdot \\mathbf{y} \\rangle = \\langle \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_d \\end{bmatrix} \\cdot \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_d \\end{bmatrix} \\rangle = x_1 \\, y_1 + x_2 \\, y_2 + \\ldots + x_d \\, y_d\nThe dot product basically sums one by one the product of the elements of each vector. The angular brackets are sometimes omitted (\\mathbf{x} \\cdot \\mathbf{y}) but we will use them in this course for clarity.\nOne can notice immediately that the dot product is symmetric:\n\\langle \\mathbf{x} \\cdot \\mathbf{y} \\rangle = \\langle \\mathbf{y} \\cdot \\mathbf{x} \\rangle\nand linear:\n\\langle (a \\, \\mathbf{x} + b\\, \\mathbf{y}) \\cdot \\mathbf{z} \\rangle = a\\, \\langle \\mathbf{x} \\cdot \\mathbf{z} \\rangle + b \\, \\langle \\mathbf{y} \\cdot \\mathbf{z} \\rangle\nThe dot product is an indirect measurement of the angle \\theta between two vectors:\n\\langle \\mathbf{x} \\cdot \\mathbf{y} \\rangle = ||\\mathbf{x}||_2 \\, ||\\mathbf{y}||_2 \\, \\cos(\\theta)\n\n\n\nThe dot product between two vectors is proportional to the cosine of the angle between the two vectors. Source: https://mathinsight.org/image/dot_product_projection_unit_vector\n\n\nIf you normalize the two vectors by dividing them by their norm (which is a scalar), we indeed have the cosine of the angle between them: The higher the normalized dot product, the more the two vectors point towards the same direction (cosine distance between two vectors).\n\\langle \\displaystyle\\frac{\\mathbf{x}}{||\\mathbf{x}||_2} \\cdot \\frac{\\mathbf{y}}{||\\mathbf{y}||_2} \\rangle =  \\cos(\\theta)\n\n\nMatrices\nMatrices are derived from vectors, so most of the previous properties will be true. Let’s consider this 4x3 matrix:\nA = \\begin{bmatrix}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23} \\\\\na_{31} & a_{32} & a_{33} \\\\\na_{41} & a_{42} & a_{43} \\\\\n\\end{bmatrix}\nEach column of the matrix is a vector with 4 elements:\n\\mathbf{a}_1 = \\begin{bmatrix}\na_{11} \\\\\na_{21} \\\\\na_{31} \\\\\na_{41} \\\\\n\\end{bmatrix} \\qquad\n\\mathbf{a}_2 = \\begin{bmatrix}\na_{12} \\\\\na_{22} \\\\\na_{32} \\\\\na_{42} \\\\\n\\end{bmatrix} \\qquad\n\\mathbf{a}_3 = \\begin{bmatrix}\na_{13} \\\\\na_{23} \\\\\na_{33} \\\\\na_{43} \\\\\n\\end{bmatrix} \\qquad\n\nA m \\times n matrix is therefore a collection of n vectors of size m put side by side column-wise:\nA = \\begin{bmatrix}\n\\mathbf{a}_1 & \\mathbf{a}_2 & \\mathbf{a}_3\\\\\n\\end{bmatrix}\nSo all properties of the vector spaces (associativity, commutativity, distributivity) also apply to matrices, as additions and multiplications with a scalar are defined.\n\\alpha \\, A + \\beta \\, B = \\begin{bmatrix}\n\\alpha\\, a_{11} + \\beta \\, b_{11} & \\alpha\\, a_{12} + \\beta \\, b_{12} & \\alpha\\, a_{13} + \\beta \\, b_{13} \\\\\n\\alpha\\, a_{21} + \\beta \\, b_{21} & \\alpha\\, a_{22} + \\beta \\, b_{22} & \\alpha\\, a_{23} + \\beta \\, b_{23} \\\\\n\\alpha\\, a_{31} + \\beta \\, b_{31} & \\alpha\\, a_{32} + \\beta \\, b_{32} & \\alpha\\, a_{33} + \\beta \\, b_{33} \\\\\n\\alpha\\, a_{41} + \\beta \\, b_{41} & \\alpha\\, a_{42} + \\beta \\, b_{42} & \\alpha\\, a_{43} + \\beta \\, b_{43} \\\\\n\\end{bmatrix}\n\n\n\n\n\n\nNote\n\n\n\nBeware, you can only add matrices of the same dimensions m\\times n. You cannot add a 2\\times 3 matrix to a 5 \\times 4 one.\n\n\nThe transpose A^T of a m \\times n matrix A is a n \\times m matrix, where the row and column indices are swapped:\nA = \\begin{bmatrix}\na_{11} & a_{12} & \\cdots & a_{1n} \\\\\na_{21} & a_{22} & \\cdots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m1} & a_{m2} & \\cdots & a_{mn} \\\\\n\\end{bmatrix}, \\qquad\nA^T = \\begin{bmatrix}\na_{11} & a_{21} & \\cdots & a_{m1} \\\\\na_{12} & a_{22} & \\cdots & a_{m2} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{1n} & a_{2n} & \\cdots & a_{mn} \\\\\n\\end{bmatrix}\n\nThis is also true for vectors, which become horizontal after transposition:\n\\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_d \\end{bmatrix}, \\qquad\n\\mathbf{x}^T = \\begin{bmatrix} x_1 & x_2 & \\ldots & x_d \\end{bmatrix}\n\nA very important operation is the matrix multiplication. If A is a m\\times n matrix and B a n \\times p matrix:\n\nA=\\begin{bmatrix}\na_{11} & a_{12} & \\cdots & a_{1n} \\\\\na_{21} & a_{22} & \\cdots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m1} & a_{m2} & \\cdots & a_{mn} \\\\\n\\end{bmatrix},\\quad\nB=\\begin{bmatrix}\nb_{11} & b_{12} & \\cdots & b_{1p} \\\\\nb_{21} & b_{22} & \\cdots & b_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nb_{n1} & b_{n2} & \\cdots & b_{np} \\\\\n\\end{bmatrix}\n\nwe can multiply them to obtain a m \\times p matrix:\n\nC = A \\times B =\\begin{bmatrix}\nc_{11} & c_{12} & \\cdots & c_{1p} \\\\\nc_{21} & c_{22} & \\cdots & c_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nc_{m1} & c_{m2} & \\cdots & c_{mp} \\\\\n\\end{bmatrix}\n\nwhere each element c_{ij} is the dot product of the ith row of A and jth column of B:\nc_{ij} = \\langle A_{i, :} \\cdot B_{:, j} \\rangle = a_{i1}b_{1j} + a_{i2}b_{2j} +\\cdots + a_{in}b_{nj}= \\sum_{k=1}^n a_{ik}b_{kj}\n\n\n\n\n\n\nNote\n\n\n\nn, the number of columns of A and rows of B, must be the same!\n\n\n\n\n\nThe element c_{ij} of C = A \\times B is the dot product between the ith row of A and the jth column of B. Source: CC BY-NC-SA; Marcia Levitus\n\n\nThinking of vectors as n \\times 1 matrices, we can multiply a matrix m \\times n with a vector:\n\n\\mathbf{y} = A \\times \\mathbf{x} = \\begin{bmatrix}\na_{11} & a_{12} & \\cdots & a_{1n} \\\\\na_{21} & a_{22} & \\cdots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m1} & a_{m2} & \\cdots & a_{mn} \\\\\n\\end{bmatrix} \\times \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_m \\end{bmatrix}\n\nThe result \\mathbf{y} is a vector of size m. In that sense, a matrix A can transform a vector of size n into a vector of size m: A represents a projection from \\Re^n to \\Re^m.\n\n\n\nA 2 \\times 3 projection matrix allows to project any 3D vector onto a 2D plane. This is for example what happens inside a camera. Source: https://en.wikipedia.org/wiki/Homogeneous_coordinate\n\n\nNote that the dot product between two vectors of size n is the matrix multiplication between the transpose of the first vector and the second one:\n\\mathbf{x}^T \\times \\mathbf{y} = \\begin{bmatrix} x_1 & x_2 & \\ldots & x_n \\end{bmatrix} \\times \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix} = x_1 \\, y_1 + x_2 \\, y_2 + \\ldots + x_n \\, y_n = \\langle \\mathbf{x} \\cdot \\mathbf{y} \\rangle\nSquare matrices of size n \\times n can be inverted. The inverse A^{-1} of a matrix A is defined by:\nA \\times A^{-1} = A^{-1} \\times A = I\nwhere I is the identity matrix (a matrix with ones on the diagonal and 0 otherwise). Not all matrices have an inverse (those who don’t are called singular or degenerate). There are plenty of conditions for a matrix to be invertible (for example its determinant is non-zero, see https://en.wikipedia.org/wiki/Invertible_matrix), but they will not matter in this course. Non-square matrices are generally not invertible, but see the pseudoinverse (https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse).\nMatrix inversion allows to solve linear systems of equations. Given the problem:\n\n\\begin{cases}\n    a_{11} \\, x_1 + a_{12} \\, x_2 + \\ldots + a_{1n} \\, x_n = b_1 \\\\\n    a_{21} \\, x_1 + a_{22} \\, x_2 + \\ldots + a_{2n} \\, x_n = b_2 \\\\\n    \\ldots \\\\\n    a_{n1} \\, x_1 + a_{n2} \\, x_2 + \\ldots + a_{nn} \\, x_n = b_n \\\\\n\\end{cases}\n\nwhich is equivalent to:\nA \\times \\mathbf{x} = \\mathbf{b}\nwe can multiply both sides to the left with A^{-1} (if it exists) and obtain:\n\\mathbf{x} = A^{-1} \\times \\mathbf{b}"
  },
  {
    "objectID": "notes/1.2-Math.html#calculus",
    "href": "notes/1.2-Math.html#calculus",
    "title": "Math basics (optional)",
    "section": "Calculus",
    "text": "Calculus\n\n\nFunctions\nA univariate function f associates to any real number x \\in \\Re (or a subset of \\Re called the support of the function) another (unique) real number f(x):\n\n\\begin{align}\nf\\colon \\quad \\Re &\\to \\Re\\\\\nx &\\mapsto f(x)\n\\end{align}\n\n\n\n\nExample of univariate function, here the quadratic function f(x) = x^2 - 2 \\, x + 1.\n\n\nA multivariate function f associates to any vector \\mathbf{x} \\in \\Re^n (or a subset) a real number f(\\mathbf{x}):\n\n\\begin{align}\nf\\colon \\quad \\Re^n &\\to \\Re\\\\\n\\mathbf{x} &\\mapsto f(\\mathbf{x})\n\\end{align}\n\nThe variables of the function are the elements of the vector. For low-dimensional vector spaces, it is possible to represent each element explicitly, for example:\n\n\\begin{align}\nf\\colon \\quad\\Re^3 &\\to \\Re\\\\\nx, y, z &\\mapsto f(x, y, z),\\end{align}\n\n\n\n\nExample of a multivariate function f(x_1, x_2) mapping \\Re^2 to \\Re. Source: https://en.wikipedia.org/wiki/Function_of_several_real_variables\n\n\nVector fields associate to any vector \\mathbf{x} \\in \\Re^n (or a subset) another vector (possibly of different size):\n\n\\begin{align}\n\\overrightarrow{f}\\colon \\quad \\Re^n &\\to \\Re^m\\\\\n\\mathbf{x} &\\mapsto \\overrightarrow{f}(\\mathbf{x}),\\end{align}\n\n\n\n\nVector field associating to each point of \\Re^2 another vector. Source: https://en.wikipedia.org/wiki/Vector_field\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe matrix-vector multiplication \\mathbf{y} = A \\times \\mathbf{x} is a linear vector field, mapping any vector \\mathbf{x} into another vector \\mathbf{y}.\n\n\n\n\nDifferentiation\n\nDerivatives\nDifferential calculus deals with the derivative of a function, a process called differentiation.\nThe derivative f'(x) or \\displaystyle\\frac{d f(x)}{dx} of a univariate function f(x) is defined as the local slope of the tangent to the function for a given value of x:\nf'(x) = \\lim_{h \\to 0} \\frac{f(x + h) - f(x)}{h}\nThe line passing through the points (x, f(x)) and (x + h, f(x + h)) becomes tangent to the function when h becomes very small:\n\n\n\nThe derivative of the function f(x) can be approximated by the slope of the line passing through (x, f(x)) and (x + h, f(x + h)) when h becomes very small.\n\n\nThe sign of the derivative tells you how the function behaves locally:\n\nIf the derivative is positive, increasing a little bit x increases the function f(x), so the function is locally increasing.\nIf the derivative is negative, increasing a little bit x decreases the function f(x), so the function is locally decreasing.\n\nIt basically allows you to measure the local influence of x on f(x): if I change a little bit the value x, what happens to f(x)? This will be very useful in machine learning.\nA special case is when the derivative is equal to 0 in x. x is then called an extremum (or optimum) of the function, i.e. it can be a maximum or minimum.\n\n\n\n\n\n\nNote\n\n\n\nIf you differentiate f'(x) itself, you obtain the second-order derivative f''(x). You can repeat that process and obtain higher order derivatives.\nFor example, if x(t) represents the position x of an object depending on time t, the first-order derivative x'(t) denotes the speed of the object and the second-order derivative x''(t) its acceleration.\n\n\nYou can tell whether an extremum is a maximum or a minimum by looking at its second-order derivative:\n\nIf f''(x) > 0, the extremum is a minimum.\nIf f''(x) < 0, the extremum is a maximum.\nIf f''(x) = 0, the extremum is a saddle point.\n\n\n\n\nQuadratic functions have only one extremum (here a minimum in -1), as their derivative is linear and is equal to zero for only one value.\n\n\nThe derivative of a multivariate function f(\\mathbf{x}) is a vector of partial derivatives called the gradient of the function \\nabla_\\mathbf{x} \\, f(\\mathbf{x}):\n\n    \\nabla_\\mathbf{x} \\, f(\\mathbf{x}) = \\begin{bmatrix}\n        \\displaystyle\\frac{\\partial f(\\mathbf{x})}{\\partial x_1} \\\\\n        \\displaystyle\\frac{\\partial f(\\mathbf{x})}{\\partial x_2} \\\\\n        \\ldots \\\\\n        \\displaystyle\\frac{\\partial f(\\mathbf{x})}{\\partial x_n} \\\\\n    \\end{bmatrix}\n\nThe subscript to the \\nabla operator denotes with respect to (w.r.t) which variable the differentiation is done.\nA partial derivative w.r.t. to particular variable (or element of the vector) is simply achieved by differentiating the function while considering all other variables to be constant. For example the function:\nf(x, y) = x^2 + 3 \\, x \\, y + 4 \\, x \\, y^2 - 1\ncan be partially differentiated w.r.t. x and y as:\n\\begin{cases}\n\\displaystyle\\frac{\\partial f(x, y)}{\\partial x} = 2 \\, x + 3\\, y + 4 \\, y^2 \\\\\n\\\\\n\\displaystyle\\frac{\\partial f(x, y)}{\\partial y} = 3 \\, x + 8\\, x \\, y\n\\end{cases}\n\nThe gradient can be generalized to vector fields, where the Jacobian or Jacobi matrix is a matrix containing all partial derivatives.\n\nJ = \\begin{bmatrix}\n    \\dfrac{\\partial \\mathbf{f}}{\\partial x_1} & \\cdots & \\dfrac{\\partial \\mathbf{f}}{\\partial x_n} \\end{bmatrix}\n= \\begin{bmatrix}\n    \\dfrac{\\partial f_1}{\\partial x_1} & \\cdots & \\dfrac{\\partial f_1}{\\partial x_n}\\\\\n    \\vdots & \\ddots & \\vdots\\\\\n    \\dfrac{\\partial f_m}{\\partial x_1} & \\cdots & \\dfrac{\\partial f_m}{\\partial x_n} \\end{bmatrix}\n\n\n\nAnalytical properties\nThe analytical form of the derivative of most standard mathematical functions is known. The following table lists the most useful ones in this course:\n\n\n\nFunction f(x)\nDerivative f'(x)\n\n\n\n\n x\n 1\n\n\nx^p\n p \\, x^{p-1}\n\n\n\\displaystyle\\frac{1}{x}\n- \\displaystyle\\frac{1}{x^2}\n\n\n e^x\ne^x\n\n\n\\ln x\n \\displaystyle\\frac{1}{x}\n\n\n\nDifferentiation is linear, which means that if we define the function:\nh(x) = a \\, f(x) + b \\, g(x)\nits derivative is:\nh'(x) = a \\, f'(x) + b \\, g'(x)\nA product of functions can also be differentiated analytically:\n(f(x) \\times g(x))' = f'(x) \\times g(x) + f(x) \\times g'(x)\n\n\n\n\n\n\nExample\n\n\n\nf(x) = x^2 \\, e^x\nf'(x) = 2 \\, x \\, e^x + x^2 \\cdot e^x\n\n\n\n\nChain rule\nA very important concept for neural networks is the chain rule, which tells how to differentiate function compositions (functions of a function) of the form:\n(f \\circ g) (x) = f(g(x))\nThe derivative of f \\circ g is:\n(f \\circ g)' (x) = (f' \\circ g) (x) \\times g'(x)\nThe chain rule may be more understandable using Leibniz’s notation:\n\\frac{d f \\circ g (x)}{dx} = \\frac{d f (g (x))}{d g(x)} \\times \\frac{d g (x)}{dx}\nBy posing y = g(x) as an intermediary variable, it becomes:\n\\frac{d f(y)}{dx} = \\frac{d f(y)}{dy} \\times \\frac{dy}{dx}\n\n\n\n\n\n\nExample\n\n\n\nThe function :\nh(x) = \\frac{1}{2 \\, x + 1}\nis the function composition of g(x) = 2 \\, x + 1 and f(x) = \\displaystyle\\frac{1}{x}, whose derivatives are known:\ng'(x) = 2 f'(x) = -\\displaystyle\\frac{1}{x^2}\nIts derivative according to the chain rule is:\nh'(x) = f'(g(x)) \\times g'(x) = -\\displaystyle\\frac{1}{(2 \\, x + 1)^2} \\times 2\n\n\nThe chain rule also applies to partial derivatives:\n\n    \\displaystyle\\frac{\\partial f \\circ g (x, y)}{\\partial x} = \\frac{\\partial f \\circ g (x, y)}{\\partial g (x, y)} \\times \\frac{\\partial g (x, y)}{\\partial x}\n\nand gradients:\n\n    \\nabla_\\mathbf{x} \\, f \\circ g (\\mathbf{x}) = \\nabla_{g(\\mathbf{x})} \\, f \\circ g (\\mathbf{x}) \\times \\nabla_\\mathbf{x} \\, g (\\mathbf{x})\n\n\n\n\nIntegration\nThe opposite operation of differentation is integration. Given a function f(x), we search a function F(x) whose derivative is f(x):\nF'(x) = f(x)\nThe integral of f is noted:\nF(x) = \\int f(x) \\, dx\ndx being an infinitesimal interval (similar h in the definition of the derivative). There are tons of formal definitions of integrals (Riemann, Lebesgue, Darboux…) and we will not get into details here as we will not use integrals a lot.\nThe most important to understand for now is maybe that the integral of a function is the area under the curve. The area under the curve of a function f on the interval [a, b] is:\n\\mathcal{S} = \\int_a^b f(x) \\, dx\n\n\n\nThe integral of f on [a, b] is the area of the surface between the function and the x-axis. Note that it can become negative when the function is mostly negative on [a, b]. Source: https://www.math24.net/riemann-sums-definite-integral/\n\n\nOne way to approximate this surface is to split the interval [a, b] into n intervals of width dx with the points x_1, x_2, \\ldots, x_n. This defines n rectangles of width dx and height f(x_i), so their surface is f(x_i) \\, dx. The area under the curve can then be approximated by the sum of the surfaces of all these rectangles.\n\n\n\nThe interval[a, b] can be split in n small intervals of width dx, defining n rectangles whose sum is close to the area under the curve. Source: https://www.math24.net/riemann-sums-definite-integral/\n\n\nWhen n \\to \\infty, or equivalently dx \\to 0, the sum of these rectangular areas (called the Riemann sum) becomes exactly the area under the curve. This is the definition of the definite integral:\n\\int_a^b f(x) \\, dx = \\lim_{dx \\to 0} \\sum_{i=1}^n f(x_i) \\, dx\nVery roughly speaking, the integral can be considered as the equivalent of a sum for continuous functions."
  },
  {
    "objectID": "notes/1.2-Math.html#probability-theory",
    "href": "notes/1.2-Math.html#probability-theory",
    "title": "Math basics (optional)",
    "section": "Probability theory",
    "text": "Probability theory\n\n\nDiscrete probability distributions\nLet’s note X a discrete random variable with n realizations (or outcomes) x_1, \\ldots, x_n.\n\nA coin has two outcomes: head and tails.\nA dice has six outcomes: 1, 2, 3, 4, 5, 6.\n\nThe probability that X takes the value x_i is defined in the frequentist sense by the relative frequency of occurrence, i.e. the proportion of samples having the value x_i, when the total number N of samples tends to infinity:\n\n    P(X = x_i) = \\frac{\\text{Number of favorable cases}}{\\text{Total number of samples}}\n\nThe set of probabilities \\{P(X = x_i)\\}_{i=1}^n define the probability distribution for the random variable (or probability mass function, pmf). By definition, we have 0 \\leq P(X = x_i) \\leq 1 and the probabilities have to respect:\n\n    \\sum_{i=1}^n P(X = x_i) = 1\n\nAn important metric for a random variable is its mathematical expectation or expected value, i.e. its “mean” realization weighted by the probabilities:\n\n    \\mathbb{E}[X] = \\sum_{i=1}^n P(X = x_i) \\, x_i\n\nThe expectation does not even need to be a valid realization:\n\n    \\mathbb{E}[\\text{Coin}] = \\frac{1}{2} \\, 0 + \\frac{1}{2} \\, 1 = 0.5\n\n\n    \\mathbb{E}[\\text{Dice}] = \\frac{1}{6} \\, (1 + 2 + 3 + 4 + 5 + 6) = 3.5\n\nWe can also compute the mathematical expectation of functions of a random variable:\n\n    \\mathbb{E}[f(X)] = \\sum_{i=1}^n P(X = x_i) \\, f(x_i)\n\nThe variance of a random variable is the squared deviation around the mean:\n\n    \\text{Var}(X) = \\mathbb{E}[(X - \\mathbb{E}[X])^2] = \\sum_{i=1}^n P(X = x_i) \\, (x_i - \\mathbb{E}[X])^2\n\nVariance of a coin:\n\n    \\text{Var}(\\text{Coin}) = \\frac{1}{2} \\, (0 - 0.5)^2 + \\frac{1}{2} \\, (1 - 0.5)^2 = 0.25\n\nVariance of a dice:\n\n    \\text{Var}(\\text{Dice}) = \\frac{1}{6} \\, ((1-3.5)^2 + (2-3.5)^2 + (3-3.5)^2 + (4-3.5)^2 + (5-3.5)^2 + (6-3.5)^2) = \\frac{105}{36}\n\n\n\nContinuous probability distributions\nContinuous random variables can take infinitely many values in a continuous interval, e.g. \\Re or some subset. The closed set of values they can take is called the support \\mathcal{D}_X of the probability distribution. The probability distribution is described by a probability density function (pdf) f(x).\n\n\n\nNormal distributions are continuous distributions. The area under the curve is always 1.\n\n\nThe pdf of a distribution must be positive (f(x) \\geq 0 \\, \\forall x \\in \\mathcal{D}_X) and its integral (area under the curve) must be equal to 1:\n\n    \\int_{x \\in \\mathcal{D}_X} f(x) \\, dx = 1\n\nThe pdf does not give the probability of taking a particular value x (it is 0), but allows to get the probability that a value lies in a specific interval:\n\n    P(a \\leq X \\leq b) = \\int_{a}^b f(x) \\, dx\n\nOne can however think of the pdf as the likelihood that a value x comes from that distribution.\nFor continuous distributions, the mathematical expectation is now defined by an integral instead of a sum:\n\n    \\mathbb{E}[X] = \\int_{x \\in \\mathcal{D}_X} f(x) \\, x \\, dx\n\nthe variance also:\n\n    \\text{Var}(X) = \\int_{x \\in \\mathcal{D}_X} f(x) \\, (x - \\mathbb{E}[X])^2 \\, dx\n\nor a function of the random variable:\n\n    \\mathbb{E}[g(X)] = \\int_{x \\in \\mathcal{D}_X} f(x) \\, g(x) \\, dx\n\nNote that the expectation operator is linear:\n\n    \\mathbb{E}[a \\, X + b \\, Y] = a \\, \\mathbb{E}[X] + b \\, \\mathbb{E}[Y]\n\nbut not the variance, even when the distributions are independent:\n\n    \\text{Var}[a \\, X + b \\, Y] = a^2 \\, \\text{Var}[X] + b^2 \\, \\text{Var}[Y]\n\n\n\nStandard distributions\nProbability distributions can in principle have any form: f(x) is unknown. However, specific parameterized distributions can be very useful: their pmf/pdf is fully determined by a couple of parameters.\n\nThe Bernouilli distribution is a binary (discrete, 0 or 1) distribution with a parameter p specifying the probability to obtain the outcome 1 (e.g. a coin):\n\n\n    P(X = 1) = p \\; \\text{and} \\; P(X=0) = 1 - p\n P(X=x) = p^x \\, (1-p)^{1-x} \\mathbb{E}[X] = p\n\nThe Multinouilli or categorical distribution is a discrete distribution with k realizations (e.g. a dice). Each realization x_i is associated with a parameter p_i >0 representing its probability. We have \\sum_i p_i = 1.\n\nP(X = x_i) = p_i\n\nThe uniform distribution has an equal and constant probability of returning values between a and b, never outside this range. It is parameterized by the start of the range a and the end of the range b. Its support is [a, b]. The pdf of the uniform distribution \\mathcal{U}(a, b) is defined on [a, b] as:\n\n\n    f(x; a, b) = \\frac{1}{b - a}\n\n\nThe normal distribution is the most frequently encountered continuous distribution. It is parameterized by two parameters: the mean \\mu and the variance \\sigma^2 (or standard deviation \\sigma). Its support is \\Re. The pdf of the normal distribution \\mathcal{N}(\\mu, \\sigma) is defined on \\Re as:\n\n\n    f(x; \\mu, \\sigma) = \\frac{1}{\\sqrt{2\\,\\pi\\,\\sigma^2}} \\, e^{-\\displaystyle\\frac{(x - \\mu)^2}{2\\,\\sigma^2}}\n\n\nThe exponential distribution is the probability distribution of the time between events in a Poisson point process, i.e., a process in which events occur continuously and independently at a constant average rate. It is parameterized by one parameter: the rate \\lambda. Its support is \\Re^+ (x > 0). The pdf of the exponential distribution is defined on \\Re^+ as:\n\n\n    f(x; \\lambda) = \\lambda \\, e^{-\\lambda \\, x}\n\n\n\nJoint and conditional probabilities\nLet’s now suppose that we have two random variables X and Y with different probability distributions P(X) and P(Y). The joint probability P(X, Y) denotes the probability of observing the realizations x and y at the same time:\nP(X=x, Y=y)\nIf the random variables are independent, we have:\nP(X=x, Y=y) = P(X=x) \\, P(Y=y)\nIf you know the joint probability, you can compute the marginal probability distribution of each variable:\nP(X=x) = \\sum_y P(X=x, Y=y)\nThe same is true for continuous probability distributions:\n\n    f(x) = \\int f(x, y) \\, dy\n\nSome useful information between two random variables is the conditional probability. P(X=x | Y=y) is the conditional probability that X=x, given that Y=y is observed.\n\nY=y is not random anymore: it is a fact (at least theoretically).\nYou wonder what happens to the probability distribution of X now that you know the value of Y.\n\nConditional probabilities are linked to the joint probability by:\n\n    P(X=x | Y=y) = \\frac{P(X=x, Y=y)}{P(Y=y)}\n\nIf X and Y are independent, we have P(X=x | Y=y) = P(X=x) (knowing Y does not change anything to the probability distribution of X). We can use the same notation for the complete probability distributions:\n\n    P(X | Y) = \\frac{P(X, Y)}{P(Y)}\n\nExample\n\n\n\nSource: https://www.elevise.co.uk/g-e-m-h-5-u.html.\n\n\nYou ask 50 people whether they like cats or dogs:\n\n18 like both cats and dogs.\n21 like only dogs.\n5 like only cats.\n6 like none of them.\n\nWe consider loving cats and dogs as random variables (and that our sample size is big enough to use probabilities…). Among the 23 who love cats, which proportion also loves dogs?\n\n\n\n\n\n\nAnswer\n\n\n\nWe have P(\\text{dog}) = \\displaystyle\\frac{18+21}{50}= \\displaystyle\\frac{39}{50} and P(\\text{cat}) = \\displaystyle\\frac{18+5}{50} = \\frac{23}{50}.\nThe joint probability of loving both cats and dogs is P(\\text{cat}, \\text{dog}) = \\displaystyle\\frac{18}{50}.\nThe conditional probability of loving dogs given one loves cats is:\nP(\\text{dog} | \\text{cat}) = \\displaystyle\\frac{P(\\text{cat}, \\text{dog})}{P(\\text{cat})} = \\frac{\\frac{18}{50}}{\\frac{23}{50}} = \\frac{18}{23}\n\n\n\n\nBayes’ rule\nNoticing that the definition of conditional probabilities is symmetric:\n\n    P(X, Y) = P(X | Y) \\, P(Y) = P(Y | X) \\, P(X)\n\nwe can obtain the Bayes’ rule:\n\n    P(Y | X) = \\frac{P(X|Y) \\, P(Y)}{P(X)}\n\nIt is very useful when you already know P(X|Y) and want to obtain P(Y|X) (Bayesian inference).\n\nP(Y | X) is called the posterior probability.\nP(X | Y) is called the likelihood.\nP(Y) is called the prior probability (belief).\nP(X) is called the model evidence or marginal likelihood.\n\nExample\nLet’s consider a disease D (binary random variable) and a medical test T (also binary). The disease affects 10% of the general population:\nP(D=1)= 0.1 \\qquad \\qquad P(D=0)=0.9\nWhen a patient has the disease, the test is positive 80% of the time (true positives):\nP(T=1 | D=1) = 0.8 \\qquad \\qquad P(T=0 | D=1) = 0.2\nWhen a patient does not have the disease, the test is still positive 10% of the time (false positives):\nP(T=1 | D=0) = 0.1 \\qquad \\qquad P(T=0 | D=0) = 0.9\nGiven that the test is positive, what is the probability that the patient is ill?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\\begin{aligned}\n    P(D=1|T=1) &= \\frac{P(T=1 | D=1) \\, P(D=1)}{P(T=1)} \\\\\n               &\\\\\n               &= \\frac{P(T=1 | D=1) \\, P(D=1)}{P(T=1 | D=1) \\, P(D=1) + P(T=1 | D=0) \\, P(D=0)} \\\\\n               &\\\\\n               &= \\frac{0.8 \\times 0.1}{0.8 \\times 0.1 + 0.1 \\times 0.9} \\\\\n               &\\\\\n               & = 0.47 \\\\\n\\end{aligned}"
  },
  {
    "objectID": "notes/1.2-Math.html#statistics",
    "href": "notes/1.2-Math.html#statistics",
    "title": "Math basics (optional)",
    "section": "Statistics",
    "text": "Statistics\n\n\nMonte Carlo sampling\nRandom sampling or Monte Carlo sampling (MC) consists of taking N samples x_i out of the distribution X (discrete or continuous) and computing the sample average:\n\n    \\mathbb{E}[X] = \\mathbb{E}_{x \\sim X} [x] \\approx \\frac{1}{N} \\, \\sum_{i=1}^N x_i\n\n\n\n\nSamples taken from a normal distribution will mostly be around the mean.\n\n\nMore samples will be obtained where f(x) is high (x is probable), so the average of the sampled data will be close to the expected value of the distribution.\nLaw of big numbers\n\nAs the number of identically distributed, randomly generated variables increases, their sample mean (average) approaches their theoretical mean.\n\nMC estimates are only correct when:\n\nthe samples are i.i.d (independent and identically distributed):\n\nindependent: the samples must be unrelated with each other.\nidentically distributed: the samples must come from the same distribution X.\n\nthe number of samples is large enough. Usually N > 30 for simple distributions.\n\nOne can estimate any function of the random variable with random sampling:\n\n    \\mathbb{E}[f(X)] = \\mathbb{E}_{x \\sim X} [f(x)] \\approx \\frac{1}{N} \\, \\sum_{i=1}^N f(x_i)\n\n\n\n\nSampling can be used to estimate \\pi: when sampling x and y uniformly in [0, 1], the proportion of points with a norm smaller than tends to \\pi/4. Source: https://towardsdatascience.com/an-overview-of-monte-carlo-methods-675384eb1694\n\n\n\n\nCentral limit theorem\nSuppose we have an unknown distribution X with expected value \\mu = \\mathbb{E}[X] and variance \\sigma^2. We can take randomly N samples from X to compute the sample average:\n\n    S_N = \\frac{1}{N} \\, \\sum_{i=1}^N x_i\n\nThe Central Limit Theorem (CLT) states that:\n\nThe distribution of sample averages is normally distributed with mean \\mu and variance \\frac{\\sigma^2}{N}.\n\nS_N \\sim \\mathcal{N}(\\mu, \\frac{\\sigma}{\\sqrt{N}})\nIf we perform the sampling multiple times, even with few samples, the average of the sampling averages will be very close to the expected value. The more samples we get, the smaller the variance of the estimates. Although the distribution X can be anything, the sampling averages are normally distributed.\n\n\n\nSource: https://en.wikipedia.org/wiki/Central_limit_theorem\n\n\n\n\nEstimators\nCLT shows that the sampling average is an unbiased estimator of the expected value of a distribution:\n\\mathbb{E}[S_N] = \\mathbb{E}[X]\nAn estimator is a random variable used to measure parameters of a distribution (e.g. its expectation). The problem is that estimators can generally be biased.\nTake the example of a thermometer M measuring the temperature T. T is a random variable (normally distributed with \\mu=20 and \\sigma=10) and the measurements M relate to the temperature with the relation:\n\n    M = 0.95 \\, T + 0.65\n\n\n\n\nLeft: measurement as a function of the temperature. Right: distribution of temperature.\n\n\nThe thermometer is not perfect, but do random measurements allow us to estimate the expected value of the temperature?\nWe could repeatedly take 100 random samples of the thermometer and see how the distribution of sample averages look like:\n\n\n\nSampled measurements.\n\n\nBut, as the expectation is linear, we actually have:\n\n    \\mathbb{E}[M] = \\mathbb{E}[0.95 \\, T + 0.65] = 0.95 \\, \\mathbb{E}[T] + 0.65 = 19.65 \\neq \\mathbb{E}[T]\n\nThe thermometer is a biased estimator of the temperature.\nLet’s note \\theta a parameter of a probability distribution X that we want to estimate (it does not have to be its mean). An estimator \\hat{\\theta} is a random variable mapping the sample space of X to a set of sample estimates.\n\nThe bias of an estimator is the mean error made by the estimator:\n\n\n    \\mathcal{B}(\\hat{\\theta}) = \\mathbb{E}[\\hat{\\theta} - \\theta] = \\mathbb{E}[\\hat{\\theta}] - \\theta\n\n\nThe variance of an estimator is the deviation of the samples around the expected value:\n\n\n    \\text{Var}(\\hat{\\theta}) = \\mathbb{E}[(\\hat{\\theta} - \\mathbb{E}[\\hat{\\theta}] )^2]\n\nIdeally, we would like estimators with:\n\nlow bias: the estimations are correct on average (= equal to the true parameter).\nlow variance: we do not need many estimates to get a correct estimate (CLT: \\frac{\\sigma}{\\sqrt{N}})\n\n\n\n\nBias-variance trade-off.\n\n\nUnfortunately, the perfect estimator does not exist in practice. One usually talks of a bias/variance trade-off: if you have a small bias, you will have a high variance, or vice versa. In machine learning, bias corresponds to underfitting, variance to overfitting."
  },
  {
    "objectID": "notes/1.2-Math.html#information-theory",
    "href": "notes/1.2-Math.html#information-theory",
    "title": "Math basics (optional)",
    "section": "Information theory",
    "text": "Information theory\n\n\nEntropy\nInformation theory (a field founded by Claude Shannon) asks how much information is contained in a probability distribution. Information is related to surprise or uncertainty: are the outcomes of a random variable surprising?\n\nAlmost certain outcomes (P \\sim 1) are not surprising because they happen all the time.\nAlmost impossible outcomes (P \\sim 0) are very surprising because they are very rare.\n\n\n\n\nSelf-information.\n\n\nA useful measurement of how surprising is an outcome x is the self-information:\n\n    I (x) = - \\log P(X = x)\n\nDepending on which log is used, self-information has different units, but it is just a rescaling, the base never matters:\n\n\\log_2: bits or shannons.\n\\log_e = \\ln: nats.\n\nThe entropy (or Shannon entropy) of a probability distribution is the expected value of the self-information of its outcomes:\n\n    H(X) = \\mathbb{E}_{x \\sim X} [I(x)] = \\mathbb{E}_{x \\sim X} [- \\log P(X = x)]\n\nIt measures the uncertainty, randomness or information content of the random variable.\nIn the discrete case:\n\n    H(X) = - \\sum_x P(x) \\, \\log P(x)\n\nIn the continuous case:\n\n    H(X) = - \\int_x f(x) \\, \\log f(x) \\, dx\n\nThe entropy of a Bernouilli variable is maximal when both outcomes are equiprobable. If a variable is deterministic, its entropy is minimal and equal to zero.\n\n\n\nThe entropy of a Bernouilli distribution is maximal when the two outcomes are equiprobable.\n\n\nThe joint entropy of two random variables X and Y is defined by:\n\n    H(X, Y) = \\mathbb{E}_{x \\sim X, y \\sim Y} [- \\log P(X=x, Y=y)]\n\nThe conditional entropy of two random variables X and Y is defined by:\n\n    H(X | Y) = \\mathbb{E}_{x \\sim X, y \\sim Y} [- \\log P(X=x | Y=y)]  = \\mathbb{E}_{x \\sim X, y \\sim Y} [- \\log \\frac{P(X=x , Y=y)}{P(Y=y)}]\n\nIf the variables are independent, we have:\n\n    H(X, Y) = H(X) + H(Y)\n \n    H(X | Y) = H(X)\n\nBoth are related by:\n\n    H(X | Y) = H(X, Y) - H(Y)\n\nThe equivalent of Bayes’ rule is:\n\n    H(Y |X) = H(X |Y) + H(Y) - H(X)\n\n\n\nMutual Information, cross-entropy and Kullback-Leibler divergence\nThe most important information measurement between two variables is the mutual information MI (or information gain):\n\n    I(X, Y) = H(X) - H(X | Y) = H(Y) - H(Y | X)\n\nIt measures how much information the variable X holds on Y:\n\nIf the two variables are independent, the MI is 0 : X is as random, whether you know Y or not.\n\n\n        I (X, Y) = 0\n\n\nIf the two variables are dependent, knowing Y gives you information on X, which becomes less random, i.e. less uncertain / surprising.\n\n\n        I (X, Y) > 0\n\nIf you can fully predict X when you know Y, it becomes deterministic (H(X|Y)=0) so the mutual information is maximal (I(X, Y) = H(X)).\nThe cross-entropy between two distributions X and Y is defined as:\n\n    H(X, Y) = \\mathbb{E}_{x \\sim X}[- \\log P(Y=x)]\n\n\n\n\n\n\n\nNote\n\n\n\nBeware that the notation H(X, Y) is the same as the joint entropy, but it is a different concept!\n\n\n\n\n\nThe cross-entropy measures the overlap between two probability distributions.\n\n\nThe cross-entropy measures the negative log-likelihood that a sample x taken from the distribution X could also come from the distribution Y. More exactly, it measures how many bits of information one would need to distinguish the two distributions X and Y.\n\n    H(X, Y) = \\mathbb{E}_{x \\sim X}[- \\log P(Y=x)]\n\nIf the two distributions are the same almost anywhere, one cannot distinguish samples from the two distributions, the cross-entropy is the same as the entropy of X. If the two distributions are completely different, one can tell whether a sample Z comes from X or Y, the cross-entropy is higher than the entropy of X.\nIn practice, the Kullback-Leibler divergence \\text{KL}(X ||Y) is a better measurement of the similarity (statistical distance) between two probability distributions:\n\n    \\text{KL}(X ||Y) = \\mathbb{E}_{x \\sim X}[- \\log \\frac{P(Y=x)}{P(X=x)}]\n\nIt is linked to the cross-entropy by:\n\n    \\text{KL}(X ||Y) = H(X, Y) - H(X)\n\nIf the two distributions are the same almost anywhere, the KL divergence is zero. If the two distributions are different, the KL divergence is positive. Minimizing the KL between two distributions is the same as making the two distributions “equal”. But remember: the KL is not a metric, as it is not symmetric.\n\n\n\n\n\n\nNote\n\n\n\nRefer https://towardsdatascience.com/entropy-cross-entropy-and-kl-divergence-explained-b09cdae917a for nice visual explanations of the cross-entropy.\n\n\n\n\n\n\nGoodfellow, I., Bengio, Y., and Courville, A. (2016). Deep Learning. MIT Press http://www.deeplearningbook.org."
  },
  {
    "objectID": "notes/1.3-Neurons.html#biological-neurons",
    "href": "notes/1.3-Neurons.html#biological-neurons",
    "title": "Neurons",
    "section": "Biological neurons",
    "text": "Biological neurons\n\nThe human brain is composed of 100 billion neurons. A biological neuron is a cell, composed of a cell body (soma), multiple dendrites and an axon. The axon of a neuron can contact the dendrites of another through synapses to transmit information. There are hundreds of different types of neurons, each with different properties.\n\n\n\nBiological neuron. Source: https://en.wikipedia.org/wiki/Neuron\n\n\nNeurons are negatively charged: they have a resting potential at around -70 mV. When a neuron receives enough input currents, its membrane potential can exceed a threshold and the neuron emits an action potential (or spike) along its axon.\n\n\n\nPropagation of an action potential along the axon. Source: https://en.wikipedia.org/wiki/Action_potential\n\n\nA spike has a very small duration (1 or 2 ms) and its amplitude is rather constant. It is followed by a refractory period where the neuron is hyperpolarized, limiting the number of spikes per second to 200.\n\n\n\nAction potential or spike. Source: https://en.wikipedia.org/wiki/Action_potential\n\n\nThe action potential arrives at the synapses and releases neurotransmitters in the synaptic cleft: glutamate (AMPA, NMDA), GABA, dopamine, serotonin, nicotin, etc… Neurotransmitters can enter the receiving neuron through receptors and change its potential: the neuron may emit a spike too. Synaptic currents change the membrane potential of the post.synaptic neuron. The change depends on the strength of the synapse called the synaptic efficiency or weight. Some synapses are stronger than others, and have a larger influence on the post-synaptic cell.\n\n\n\nNeurotransmitter release at the synapse. Source: https://en.wikipedia.org/wiki/Neuron\n\n\n\nThe two important dimensions of the information exchanged by neurons are:\n\nThe instantaneous frequency or firing rate: number of spikes per second (Hz).\nThe precise timing of the spike trains.\n\n\n\n\nNeurons emit spikes at varying frequencies (firing rate) and variable timings. Source: https://en.wikipedia.org/wiki/Neural_oscillation\n\n\nThe shape of the spike (amplitude, duration) does not matter much for synaptic transission: spikes can be considered as binary signals (0 or 1) occuring at precise moments of time.\nSome neuron models called rate-coded models only represent the firing rate of a neuron and ignore spike timing at all. Other models called spiking models represent explicitly the spiking behavior."
  },
  {
    "objectID": "notes/1.3-Neurons.html#hodgkin-huxley-neurons",
    "href": "notes/1.3-Neurons.html#hodgkin-huxley-neurons",
    "title": "Neurons",
    "section": "Hodgkin-Huxley neurons",
    "text": "Hodgkin-Huxley neurons\n\nAlan Hodgkin and Andrew Huxley (Nobel prize 1963) were the first to propose a detailed mathematical model of the giant squid neuron. The membrane potential V of the neuron is governed by an electrical circuit, including sodium and potassium channels. The membrane has a capacitance C that models the dynamics of the membrane (time constant). The conductance g_L allows the membrane potential to relax back to its resting potential E_L in the absence of external currents. External currents (synaptic inputs) perturb the membrane potential and can bring the neuron to fire an action potential.\nTheir neuron model include:\n\nAn ordinary differential equation (ODE) for the membrane potential v.\nThree ODEs for n, m and h representing potassium channel activation, sodium channel activation, and sodium channel inactivation.\nSeveral parameters determined experimentally.\n\n\n\\begin{aligned}\n    a_n &= 0.01 \\, (v + 60) / (1.0 - \\exp(-0.1\\, (v + 60) ) ) \\\\\n    a_m &= 0.1 \\, (v + 45) / (1.0 - \\exp (- 0.1 \\, ( v + 45 ))) \\\\\n    a_h &= 0.07 \\, \\exp(- 0.05 \\, ( v + 70 )) \\\\\n    b_n &= 0.125 \\, \\exp (- 0.0125 \\, (v + 70)) \\\\\n    b_m &= 4 \\,  \\exp (- (v + 70) / 80) \\\\\n    b_h &= 1/(1 + \\exp (- 0.1 \\, ( v + 40 )) ) \\\\\n    & \\\\\n    \\frac{dn}{dt} &= a_n \\, (1 - n) - b_n \\, n  \\\\\n    \\frac{dm}{dt} &= a_m \\, (1 - m) - b_m \\, m  \\\\\n    \\frac{dh}{dt} &= a_h \\, (1 - h) - b_h \\, h  \\\\\n\\end{aligned}\n\n\n\\begin{aligned}\n    C \\, \\frac{dv}{dt} = g_L \\, (V_L - v) &+ g_K \\, n^4 \\, (V_K - v) \\\\\n        & + g_\\text{Na} \\, m^3 \\, h \\, (V_\\text{Na} - v) + I \\\\\n\\end{aligned}\n\nThese equations allow to describe very precisely how an action potential is created from external currents.\n\n\n\nAction potential for a Hodgkin-Huxley neuron."
  },
  {
    "objectID": "notes/1.3-Neurons.html#spiking-neurons",
    "href": "notes/1.3-Neurons.html#spiking-neurons",
    "title": "Neurons",
    "section": "Spiking neurons",
    "text": "Spiking neurons\nAs action potentials are stereotypical, it is a waste of computational resources to model their generation precisely. What actually matters are the sub-threshold dynamics, i.e. what happens before the spike is emitted.\nThe leaky integrate-and-fire (LIF; Lapicque, 1907) neuron integrates its input current and emits a spike if the membrane potential exceeds a threshold.\n\n    C \\, \\frac{dv}{dt} = - g_L \\, (v - V_L) + I\n\n\n    \\text{if} \\; v > V_T \\; \\text{emit a spike and reset.}\n\n\n\n\nSpike emission for a LIF neuron.\n\n\nOther well-known spiking neuron models include:\n\nIzhikevich quadratic IF (Izhikevich, 2003), using a quadratic function of the membrane potential and an adaptation variable u.\n\n\n    \\frac{dv}{dt} = 0.04 \\, v^2 + 5 \\, v + 140 - u + I\n \n    \\frac{du}{dt} = a \\, (b \\, v - u)\n\n\nAdaptive exponential IF (AdEx, (Brette and Gerstner, 2005)), using an exponential function.\n\n\n\\begin{aligned}\n    C \\, \\frac{dv}{dt} = -g_L \\ (v - E_L) + & g_L \\, \\Delta_T \\, \\exp(\\frac{v - v_T}{\\Delta_T}) \\\\\n                                            & + I - w\n\\end{aligned}\n \n    \\tau_w \\, \\frac{dw}{dt} = a \\, (v - E_L) - w\n\n\n\n\nDifferent subthreshold dynamics between the LIF, Izhikevich and AdEx neuron models.\n\n\nContrary to the simple LIF model, these realistic neuron models can reproduce a variety of dynamics, as biological neurons do not all respond the same to an input current. Some fire regularly, some slow down with time, while others emit bursts of spikes. Modern spiking neuron models allow to recreate these variety of dynamics by changing a few parameters.\n\n\n\nDifferent parameters of the AdEx neuron model produce different spiking patterns."
  },
  {
    "objectID": "notes/1.3-Neurons.html#rate-coded-neurons",
    "href": "notes/1.3-Neurons.html#rate-coded-neurons",
    "title": "Neurons",
    "section": "Rate-coded neurons",
    "text": "Rate-coded neurons\n\nAt the population level, interconnected networks of spiking neurons tend to fire synchronously (code redundancy). What if the important information was not the precise spike timings, but the firing rate of a small population? The instantaneous firing rate is defined in Hz (number of spikes per second). It can be estimated by an histogram of the spikes emitted by a network of similar neurons, or by repeating the same experiment multiple times for a single neuron. One can also build neural models that directly model the firing rate of (a population of) neuron(s): the rate-coded neuron.\n\n\n\nThe spiking pattern (raster plot) of a population of interconnected neurons can be approximated by its mean firing rate.\n\n\nA rate-coded neuron is represented by two time-dependent variables:\n\nThe “membrane potential” v(t) which evolves over time using an ODE.\n\n\n    \\tau \\, \\frac{d v(t)}{dt} + v(t) = \\sum_{i=1}^d w_{i, j} \\, r_i(t) + b\n\n\nThe firing rate r(t) which transforms the membrane potential into a single continuous value using a transfer function or activation function.\n\n\n    r(t) = f(v(t))\n\n\n\n\nRate-coded neuron.\n\n\nThe membrane potential uses a weighted sum of inputs (the firing rates r_i(t) of other neurons) by multiplying each rate with a weight w_i and adds a constant value b (the bias). The activation function can be any non-linear function, usually making sure that the firing rate is positive.\n\n\n\nFiring rate of a rate-coded neuron for a step input.\n\n\nRemarks on ODEs\nLet’s consider a simple rate-coded neuron taking a step signal I(t) as input:\n\n    \\tau \\, \\frac{d v(t)}{dt} + v(t) = I(t)\n\n\n    r(t) = (v(t))^+\n\nThe “speed” of v(t) is given by its temporal derivative:\n\n    \\frac{d v(t)}{dt} = \\frac{I(t) - v(t)}{\\tau}\n\nWhen v(t) is quite different from I(t), the membrane potential “accelerates” to reduce the difference. When v(t) is similar to I(t), the membrane potential stays constant.\n\n\n\nThe time constant \\tau of a rate-coded neuron influences the speed at which it reacts to inputs.\n\n\nThe membrane potential follows an exponential function which tries to “match” its input with a speed determined by the time constant \\tau. The time constant \\tau determines how fast the rate-coded neuron matches its inputs. Biological neurons have time constants between 5 and 30 ms depending on the cell type.\nThere exists a significant number of transfer functions that can be used:\n\n\n\nTypical transfer functions used in neural networks include the rectifier (ReLU), piece-wise linear, sigmoid (or logistic) and tanh functions..\n\n\nWhen using the rectifier activation function (ReLU):\n\n    f(x) = \\max(0, x)\n\nthe membrane potential v(t) can take any value, but the firing rate r(t) is only positive.\n\n\n\nThe rectifier function only keeps the positive part of the membrane potential.\n\n\nWhen using the logistic (or sigmoid) activation function:\n\n    f(x) = \\frac{1}{1 + \\exp(-x)}\n\nthe firing rate r(t) is bounded between 0 and 1, but responds for negative membrane potentials.\n\n\n\nThe sigmoid/logistic function bounds the firing rate between 0 and 1, even if the membrane potential is negative."
  },
  {
    "objectID": "notes/1.3-Neurons.html#artificial-neurons",
    "href": "notes/1.3-Neurons.html#artificial-neurons",
    "title": "Neurons",
    "section": "Artificial neurons",
    "text": "Artificial neurons\n\nBy omitting the dynamics of the rate-coded neuron, one obtains the very simple artificial neuron (McCulloch and Pitts, 1943):\n\n\n\nArtificial neuron.\n\n\nAn artificial neuron sums its inputs x_1, \\ldots, x_d by multiplying them with weights w_1, \\ldots, w_d, adds a bias b and transforms the result into an output y using an activation function f.\n\n    y = f( \\sum_{i=1}^d w_i \\, x_i + b)\n\nThe output y directly reflects the input, without temporal integration. The weighted sum of inputs + bias \\sum_{i=1}^d w_i \\, x_i + b is called the net activation.\nThis overly simplified neuron model is the basic unit of the artificial neural networks (ANN) used in machine learning / deep learning.\nArtificial neurons and hyperplanes\nLet’s consider an artificial neuron with only two inputs x_1 and x_2.\nThe net activation w_1 \\, x_1 + w_2 \\, x_2 + b is the equation of a line in the space (x_1, x_2).\n\n    w_1 \\, x_1 + w_2 \\, x_2 + b = 0 \\Leftrightarrow x_2 = - \\frac{w_1}{w_2} \\, x_1 - \\frac{b}{w_2}\n\n\n\n\nThe net activation represents an hyperplane in 2D.\n\n\nThe net activation is a line in 2D, a plane in 3D, etc. Generally, the net activation describes an hyperplane in the input space with d dimensions (x_1, x_2, \\ldots, x_d). An hyperplane has one dimension less than the space.\n\n\n\nHyperplane in 3D. Source: https://newvitruvian.com/explore/vector-planes/#gal_post_7186_nonzero-vector.gif\n\n\nWe can write the net activation using a weight vector \\mathbf{w} and a bias b:\n\n    \\sum_{i=1}^d w_i \\, x_i + b  = \\langle\\mathbf{w} \\cdot \\mathbf{x} \\rangle + b\n\nwith:\n\n    \\mathbf{w} = \\begin{bmatrix} w_1 \\\\ w_2 \\\\ \\ldots \\\\ w_d \\end{bmatrix} \\qquad \\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\ldots \\\\ x_d \\end{bmatrix}\n\n\\langle \\cdot \\rangle is the dot product (aka inner product, scalar product) between the input vector \\mathbf{x} and the weight vector \\mathbf{w}.\nThe weight vector is orthogonal to the hyperplane (\\mathbf{w}, b) and defines its orientation. b is the “signed distance” between the hyperplane and the origin. The hyperplane separates the input space into two parts:\n\n\\langle\\mathbf{w} \\cdot \\mathbf{x} \\rangle + b > 0 for all points \\mathbf{x} above the hyperplane.\n\\langle\\mathbf{w} \\cdot \\mathbf{x} \\rangle + b < 0 for all points \\mathbf{x} below the hyperplane.\n\nBy looking at the sign of the net activation, we can separate the input space into two classes. This will be the main principle of linear classification.\n\n\n\nThe sign of the projection of an input \\mathbf{x} on the hyperplane tells whether the input is above or below the hyperplane.\n\n\n\n\n\n\nBrette, R., and Gerstner, W. (2005). Adaptive Exponential Integrate-and-Fire Model as an Effective Description of Neuronal Activity. Journal of Neurophysiology 94, 3637–3642. doi:10.1152/jn.00686.2005.\n\n\nIzhikevich, E. M. (2003). Simple model of spiking neurons. IEEE transactions on neural networks 14, 1569–72. doi:10.1109/TNN.2003.820440."
  },
  {
    "objectID": "notes/2.1-Optimization.html#analytic-optimization",
    "href": "notes/2.1-Optimization.html#analytic-optimization",
    "title": "Optimization",
    "section": "Analytic optimization",
    "text": "Analytic optimization\n\nMachine learning is all about optimization:\n\nSupervised learning minimizes the error between the prediction and the data.\nUnsupervised learning maximizes the fit between the model and the data\nReinforcement learning maximizes the collection of rewards.\n\nThe function to be optimized is called the objective function, cost function or loss function. ML searches for the value of free parameters which optimize the objective function on the data set. The simplest optimization method is the gradient descent (or ascent) method.\nThe easiest method to find the optima of a function f(x) is to look where its first-order derivative is equal to 0:\n\n    x^* = \\min_x f(x) \\Leftrightarrow f'(x^*) = 0 \\; \\text{and} \\; f''(x^*) > 0\n\n\n    x^* = \\max_x f(x) \\Leftrightarrow f'(x^*) = 0 \\; \\text{and} \\; f''(x^*) < 0\n\nThe sign of the second order derivative tells us whether it is a maximum or minimum. There can be multiple minima or maxima (or none) depending on the function. The “best” minimum (with the lowest value among all minima) is called the global minimum. The others are called local minima.\n\n\n\nFunctions (may) have one global minimum but several local minima.\n\n\nMultivariate functions\nA multivariate function is a function of more than one variable, e.g. f(x, y). A point (x^*, y^*) is an optimum of f if all partial derivatives are zero:\n\n    \\begin{cases}\n        \\dfrac{\\partial f(x^*, y^*)}{\\partial x} = 0 \\\\\n        \\dfrac{\\partial f(x^*, y^*)}{\\partial y} = 0 \\\\\n    \\end{cases}\n\nThe vector of partial derivatives is called the gradient of the function:\n\n    \\nabla_{x, y} \\, f(x^*, y^*) = \\begin{bmatrix} \\dfrac{\\partial f(x^*, y^*)}{\\partial x} \\\\ \\dfrac{\\partial f(x^*, y^*)}{\\partial y} \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\n\n\n\n\nMultivariate optimization of f(x, y) = (x - 1)^2 + y^2 + 1. The minimum is in (1, 0)."
  },
  {
    "objectID": "notes/2.1-Optimization.html#gradient-descent",
    "href": "notes/2.1-Optimization.html#gradient-descent",
    "title": "Optimization",
    "section": "Gradient descent",
    "text": "Gradient descent\n\nIn machine learning, we generally do not have access to the analytical form of the objective function. We can not therefore get its derivative and search where it is 0. However, we have access to its value (and derivative) for certain values, for example:\n\n    f(0, 1) = 2 \\qquad f'(0, 1) = -1.5\n\nWe can “ask” the model for as many values as we want, but we never get its analytical form. For most useful problems, the function would be too complex to differentiate anyway.\n\n\n\nEuler method: the derivative of a function is the slope of its tangent.\n\n\nLet’s remember the definition of the derivative of a function. The derivative f'(x) is defined by the slope of the tangent of the function:\n\n    f'(x) = \\lim_{h \\to 0} \\frac{f(x + h) - f(x)}{x + h - x} = \\lim_{h \\to 0} \\frac{f(x + h) - f(x)}{h}\n\nIf we take h small enough, we have the following approximation:\n\n    f(x + h) - f(x) \\approx h \\, f'(x)\n\nIf we want x+h to be closer to the minimum than x, we want:\n\n    f(x + h) < f(x)\n\nor:\n\n    f(x + h) - f(x) < 0\n\nWe therefore want that:\n\n    h \\, f'(x) < 0\n\nThe change h in the value of x must have the opposite sign of f'(x) in order to get closer to the minimum. If the function is increasing in x, the minimum is smaller (to the left) than x. If the function is decreasing in x, the minimum is bigger than x (to the right).\nGradient descent (GD) is a first-order method to iteratively find the minimum of a function f(x). It starts with a random estimate x_0 and iteratively changes its value so that it becomes closer to the minimum.\n\n\n\nGradient descent iteratively modifies the estimate x_n in the opposite direction of the derivative.\n\n\nIt creates a series of estimates [x_0, x_1, x_2, \\ldots] that converge to a local minimum of f. Each element of the series is calculated based on the previous element and the derivative of the function in that element:\n\n    x_{n+1} = x_n + \\Delta x =  x_n - \\eta \\, f'(x_n)\n\nIf the function is locally increasing (resp. decreasing), the new estimate should be smaller (resp. bigger) than the previous one. \\eta is a small parameter between 0 and 1 called the learning rate that controls the speed of convergence (more on that later).\nGradient descent algorithm:\n\nWe start with an initially wrong estimate of x: x_0\nfor n \\in [0, \\infty]:\n\nWe compute or estimate the derivative of the loss function in x_{n}: f'(x_{n})\nWe compute a new value x_{n+1} for the estimate using the gradient descent update rule:\n\n\n      \\Delta x = x_{n+1} - x_n =  - \\eta \\, f'(x_n)\n  \n\nThere is theoretically no end to the GD algorithm: we iterate forever and always get closer to the minimum. The algorithm can be stopped when the change \\Delta x is below a threshold.\n\n\n\nVisualization of Gradient Descent on a quadratic function. Notice how the speed of convergence slows down when approaching the minimum.\n\n\nGradient descent can be applied to multivariate functions:\n\n    \\min_{x, y, z} \\qquad f(x, y, z)\n\nEach variable is updated independently using partial derivatives:\n\n    \\Delta x = x_{n+1} - x_{n} = - \\eta \\, \\frac{\\partial f(x_n, y_n, z_n)}{\\partial x}\n \n    \\Delta y = y_{n+1} - y_{n} = - \\eta \\, \\frac{\\partial f(x_n, y_n, z_n)}{\\partial y}\n \n    \\Delta z = z_{n+1} - z_{n} = - \\eta \\, \\frac{\\partial f(x_n, y_n, z_n)}{\\partial z}\n\nWe can also use the vector notation to use the gradient operator:\n\n    \\mathbf{x}_n = \\begin{bmatrix} x_n \\\\ y_n \\\\ z_n \\end{bmatrix} \\quad \\text{and} \\quad \\nabla_\\mathbf{x} f(\\mathbf{x}) = \\begin{bmatrix} \\frac{\\partial f(x, y, z)}{\\partial x} \\\\ \\frac{\\partial f(x, y, z)}{\\partial y} \\\\ \\frac{\\partial f(x, y, z)}{\\partial z} \\end{bmatrix}\n    \\qquad \\rightarrow \\qquad \\Delta \\mathbf{x} = - \\eta \\, \\nabla_\\mathbf{x} f(\\mathbf{x}_n)\n\nThe change in the estimation is in the opposite direction of the gradient, hence the name gradient descent.\n\n\n\nVisualization of Gradient Descent on a multivariate function in 2 dimensions.\n\n\nThe choice of the learning rate \\eta is critical:\n\nIf it is too small, the algorithm will need a lot of iterations to converge.\nIf it is too big, the algorithm can oscillate around the desired values without ever converging.\n\n\n\n\nInfluence of the learning on convergence: too small (red) and it takes forever, too high (green) and convergence is unstable. Finding its optimal value (blue) is hard as it depends on the function itself.\n\n\nGradient descent is not optimal: it always finds a local minimum, but there is no guarantee that it is the global minimum. The found solution depends on the initial choice of x_0. If you initialize the parameters near to the global minimum, you are lucky. But how? This will be a big issue in neural networks."
  },
  {
    "objectID": "notes/2.1-Optimization.html#regularization",
    "href": "notes/2.1-Optimization.html#regularization",
    "title": "Optimization",
    "section": "Regularization",
    "text": "Regularization\n\n\nL2 - Regularization\nMost of the time, there are many minima to a function, if not an infinity. As GD only converges to the “closest” local minimum, you are never sure that you get a good solution. Consider the following function:\n\n    f(x, y) = (x -1)^2\n\nAs it does not depend on y, whatever initial value y_0 will be considered as a solution. As we will see later, this is something we do not want.\n\n\n\nFunction with an infinity of minima: as long as x=1, each point on the vertical line is a minimum.\n\n\nTo obtain a single solution, we may want to put the additional constraint that both x and y should be as small as possible. One possibility is to also minimize the Euclidian norm (or L2-norm) of the vector \\mathbf{x} = [x, y].\n\n    \\min_{x, y} ||\\mathbf{x}||^2 = x^2 + y^2\n\nNote that this objective is in contradiction with the original objective: (0, 0) minimizes the norm, but not the function f(x, y). We construct a new function as the sum of f(x, y) and the norm of \\mathbf{x}, weighted by the regularization parameter \\lambda:\n\n    \\mathcal{L}(x, y) = f(x, y) + \\lambda \\, (x^2 + y^2)\n\nFor a fixed value of \\lambda (for example 0.1), we now minimize using gradient descent this new loss function. To do that, we just need to compute its gradient:\n\n    \\nabla_{x, y} \\, \\mathcal{L}(x, y) = \\begin{bmatrix} \\frac{\\partial f(x, y)}{\\partial x} + 2\\, \\lambda \\, x \\\\ \\frac{\\partial f(x, y)}{\\partial y} + 2\\, \\lambda \\, y \\end{bmatrix}\n\nand apply gradient descent iteratively:\n\n    \\Delta \\begin{bmatrix} x \\\\ y \\end{bmatrix} = - \\eta \\, \\nabla_{x, y} \\, \\mathcal{L}(x, y) = - \\eta \\, \\begin{bmatrix} \\frac{\\partial f(x, y)}{\\partial x} + 2\\, \\lambda \\, x \\\\ \\frac{\\partial f(x, y)}{\\partial y} + 2\\, \\lambda \\, y \\end{bmatrix}\n\n\n\n\nGradient descent with L2 regularization, using \\lambda = 0.1.\n\n\nYou may notice that the result of the optimization is a bit off, it is not exactly (1, 0). This is because we do not optimize f(x, y) directly, but \\mathcal{L}(x, y). Let’s have a look at the landscape of the loss function:\n\n\n\nLandscape of the loss function \\mathcal{L}(x, y) = f(x, y) + \\lambda \\, (x^2 + y^2) with \\lambda = 0.1.\n\n\nThe optimization with GD indeed works, it is just that the function is different. The constraint on the Euclidian norm “attracts” or “distorts” the function towards (0, 0). This may seem counter-intuitive, but we will see with deep networks that we can live with it. Let’s now look at what happens when we increase \\lambda to 5:\n\n\n\nGradient descent with L2 regularization, using \\lambda = 5.\n\n\n\n\n\nLandscape of the loss function \\mathcal{L}(x, y) = f(x, y) + \\lambda \\, (x^2 + y^2) with \\lambda = 5.\n\n\nNow the result of the optimization is totally wrong: the constraint on the norm completely dominates the optimization process.\n\n    \\mathcal{L}(x, y) = f(x, y) + \\lambda \\, (x^2 + y^2)\n\n\\lambda controls which of the two objectives, f(x, y) or x^2 + y^2, has the priority:\n\nWhen \\lambda is small, f(x, y) dominates and the norm of \\mathbf{x} can be anything.\nWhen \\lambda is big, x^2 + y^2 dominates, the result will be very small but f(x, y) will have any value.\n\nThe right value for \\lambda is hard to find. We will see later methods to experimentally find its most adequate value.\n\n\n\n\n\n\nNote\n\n\n\nRegularization is a form of constrained optimization. What we actually want to solve is the constrained optimization problem:\n\n    \\min_{x, y} \\qquad f(x, y)\n so that: \n    x^2 + y^2 < \\delta\n\ni.e. minimize f(x, y) while keeping the norm of [x, y] below a threshold \\delta. Lagrange optimization (technically KKT optimization; see the course Introduction to AI) allows to solve that problem by searching the minimum of the generalized Lagrange function:\n\n    \\mathcal{L}(x, y, \\lambda) = f(x, y) + \\lambda \\, (x^2 + y^2 - \\delta)\n\nRegularization is a special case of Lagrange optimization, as it considers \\lambda to be fixed, while it is an additional variable in Lagrange optimization. When differentiating this function, \\delta disappears anyway, so it is equivalent to our regularized loss function.\n\n\n\n\nL1 - Regularization\nAnother form of regularization is L1 - regularization using the L1-norm (absolute values):\n\n    \\mathcal{L}(x, y) = f(x, y) + \\lambda \\, (|x| + |y|)\n\nIts gradient only depend on the sign of x and y:\n\n    \\nabla_{x, y} \\, \\mathcal{L}(x, y) = \\begin{bmatrix} \\frac{\\partial f(x, y)}{\\partial x} + \\lambda \\, \\text{sign}(x) \\\\ \\frac{\\partial f(x, y)}{\\partial y} + \\lambda \\, \\text{sign}(y) \\end{bmatrix}\n\nIt tends to lead to sparser value of (x, y), i.e. either x or y will be close or equal to 0.\n\n\n\nGradient descent with L1 regularization, using \\lambda = 0.1.\n\n\nBoth L1 and L2 regularization can be used in neural networks depending on the desired effect."
  },
  {
    "objectID": "notes/2.2-LinearRegression.html#linear-regression",
    "href": "notes/2.2-LinearRegression.html#linear-regression",
    "title": "Linear regression",
    "section": "Linear regression",
    "text": "Linear regression\n\n\n\n\nSimple linear regression. x is the input, y the output. The data is represented by blue dots, the model by the black line.\n\n\nLet’s consider a training set of N examples \\mathcal{D} = (x_i, t_i)_{i=1..N}. In linear regression, we want to learn a linear model (hypothesis) y that is linearly dependent on the input x:\n\n    y = f_{w, b}(x) = w \\, x + b\n\nThe free parameters of the model are the slope w and the intercept b. This model corresponds to a single artificial neuron with output y, having one input x, one weight w, one bias b and a linear activation function f(x) = x.\n\n\n\nArtificial neuron with multiple inputs.\n\n\nThe goal of the linear regression (or least mean squares - LMS) is to minimize the mean square error (mse) between the targets and the predictions. This loss function is defined as the mathematical expectation of the quadratic error over the training set:\n\n    \\mathcal{L}(w, b) =  \\mathbb{E}_{x_i, t_i \\in \\mathcal{D}} [ (t_i - y_i )^2 ]\n\nAs the training set is finite and the samples i.i.d (independent and identically distributed), we can simply replace the expectation by a sampling average over the training set:\n\n    \\mathcal{L}(w, b) = \\frac{1}{N} \\, \\sum_{i=1}^{N} (t_i - y_i )^2\n\nThe minimum of the mse is achieved when the prediction y_i = f_{w, b}(x_i) is equal to the true value t_i for all training examples. In other words, we want to minimize the residual error of the model on the data. It is not always possible to obtain the global minimum (0) as the data may be noisy, but the closer, the better.\n\n\n\nA good fit to the data is when the prediction y_i (on the line) is close to the data t_i for all training examples.\n\n\n\nLeast Mean Squares\nWe search for w and b which minimize the mean square error:\n\n    \\mathcal{L}(w, b) = \\frac{1}{N} \\, \\sum_{i=1}^{N} (t_i - y_i )^2\n\nWe will apply gradient descent to iteratively modify estimates of w and b:\n\n    \\Delta w = - \\eta \\, \\frac{\\partial \\mathcal{L}(w, b)}{\\partial w}\n \n    \\Delta b = - \\eta \\, \\frac{\\partial \\mathcal{L}(w, b)}{\\partial b}\n\nLet’s search for the partial derivative of the mean square error with respect to w:\n\n    \\frac{\\partial \\mathcal{L}(w, b)}{\\partial w} = \\frac{\\partial}{\\partial w} [\\frac{1}{N} \\, \\sum_{i=1}^{N} (t_i - y_i )^2]\n\nPartial derivatives are linear, so the derivative of a sum is the sum of the derivatives:\n\n    \\frac{\\partial \\mathcal{L}(w, b)}{\\partial w} = \\frac{1}{N} \\, \\sum_{i=1}^{N} \\frac{\\partial}{\\partial w} (t_i - y_i )^2\n\nThis means we can compute a gradient for each training example instead of for the whole training set (see later the distinction batch/online):\n\n    \\frac{\\partial \\mathcal{L}(w, b)}{\\partial w} = \\frac{1}{N} \\, \\sum_{i=1}^{N} \\frac{\\partial}{\\partial w} \\mathcal{l}_i(w, b)\n    \\qquad \\text{with} \\qquad \\mathcal{l}_i(w, b) = (t_i - y_i )^2\n\nThe individual loss \\mathcal{l}_i(w, b) = (t_i - y_i )^2 is the composition of two functions:\n\na square error function g_i(y_i) = (t_i - y_i)^2.\nthe prediction y_i = f_{w, b}(x_i) = w \\, x_i + b.\n\nThe chain rule tells us how to derive such composite functions:\n\n    \\frac{ d f(g(x))}{dx} = \\frac{ d f(g(x))}{d g(x)} \\times \\frac{ d g(x)}{dx} = \\frac{ d f(y)}{dy} \\times \\frac{ d g(x)}{dx}\n\nThe first derivative considers g(x) to be a single variable. Applied to our problem, this gives:\n\n     \\frac{\\partial}{\\partial w} \\mathcal{l}_i(w, b) =  \\frac{\\partial g_i(y_i)}{\\partial y_i} \\times  \\frac{\\partial y_i}{\\partial w}\n\nThe square error function g_i(y) = (t_i - y)^2 is easy to differentiate w.r.t y:\n\n    \\frac{\\partial g_i(y_i)}{\\partial y_i} = - 2 \\, (t_i - y_i)\n\nThe prediction y_i = w \\, x_i + b also w.r.t w and b:\n\n   \\frac{\\partial  y_i}{\\partial w} = x_i\n\n\n   \\frac{\\partial  y_i}{\\partial b} = 1\n\nThe partial derivative of the individual loss is:\n\n    \\frac{\\partial \\mathcal{l}_i(w, b)}{\\partial w} = - 2 \\, (t_i - y_i) \\, x_i\n\n\n    \\frac{\\partial \\mathcal{l}_i(w, b)}{\\partial b} = - 2 \\, (t_i - y_i)\n\nThis gives us:\n\n    \\frac{\\partial \\mathcal{L}(w, b)}{\\partial w} = - \\frac{2}{N} \\sum_{i=1}^{N} (t_i - y_i) \\, x_i\n\n\n    \\frac{\\partial \\mathcal{L}(w, b)}{\\partial b} = - \\frac{2}{N} \\sum_{i=1}^{N} (t_i - y_i)\n\nGradient descent is then defined by the learning rules (absorbing the 2 in \\eta):\n\n    \\Delta w = \\eta \\, \\frac{1}{N} \\sum_{i=1}^{N} (t_i - y_i) \\, x_i\n\n\n    \\Delta b = \\eta \\, \\frac{1}{N} \\sum_{i=1}^{N} (t_i - y_i)\n\nLeast Mean Squares (LMS) or Ordinary Least Squares (OLS) is a batch algorithm: the parameter changes are computed over the whole dataset.\n\n    \\Delta w = \\eta \\, \\frac{1}{N} \\sum_{i=1}^{N} (t_i - y_i) \\, x_i\n \n    \\Delta b = \\eta \\, \\frac{1}{N} \\sum_{i=1}^{N} (t_i - y_i)\n\nThe parameter changes have to be applied multiple times (epochs) in order for the parameters to converge. One can stop when the parameters do not change much, or after a fixed number of epochs.\n\n\n\n\n\n\nLMS algorithm\n\n\n\n\nw=0 \\quad;\\quad b=0\nfor M epochs:\n\ndw=0 \\quad;\\quad db=0\nfor each sample (x_i, t_i):\n\ny_i = w \\, x_i + b\ndw = dw + (t_i - y_i) \\, x_i\ndb = db + (t_i - y_i)\n\n\\Delta w = \\eta \\, \\frac{1}{N} dw\n\\Delta b = \\eta \\, \\frac{1}{N} db\n\n\n\n\n\n\n\nVisualization of least mean squares applied to a simple regression problem with \\eta=0.1. Each step of the animation corresponds to one epoch (iteration over the training set).\n\n\nDuring learning, the mean square error (mse) decreases with the number of epochs but does not reach zero because of the noise in the data.\n\n\n\nEvolution of the loss function during training.\n\n\n\n\nDelta learning rule\nLMS is very slow, because it changes the weights only after the whole training set has been evaluated. It is also possible to update the weights immediately after each example using the delta learning rule, which is the online version of LMS:\n\\Delta w = \\eta \\, (t_i - y_i) \\, x_i\n\\Delta b = \\eta \\, (t_i - y_i)\n\n\n\n\n\n\nDelta learning rule\n\n\n\n\nw=0 \\quad;\\quad b=0\nfor M epochs:\n\nfor each sample (x_i, t_i):\n\ny_i = w \\, x_i + b\n\\Delta w = \\eta \\, (t_i - y_i ) \\, x_i\n\\Delta b = \\eta \\, (t_i - y_i)\n\n\n\n\n\nThe batch version is more stable, but the online version is faster: the weights have already learned something when arriving at the end of the first epoch. Note that the loss function is slightly higher at the end of learning (see Exercise 3 for a deeper discussion).\n\n\n\nVisualization of the delta learning rule applied to a simple regression problem with \\eta = 0.1. Each step of the animation corresponds to one epoch (iteration over the training set).\n\n\n\n\n\nEvolution of the loss function during training. With the same learning rate, the delta learning rule converges much faster but reaches a poorer minimum. Lowering the learning rate slows down learning but reaches a better minimum."
  },
  {
    "objectID": "notes/2.2-LinearRegression.html#multiple-linear-regression",
    "href": "notes/2.2-LinearRegression.html#multiple-linear-regression",
    "title": "Linear regression",
    "section": "Multiple linear regression",
    "text": "Multiple linear regression\n\nThe key idea of linear regression (one input x, one output y) can be generalized to multiple inputs and outputs.\nMultiple Linear Regression (MLR) predicts several output variables based on several explanatory variables:\n\n\\begin{cases}\ny_1 = w_1 \\, x_1 + w_2 \\, x_2 + b_1\\\\\n\\\\\ny_2 = w_3 \\, x_1 + w_3 \\, x_2 + b_2\\\\\n\\end{cases}\n\n\n\n\n\n\n\nExample: fuel consumption and CO2 emissions\n\n\n\nLet’s suppose you have 13971 measurements in some Excel file, linking engine size, number of cylinders, fuel consumption and CO2 emissions of various cars. You want to predict fuel consumption and CO2 emissions when you know the engine size and the number of cylinders.\n\n\n\nEngine size\nCylinders\nFuel consumption\nCO2 emissions\n\n\n\n\n2\n 4\n8.5\n196\n\n\n2.4\n4\n9.6\n221\n\n\n1.5\n4\n5.9\n136\n\n\n3.5\n6\n11\n255\n\n\n…\n…\n…\n…\n\n\n\n\n\n\nCO2 emissions and fuel consumption depend almost linearly on the engine size and number of cylinders.\n\n\n\n\n\nCO2 emissions and fuel consumption depend almost linearly on the engine size and number of cylinders.\n\n\nWe can notice that the output variables seem to linearly depend on the inputs. Noting the input variables x_1, x_2 and the output ones y_1, y_2, we can define our problem as a multiple linear regression:\n\n\\begin{cases}\ny_1 = w_1 \\, x_1 + w_2 \\, x_2 + b_1\\\\\n\\\\\ny_2 = w_3 \\, x_1 + w_3 \\, x_2 + b_2\\\\\n\\end{cases}\n\nand solve it using the least mean squares method by minimizing the mse between the model and the data.\n\n\n\nThe result of MLR is a plane in the input space.\n\n\nUsing the Python library scikit-learn (https://scikit-learn.org), this is done in two lines of code:\nfrom sklearn.linear_model import LinearRegression\nreg = LinearRegression().fit(X, y)\n\n\nThe system of equations:\n\n\\begin{cases}\ny_1 = w_1 \\, x_1 + w_2 \\, x_2 + b_1\\\\\n\\\\\ny_2 = w_3 \\, x_1 + w_4 \\, x_2 + b_2\\\\\n\\end{cases}\n\ncan be put in a matrix-vector form:\n\n    \\begin{bmatrix} y_1 \\\\ y_2 \\\\\\end{bmatrix} = \\begin{bmatrix} w_1 & w_2 \\\\ w_3 & w_4 \\\\\\end{bmatrix} \\times \\begin{bmatrix} x_1 \\\\ x_2 \\\\\\end{bmatrix} + \\begin{bmatrix} b_1 \\\\ b_2 \\\\\\end{bmatrix}\n\nWe simply create the corresponding vectors and matrices:\n\n    \\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\\\end{bmatrix} \\qquad \\mathbf{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\\\end{bmatrix} \\qquad \\mathbf{t} = \\begin{bmatrix} t_1 \\\\ t_2 \\\\\\end{bmatrix} \\qquad \\mathbf{b} = \\begin{bmatrix} b_1 \\\\ b_2 \\\\\\end{bmatrix} \\qquad W = \\begin{bmatrix} w_1 & w_2 \\\\ w_3 & w_4 \\\\\\end{bmatrix}\n\n\\mathbf{x} is the input vector, \\mathbf{y} is the output vector, \\mathbf{t} is the target vector. W is called the weight matrix and \\mathbf{b} the bias vector.\nThe model is now defined by:\n\n    \\mathbf{y} = f_{W, \\mathbf{b}}(\\mathbf{x}) = W \\times \\mathbf{x} + \\mathbf{b}\n\nThe problem is exactly the same as before, except that we use vectors and matrices instead of scalars: \\mathbf{x} and \\mathbf{y} can have any number of dimensions, the same procedure will apply. This corresponds to a linear neural network (or linear perceptron), with one output neuron per predicted value y_i using the linear activation function.\n\n\n\nA linear perceptron is a single layer of artificial neurons. The output vector \\mathbf{y} is compared to the ground truth vector \\mathbf{t} using the mse loss.\n\n\nThe mean square error still needs to be a scalar in order to be minimized. We can define it as the squared norm of the error vector:\n\n    \\min_{W, \\mathbf{b}} \\, \\mathcal{L}(W, \\mathbf{b}) = \\mathbb{E}_\\mathcal{D} [ ||\\mathbf{t} - \\mathbf{y}||^2 ] = \\mathbb{E}_\\mathcal{D} [ ((t_1 - y_1)^2 + (t_2 - y_2)^2) ]\n\nIn order to apply gradient descent, one needs to calculate partial derivatives w.r.t the weight matrix W and the bias vector \\mathbf{b}, i.e. gradients:\n\n    \\begin{cases}\n    \\Delta W = - \\eta \\, \\nabla_W \\, \\mathcal{L}(W, \\mathbf{b}) \\\\\n    \\\\\n    \\Delta \\mathbf{b} = - \\eta \\, \\nabla_\\mathbf{b} \\, \\mathcal{L}(W, \\mathbf{b}) \\\\\n    \\end{cases}\n\n\n\n\n\n\n\nNote\n\n\n\nSome more advanced linear algebra becomes important to know how to compute these gradients:\nhttps://web.stanford.edu/class/cs224n/readings/gradient-notes.pdf\n\n\nWe search the minimum of the mse loss function:\n\n    \\min_{W, \\mathbf{b}} \\, \\mathcal{L}(W, \\mathbf{b}) = \\mathbb{E}_\\mathcal{D} [ ||\\mathbf{t} - \\mathbf{y}||^2 ] \\approx \\frac{1}{N} \\, \\sum_{i=1}^N ||\\mathbf{t}_i - \\mathbf{y}_i||^2 = \\frac{1}{N} \\, \\sum_{i=1}^N \\mathcal{l}_i(W, \\mathbf{b})\n\nThe individual loss function \\mathcal{l}_i(W, \\mathbf{b}) is the squared \\mathcal{L}^2-norm of the error vector, what can be expressed as a dot product or a vector multiplication:\n\n    \\mathcal{l}_i(W, \\mathbf{b}) = ||\\mathbf{t}_i - \\mathbf{y}_i||^2 = \\langle \\mathbf{t}_i - \\mathbf{y}_i \\cdot \\mathbf{t}_i - \\mathbf{y}_i \\rangle = (\\mathbf{t}_i - \\mathbf{y}_i)^T \\times (\\mathbf{t}_i - \\mathbf{y}_i)\n\n\n\n\n\n\n\nNote\n\n\n\nRemember:\n\\mathbf{x}^T \\times \\mathbf{x} = \\begin{bmatrix} x_1 & x_2 & \\ldots & x_n \\end{bmatrix} \\times \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix} = x_1 \\, x_1 + x_2 \\, x_2 + \\ldots + x_n \\, x_n = \\langle \\mathbf{x} \\cdot \\mathbf{x} \\rangle = ||\\mathbf{x}||^2_2\n\n\nThe chain rule tells us in principle that:\n\\nabla_{W} \\, \\mathcal{l}_i(W, \\mathbf{b}) = \\nabla_{\\mathbf{y}_i} \\, \\mathcal{l}_i(W, \\mathbf{b}) \\times \\nabla_{W} \\, \\mathbf{y}_i\nThe gradient w.r.t the output vector \\mathbf{y}_i is quite easy to obtain, as it a quadratic function of \\mathbf{t}_i - \\mathbf{y}_i:\n\\nabla_{\\mathbf{y}_i} \\, \\mathcal{l}_i(W, \\mathbf{b}) = \\nabla_{\\mathbf{y}_i} \\, (\\mathbf{t}_i - \\mathbf{y}_i)^T \\times (\\mathbf{t}_i - \\mathbf{y}_i)\nThe proof relies on product differentiation (f\\times g)' = f' \\, g + f \\, g':\n\\begin{aligned}\n    \\nabla_{\\mathbf{y}_i} \\, (\\mathbf{t}_i - \\mathbf{y}_i)^T \\times (\\mathbf{t}_i - \\mathbf{y}_i) & = ( \\nabla_{\\mathbf{y}_i} \\, (\\mathbf{t}_i - \\mathbf{y}_i) ) \\times (\\mathbf{t}_i - \\mathbf{y}_i) + (\\mathbf{t}_i - \\mathbf{y}_i) \\times \\nabla_{\\mathbf{y}_i} \\, (\\mathbf{t}_i - \\mathbf{y}_i)  \\\\\n    &\\\\\n    &= - (\\mathbf{t}_i - \\mathbf{y}_i) - (\\mathbf{t}_i - \\mathbf{y}_i) \\\\\n    &\\\\\n    &= - 2 \\, (\\mathbf{t}_i - \\mathbf{y}_i) \\\\\n\\end{aligned}\n\n\n\n\n\n\n\nNote\n\n\n\nWe use the properties \\nabla_{\\mathbf{x}}\\, \\mathbf{x}^T \\times \\mathbf{z} = \\mathbf{z} and \\nabla_{\\mathbf{z}} \\, \\mathbf{x}^T \\times \\mathbf{z} = \\mathbf{x} to get rid of the transpose.\n\n\nThe “problem” is when computing \\nabla_{W} \\, \\mathbf{y}_i = \\nabla_{W} \\, (W \\times \\mathbf{x}_i + \\mathbf{b}): * \\mathbf{y}_i is a vector and W a matrix. * \\nabla_{W} \\, \\mathbf{y}_i is then a Jacobian (matrix), not a gradient (vector).\nIntuitively, differentiating W \\times \\mathbf{x}_i + \\mathbf{b} w.r.t W should return \\mathbf{x}_i, but it is a vector, not a matrix…\nActually, only the gradient (or Jacobian) of \\mathcal{l}_i(W, \\mathbf{b}) w.r.t W should be a matrix of the same size as W so that we can apply gradient descent:\n\\Delta W = - \\eta \\, \\nabla_W \\, \\mathcal{L}(W, \\mathbf{b})\nWe already know that:\n\\nabla_{W} \\, \\mathcal{l}_i(W, \\mathbf{b}) = - 2\\, (\\mathbf{t}_i - \\mathbf{y}_i) \\times \\nabla_{W} \\, \\mathbf{y}_i\nIf \\mathbf{x}_i has n elements and \\mathbf{y}_i m elements, W is a m \\times n matrix.\n\n\n\n\n\n\nNote\n\n\n\nRemember the outer product between two vectors:\n\n\\mathbf{u} \\times \\mathbf{v}^\\textsf{T} =\n  \\begin{bmatrix}u_1 \\\\ u_2 \\\\ u_3 \\\\ u_4\\end{bmatrix}\n    \\begin{bmatrix}v_1 & v_2 & v_3\\end{bmatrix} =\n  \\begin{bmatrix}\n    u_1v_1 & u_1v_2 & u_1v_3 \\\\\n    u_2v_1 & u_2v_2 & u_2v_3 \\\\\n    u_3v_1 & u_3v_2 & u_3v_3 \\\\\n    u_4v_1 & u_4v_2 & u_4v_3\n  \\end{bmatrix}.\n\n\n\nIt is easy to see that the outer product between (\\mathbf{t}_i - \\mathbf{y}_i) and \\mathbf{x}_i gives a m \\times n matrix:\n\n    \\nabla_W \\, \\mathcal{l}_i(W, \\mathbf{b}) = - 2 \\, (\\mathbf{t}_i - \\mathbf{y}_i) \\times \\mathbf{x}_i^T\\\\\n\nLet’s prove it element per element on a small matrix:\n\n    \\mathbf{y} = W \\times \\mathbf{x} + \\mathbf{b}\n\n\n    \\begin{bmatrix} y_1 \\\\ y_2 \\\\\\end{bmatrix} = \\begin{bmatrix} w_1 & w_2 \\\\ w_3 & w_4 \\\\\\end{bmatrix} \\times \\begin{bmatrix} x_1 \\\\ x_2 \\\\\\end{bmatrix} + \\begin{bmatrix} b_1 \\\\ b_2 \\\\\\end{bmatrix}\n\n\n\\mathcal{l}(W, \\mathbf{b}) = (\\mathbf{t} - \\mathbf{y})^T \\times (\\mathbf{t} - \\mathbf{y}) = \\begin{bmatrix} t_1 - y_1 & t_2 - y_2 \\\\\\end{bmatrix} \\times \\begin{bmatrix} t_1 - y_1 \\\\ t_2 - y_2 \\\\\\end{bmatrix} = (t_1 - y_1)^2 + (t_2 - y_2)^2\n\nThe Jacobian w.r.t W can be explicitly formed using partial derivatives:\n\n\\nabla_W \\, \\mathcal{l}(W, \\mathbf{b}) = \\begin{bmatrix}\n\\dfrac{\\partial \\mathcal{l}(W, \\mathbf{b})}{\\partial w_1} & \\dfrac{\\partial \\mathcal{l}(W, \\mathbf{b})}{\\partial w_2} \\\\ \\dfrac{\\partial \\mathcal{l}(W, \\mathbf{b})}{\\partial w_3} & \\dfrac{\\partial \\mathcal{l}(W, \\mathbf{b})}{\\partial w_4} \\\\\n\\end{bmatrix}\n= \\begin{bmatrix}\n-2 \\, (t_1 - y_1) \\, x_1 & -2 \\, (t_1 - y_1) \\, x_2 \\\\ -2 \\, (t_2 - y_2) \\, x_1 & -2 \\, (t_2 - y_2) \\, x_2 \\\\\n\\end{bmatrix}\n\nWe can rearrange this matrix as an outer product:\n\n\\nabla_W \\, \\mathcal{l}(W, \\mathbf{b}) = -2 \\, \\begin{bmatrix}\nt_1 - y_1  \\\\  t_2 - y_2 \\\\\n\\end{bmatrix} \\times \\begin{bmatrix}\nx_1 & x_2 \\\\\n\\end{bmatrix}\n= - 2 \\, (\\mathbf{t} - \\mathbf{y}) \\times \\mathbf{x}^T\n\nMultiple linear regression\n\nBatch version (least mean squares):\n\n\\begin{cases}\n    \\Delta W = \\eta \\, \\dfrac{1}{N} \\sum_{i=1}^N \\, (\\mathbf{t}_i - \\mathbf{y}_i ) \\times \\mathbf{x}_i^T \\\\\n    \\\\\n    \\Delta \\mathbf{b} = \\eta \\, \\dfrac{1}{N} \\sum_{i=1}^N \\, (\\mathbf{t}_i - \\mathbf{y}_i) \\\\\n\\end{cases}\n\n\nOnline version (delta learning rule):\n\n\\begin{cases}\n    \\Delta W = \\eta \\, (\\mathbf{t}_i - \\mathbf{y}_i ) \\times \\mathbf{x}_i^T \\\\\n    \\\\\n    \\Delta \\mathbf{b} = \\eta \\, (\\mathbf{t}_i - \\mathbf{y}_i) \\\\\n\\end{cases}\n\nThe matrix-vector notation is completely equivalent to having one learning rule per parameter:\n\n\\begin{cases}\n    \\Delta w_1 = \\eta \\, (t_1 - y_1) \\, x_1 \\\\\n    \\Delta w_2 = \\eta \\, (t_1 - y_1) \\, x_2 \\\\\n    \\Delta w_3 = \\eta \\, (t_2 - y_2) \\, x_1 \\\\\n    \\Delta w_4 = \\eta \\, (t_2 - y_2) \\, x_2 \\\\\n\\end{cases}\n\\qquad\n\\begin{cases}\n    \\Delta b_1 = \\eta \\, (t_1 - y_1) \\\\\n    \\Delta b_2 = \\eta \\, (t_2 - y_2) \\\\\n\\end{cases}\n\n\n\n\n\n\n\nNote\n\n\n\nThe delta learning rule is always of the form: \\Delta w = eta * error * input. Biases have an input of 1."
  },
  {
    "objectID": "notes/2.2-LinearRegression.html#logistic-regression",
    "href": "notes/2.2-LinearRegression.html#logistic-regression",
    "title": "Linear regression",
    "section": "Logistic regression",
    "text": "Logistic regression\n\nLet’s suppose we want to perform a regression, but where the outputs t_i are bounded between 0 and 1. We could use a logistic (or sigmoid) function instead of a linear function in order to transform the input into an output:\n\n    y = \\sigma(w \\, x + b )  = \\displaystyle\\frac{1}{1+\\exp(-w \\, x - b )}\n\n\n\n\nLogistic or sigmoid function \\sigma(x)=\\displaystyle\\frac{1}{1+\\exp(-x)}.\n\n\nBy definition of the logistic function, the prediction y will be bounded between 0 and 1, what matches the targets t. Let’s now apply gradient descent on the mse loss using this new model. The individual loss will be:\nl_i(w, b) = (t_i - \\sigma(w \\, x_i + b) )^2 \nThe partial derivative of the individual loss is easy to find using the chain rule:\n\n\\begin{aligned}\n    \\displaystyle\\frac{\\partial l_i(w, b)}{\\partial w}\n        &= 2 \\, (t_i - y_i)  \\, \\frac{\\partial}{\\partial w}  (t_i - \\sigma(w \\, x_i + b ))\\\\\n        &\\\\\n        &= - 2 \\, (t_i - y_i) \\, \\sigma'(w \\, x_i + b ) \\,  x_i \\\\\n\\end{aligned}\n\nThe non-linear transfer function \\sigma(x) therefore adds its derivative into the gradient:\n\n    \\Delta w = \\eta \\, (t_i - y_i) \\, \\sigma'(w \\, x_i + b ) \\, x_i\n\nThe logistic function \\sigma(x)=\\frac{1}{1+\\exp(-x)} has the nice property that its derivative can be expressed easily:\n\n    \\sigma'(x) = \\sigma(x) \\, (1 - \\sigma(x) )\n\n\n\n\n\n\n\nNote\n\n\n\nHere is the proof using the fact that the derivative of \\displaystyle\\frac{1}{f(x)} is \\displaystyle\\frac{- f'(x)}{f^2(x)} :\n\\begin{aligned}\n    \\sigma'(x) & = \\displaystyle\\frac{-1}{(1+\\exp(-x))^2} \\, (- \\exp(-x)) \\\\\n    &\\\\\n    &= \\frac{1}{1+\\exp(-x)} \\times \\frac{\\exp(-x)}{1+\\exp(-x)}\\\\\n    &\\\\\n    &= \\frac{1}{1+\\exp(-x)} \\times \\frac{1 + \\exp(-x) - 1}{1+\\exp(-x)}\\\\\n    &\\\\\n    &= \\frac{1}{1+\\exp(-x)} \\times (1 - \\frac{1}{1+\\exp(-x)})\\\\\n    &\\\\\n    &= \\sigma(x) \\, (1 - \\sigma(x) )\\\\\n\\end{aligned}\n\n\n\nThe delta learning rule for the logistic regression model is therefore easy to obtain:\n\n\\begin{cases}\n    \\Delta w = \\eta \\, (t_i - y_i) \\, y_i \\, ( 1 - y_i ) \\, x_i \\\\\n\\\\\n    \\Delta b = \\eta \\, (t_i - y_i) \\, y_i \\, ( 1 - y_i ) \\\\\n\\end{cases}\n\nGeneralized form of the delta learning rule\n\n\n\nArtificial neuron with multiple inputs.\n\n\nFor a linear perceptron with parameters W and \\mathbf{b} and any activation function f:\n\n    \\mathbf{y} = f(W \\times \\mathbf{x} + \\mathbf{b} )  \n\nand the mse loss function:\n\n    \\mathcal{L}(W, \\mathbf{b}) = \\mathbb{E}_{\\mathcal{D}}[||\\mathbf{t} - \\mathbf{y}||^2]\n\nthe delta learning rule has the form:\n\n\\begin{cases}\n    \\Delta W = \\eta \\, [(\\mathbf{t} - \\mathbf{y}) \\odot f'(W \\times \\mathbf{x} + \\mathbf{b}) ] \\times \\mathbf{x}^T \\\\\n\\\\\n    \\Delta \\mathbf{b} = \\eta \\, (\\mathbf{t} - \\mathbf{y}) \\odot f'(W \\times \\mathbf{x} + \\mathbf{b}) \\\\\n\\end{cases}\n\n\\odot denotes element-wise multiplication, i.e. (\\mathbf{t} - \\mathbf{y}) \\odot f'(W \\times \\mathbf{x} + \\mathbf{b}) is also a vector.\nIn the linear case, f'(x) = 1. One can use any non-linear function, e.g hyperbolic tangent tanh(), ReLU, etc. Transfer functions are chosen for neural networks so that we can compute their derivative easily."
  },
  {
    "objectID": "notes/2.2-LinearRegression.html#polynomial-regression",
    "href": "notes/2.2-LinearRegression.html#polynomial-regression",
    "title": "Linear regression",
    "section": "Polynomial regression",
    "text": "Polynomial regression\n\n\n\n\n\n\nThe functions underlying real data are rarely linear plus some noise around the ideal value. In the figure above, the input/output function is better modeled by a second-order polynomial:\ny = f_{\\mathbf{w}, b}(x) = w_1 \\, x + w_2 \\, x^2 +b\nWe can transform the input into a vector of coordinates:\n\\mathbf{x} = \\begin{bmatrix} x \\\\ x^2 \\\\ \\end{bmatrix} \\qquad \\mathbf{w} = \\begin{bmatrix} w_1 \\\\ w_2 \\\\ \\end{bmatrix}\nThe problem becomes:\ny = \\langle \\mathbf{w} . \\mathbf{x} \\rangle + b = \\sum_j w_j \\, x_j + b\nWe can simply apply multiple linear regression (MLR) to find \\mathbf{w} and b:\n\\begin{cases}\n\\Delta \\mathbf{w} =  \\eta \\, (t - y) \\, \\mathbf{x}\\\\\n\\\\\n\\Delta b =  \\eta \\, (t - y)\\\\\n\\end{cases}\nThis generalizes to polynomials of any order p:\ny = f_{\\mathbf{w}, b}(x) = w_1 \\, x + w_2 \\, x^2 + \\ldots + w_p \\, x^p + b\nWe create a vector of powers of x:\n\\mathbf{x} = \\begin{bmatrix} x \\\\ x^2 \\\\ \\ldots \\\\ x^p \\end{bmatrix} \\qquad \\mathbf{w} = \\begin{bmatrix} w_1 \\\\ w_2 \\\\ \\ldots \\\\ w_p \\end{bmatrix}\nad apply multiple linear regression (MLR) to find \\mathbf{w} and b:\n\\begin{cases}\n\\Delta \\mathbf{w} =  \\eta \\, (t - y) \\, \\mathbf{x}\\\\\n\\\\\n\\Delta b =  \\eta \\, (t - y)\\\\\n\\end{cases}\n\nNon-linear problem solved! The only unknown is which order for the polynomial matches best the data. One can perform regression with any kind of parameterized function using gradient descent."
  },
  {
    "objectID": "notes/2.2-LinearRegression.html#a-bit-of-learning-theory",
    "href": "notes/2.2-LinearRegression.html#a-bit-of-learning-theory",
    "title": "Linear regression",
    "section": "A bit of learning theory",
    "text": "A bit of learning theory\n\nBefore going further, let’s think about what we have been doing so far. We had a bunch of data samples \\mathcal{D} = (\\mathbf{x}_i, t_i)_{i=1..N} (the training set). We decided to apply a (linear) model on it:\ny_i = \\langle \\mathbf{w} . \\mathbf{x}_i \\rangle + b\nWe then minimized the mean square error (mse) on that training set using gradient descent:\n\n    \\mathcal{L}(w, b) = \\mathbb{E}_{\\mathbf{x}, t \\in \\mathcal{D}} [(t_i - y_i )^2]\n\nAt the end of learning, we can measure the residual error of the model on the data:\n\n    \\epsilon_\\mathcal{D} = \\frac{1}{N} \\, \\sum_{i=1}^{N} (t_i - y_i )^2\n\nWe get a number, for example 0.04567. Is that good?\nThe mean square error mse is not very informative, as its value depends on how the outputs are scaled: multiply the targets and prediction by 10 and the mse is 100 times higher.\n\n\n\nThe residual error measures the quality of the fit, but it is sensible to the scaling of the outputs.\n\n\nThe coefficient of determination R^2 is a rescaled variant of the mse comparing the variance of the residuals to the variance of the data around its mean \\hat{t}:\n\n    R^2 = 1 - \\frac{\\text{Var}(\\text{residuals})}{\\text{Var}(\\text{data})} = 1 - \\frac{\\sum_{i=1}^N (t_i- y_i)^2}{\\sum_{i=1}^N (t_i - \\hat{t})^2}\n\nR^2 should be as close from 1 as possible. For example, if R^2 = 0.8, we can say that the model explains 80% of the variance of the data.\n\n\n\nThe coefficient of determination compares the variance of the residuals to the variance of the data. Source: https://towardsdatascience.com/introduction-to-linear-regression-in-python-c12a072bedf0\n\n\n\nSensibility to outliers\nSuppose we have a training set with one outlier (bad measurement, bad luck, etc).\n\n\n\nLinear data with one outlier.\n\n\nLMS would find the minimum of the mse, but it is clearly a bad fit for most points.\n\n\n\nLMS is attracted by the outlier, leading to a bad prediction for all points.\n\n\nThis model feels much better, but its residual mse is actually higher…\n\n\n\nBy ignoring the outlier, the prediction would be correct for most points.\n\n\nLet’s visualize polynomial regression with various orders of the polynomial on a small dataset.\n\n\n\nPolynomial regression with various orders.\n\n\nWhen only looking at the residual mse on the training data, one could think that the higher the order of the polynomial, the better. But it is obvious that the interpolation quickly becomes very bad when the order is too high. A complex model (with a lot of parameters) is useless for predicting new values. We actually do not care about the error on the training set, but about generalization.\n\n\n\nResidual mse of polynomial regression depending on the order of the polynomial.\n\n\n\n\nCross-validation\nLet’s suppose we dispose of m models \\mathcal{M} = \\{ M_1, ..., M_m\\} that could be used to fit (or classify) some data \\mathcal{D} = \\{\\mathbf{x}_i, t_i\\}_{i=1}^N. Such a class could be the ensemble of polynomes with different orders, different algorithms (NN, SVM) or the same algorithm with different values for the hyperparameters (learning rate, regularization parameters…).\nThe naive and wrong method to find the best hypothesis would be:\n\n\n\n\n\n\nDo not do this!\n\n\n\n\nFor all models M_i:\n\nTrain M_i on \\mathcal{D} to obtain an hypothesis h_i.\nCompute the training error \\epsilon_\\mathcal{D}(h_i) of h_i on \\mathcal{D} :\n\n\n      \\epsilon_\\mathcal{D}(h_i) =  \\mathbb{E}_{(\\mathbf{x}, t) \\in \\mathcal{D}} [(h_i(\\mathbf{x}) - t)^2]\n  \nSelect the hypothesis h_{i}^* with the minimal training error : h_{i}^* = \\text{argmin}_{h_i \\in \\mathcal{M}} \\quad \\epsilon_\\mathcal{D}(h_i)\n\n\n\nThis method leads to overfitting, as only the training error is used.\nThe solution is randomly take some samples out of the training set to form the test set. Typical values are 20 or 30 % of the samples in the test set.\n\nTrain the model on the training set (70% of the data).\nTest the performance of the model on the test set (30% of the data).\n\n\n\n\nPolynomial data split in a training set and a test set.\n\n\nThe test performance will better measure how well the model generalizes to new examples.\n\n\n\n\n\n\nSimple hold-out cross-validation\n\n\n\n\nSplit the training data \\mathcal{D} into \\mathcal{S}_{\\text{train}} and \\mathcal{S}_{\\text{test}}.\nFor all models M_i:\n\nTrain M_i on \\mathcal{S}_{\\text{train}} to obtain an hypothesis h_i.\nCompute the empirical error \\epsilon_{\\text{test}}(h_i) of h_i on \\mathcal{S}_{\\text{test}} :\n\n\\epsilon_{\\text{test}}(h_i) = \\mathbb{E}_{(\\mathbf{x}, t) \\in  \\mathcal{S}_{\\text{test}}} [(h_i(\\mathbf{x}) - t)^2]\nSelect the hypothesis h_{i}^* with the minimal empirical error : h_{i}^* = \\text{argmin}_{h_i \\in \\mathcal{M}} \\quad \\epsilon_{\\text{test}}(h_i)\n\n\n\nThe disadvantage of simple hold-out cross-validation is that 20 or 30% of the data is wasted and not used for learning. It may be a problem when data is rare or expensive.\nk-fold cross-validation allows a more efficient use os the available data and a better measure of the generalization error. The idea is to build several different training/test sets with the same data, train and test each model repeatedly on each partition and choose the hypothesis that works best on average.\n\n\n\nk-fold cross-validation. Source https://upload.wikimedia.org/wikipedia/commons/1/1c/K-fold_cross_validation_EN.jpg\n\n\n\n\n\n\n\n\nk-fold cross-validation\n\n\n\n\nRandomly split the data \\mathcal{D} into k subsets of \\frac{N}{k} examples \\{ \\mathcal{S}_{1}, \\dots , \\mathcal{S}_{k}\\}\nFor all models M_i:\n\nFor all k subsets \\mathcal{S}_j:\n\nTrain M_i on \\mathcal{D} - \\mathcal{S}_j to obtain an hypothesis h_{ij}\nCompute the empirical error \\epsilon_{\\mathcal{S}_j}(h_{ij}) of h_{ij} on \\mathcal{S}_j\n\nThe empirical error of the model M_i on \\mathcal{D} is the average of empirical errors made on (\\mathcal{S}_j)_{j=1}^{k}\n\n      \\epsilon_{\\mathcal{D}} (M_i) = \\frac{1}{k} \\cdot \\sum_{j=1}^{k} \\epsilon_{\\mathcal{S}_j}(h_{ij})\n  \n\nSelect the model M_{i}^* with the minimal empirical error on \\mathcal{D}.\n\n\n\nIn general, you can take k=10 partitions. The extreme case is to take k=N partition, i.e. the test set has only one sample each time: leave-one-out cross-validation. k-fold cross-validation works well, but needs a lot of repeated learning.\n\n\nUnderfitting - overfitting\nWhile the training mse always decrease with more complex models, the test mse increases after a while. This is called overfitting: learning by heart the data without caring about generalization. The two curves suggest that we should chose a polynomial order between 2 and 9.\n\n\n\nTraining and test mse of polynomial regression.\n\n\nA model not complex enough for the data will underfit: its training error is high. A model too complex for the data will overfit: its test error is high. In between, there is the right complexity for the model: it learns the data correctly but does not overfit.\n\n\n\nUnderfitting and overfitting.\n\n\nWhat does complexity mean? In polynomial regression, the complexity is related to the order of the polynomial, i.e. the number of coefficients to estimate:\ny = f_{\\mathbf{w}, b}(x) = \\sum_{k=1}^p w_k \\, x^k + b\n\\mathbf{x} = \\begin{bmatrix} x \\\\ x^2 \\\\ \\ldots \\\\ x^p \\end{bmatrix} \\qquad \\mathbf{w} = \\begin{bmatrix} w_1 \\\\ w_2 \\\\ \\ldots \\\\ w_p \\end{bmatrix}\nA polynomial of order p has p+1 unknown parameters (free parameters): the p weights and the bias. Generally, the complexity of a model relates to its number of free parameters:\n\nThe more free parameters, the more complex the model is, the more likely it will overfit.\n\n\n\n\n\n\n\nBias-variance trade-off\n\n\n\nUnder-/Over-fitting relates to the statistical concept of bias-variance trade-off. The bias is the training error that the hypothesis would make if the training set was infinite (accuracy, flexibility of the model): a model with high bias is underfitting. The variance is the error that will be made by the hypothesis on new examples taken from the same distribution (spread, the model is correct on average, but not for individual samples): a model with high variance is overfitting.\n\n\n\nBias and variance of an estimator. Source: http://scott.fortmann-roe.com/docs/BiasVariance.html\n\n\nThe bias decreases when the model becomes complex; the variance increases when the model becomes complex. The generalization error is a combination of the bias and variance:\n\n    \\text{generalization error} = \\text{bias}^2 + \\text{variance}\n\nWe search for the model with the optimum complexity realizing the trade-off between bias and variance. It is better to have a model with a slightly higher bias (training error) but with a smaller variance (generalization error).\n\n\n\nThe optimal complexity of an algorithm is a trade-off between bias and variance. Source: http://scott.fortmann-roe.com/docs/BiasVariance.html"
  },
  {
    "objectID": "notes/2.2-LinearRegression.html#regularized-regression",
    "href": "notes/2.2-LinearRegression.html#regularized-regression",
    "title": "Linear regression",
    "section": "Regularized regression",
    "text": "Regularized regression\n\nLinear regression can either underfit or overfit depending on the data.\n\n\n\nLinear regression underfits non-linear data.\n\n\n\n\n\nLinear regression overfits outliers.\n\n\nWhen linear regression underfits (both training and test errors are high), the data is not linear: we need to use a neural network. When linear regression overfits (the test error is higher than the training error), we would like to decrease its complexity.\nThe problem is that the number of free parameters in linear regression only depends on the number of inputs (dimensions of the input space).\n\n    y = \\sum_{i=1}^d w_i \\, x_i + b\n\nFor d inputs, there are d+1 free parameters: the d weights and the bias.\nWe must find a way to reduce the complexity of the linear regression without changing the number of parameters, which is impossible. The solution is to constrain the values that the parameters can take: regularization. Regularization reduces the variance at the cost of increasing the bias.\n\nL2 regularization - Ridge regression\nUsing L2 regularization for linear regression leads to the Ridge regression algorithm. The individual loss function is defined as:\n\n    \\mathcal{l}_i(\\mathbf{w}, b) = (t_i - y_i)^2 + \\lambda \\, ||\\mathbf{w}||^2\n\nThe first part of the loss function is the classical mse on the training set: its role is to reduce the bias. The second part minimizes the L2 norm of the weight vector (or matrix), reducing the variance:\n\n    ||\\mathbf{w}||^2 = \\sum_{i=1}^d w_i^2\n\nDeriving the regularized delta learning rule is straightforward:\n\n    \\Delta w_i = \\eta \\, ((t_i - y_i) \\ x_i - \\lambda \\, w_i)\n\nRidge regression is also called weight decay: even if there is no error, all weights will decay to 0.\n\n\n\nRidge regression finds the smallest value for the weights that minimize the mse. Source: https://www.mlalgorithms.org/articles/l1-l2-regression/\n\n\n\n\nL1 regularization - LASSO regression\nUsing L1 regularization for linear regression leads to the LASSO regression algorithm (least absolute shrinkage and selection operator). The individual loss function is defined as:\n\n    \\mathcal{l}_i(\\mathbf{w}, b) =  (t_i - y_i)^2 + \\lambda \\, |\\mathbf{w}|\n\nThe second part minimizes this time the L1 norm of the weight vector, i.e. its absolute value:\n\n    |\\mathbf{w}| = \\sum_{i=1}^d |w_i|\n\nRegularized delta learning rule with LASSO:\n\n    \\Delta w_i = \\eta \\, ((t_i - y_i) \\ x_i - \\lambda \\, \\text{sign}(w_i))\n\nWeight decay does not depend on the value of the weight, only its sign. Weights can decay very fast to 0.\n\n\n\nLASSO regression tries to set as many weight to 0 as possible (sparse code). Source: https://www.mlalgorithms.org/articles/l1-l2-regression/\n\n\nBoth methods depend on the regularization parameter \\lambda. Its value determines how important the regularization term should. Regularization introduce a bias, as the solution found is not the minimum of the mse, but reduces the variance of the estimation, as small weights are less sensible to noise.\nLASSO allows feature selection: features with a zero weight can be removed from the training set.\n\n\n\nLinear regression tends to assign values to all weights. Source: https://www.analyticsvidhya.com/blog/2017/06/a-comprehensive-guide-for-linear-ridge-and-lasso-regression/\n\n\n\n\n\nLASSO regression tries to set as many weights to 0 as possible (sparse code). Source: https://www.analyticsvidhya.com/blog/2017/06/a-comprehensive-guide-for-linear-ridge-and-lasso-regression/\n\n\n\n\nL1+L2 regularization - ElasticNet\nAn ElasticNet is a linear regression using both L1 and L2 regression:\n\n    \\mathcal{l}_i(\\mathbf{w}, b) =  (t_i - y_i)^2 + \\lambda_1 \\, |\\mathbf{w}| + \\lambda_2 \\, ||\\mathbf{w}||^2\n\nIt combines the advantages of Ridge and LASSO, at the cost of having now two regularization parameters to determine."
  },
  {
    "objectID": "notes/2.3-LinearClassification.html#hard-linear-classification",
    "href": "notes/2.3-LinearClassification.html#hard-linear-classification",
    "title": "Linear classification",
    "section": "Hard linear classification",
    "text": "Hard linear classification\n\nThe training data \\mathcal{D} is composed of N examples (\\mathbf{x}_i, t_i)_{i=1..N} , with a d-dimensional input vector \\mathbf{x}_i \\in \\Re^d and a binary output t_i \\in \\{-1, +1\\}. The data points where t = + 1 are called the positive class, the other the negative class.\n\n\n\nBinary linear classification of 2D data.\n\n\nFor example, the inputs \\mathbf{x}_i can be images (one dimension per pixel) and the positive class corresponds to cats (t_i = +1), the negative class to dogs (t_i = -1).\n\n\n\nBinary linear classification of cats vs. dogs images. Source: http://adilmoujahid.com/posts/2016/06/introduction-deep-learning-python-caffe\n\n\nWe want to find the hyperplane (\\mathbf{w}, b) of \\Re^d that correctly separates the two classes.\n\n\n\nThe hyperplane separates the input space into two regions.\n\n\nFor a point \\mathbf{x} \\in \\mathcal{D}, \\langle \\mathbf{w} \\cdot \\mathbf{x} \\rangle +b is the projection of \\mathbf{x} onto the hyperplane (\\mathbf{w}, b).\n\nIf \\langle \\mathbf{w} \\cdot \\mathbf{x} \\rangle +b > 0, the point is above the hyperplane.\nIf \\langle \\mathbf{w} \\cdot \\mathbf{x} \\rangle +b < 0, the point is below the hyperplane.\nIf \\langle \\mathbf{w} \\cdot \\mathbf{x} \\rangle +b = 0, the point is on the hyperplane.\n\n\n\n\nProjection on an hyperplane.\n\n\nBy looking at the sign of \\langle \\mathbf{w} \\cdot \\mathbf{x} \\rangle +b, we can predict the class of the input.\n\\text{sign}(\\langle \\mathbf{w} \\cdot \\mathbf{x} \\rangle +b) = \\begin{cases} +1 \\; \\text{if} \\; \\langle \\mathbf{w} \\cdot \\mathbf{x} \\rangle +b \\geq 0 \\\\ -1 \\; \\text{if} \\; \\langle \\mathbf{w} \\cdot \\mathbf{x} \\rangle +b < 0 \\\\ \\end{cases}\nBinary linear classification can therefore be made by a single artificial neuron using the sign transfer function.\n\ny = f_{\\mathbf{w}, b} (\\mathbf{x}) = \\text{sign} ( \\langle \\mathbf{w} \\cdot \\mathbf{x} \\rangle +b )  = \\text{sign} ( \\sum_{j=1}^d w_j \\, x_j +b )\n\n\\mathbf{w} is the weight vector and b is the bias.\nLinear classification is the process of finding an hyperplane (\\mathbf{w}, b) that correctly separates the two classes. If such an hyperplane can be found, the training set is said linearly separable. Otherwise, the problem is non-linearly separable and other methods have to be applied (MLP, SVM…).\n\n\n\nLinearly and non-linearly separable datasets.\n\n\n\nPerceptron algorithm\nThe Perceptron algorithm tries to find the weights and biases minimizing the mean square error (mse) or quadratic loss:\n\\mathcal{L}(\\mathbf{w}, b) = \\mathbb{E}_\\mathcal{D} [(t_i - y_i)^2] \\approx \\frac{1}{N} \\, \\sum_{i=1}^{N} (t_i - y_i)^2\nWhen the prediction y_i is the same as the data t_i for all examples in the training set (perfect classification), the mse is minimal and equal to 0. We can apply gradient descent to find this minimum.\n\n\\begin{cases}\n    \\Delta \\mathbf{w} = - \\eta \\, \\nabla_\\mathbf{w} \\, \\mathcal{L}(\\mathbf{w}, b)\\\\\n    \\\\\n    \\Delta b = - \\eta \\, \\nabla_b \\, \\mathcal{L}(\\mathbf{w}, b)\\\\\n\\end{cases}\n\nLet’s search for the partial derivative of the quadratic error function with respect to the weight vector:\n\n    \\nabla_\\mathbf{w} \\, \\mathcal{L}(\\mathbf{w}, b) = \\nabla_\\mathbf{w} \\,  \\frac{1}{N} \\, \\sum_{i=1}^{N} (t_i - y_i )^2 = \\frac{1}{N} \\, \\sum_{i=1}^{N} \\nabla_\\mathbf{w} \\,  (t_i - y_i )^2 = \\frac{1}{N} \\, \\sum_{i=1}^{N} \\nabla_\\mathbf{w} \\,  \\mathcal{l}_i (\\mathbf{w}, b)\n\nEverything is similar to linear regression until we get:\n\n    \\nabla_\\mathbf{w} \\,  \\mathcal{l}_i (\\mathbf{w}, b) = - 2 \\, (t_i - y_i) \\, \\nabla_\\mathbf{w} \\, \\text{sign}( \\langle \\mathbf{w} \\cdot \\mathbf{x}_i \\rangle +b)\n\nIn order to continue with the chain rule, we would need to differentiate \\text{sign}(x).\n\n    \\nabla_\\mathbf{w} \\,  \\mathcal{l}_i (\\mathbf{w}, b) = - 2 \\, (t_i - y_i) \\, \\text{sign}'( \\langle \\mathbf{w} \\cdot \\mathbf{x}_i \\rangle +b) \\,  \\mathbf{x}_i\n\nBut the sign function is not differentiable… We will simply pretend that the sign() function is linear, with a derivative of 1:\n\n    \\nabla_\\mathbf{w} \\,  \\mathcal{l}_i (\\mathbf{w}, b) = - 2 \\, (t_i - y_i) \\,   \\mathbf{x}_i\n\nThe update rule for the weight vector \\mathbf{w} and the bias b is therefore the same as in linear regression:\n\n    \\Delta \\mathbf{w} =  \\eta \\, \\frac{1}{N} \\, \\sum_{i=1}^{N} (t_i - y_i) \\, \\mathbf{x}_i\n\n\n    \\Delta b = \\eta \\, \\frac{1}{N} \\, \\sum_{i=1}^{N} (t_i - y_i )\n\nBy applying gradient descent on the quadratic error function, one obtains the following algorithm:\n\n\n\n\n\n\nBatch perceptron\n\n\n\n\nfor M epochs:\n\n\\mathbf{dw} = 0 \\qquad db = 0\nfor each sample (\\mathbf{x}_i, t_i):\n\ny_i = \\text{sign}( \\langle \\mathbf{w} \\cdot \\mathbf{x}_i \\rangle + b)\n\\mathbf{dw} = \\mathbf{dw} + (t_i - y_i) \\, \\mathbf{x}_i\ndb = db + (t_i - y_i)\n\n\\Delta \\mathbf{w} = \\eta \\, \\frac{1}{N} \\, \\mathbf{dw}\n\\Delta b = \\eta \\, \\frac{1}{N} \\, db\n\n\n\n\nThis is called the batch version of the Perceptron algorithm. If the data is linearly separable and \\eta is well chosen, it converges to the minimum of the mean square error.\n\n\n\nBatch perceptron algorithm.\n\n\nThe Perceptron algorithm was invented by the psychologist Frank Rosenblatt in 1958. It was the first algorithmic neural network able to learn linear classification.\n\n\n\n\n\n\nOnline perceptron algorithm\n\n\n\n\nfor M epochs:\n\nfor each sample (\\mathbf{x}_i, t_i):\n\ny_i = \\text{sign}( \\langle \\mathbf{w} \\cdot \\mathbf{x}_i \\rangle + b)\n\\Delta \\mathbf{w} = \\eta \\, (t_i - y_i) \\, \\mathbf{x}_i\n\\Delta b = \\eta \\, (t_i - y_i)\n\n\n\n\n\nThis algorithm iterates over all examples of the training set and applies the delta learning rule to each of them immediately, not at the end on the whole training set. One could check whether there are still classification errors on the training set at the end of each epoch and stop the algorithm. The delta learning rule depends as always on the learning rate \\eta, the error made by the prediction (t_i - y_i) and the input \\mathbf{x}_i.\n\n\n\nOnline perceptron algorithm.\n\n\n\n\nStochastic Gradient descent\nThe mean square error is defined as the expectation over the data:\n\\mathcal{L}(\\mathbf{w}, b) = \\mathbb{E}_\\mathcal{D} [(t_i - y_i)^2]\nBatch learning uses the whole training set as samples to estimate the mse:\n\\mathcal{L}(\\mathbf{w}, b) \\approx \\frac{1}{N} \\, \\sum_{i=1}^{N} (t_i - y_i)^2\n\n    \\Delta \\mathbf{w} = \\eta \\, \\frac{1}{N} \\sum_{i=1}^{N} (t_i - y_i ) \\, \\mathbf{x_i}\n\nOnline learning uses a single sample to estimate the mse:\n\\mathcal{L}(\\mathbf{w}, b) \\approx (t_i - y_i)^2\n\n    \\Delta \\mathbf{w} = \\eta \\, (t_i - y_i) \\, \\mathbf{x_i}\n\nBatch learning has less bias (central limit theorem) and is less sensible to noise in the data, but is very slow. Online learning converges faster, but can be instable and overfits (high variance).\nIn practice, we use a trade-off between batch and online learning called Stochastic Gradient Descent (SGD) or Minibatch Gradient Descent.\nThe training set is randomly split at each epoch into small chunks of data (a minibatch, usually 32 or 64 examples) and the batch learning rule is applied on each chunk.\n\n    \\Delta \\mathbf{w} = \\eta \\, \\frac{1}{K} \\sum_{i=1}^{K} (t_i - y_i) \\, \\mathbf{x_i}\n\nIf the batch size is well chosen, SGD is as stable as batch learning and as fast as online learning. The minibatches are randomly selected at each epoch (i.i.d).\n\n\n\n\n\n\nNote\n\n\n\nOnline learning is a stochastic gradient descent with a batch size of 1."
  },
  {
    "objectID": "notes/2.3-LinearClassification.html#maximum-likelihood-estimation",
    "href": "notes/2.3-LinearClassification.html#maximum-likelihood-estimation",
    "title": "Linear classification",
    "section": "Maximum Likelihood Estimation",
    "text": "Maximum Likelihood Estimation\n\nLet’s consider N samples \\{x_i\\}_{i=1}^N independently taken from a normal distribution X. The probability density function (pdf) of a normal distribution is:\n\n    f(x ; \\mu, \\sigma) =  \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\, \\exp{- \\frac{(x - \\mu)^2}{2\\sigma^2}}\n\nwhere \\mu is the mean of the distribution and \\sigma its standard deviation.\n\n\n\nNormal distributions with different parameters \\mu and \\sigma explain the data with different likelihoods.\n\n\nThe problem is to find the values of \\mu and \\sigma which explain best the observations \\{x_i\\}_{i=1}^N.\nThe idea of MLE is to maximize the joint density function for all observations. This function is expressed by the likelihood function:\n\n    L(\\mu, \\sigma) = P( x ; \\mu , \\sigma  )  = \\prod_{i=1}^{N} f(x_i ; \\mu, \\sigma )\n\nWhen the pdf takes high values for all samples, it is quite likely that the samples come from this particular distribution. The likelihood function reflects the probability that the parameters \\mu and \\sigma explain the observations \\{x_i\\}_{i=1}^N.\nWe therefore search for the values \\mu and \\sigma which maximize the likelihood function.\n\n    \\text{max}_{\\mu, \\sigma} \\quad L(\\mu, \\sigma) = \\prod_{i=1}^{N} f(x_i ; \\mu, \\sigma )\n\nFor the normal distribution, the likelihood function is:\n\n\\begin{aligned}\n    L(\\mu, \\sigma) & = \\prod_{i=1}^{N} f(x_i ; \\mu, \\sigma ) \\\\\n                   & = \\prod_{i=1}^{N} \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\, \\exp{- \\frac{(x_i - \\mu)^2}{2\\sigma^2}}\\\\\n                   & =  (\\frac{1}{\\sqrt{2\\pi \\sigma^2}})^N \\, \\prod_{i=1}^{N} \\exp{- \\frac{(x_i - \\mu)^2}{2\\sigma^2}}\\\\\n                   & =  (\\frac{1}{\\sqrt{2\\pi \\sigma^2}})^N \\, \\exp{- \\frac{\\sum_{i=1}^{N}(x_i - \\mu)^2}{2\\sigma^2}}\\\\\n\\end{aligned}\n\nTo find the maximum of L(\\mu, \\sigma), we need to search where the gradient is equal to zero:\n\n\\begin{cases}\n    \\dfrac{\\partial L(\\mu, \\sigma)}{\\partial \\mu} = 0 \\\\\n    \\dfrac{\\partial L(\\mu, \\sigma)}{\\partial \\sigma} = 0 \\\\\n\\end{cases}\n\nThe likelihood function is complex to differentiate, so we consider its logarithm l(\\mu, \\sigma) = \\log(L(\\mu, \\sigma)) which has a maximum for the same value of (\\mu, \\sigma) as the log function is monotonic.\n\n\\begin{aligned}\n    l(\\mu, \\sigma) & = \\log(L(\\mu, \\sigma)) \\\\\n                   & =  \\log \\left((\\frac{1}{\\sqrt{2\\pi \\sigma^2}})^N \\, \\exp{- \\frac{\\sum_{i=1}^{N}(x_i - \\mu)^2}{2\\sigma^2}} \\right)\\\\\n                   & =  - \\frac{N}{2} \\log (2\\pi \\sigma^2) - \\frac{\\sum_{i=1}^{N}(x_i - \\mu)^2}{2\\sigma^2}\\\\\n\\end{aligned}\n\nl(\\mu, \\sigma) is called the log-likelihood function. The maximum of the log-likelihood function respects:\n\n\\begin{aligned}\n    \\frac{\\partial l(\\mu, \\sigma)}{\\partial \\mu} & = \\frac{\\sum_{i=1}^{N}(x_i - \\mu)}{\\sigma^2} = 0 \\\\\n    \\frac{\\partial l(\\mu, \\sigma)}{\\partial \\sigma} & = - \\frac{N}{2} \\frac{4 \\pi \\sigma}{2 \\pi \\sigma^2} + \\frac{\\sum_{i=1}^{N}(x_i - \\mu)^2}{\\sigma^3} \\\\\n                                                    & = - \\frac{N}{\\sigma} + \\frac{\\sum_{i=1}^{N}(x_i - \\mu)^2}{\\sigma^3} = 0\\\\\n\\end{aligned}\n\nWe obtain:\n\n    \\mu = \\frac{1}{N} \\sum_{i=1}^{N} x_i  \\qquad\\qquad    \\sigma^2 = \\frac{1}{N} \\sum_{i=1}^{N}(x_i - \\mu)^2\n\nUnsurprisingly, the mean and variance of the normal distribution which best explains the data are the mean and variance of the data…\nThe same principle can be applied to estimate the parameters of any distribution: normal, exponential, Bernouilli, Poisson, etc… When a machine learning method has an probabilistic interpretation (i.e. it outputs probabilities), MLE can be used to find its parameters. One can use global optimization like here, or gradient descent to estimate the parameters iteratively."
  },
  {
    "objectID": "notes/2.3-LinearClassification.html#soft-linear-classification-logistic-regression",
    "href": "notes/2.3-LinearClassification.html#soft-linear-classification-logistic-regression",
    "title": "Linear classification",
    "section": "Soft linear classification : Logistic regression",
    "text": "Soft linear classification : Logistic regression\n\nIn logistic regression, we want to perform a regression, but where the targets t_i are bounded betwen 0 and 1. We can use a logistic function instead of a linear function in order to transform the net activation into an output:\n\n\\begin{aligned}\n    y = \\sigma(w \\, x + b )  = \\frac{1}{1+\\exp(-w \\, x - b )}\n\\end{aligned}\n\nLogistic regression can be used in binary classification if we consider y = \\sigma(w \\, x + b ) as the probability that the example belongs to the positive class (t=1).\n\n    P(t = 1 | x; w, b) = y ; \\qquad P(t = 0 | x; w, b) = 1 - y\n\nThe output t therefore comes from a Bernouilli distribution \\mathcal{B} of parameter p = y = f_{w, b}(x). The probability mass function (pmf) is:\nf(t | x; w, b) = y^t \\, (1- y)^{1-t}\nIf we consider our training samples (x_i, t_i) as independently taken from this distribution, our task is to find the parameterized distribution that best explains the data, which means to find the parameters w and b maximizing the likelihood that the samples t come from a Bernouilli distribution when x, w and b are given. We only need to apply Maximum Likelihood Estimation (MLE) on this Bernouilli distribution!\nThe likelihood function for logistic regression is :\n\n\\begin{aligned}\n    L( w, b) &= P( t | x; w,  b )  = \\prod_{i=1}^{N} f(t_i | x_i;  w,  b ) \\\\\n    &= \\prod_{i=1}^{N}  y_i^{t_i} \\, (1- y_i)^{1-t_i}\n\\end{aligned}\n\nThe likelihood function is quite hard to differentiate, so we take the log-likelihood function:\n\n\\begin{aligned}\n    l( w, b) &= \\log L( w, b) \\\\\n    &=  \\sum_{i=1}^{N} [t_i \\, \\log y_i + (1 - t_i) \\, \\log( 1- y_i)]\\\\\n\\end{aligned}\n\nor even better: the negative log-likelihood which will be minimized using gradient descent:\n\n    \\mathcal{L}( w, b) =  - \\sum_{i=1}^{N} [t_i \\, \\log y_i + (1 - t_i) \\, \\log( 1- y_i)]\n\nWe then search for the minimum of the negative log-likelihood function by computing its gradient (here for a single sample):\n\n\\begin{aligned}\n    \\frac{\\partial \\mathcal{l}_i(w, b)}{\\partial w}\n        &= -\\frac{\\partial}{\\partial w} [ t_i \\, \\log y_i + (1 - t_i) \\, \\log( 1- y_i) ] \\\\\n        &= - t_i \\, \\frac{\\partial}{\\partial w} \\log y_i - (1 - t_i) \\, \\frac{\\partial}{\\partial w}\\log( 1- y_i) \\\\\n        &= - t_i \\, \\frac{\\frac{\\partial}{\\partial w} y_i}{y_i} - (1 - t_i) \\, \\frac{\\frac{\\partial}{\\partial w}( 1- y_i)}{1- y_i} \\\\\n        &= - t_i \\, \\frac{y_i \\, (1 - y_i) \\, x_i}{y_i} + (1 - t_i) \\, \\frac{y_i \\, (1-y_i) \\, x_i}{1 - y_i}\\\\\n        &= - ( t_i - y_i ) \\, x_i\\\\\n\\end{aligned}\n\nWe obtain the same gradient as the linear perceptron, but with a non-linear output function! Logistic regression is therefore a regression method used for classification. It uses a non-linear transfer function \\sigma(x)=\\frac{1}{1+\\exp(-x)} applied on the net activation:\n\n    y_i = \\sigma(\\langle \\mathbf{w} \\cdot \\mathbf{x}_i \\rangle + b )\n\nThe continuous output y is interpreted as the probability of belonging to the positive class.\n\n   P(t_i = 1 | \\mathbf{x}_i; \\mathbf{w}, b) = y_i ; \\qquad P(t_i = 0 | \\mathbf{x}_i; \\mathbf{w}, b) = 1 - y_i\n\nWe minimize the negative log-likelihood loss function:\n\n    \\mathcal{L}(\\mathbf{w}, b) =  - \\sum_{i=1}^{N} [t_i \\, \\log y_i + (1 - t_i) \\, \\log( 1- y_i)]\n\nGradient descent leads to the delta learning rule, using the class as a target and the probability as a prediction:\n\n    \\begin{cases}\n    \\Delta \\mathbf{w} = \\eta \\, ( t_i - y_i ) \\, \\mathbf{x}_i \\\\\n    \\\\\n    \\Delta b = \\eta \\, ( t_i - y_i ) \\\\\n    \\end{cases}\n\n\n\n\n\n\n\nLogistic regression\n\n\n\n\n\\mathbf{w} = 0 \\qquad b = 0\nfor M epochs:\n\nfor each sample (\\mathbf{x}_i, t_i):\n\ny_i = \\sigma( \\langle \\mathbf{w} \\cdot \\mathbf{x}_i \\rangle + b)\n\\Delta \\mathbf{w} = \\eta \\, (t_i - y_i) \\, \\mathbf{x}_i\n\\Delta b = \\eta \\, (t_i - y_i)\n\n\n\n\n\nLogistic regression works just like linear classification, except in the way the prediction is done. To know to which class \\mathbf{x}_i belongs, simply draw a random number between 0 and 1:\n\nif it is smaller than y_i (probability y_i), it belongs to the positive class.\nif it is bigger than y_i (probability 1-y_i), it belongs to the negative class.\n\nAlternatively, you can put a hard limit at 0.5:\n\nif y_i > 0.5 then the class is positive.\nif y_i < 0.5 then the class is negative.\n\n\n\n\nLogistic regression for soft classification. The confidence scores tells how certain the classification is.\n\n\nLogistic regression also provides a confidence score: the closer y is from 0 or 1, the more confident we can be that the classification is correct. This is particularly important in safety critical applications: if you detect the positive class but with a confidence of 0.51, you should perhaps not trust the prediction. If the confidence score is 0.99, you can probably trust the prediction."
  },
  {
    "objectID": "notes/2.3-LinearClassification.html#multi-class-classification",
    "href": "notes/2.3-LinearClassification.html#multi-class-classification",
    "title": "Linear classification",
    "section": "Multi-class classification",
    "text": "Multi-class classification\n\nCan we perform multi-class classification using the previous methods when t \\in \\{A, B, C\\} instead of t = +1 or -1? There are two main solutions:\n\nOne-vs-All (or One-vs-the-rest): one trains simultaneously a binary (linear) classifier for each class. The examples belonging to this class form the positive class, all others are the negative class:\n\nA vs. B and C\nB vs. A and C\nC vs. A and B\n\n\nIf multiple classes are predicted for a single example, ones needs a confidence level for each classifier saying how sure it is of its prediction.\n\nOne-vs-One: one trains a classifier for each pair of class:\n\nA vs. B\nB vs. C\nC vs. A\n\n\nA majority vote is then performed to find the correct class.\n\n\n\nExample of One-vs-All classification: one binary classifier per class. Source http://cs231n.github.io/linear-classify\n\n\n\nSoftmax linear classifier\nSuppose we have C classes (dog vs. cat vs. ship vs…). The One-vs-All scheme involves C binary classifiers (\\mathbf{w}_i, b_i), each with a weight vector and a bias, working on the same input \\mathbf{x}.\ny_i = f(\\langle \\mathbf{w}_i \\cdot \\mathbf{x} \\rangle + b_i)\nPutting all neurons together, we obtain a linear perceptron similar to multiple linear regression:\n\n    \\mathbf{y} = f(W \\times \\mathbf{x} + \\mathbf{b})\n\nThe C weight vectors form a C \\times d weight matrix W, the biases form a vector \\mathbf{b}.\n\n\n\nLinear perceptron for images. The output is the logit score. Source http://cs231n.github.io/linear-classify\n\n\nThe net activations form a vector \\mathbf{z}:\n\n    \\mathbf{z} = f_{W, \\mathbf{b}}(\\mathbf{x}) = W \\times \\mathbf{x} + \\mathbf{b}\n\nEach element z_j of the vector \\mathbf{z} is called the logit score of the class: the higher the score, the more likely the input belongs to this class. The logit scores are not probabilities, as they can be negative and do not sum to 1.\n\nOne-hot encoding\nHow do we represent the ground truth \\mathbf{t} for each neuron? The target vector \\mathbf{t} is represented using one-hot encoding. The binary vector has one element per class: only one element is 1, the others are 0. Example:\n\n    \\mathbf{t} = [\\text{cat}, \\text{dog}, \\text{ship}, \\text{house}, \\text{car}] = [0, 1, 0, 0, 0]\n\nThe labels can be seen as a probability distribution over the training set, in this case a multinomial distribution (a dice with C sides). For a given image \\mathbf{x} (e.g. a picture of a dog), the conditional pmf is defined by the one-hot encoded vector \\mathbf{t}:\nP(\\mathbf{t} | \\mathbf{x}) = [P(\\text{cat}| \\mathbf{x}), P(\\text{dog}| \\mathbf{x}), P(\\text{ship}| \\mathbf{x}), P(\\text{house}| \\mathbf{x}), P(\\text{car}| \\mathbf{x})] = [0, 1, 0, 0, 0]\n\n\n\nThe logit scores \\mathbf{z} cannot be compared to the targets \\mathbf{t}: we need to transform them into a probability distribution \\mathbf{y}.\n\n\nWe need to transform the logit score \\mathbf{z} into a probability distribution P(\\mathbf{y} | \\mathbf{x}) that should be as close as possible from P(\\mathbf{t} | \\mathbf{x}).\n\n\nSoftmax activation\nThe softmax operator makes sure that the sum of the outputs \\mathbf{y} = \\{y_i\\} over all classes is 1.\n\n    y_j = P(\\text{class = j} | \\mathbf{x}) = \\mathcal{S}(z_j) = \\frac{\\exp(z_j)}{\\sum_k \\exp(z_k)}\n\n\n\n\nSoftmax activation transforms the logit score into a probability distribution. Source http://cs231n.github.io/linear-classify\n\n\nThe higher z_j, the higher the probability that the example belongs to class j. This is very similar to logistic regression for soft classification, except that we have multiple classes.\n\n\nCross-entropy loss function\nWe cannot use the mse as a loss function, as the softmax function would be hard to differentiate:\n\n    \\text{mse}(W, \\mathbf{b}) = \\sum_j (t_{j} - \\frac{\\exp(z_j)}{\\sum_k \\exp(z_k)})^2\n\nWe actually want to minimize the statistical distance netween two distributions:\n\nThe model outputs a multinomial probability distribution \\mathbf{y} for an input \\mathbf{x}: P(\\mathbf{y} | \\mathbf{x}; W, \\mathbf{b}).\nThe one-hot encoded classes also come from a multinomial probability distribution P(\\mathbf{t} | \\mathbf{x}).\n\nWe search which parameters (W, \\mathbf{b}) make the two distributions P(\\mathbf{y} | \\mathbf{x}; W, \\mathbf{b}) and P(\\mathbf{t} | \\mathbf{x}) close. The training data \\{\\mathbf{x}_i, \\mathbf{t}_i\\} represents samples from P(\\mathbf{t} | \\mathbf{x}). P(\\mathbf{y} | \\mathbf{x}; W, \\mathbf{b}) is a good model of the data when the two distributions are close, i.e. when the negative log-likelihood of each sample under the model is small.\n\n\n\nCross-entropy between two distributions X and Y: are samples of X likely under Y?\n\n\nFor an input \\mathbf{x}, we minimize the cross-entropy between the target distribution and the predicted outputs:\n\n    \\mathcal{l}(W, \\mathbf{b}) = \\mathcal{H}(\\mathbf{t} | \\mathbf{x}, \\mathbf{y} | \\mathbf{x}) =  \\mathbb{E}_{t \\sim P(\\mathbf{t} | \\mathbf{x})} [ - \\log P(\\mathbf{y} = t | \\mathbf{x})]\n\nThe cross-entropy samples from \\mathbf{t} | \\mathbf{x}:\n\n    \\mathcal{l}(W, \\mathbf{b}) = \\mathcal{H}(\\mathbf{t} | \\mathbf{x}, \\mathbf{y} | \\mathbf{x}) =  \\mathbb{E}_{t \\sim P(\\mathbf{t} | \\mathbf{x})} [ - \\log P(\\mathbf{y} = t | \\mathbf{x})]\n\nFor a given input \\mathbf{x}, \\mathbf{t} is non-zero only for the correct class t^*, as \\mathbf{t} is a one-hot encoded vector [0, 1, 0, 0, 0]:\n\n    \\mathcal{l}(W, \\mathbf{b}) =  - \\log P(\\mathbf{y} = t^* | \\mathbf{x})\n\nIf we note j^* the index of the correct class t^*, the cross entropy is simply:\n\n    \\mathcal{l}(W, \\mathbf{b}) =  - \\log y_{j^*}\n\nAs only one element of \\mathbf{t} is non-zero, the cross-entropy is the same as the negative log-likelihood of the prediction for the true label:\n\n    \\mathcal{l}(W, \\mathbf{b}) =  - \\log y_{j^*}\n\nThe minimum of - \\log y is obtained when y =1: We want to classifier to output a probability 1 for the true label. Because of the softmax activation function, the probability for the other classes should become closer from 0.\n\n    y_j = P(\\text{class = j}) = \\frac{\\exp(z_j)}{\\sum_k \\exp(z_k)}\n\nMinimizing the cross-entropy / negative log-likelihood pushes the output distribution \\mathbf{y} | \\mathbf{x} to be as close as possible to the target distribution \\mathbf{t} | \\mathbf{x}.\n\n\n\nMinimizing the cross-entropy between \\mathbf{t} | \\mathbf{x} and \\mathbf{y} | \\mathbf{x} makes them similar.\n\n\nAs \\mathbf{t} is a binary vector [0, 1, 0, 0, 0], the cross-entropy / negative log-likelihood can also be noted as the dot product between \\mathbf{t} and \\log \\mathbf{y}:\n\n    \\mathcal{l}(W, \\mathbf{b}) = - \\langle \\mathbf{t} \\cdot \\log \\mathbf{y} \\rangle = - \\sum_{j=1}^C t_j \\, \\log y_j =  - \\log y_{j^*}\n\nThe cross-entropy loss function is then the expectation over the training set of the individual cross-entropies:\n\n    \\mathcal{L}(W, \\mathbf{b}) = \\mathbb{E}_{\\mathbf{x}, \\mathbf{t} \\sim \\mathcal{D}} [- \\langle \\mathbf{t} \\cdot \\log \\mathbf{y} \\rangle ] \\approx \\frac{1}{N} \\sum_{i=1}^N - \\langle \\mathbf{t}_i \\cdot \\log \\mathbf{y}_i \\rangle\n\nThe nice thing with the cross-entropy loss function, when used on a softmax activation function, is that the partial derivative w.r.t the logit score \\mathbf{z} is simple:\n\n\\begin{split}\n    \\frac{\\partial {l}(W, \\mathbf{b})}{\\partial z_i} & = - \\sum_j \\frac{\\partial}{\\partial z_i}  t_j \\log(y_j)=\n- \\sum_j t_j \\frac{\\partial \\log(y_j)}{\\partial z_i} = - \\sum_j t_j \\frac{1}{y_j} \\frac{\\partial y_j}{\\partial z_i} \\\\\n& = - \\frac{t_i}{y_i} \\frac{\\partial y_i}{\\partial z_i} - \\sum_{j \\neq i}^C \\frac{t_j}{y_j} \\frac{\\partial y_j}{\\partial z_i}\n= - \\frac{t_i}{y_i} y_i (1-y_i) - \\sum_{j \\neq i}^C \\frac{t_j}{y_i} (-y_j \\, y_i) \\\\\n& = - t_i + t_i \\, y_i + \\sum_{j \\neq i}^C t_j \\, y_i = - t_i + \\sum_{j = 1}^C t_j y_i\n= -t_i + y_i \\sum_{j = 1}^C t_j \\\\\n& = - (t_i - y_i)\n\\end{split}\n\ni.e. the same as with the mse in linear regression! Refer https://peterroelants.github.io/posts/cross-entropy-softmax/ for more explanations on the proof.\n\n\n\n\n\n\nNote\n\n\n\nWhen differentiating a softmax probability y_j = \\dfrac{\\exp(z_j)}{\\sum_k \\exp(z_k)} w.r.t a logit score z_i, i.e. \\dfrac{\\partial y_j}{\\partial z_i}, we need to consider two cases:\n\nIf i=j, \\exp(z_i) appears both at the numerator and denominator of \\frac{\\exp(z_i)}{\\sum_k \\exp(z_k)}. The product rule (f\\times g)' = f'\\, g + f \\, g' gives us:\n\n\\begin{aligned}\n\\dfrac{\\partial \\log(y_i)}{\\partial z_i} &= \\dfrac{\\exp(z_i)}{\\sum_k \\exp(z_k)} + \\exp(z_i) \\, \\dfrac{- \\exp(z_i)}{(\\sum_k \\exp(z_k))^2} \\\\\n&= \\dfrac{\\exp(z_i)  \\, \\sum_k \\exp(z_k) - \\exp(z_i)^2}{(\\sum_k \\exp(z_k))^2} \\\\\n&= \\dfrac{\\exp(z_i)}{\\sum_k \\exp(z_k)} \\, (1- \\dfrac{\\exp(z_i)}{\\sum_k \\exp(z_k)})\\\\\n&= y_i \\, (1 - y_i)\\\\\n\\end{aligned}\n\nThis is similar to the derivative of the logistic function.\n\nIf i \\neq j, z_i only appears at the denominator, so we only need the chain rule:\n\n\\begin{aligned}\n\\dfrac{\\partial \\log(y_j)}{\\partial z_i} &= - \\exp(z_j) \\, \\dfrac{\\exp(z_i)}{(\\sum_k \\exp(z_k))^2} \\\\\n&= - \\dfrac{\\exp(z_i)}{\\sum_k \\exp(z_k)} \\, \\dfrac{\\exp(z_j)}{\\sum_k \\exp(z_k)} \\\\\n&= - y_i \\, y_j \\\\\n\\end{aligned}\n\n\n\nUsing the vector notation, we get:\n\n    \\frac{\\partial \\mathcal{l}(W, \\mathbf{b})}{\\partial \\mathbf{z}} =  -  (\\mathbf{t} - \\mathbf{y} )\n\nAs:\n\n    \\mathbf{z} = W \\times \\mathbf{x} + \\mathbf{b}\n\nwe can obtain the partial derivatives:\n\n\\begin{cases}\n    \\dfrac{\\partial \\mathcal{l}(W, \\mathbf{b})}{\\partial W} = \\dfrac{\\partial \\mathcal{l}(W, \\mathbf{b})}{\\partial \\mathbf{z}} \\times \\dfrac{\\partial \\mathbf{z}}{\\partial W} = - (\\mathbf{t} - \\mathbf{y} ) \\times \\mathbf{x}^T \\\\\n    \\\\\n    \\dfrac{\\partial \\mathcal{l}(W, \\mathbf{b})}{\\partial \\mathbf{b}} = \\dfrac{\\partial \\mathcal{l}(W, \\mathbf{b})}{\\partial \\mathbf{z}} \\times \\dfrac{\\partial \\mathbf{z}}{\\partial \\mathbf{b}} = - (\\mathbf{t} - \\mathbf{y} ) \\\\\n\\end{cases}\n\nSo gradient descent leads to the delta learning rule:\n\n\\begin{cases}\n    \\Delta W = \\eta \\,  (\\mathbf{t} - \\mathbf{y} ) \\times \\mathbf{x}^T \\\\\n    \\\\\n    \\Delta \\mathbf{b} = \\eta \\,  (\\mathbf{t} - \\mathbf{y} ) \\\\\n\\end{cases}\n\n\n\n\n\n\n\nSoftmax linear classifier\n\n\n\n\n\n\n\n\n\nWe first compute the logit scores \\mathbf{z} using a linear layer:\n\n\n    \\mathbf{z} = W \\times \\mathbf{x} + \\mathbf{b}\n\n\nWe turn them into probabilities \\mathbf{y} using the softmax activation function:\n\n\n    y_j = \\frac{\\exp(z_j)}{\\sum_k \\exp(z_k)}\n\n\nWe minimize the cross-entropy / negative log-likelihood on the training set:\n\n\n    \\mathcal{L}(W, \\mathbf{b}) = \\mathbb{E}_{\\mathbf{x}, \\mathbf{t} \\sim \\mathcal{D}} [ - \\langle \\mathbf{t} \\cdot \\log \\mathbf{y} \\rangle]\n\nwhich simplifies into the delta learning rule:\n\n\\begin{cases}\n    \\Delta W = \\eta \\,  (\\mathbf{t} - \\mathbf{y} ) \\times \\mathbf{x}^T \\\\\n    \\\\\n    \\Delta \\mathbf{b} = \\eta \\,  (\\mathbf{t} - \\mathbf{y} ) \\\\\n\\end{cases}\n\n\n\n\n\n\nComparison of linear classification and regression\nClassification and regression differ in the nature of their outputs: in classification they are discrete, in regression they are continuous values. However, when trying to minimize the mismatch between a model \\mathbf{y} and the real data \\mathbf{t}, we have found the same delta learning rule:\n\n\\begin{cases}\n    \\Delta W = \\eta \\,  (\\mathbf{t} - \\mathbf{y} ) \\times \\mathbf{x}^T \\\\\n    \\\\\n    \\Delta \\mathbf{b} = \\eta \\,  (\\mathbf{t} - \\mathbf{y} ) \\\\\n\\end{cases}\n\nRegression and classification are in the end the same problem for us. The only things that needs to be adapted is the activation function of the output and the loss function:\n\nFor regression, we use regular activation functions and the mean square error (mse):\n\n  \\mathcal{L}(W, \\mathbf{b}) = \\mathbb{E}_{\\mathbf{x}, \\mathbf{t} \\in \\mathcal{D}} [ ||\\mathbf{t} - \\mathbf{y}||^2 ]\n  \nFor classification, we use the softmax activation function and the cross-entropy (negative log-likelihood) loss function:\n\\mathcal{L}(W, \\mathbf{b}) = \\mathbb{E}_{\\mathbf{x}, \\mathbf{t} \\sim \\mathcal{D}} [ - \\langle \\mathbf{t} \\cdot \\log \\mathbf{y} \\rangle]\n\n\n\nMulti-label classification\nWhat if there is more than one label on the image? The target vector \\mathbf{t} does not represent a probability distribution anymore:\n\n    \\mathbf{t} = [\\text{cat}, \\text{dog}, \\text{ship}, \\text{house}, \\text{car}] = [1, 1, 0, 0, 0]\n\nNormalizing the vector does not help: it is not a dog or a cat, it is a dog and a cat.\n\n    \\mathbf{t} = [\\text{cat}, \\text{dog}, \\text{ship}, \\text{house}, \\text{car}] = [0.5, 0.5, 0, 0, 0]\n\nFor multi-label classification, we can simply use the logistic activation function for the output neurons:\n\n    \\mathbf{y} = \\sigma(W \\times \\mathbf{x} + \\mathbf{b})\n\nThe outputs are between 0 and 1, but they do not sum to one. Each output neuron performs a logistic regression for soft classification on their class:\ny_j = P(\\text{class} = j | \\mathbf{x})\nEach output neuron y_j has a binary target t_j (one-vs-the-rest) and has to minimize the negative log-likelihood:\n\n\\mathcal{l}_j(W, \\mathbf{b}) =  - t_j \\, \\log y_j + (1 - t_j) \\, \\log( 1- y_j)\n\nThe binary cross-entropy loss for the whole network is the sum of the negative log-likelihood for each class:\n\n\\mathcal{L}(W, \\mathbf{b}) =  \\mathbb{E}_{\\mathcal{D}} [- \\sum_{j=1}^C t_j \\, \\log y_j + (1 - t_j) \\, \\log( 1- y_j)]"
  },
  {
    "objectID": "notes/2.4-LearningTheory.html#error-measurements",
    "href": "notes/2.4-LearningTheory.html#error-measurements",
    "title": "Learning theory",
    "section": "Error measurements",
    "text": "Error measurements\n\nThe training error is the error made on the training set. It is easy to measure for classification as the number of misclassified examples divided by the total number of examples.\n\\epsilon_\\mathcal{D} = \\dfrac{\\text{Number of misclassifications}}{\\text{Total number of examples}}\nThe training error is totally irrelevant on usage: reading the training set has a training error of 0%. What matters is the generalization error, which is the error that will be made on new examples (not used during learning). It is much harder to measure (potentially infinite number of new examples, what is the correct answer?). The generalization error is often approximated by the empirical error: one keeps a number of training examples out of the learning phase and one tests the performance on them.\nClassification errors can also depend on the class:\n\nFalse Positive errors (FP, false alarm, type I) is when the classifier predicts a positive class for a negative example.\nFalse Negative errors (FN, miss, type II) is when the classifier predicts a negative class for a positive example.\nTrue Positive (TP) and True Negative (TN) are correctly classified examples.\n\nIs it better to fail to detect a cancer (FN) or to incorrectly predict one (FP)?\nSome other metrics:\n\nAccuracy (1 - error)\n\n\n    \\text{acc} = \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{FP} + \\text{TN} + \\text{FN}}\n\n\nRecall (hit rate, sensitivity)\n\n\n    R = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\n\n\nPrecision (specificity)\n\n\n    P = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}\n\n\nF1 score = harmonic mean of precision and recall\n\n\n    \\text{F1} = \\frac{2\\, P \\, R}{P + R}\n\nFor multiclass classification problems, the confusion matrix tells how many examples are correctly classified and where confusion happens. One axis is the predicted class, the other is the target class. Each element of the matrix tells how many examples are classified or misclassified. The matrix should be as diagonal as possible.\n\n\n\nConfusion matrix.\n\n\n\n\n\n\n\n\nNote\n\n\n\nUsing scikit-learn:\nfrom sklearn.metrics import confusion_matrix\n\nm = confusion_matrix(t, y)"
  },
  {
    "objectID": "notes/2.4-LearningTheory.html#cross-validation",
    "href": "notes/2.4-LearningTheory.html#cross-validation",
    "title": "Learning theory",
    "section": "Cross-validation",
    "text": "Cross-validation\n\n\n\nOverfitting in regression.\n\n\n\n\n\nOverfitting in classification.\n\n\nIn classification too, cross-validation has to be used to prevent overfitting. The classifier is trained on the training set and tested on the test set. Optionally, a third validation set can be used to track overfitting during training.\n\n\n\nTraining, validation and test sets. Source: https://developers.google.com/machine-learning/crash-course/validation/another-partition\n\n\n\n\n\n\n\n\nNote\n\n\n\nBeware: the test data must come from the same distribution as the training data, otherwise it makes no sense."
  },
  {
    "objectID": "notes/2.4-LearningTheory.html#vapnik-chervonenkis-dimension",
    "href": "notes/2.4-LearningTheory.html#vapnik-chervonenkis-dimension",
    "title": "Learning theory",
    "section": "Vapnik-Chervonenkis dimension",
    "text": "Vapnik-Chervonenkis dimension\n\nHow many data examples can be correctly classified by a linear model in \\Re^d? In \\Re^2, all dichotomies of three non-aligned examples can be correctly classified by a linear model (y = w_1 \\, x_1 + w_2 \\, x_2 + b).\n\n\n\nA linear classifier in 2D can classify any configuration of three points.\n\n\nHowever, there exists sets of four examples in \\Re^2 which can NOT be correctly classified by a linear model, i.e. they are not linearly separable.\n\n\n\nThere exists configurations of four points in 2D that cannot be linearly classified.\n\n\nThe XOR function in \\Re^2 is for example not linearly separable, i.e. the Perceptron algorithm can not converge.\nThe probability that a set of 3 (non-aligned) points in \\Re^2 is linearly separable is 1, but the probability that a set of four points is linearly separable is smaller than 1 (but not zero). When a class of hypotheses \\mathcal{H} can correctly classify all points of a training set \\mathcal{D}, we say that \\mathcal{H} shatters \\mathcal{D}.\nThe Vapnik-Chervonenkis dimension \\text{VC}_\\text{dim} (\\mathcal{H}) of an hypothesis class \\mathcal{H} is defined as the maximal number of training examples that \\mathcal{H} can shatter. We saw that in \\Re^2, this dimension is 3:\n\\text{VC}_\\text{dim} (\\text{Linear}(\\Re^2) ) = 3\nThis can be generalized to linear classifiers in \\Re^d:\n\\text{VC}_\\text{dim} (\\text{Linear}(\\Re^d) ) = d+1\nThis corresponds to the number of free parameters of the linear classifier: d parameters for the weight vector, 1 for the bias. Given any set of (d+1) examples in \\Re^d, there exists a linear classifier able to classify them perfectly. For other types of (non-linear) hypotheses, the VC dimension is generally proportional to the number of free parameters, but regularization reduces the VC dimension of the classifier.\n\n\n\n\n\n\nVapnik-Chervonenkis theorem\n\n\n\nThe generalization error \\epsilon(h) of an hypothesis h taken from a class \\mathcal{H} of finite VC dimension and trained on N samples of \\mathcal{S} is bounded by the sum of the training error \\hat{\\epsilon}_{\\mathcal{S}}(h) and the VC complexity term:\n\n    \\epsilon(h) \\leq \\hat{\\epsilon}_{\\mathcal{S}}(h) + \\sqrt{\\frac{\\text{VC}_\\text{dim} (\\mathcal{H}) \\cdot (1 + \\log(\\frac{2\\cdot N}{\\text{VC}_\\text{dim} (\\mathcal{H})})) - \\log(\\frac{\\delta}{4})}{N}}\n\nwith probability 1-\\delta, if \\text{VC}_\\text{dim} (\\mathcal{H}) << N.\nVapnik, Vladimir (2000). The nature of statistical learning theory. Springer."
  },
  {
    "objectID": "notes/2.4-LearningTheory.html#structural-risk-minimization",
    "href": "notes/2.4-LearningTheory.html#structural-risk-minimization",
    "title": "Learning theory",
    "section": "Structural risk minimization",
    "text": "Structural risk minimization\n\n\n\nStructural risk minimization.\n\n\nThe generalization error increases with the VC dimension, while the training error decreases. Structural risk minimization is an alternative method to cross-validation. The VC dimensions of various classes of hypothesis are already known (~ number of free parameters). The VC bounds tells how many training samples are needed by a given hypothesis class in order to obtain a satisfying generalization error.\n\\epsilon(h) \\leq \\hat{\\epsilon}_{\\mathcal{S}(h)} + \\sqrt{\\frac{\\text{VC}_\\text{dim} (\\mathcal{H}) \\cdot (1 + \\log(\\frac{2\\cdot N}{\\text{VC}_\\text{dim} (\\mathcal{H})})) - \\log(\\frac{\\delta}{4})}{N}}\nThe more complex the model, the more training data you will need to get a good generalization error!\nRule of thumb:\n\n        \\epsilon(h) \\approx \\frac{\\text{VC}_\\text{dim} (\\mathcal{H})}{N}\n\nA learning algorithm should only try to minimize the training error, as the VC complexity term only depends on the model. This term is only an upper bound: most of the time, the real bound is usually 100 times smaller.\nThe VC dimension of linear classifiers in \\Re^d is:\n\\text{VC}_\\text{dim} (\\text{Linear}(\\Re^d) ) = d+1\nGiven any set of (d+1) examples in \\Re^d, there exists a linear classifier able to classify them perfectly. For N >> d the probability of having training errors becomes huge (the data is generally not linearly separable).\n\nIf we project the input data onto a space with sufficiently high dimensions, it becomes then possible to find a linear classifier on this projection space that is able to classify the data!\n\nHowever, if the space has too many dimensions, the VC dimension will increase and the generalization error will increase. This is the basic principle of all non-linear ML methods: multi-layer perceptron, radial-basis-function networks, support-vector machines…"
  },
  {
    "objectID": "notes/2.4-LearningTheory.html#feature-space",
    "href": "notes/2.4-LearningTheory.html#feature-space",
    "title": "Learning theory",
    "section": "Feature space",
    "text": "Feature space\n\n\n\n\n\n\n\nCover’s theorem on the separability of patterns (1965)\n\n\n\nA complex pattern-classification problem, cast in a high dimensional space non-linearly, is more likely to be linearly separable than in a low-dimensional space, provided that the space is not densely populated.\n\n\n\n\n\nProjection to a feature space.\n\n\nThe highly dimensional space where the input data is projected is called the feature space When the number of dimensions of the feature space increases, the training error decreases (the pattern is more likely linearly separable) but the generalization error increases (the VC dimension increases).\n\nPolynomial features\nFor the polynomial regression of order p:\ny = f_{\\mathbf{w}, b}(x) = w_1 \\, x + w_2 \\, x^2 + \\ldots + w_p \\, x^p + b\nthe vector \\mathbf{x} = \\begin{bmatrix} x \\\\ x^2 \\\\ \\ldots \\\\ x^p \\end{bmatrix} defines a feature space for the input x. The elements of the feature space are called polynomial features. We can define polynomial features of more than one variable, e.g. x^2 \\, y, x^3 \\, y^4, etc. We then apply multiple linear regression (MLR) on the polynomial feature space to find the parameters:\n\\Delta \\mathbf{w} =  \\eta \\, (t - y) \\, \\mathbf{x}\n\n\nRadial-basis function networks\n\n\n\nRadial basis function network. Source: https://mccormickml.com/2013/08/15/radial-basis-function-network-rbfn-tutorial/\n\n\nRadial-basis function (RBF) networks samples a subset of K training examples and form the feature space using a gaussian kernel:\n\\phi(\\mathbf{x}) = \\begin{bmatrix} \\varphi(\\mathbf{x} - \\mathbf{x}_1) \\\\ \\varphi(\\mathbf{x} - \\mathbf{x}_2) \\\\ \\ldots \\\\ \\varphi(\\mathbf{x} - \\mathbf{x}_K) \\end{bmatrix}\nwith \\varphi(\\mathbf{x} - \\mathbf{x}_i) = \\exp - \\beta \\, ||\\mathbf{x} - \\mathbf{x}_i||^2 decreasing with the distance between the vectors.\n\n\n\nSelection of protypes among the training data. Source: https://mccormickml.com/2013/08/15/radial-basis-function-network-rbfn-tutorial/\n\n\n\n\n\nGaussian kernel. Source: https://mccormickml.com/2013/08/15/radial-basis-function-network-rbfn-tutorial/\n\n\nBy applying a linear classification algorithm on the RBF feature space:\n\\mathbf{y} = f(W \\times \\phi(\\mathbf{x}) + \\mathbf{b})\nwe obtain a smooth non-linear partition of the input space. The width of the gaussian kernel allows distance-based generalization.\n\n\n\nRBF networks learn linearly smooth transitions between the training examples. Source: https://mccormickml.com/2013/08/15/radial-basis-function-network-rbfn-tutorial/\n\n\n\n\nKernel perceptron (optional)\nWhat happens during online Perceptron learning?\n\n\n\n\n\n\nPrimal form of the online Perceptron algorithm\n\n\n\n\nfor M epochs:\n\nfor each sample (\\mathbf{x}_i, t_i):\n\ny_i = \\text{sign}( \\langle \\mathbf{w} \\cdot \\mathbf{x}_i \\rangle + b)\n\\Delta \\mathbf{w} = \\eta \\, (t_i - y_i) \\, \\mathbf{x}_i\n\\Delta b = \\eta \\, (t_i - y_i)\n\n\n\n\n\nIf an example \\mathbf{x}_i is correctly classified (y_i = t_i), the weight vector does not change.\n\\mathbf{w} \\leftarrow \\mathbf{w}\nIf an example \\mathbf{x}_i is miscorrectly classified (y_i \\neq t_i), the weight vector is increased from t_i \\, \\mathbf{x}_i.\n\\mathbf{w} \\leftarrow \\mathbf{w} + 2 \\, \\eta \\, t_i \\, \\mathbf{x}_i\nIf you initialize the weight vector to 0, its final value will therefore be a linear combination of the input samples:\n\\mathbf{w} = \\sum_{i=1}^N \\alpha_i \\, t_i \\, \\mathbf{x}_i\nThe coefficients \\alpha_i represent the embedding strength of each example, i.e. how often they were misclassified.\nWith \\mathbf{w} = \\sum_{i=1}^N \\alpha_i \\, t_i \\, \\mathbf{x}_i, the prediction for an input \\mathbf{x} only depends on the training samples and their \\alpha_i value:\ny =  \\text{sign}( \\sum_{i=1}^N \\alpha_i \\, t_i \\, \\langle \\mathbf{x}_i \\cdot \\mathbf{x} \\rangle)\nTo make a prediction y, we need the dot product between the input \\mathbf{x} and all training examples \\mathbf{x}_i. We ignore the bias here, but it can be added back.\n\n\n\n\n\n\nDual form of the online Perceptron algorithm\n\n\n\n\nfor M epochs:\n\nfor each sample (\\mathbf{x}_i, t_i):\n\ny_i = \\text{sign}( \\sum_{j=1}^N \\alpha_j \\, t_j \\, \\langle \\mathbf{x}_j \\cdot \\mathbf{x}_i \\rangle)\nif y_i \\neq t_i :\n\n\\alpha_i \\leftarrow \\alpha_i + 1\n\n\n\n\n\n\nThis dual form of the Perceptron algorithm is strictly equivalent to its primal form. It needs one parameter \\alpha_i per training example instead of a weight vector (N >> d), but relies on dot products between vectors.\nWhy is it interesting to have an algorithm relying on dot products? You can project the inputs \\mathbf{x} to a feature space \\phi(\\mathbf{x}) and apply the same algorithm:\ny =  \\text{sign}( \\sum_{i=1}^N \\alpha_i \\, t_i \\, \\langle \\phi(\\mathbf{x}_i) \\cdot \\phi(\\mathbf{x}) \\rangle)\nBut you do not need to compute the dot product in the feature space, all you need to know is its result.\nK(\\mathbf{x}_i, \\mathbf{x}) = \\langle \\phi(\\mathbf{x}_i) \\cdot \\phi(\\mathbf{x}) \\rangle\n\nKernel trick: A kernel K(\\mathbf{x}, \\mathbf{z}) allows to compute the dot product between the feature space representation of two vectors without ever computing these representations!\n\nLet’s consider the quadratic kernel in \\Re^3:\n\n\\begin{aligned}\n\\forall (\\mathbf{x}, \\mathbf{z}) \\in \\Re^3 \\times \\Re^3 & \\\\\n                            & \\\\\n  K(\\mathbf{x}, \\mathbf{z}) &= ( \\langle \\mathbf{x} \\cdot  \\mathbf{z} \\rangle)^2 \\\\\n                            &=  (\\sum_{i=1}^3 x_i \\cdot z_i) \\cdot (\\sum_{j=1}^3 x_j \\cdot z_j) \\\\\n                            &=  \\sum_{i=1}^3 \\sum_{j=1}^3 (x_i \\cdot x_j) \\cdot ( z_i \\cdot z_j) \\\\\n                            &=  \\langle \\phi(\\mathbf{x}) \\cdot \\phi(\\mathbf{z}) \\rangle \\\\\n\\end{aligned}\n\nwith:\n\n  \\phi(\\mathbf{x}) = \\begin{bmatrix}\n                            x_1 \\cdot x_1 \\\\\n                            x_1 \\cdot x_2 \\\\\n                            x_1 \\cdot x_3 \\\\\n                            x_2 \\cdot x_1 \\\\\n                            x_2 \\cdot x_2 \\\\\n                            x_2 \\cdot x_3 \\\\\n                            x_3 \\cdot x_1 \\\\\n                            x_3 \\cdot x_2 \\\\\n                            x_3 \\cdot x_3 \\end{bmatrix}\n\nThe quadratic kernel implicitely transforms an input space with three dimensions into a feature space of 9 dimensions.\nMore generally, the polynomial kernel in \\Re^d of degree p:\n\n\\begin{align*}\n\\forall (\\mathbf{x}, \\mathbf{z}) \\in \\Re^d \\times \\Re^d \\qquad  K(\\mathbf{x}, \\mathbf{z}) &= ( \\langle \\mathbf{x} \\cdot  \\mathbf{z} \\rangle)^p \\\\\n                                            &=  \\langle \\phi(\\mathbf{x}) \\cdot \\phi(\\mathbf{z}) \\rangle\n\\end{align*}\n\ntransforms the input from a space with d dimensions into a feature space of d^p dimensions.\nWhile the inner product in the feature space would require O(d^p) operations, the calculation of the kernel directly in the input space only requires O(d) operations. This is called the kernel trick: when a linear algorithm only relies on the dot product between input vectors, it can be safely projected into a higher dimensional feature space through a kernel function, without increasing too much its computational complexity, and without ever computing the values in the feature space.\nThe kernel perceptron is the dual form of the Perceptron algorithm using a kernel:\n\n\n\n\n\n\nKernel perceptron\n\n\n\n\nfor M epochs:\n\nfor each sample (\\mathbf{x}_i, t_i):\n\ny_i = \\text{sign}( \\sum_{j=1}^N \\alpha_j \\, t_j \\, K(\\mathbf{x}_j, \\mathbf{x}_i))\nif y_i \\neq t_i :\n\n\\alpha_i \\leftarrow \\alpha_i + 1\n\n\n\n\n\n\nDepending on the kernel, the implicit dimensionality of the feature space can even be infinite! Some kernels:\n\nLinear kernel: d dimensions.\n\n\nK(\\mathbf{x},\\mathbf{z}) = \\langle \\mathbf{x} \\cdot \\mathbf{z} \\rangle\n\n\nPolynomial kernel: d^p dimensions.\n\n\nK(\\mathbf{x},\\mathbf{z}) = (\\langle \\mathbf{x} \\cdot \\mathbf{z} \\rangle)^p\n\n\nGaussian kernel (or RBF kernel): \\infty dimensions.\n\n\nK(\\mathbf{x},\\mathbf{z}) = \\exp(-\\frac{\\| \\mathbf{x} - \\mathbf{z} \\|^2}{2\\sigma^2})\n\n\nHyperbolic tangent kernel: \\infty dimensions.\n\n\nk(\\mathbf{x},\\mathbf{z})=\\tanh(\\langle \\kappa \\mathbf{x} \\cdot \\mathbf{z} \\rangle +c)\n\nIn practice, the choice of the kernel family depends more on the nature of data (text, image…) and its distribution than on the complexity of the learning problem. RBF kernels tend to “group” positive examples together. Polynomial kernels are more like “distorted” hyperplanes. Kernels have parameters (p, \\sigma…) which have to found using cross-validation.\n\n\n\nDifferent kernels lead to different decision functions. Source: http://beta.cambridgespark.com/courses/jpm/05-module.html\n\n\n\n\nSupport vector machines (optional)\nSupport vector machines (SVM) extend the idea of a kernel perceptron using a different linear learning algorithm, the maximum margin classifier. Using Lagrange optimization and regularization, the maximal margin classifer tries to maximize the “safety zone” (geometric margin) between the classifier and the training examples. It also tries to reduce the number of non-zero \\alpha_i coefficients to keep the complexity of the classifier bounded, thereby improving the generalization:\n\n\\mathbf{y} = \\text{sign}(\\sum_{i=1}^{N_{SV}} \\alpha_i \\, t_i \\, K(\\mathbf{x}_i, \\mathbf{x}) + b)\n\n\n\n\nSupport-vectors are the closest examples to the hyperplane. They have a non-zero \\alpha coefficient.\n\n\nCoupled with a good kernel, a SVM can efficiently solve non-linear classification problems without overfitting. SVMs were the weapon of choice before the deep learning era, which deals better with huge datasets."
  },
  {
    "objectID": "notes/3.1-NeuralNetworks.html#multi-layer-perceptron",
    "href": "notes/3.1-NeuralNetworks.html#multi-layer-perceptron",
    "title": "Multi-layer perceptron",
    "section": "Multi-layer perceptron",
    "text": "Multi-layer perceptron\n\nA Multi-Layer Perceptron (MLP) or feedforward neural network is composed of:\n\nan input layer for the input vector \\mathbf{x}\none or several hidden layers allowing to project non-linearly the input into a space of higher dimensions \\mathbf{h}_1, \\mathbf{h}_2, \\mathbf{h}_3, \\ldots.\nan output layer for the output \\mathbf{y}.\n\n\n\n\nMulti-layer perceptron with one hidden layer.\n\n\nIf there is a single hidden layer \\mathbf{h} (shallow network), it corresponds to the feature space. Each layer takes inputs from the previous layer. If the hidden layer is adequately chosen, the output neurons can learn to replicate the desired output \\mathbf{t}.\n\nFully-connected layer\nThe operation performed by each layer can be written in the form of a matrix-vector multiplication:\n\n    \\mathbf{h} = f(\\textbf{net}_\\mathbf{h}) = f(W^1 \\, \\mathbf{x} + \\mathbf{b}^1)\n \n    \\mathbf{y} = f(\\textbf{net}_\\mathbf{y}) = f(W^2 \\, \\mathbf{h} + \\mathbf{b}^2)\n\n\n\n\nFully-connected layer.\n\n\nFully-connected layers (FC) transform an input vector \\mathbf{x} into a new vector \\mathbf{h} by multiplying it by a weight matrix W and adding a bias vector \\mathbf{b}. A non-linear activation function transforms each element of the net activation.\n\n\nActivation functions\nHere are some of the most useful activation functions in a MLP.\n\n\n\nSome classical activation functions in MLPs.\n\n\n\nThreshold function (output is binary)\n\n\n    f(x) = \\begin{cases} 1 \\; \\text{if} \\; x \\geq 0 \\\\ 0 \\; \\text{otherwise.} \\end{cases}\n\n\nLogistic / sigmoid function (output is continuous and bounded between 0 and 1)\n\n\n    f(x) = \\dfrac{1}{1 + \\exp -x}\n\n\nHyperbolic function (output is continuous and bounded between -1 and 1)\n\n\n    f(x) = \\text{tanh}(x)\n\n\nRectified linear function - ReLU (output is continuous and positive).\n\n\n    f(x) = \\max(0, x) = \\begin{cases} x \\quad \\text{if} \\quad x \\geq 0 \\\\ 0 \\quad \\text{otherwise.} \\end{cases}\n\n\nParametric Rectifier Linear Unit - PReLU (output is continuous and unbounded).\n\n\n    f(x) = \\begin{cases} x \\quad \\text{if} \\quad x \\geq 0 \\\\ \\alpha \\, x  \\quad \\text{otherwise.}\\end{cases}\n\n\n\n\nReLU vs. PReLU activation functions.\n\n\nFor classification problems, the softmax activation function can be used in the output layer to make sure that the sum of the outputs \\mathbf{y} = \\{y_j\\} over all output neurons is one.\n\n    y_j = P(\\text{class = j}) = \\frac{\\exp(\\text{net}_j)}{\\sum_k \\exp(\\text{net}_k)}\n\nThe higher the net activation \\text{net}_j, the higher the probability that the example belongs to class j. Softmax is not per se a transfer function (not local to each neuron), but the idea is similar.\n\n\n\n\n\n\nNote\n\n\n\nWhy not use the linear function f(x) = x in the hidden layer?\n\n    \\mathbf{h} = W^1 \\, \\mathbf{x} + \\mathbf{b}^1\n \n    \\mathbf{y} = W^2 \\, \\mathbf{h} + \\mathbf{b}^2\n\nWe would have:\n\n\\begin{align*}\n    \\mathbf{y} &= W^2 \\, (W^1 \\, \\mathbf{x} + \\mathbf{b}^1) + \\mathbf{b}^2 \\\\\n               &= (W^2 \\, W^1) \\, \\mathbf{x} + (W^2 \\, \\mathbf{b}^1 + \\mathbf{b}^2) \\\\\n               &= W \\, \\mathbf{x} + \\mathbf{b} \\\\\n\\end{align*}\n\nso the equivalent function would be linear…\nRemember Cover’s theorem:\n\nA complex pattern-classification problem, cast in a high dimensional space non-linearly, is more likely to be linearly separable than in a low-dimensional space, provided that the space is not densely populated.\n\nIn practice it does not matter how non-linear the function is (e.g PReLU is almost linear), but there must be at least one non-linearity.\n\n\n\n\nLoss function\nWe have a training set composed of N input/output pairs (\\mathbf{x}_i, \\mathbf{t}_i)_{i=1..N}. What are the free parameters \\theta (weights W^1, W^2 and biases \\textbf{b}^1, \\textbf{b}^2) making the prediction \\mathbf{y} as close as possible from the desired output \\mathbf{t}?\nWe define a loss function \\mathcal{L}(\\theta) of the free parameters which should be minimized:\n\nFor regression problems, we take the mean square error (mse):\n\n\n    \\mathcal{L}_\\text{reg}(\\theta) = \\mathbb{E}_{\\mathbf{x}, \\mathbf{t} \\in \\mathcal{D}} [ ||\\mathbf{t} - \\mathbf{y}||^2 ]\n\n\nFor classification problems, we take the cross-entropy or negative log-likelihood on a softmax output layer:\n\n\\mathcal{L}_\\text{class}(\\theta) = \\mathbb{E}_{\\mathbf{x}, \\mathbf{t} \\sim \\mathcal{D}} [ - \\langle \\mathbf{t} \\cdot \\log \\mathbf{y} \\rangle]\n\n\nOptimizer\nTo minimize the chosen loss function, we are going to use stochastic gradient descent iteratively until the network converges:\n\\begin{cases}\n    \\Delta W^1 = - \\eta \\, \\nabla_{W^1} \\, \\mathcal{L}(\\theta) \\\\\n    \\\\\n    \\Delta \\mathbf{b}^1 = - \\eta \\, \\nabla_{\\mathbf{b}^1} \\, \\mathcal{L}(\\theta) \\\\\n    \\\\\n    \\Delta W^2 = - \\eta \\, \\nabla_{W^2} \\, \\mathcal{L}(\\theta) \\\\\n    \\\\\n    \\Delta \\mathbf{b}^2 = - \\eta \\, \\nabla_{\\mathbf{b}^2} \\, \\mathcal{L}(\\theta)\\\\\n\\end{cases}\n\nWe will see later that other optimizers than SGD can be used. The question is now how to compute efficiently these gradients w.r.t all the weights and biases. The algorithm to achieve this is called backpropagation (Rumelhart et al., 1986), which is simply a smart implementation of the chain rule.\n\n\n\n\n\n\nNote\n\n\n\n(Rumelhart et al., 1986) did not actually invent backpropagation but merely popularized it in an article in Nature. Seppo Linnainmaa published the reverse mode of automatic differentiation in his Master thesis back in 1970 (Linnainmaa, 1970) and Paul Werbos applied it to deep neural networks in 1982 (Werbos, 1982).\nFor a controversy about the origins of backpropagation and who should get credit for deep learning, see this strongly-worded post by Jürgen Schmidhuber:\nhttps://people.idsia.ch/~juergen/scientific-integrity-turing-award-deep-learning.html"
  },
  {
    "objectID": "notes/3.1-NeuralNetworks.html#backpropagation",
    "href": "notes/3.1-NeuralNetworks.html#backpropagation",
    "title": "Multi-layer perceptron",
    "section": "Backpropagation",
    "text": "Backpropagation\n\n\nBackpropagation on a shallow network\nA shallow MLP computes:\n\n    \\mathbf{h} = f(\\textbf{net}_\\mathbf{h}) = f(W^1 \\, \\mathbf{x} + \\mathbf{b}^1)\n \n    \\mathbf{y} = f(\\textbf{net}_\\mathbf{y}) = f(W^2 \\, \\mathbf{h} + \\mathbf{b}^2)\n\nThe chain rule gives us for the parameters of the output layer:\n\n    \\frac{\\partial \\mathcal{L}(\\theta)}{\\partial W^2} = \\frac{\\partial \\mathcal{L}(\\theta)}{\\partial \\mathbf{y}} \\times \\frac{\\partial \\mathbf{y}}{\\partial \\textbf{net}_\\mathbf{y}} \\times \\frac{\\partial \\textbf{net}_\\mathbf{y}}{\\partial W^2}\n\n\n    \\frac{\\partial \\mathcal{L}(\\theta)}{\\partial \\mathbf{b}^2} = \\frac{\\partial \\mathcal{L}(\\theta)}{\\partial \\mathbf{y}} \\times \\frac{\\partial \\mathbf{y}}{\\partial \\textbf{net}_\\mathbf{y}} \\times \\frac{\\partial \\textbf{net}_\\mathbf{y}}{\\partial \\mathbf{b}^2}\n\nand for the hidden layer:\n\n    \\frac{\\partial \\mathcal{L}(\\theta)}{\\partial W^1} = \\frac{\\partial \\mathcal{L}(\\theta)}{\\partial \\mathbf{y}} \\times \\frac{\\partial \\mathbf{y}}{\\partial \\textbf{net}_\\mathbf{y}} \\times \\frac{\\partial \\textbf{net}_\\mathbf{y}}{\\partial \\mathbf{h}} \\times \\frac{\\partial \\mathbf{h}}{\\partial \\textbf{net}_\\mathbf{h}} \\times \\frac{\\partial \\textbf{net}_\\mathbf{h}}{\\partial W^1}\n\n\n    \\frac{\\partial \\mathcal{L}(\\theta)}{\\partial \\mathbf{b}^1} = \\frac{\\partial \\mathcal{L}(\\theta)}{\\partial \\mathbf{y}} \\times \\frac{\\partial \\mathbf{y}}{\\partial \\textbf{net}_\\mathbf{y}} \\times \\frac{\\partial \\textbf{net}_\\mathbf{y}}{\\partial \\mathbf{h}} \\times \\frac{\\partial \\mathbf{h}}{\\partial \\textbf{net}_\\mathbf{h}} \\times \\frac{\\partial \\textbf{net}_\\mathbf{h}}{\\partial \\mathbf{b}^1}\n\nIf we can compute all these partial derivatives / gradients individually, the problem is solved.\nWe have already seen for the linear algorithms that the derivative of the loss function w.r.t the net activation of the output \\textbf{net}_\\mathbf{y} is proportional to the prediction error \\mathbf{t} - \\mathbf{y}:\n\nmse for regression:\n\n\n    \\mathbf{\\delta_y} = - \\frac{\\partial \\mathcal{l}_\\text{reg}(\\theta)}{\\partial \\textbf{net}_\\mathbf{y}} = - \\frac{\\partial \\mathcal{l}_\\text{reg}(\\theta)}{\\partial \\mathbf{y}} \\times \\frac{\\partial \\mathbf{y}}{\\partial \\textbf{net}_\\mathbf{y}} = 2 \\, (\\mathbf{t} - \\mathbf{y}) \\, f'(\\textbf{net}_\\mathbf{y})\n\n\ncross-entropy using a softmax output layer:\n\n\n    \\mathbf{\\delta_y} = - \\frac{\\partial \\mathcal{l}_\\text{class}(\\theta)}{\\partial \\textbf{net}_\\mathbf{y}} = (\\mathbf{t} - \\mathbf{y})\n\n\\mathbf{\\delta_y} = - \\dfrac{\\partial \\mathcal{l}(\\theta)}{\\partial \\textbf{net}_\\mathbf{y}} is called the output error. The output error is going to appear in all partial derivatives, i.e. in all learning rules. The backpropagation algorithm is sometimes called backpropagation of the error.\nWe now have everything we need to train the output layer:\n\n    \\frac{\\partial \\mathcal{l}(\\theta)}{\\partial W^2} = \\frac{\\partial \\mathcal{l}(\\theta)}{\\partial \\textbf{net}_\\mathbf{y}}  \\times \\frac{\\partial \\textbf{net}_\\mathbf{y}}{\\partial W^2} = - \\mathbf{\\delta_y}  \\times \\frac{\\partial \\textbf{net}_\\mathbf{y}}{\\partial W^2}\n\n\n    \\frac{\\partial \\mathcal{l}(\\theta)}{\\partial \\mathbf{b}^2} = \\frac{\\partial \\mathcal{l}(\\theta)}{\\partial \\textbf{net}_\\mathbf{y}}  \\times \\frac{\\partial \\textbf{net}_\\mathbf{y}}{\\partial \\mathbf{b}^2} = - \\mathbf{\\delta_y} \\times \\frac{\\partial \\textbf{net}_\\mathbf{y}}{\\partial \\mathbf{b}^2}\n\nAs \\textbf{net}_\\mathbf{y} = W^2 \\, \\mathbf{h} + \\mathbf{b}^2, we get for the cross-entropy loss:\n\n    \\frac{\\partial \\mathcal{l}(\\theta)}{\\partial W^2} = - \\mathbf{\\delta_y} \\times \\mathbf{h}^T\n\n\n    \\frac{\\partial \\mathcal{l}(\\theta)}{\\partial \\mathbf{b}^2} = - \\mathbf{\\delta_y}\n\ni.e. exactly the same delta learning rule as a softmax linear classifier or multiple linear regression using the vector \\mathbf{h} as an input.\n\n\\begin{cases}\n    \\Delta W^2 = \\eta \\, \\mathbf{\\delta_y} \\times \\mathbf{h}^T = \\eta \\,  (\\mathbf{t} - \\mathbf{y} ) \\times \\mathbf{h}^T \\\\\n    \\\\\n    \\Delta \\mathbf{b}^2 = \\eta \\,  \\mathbf{\\delta_y} = \\eta \\,  (\\mathbf{t} - \\mathbf{y} ) \\\\\n\\end{cases}\n\nLet’s now note \\mathbf{\\delta_h} the hidden error, i.e. minus the gradient of the loss function w.r.t the net activation of the hidden layer:\n\n    \\mathbf{\\delta_h} = - \\frac{\\partial \\mathcal{l}(\\theta)}{\\partial \\textbf{net}_\\mathbf{h}} = - \\frac{\\partial \\mathcal{l}(\\theta)}{\\partial \\textbf{net}_\\mathbf{y}} \\times \\frac{\\partial \\textbf{net}_\\mathbf{y}}{\\partial \\mathbf{h}} \\times \\frac{\\partial \\mathbf{h}}{\\partial \\textbf{net}_\\mathbf{h}}  = \\mathbf{\\delta_y} \\times \\frac{\\partial \\textbf{net}_\\mathbf{y}}{\\partial \\mathbf{h}} \\times \\frac{\\partial \\mathbf{h}}{\\partial \\textbf{net}_\\mathbf{h}}\n\nUsing this hidden error, we can compute the gradients w.r.t W^1 and \\mathbf{b}^1:\n\n    \\frac{\\partial \\mathcal{l}(\\theta)}{\\partial W^1} = \\frac{\\partial \\mathcal{l}(\\theta)}{\\partial \\textbf{net}_\\mathbf{h}} \\times \\frac{\\partial \\textbf{net}_\\mathbf{h}}{\\partial W^1} = - \\mathbf{\\delta_h} \\times \\frac{\\partial \\textbf{net}_\\mathbf{h}}{\\partial W^1}\n\n\n    \\frac{\\partial \\mathcal{l}(\\theta)}{\\partial \\mathbf{b}^1} = \\frac{\\partial \\mathcal{l}(\\theta)}{\\partial \\textbf{net}_\\mathbf{h}} \\times \\frac{\\partial \\textbf{net}_\\mathbf{h}}{\\partial \\mathbf{b}^1} = - \\mathbf{\\delta_h} \\times \\frac{\\partial \\textbf{net}_\\mathbf{h}}{\\partial \\mathbf{b}^1}\n\nAs \\textbf{net}_\\mathbf{h} = W^1 \\, \\mathbf{x} + \\mathbf{b}^1, we get:\n\n    \\frac{\\partial \\mathcal{l}(\\theta)}{\\partial W^1} =  - \\mathbf{\\delta_h} \\times \\mathbf{x}^T\n\n\n    \\frac{\\partial \\mathcal{l}(\\theta)}{\\partial \\mathbf{b}^1} =  - \\mathbf{\\delta_h}\n\nIf we know the hidden error \\mathbf{\\delta_h}, the update rules for the input weights W^1 and \\mathbf{b}^1 also take the form of the delta learning rule:\n\n\\begin{cases}\n    \\Delta W^1 = \\eta \\,   \\mathbf{\\delta_h} \\times \\mathbf{x}^T \\\\\n    \\\\\n    \\Delta \\mathbf{b}^1 = \\eta \\,  \\mathbf{\\delta_h} \\\\\n\\end{cases}\n\nThis is the classical form eta * error * input. All we need to know is the backpropagated error \\mathbf{\\delta_h} and we can apply the delta learning rule!\nThe backpropagated error \\mathbf{\\delta_h} is a vector assigning an error to each of the hidden neurons:\n\n    \\mathbf{\\delta_h} = - \\frac{\\partial \\mathcal{l}(\\theta)}{\\partial \\textbf{net}_\\mathbf{h}}  = \\mathbf{\\delta_y} \\times \\frac{\\partial \\textbf{net}_\\mathbf{y}}{\\partial \\mathbf{h}} \\times \\frac{\\partial \\mathbf{h}}{\\partial \\textbf{net}_\\mathbf{h}}\n\nAs :\n\\textbf{net}_\\mathbf{y} = W^2 \\, \\mathbf{h} + \\mathbf{b}^2\n\\mathbf{h} = f(\\textbf{net}_\\mathbf{h})\nwe obtain:\n\n    \\mathbf{\\delta_h} = f'(\\textbf{net}_\\mathbf{h}) \\, (W^2)^T \\times \\mathbf{\\delta_y}\n\nIf \\mathbf{h} and \\mathbf{\\delta_h} have K elements and \\mathbf{y} and \\mathbf{\\delta_y} have C elements, the matrix W^2 is C \\times K as W^2 \\times \\mathbf{h} must be a vector with C elements. (W^2)^T \\times \\mathbf{\\delta_y} is therefore a vector with K elements, which is then multiplied element-wise with the derivative of the transfer function to obtain \\mathbf{\\delta_h}.\n\n\n\n\n\n\nBackpropagation for a shallow MLP\n\n\n\n\n\n\n\n\nFor a shallow MLP with one hidden layer:\n\n    \\mathbf{h} = f(\\textbf{net}_\\mathbf{h}) = f(W^1 \\, \\mathbf{x} + \\mathbf{b}^1)\n \n    \\mathbf{y} = f(\\textbf{net}_\\mathbf{y}) = f(W^2 \\, \\mathbf{h} + \\mathbf{b}^2)\n\nthe output error:\n\n        \\mathbf{\\delta_y} = - \\frac{\\partial \\mathcal{l}(\\theta)}{\\partial \\textbf{net}_\\mathbf{y}} = (\\mathbf{t} - \\mathbf{y})\n    \nis backpropagated to the hidden layer:\n\n    \\mathbf{\\delta_h} = f'(\\textbf{net}_\\mathbf{h}) \\, (W^2)^T \\times \\mathbf{\\delta_y}\n\nwhat allows to apply the delta learning rule to all parameters:\n\n\\begin{cases}\n    \\Delta W^2 = \\eta \\, \\mathbf{\\delta_y} \\times \\mathbf{h}^T \\\\\n    \\Delta \\mathbf{b}^2 = \\eta \\,  \\mathbf{\\delta_y} \\\\\n    \\Delta W^1 = \\eta \\, \\mathbf{\\delta_h}  \\times  \\mathbf{x}^T \\\\\n    \\Delta \\mathbf{b}^1 = \\eta \\, \\mathbf{\\delta_h}  \\\\\n\\end{cases}\n\n\n\nThe usual transfer functions are easy to derive (that is why they are chosen…).\n\nThreshold and sign functions are not differentiable, we simply consider the derivative is 1.\n\n\nf(x) = \\begin{cases} 1 \\quad \\text{if} \\quad x \\geq 0 \\\\ 0 \\text{ or } 1 \\quad \\text{otherwise.} \\end{cases} \\qquad \\rightarrow \\qquad f'(x) = 1\n\n\nThe logistic or sigmoid function has the nice property that its derivative can be expressed as a function of itself:\n\n\nf(x) = \\frac{1}{1+\\exp(-x)} \\qquad \\rightarrow \\qquad f'(x) = f(x) \\, (1 - f(x))\n\n\nThe hyperbolic tangent function too:\n\n\nf(x) = \\tanh(x) \\qquad \\rightarrow \\qquad f'(x) = 1 - f(x)^2\n\n\nReLU is even simpler:\n\n\nf(x) = \\max(0, x) = \\begin{cases} x \\quad \\text{if} \\quad x \\geq 0 \\\\ 0 \\quad \\text{otherwise.} \\end{cases} \\qquad \\rightarrow \\qquad f'(x) = \\begin{cases} 1 \\quad \\text{if} \\quad x \\geq 0 \\\\ 0 \\quad \\text{otherwise.}\\end{cases}\n\n\n\nBackpropagation at the neuron level\nLet’s have a closer look at what is backpropagated using single neurons and weights. The output neuron y_k computes:\ny_k = f(\\sum_{j=1}^K W^2_{jk} \\, h_j + b^2_k)\nAll output weights W^2_{jk} are updated proportionally to the output error of the neuron y_k:\n\\Delta W^2_{jk} = \\eta \\, \\delta_{{y}_k} \\, h_j = \\eta \\, (t_k - y_k) \\, h_j\nThis is possible because we know the output error directly from the data t_k.\nThe hidden neuron h_j computes:\nh_j = f(\\sum_{i=1}^d W^1_{ij} \\, x_i + b^1_j)\nWe want to learn the hidden weights W^1_{ij} using the delta learning rule:\n\\Delta W^1_{ij} = \\eta \\, \\delta_{{h}_j} \\, x_i\nbut we do not know the ground truth of the hidden neuron in the data:\n\\delta_{{h}_j} = (? - h_j)\nWe need to estimate the backpropagated error using the output error. If we omit the derivative of the transfer function, the backpropagated error for the hidden neuron h_j is:\n\\delta_{{h}_j} = \\sum_{k=1}^C W^2_{jk} \\, \\delta_{{y}_k}\nThe backpropagated error is an average of the output errors \\delta_{{y}_k}, weighted by the output weights between the hidden neuron h_j and the output neurons y_k.\nThe backpropagated error is the contribution of each hidden neuron h_j to the output error:\n\nIf there is no output error, there is no hidden error.\nIf a hidden neuron sends strong weights |W^2_{jk}| to an output neuron y_k with a strong prediction error \\delta_{{y}_k}, this means that it participates strongly to the output error and should learn from it.\nIf the weight |W^2_{jk}| is small, it means that the hidden neuron does not take part in the output error.\n\n\n\nUniversal approximation theorem\n\n\n\n\n\n\nUniversal approximation theorem\n\n\n\nCybenko, 1989\nLet \\varphi() be a nonconstant, bounded, and monotonically-increasing continuous function. Let I_{m_0} denote the m_0-dimensional unit hypercube [0,1]^{m_0}. The space of continuous functions on I_{m_0} is denoted by C(I_{m_0}). Then, given any function f \\in C(I_{m_0}) and \\epsilon > 0, there exists an integer m_1 and sets of real constants \\alpha_i, b_i and w_{ij} \\in \\Re, where i = 1, ..., m_1 and j = 1, ..., m_0 such that we may define:\n\n    F(\\mathbf{x}) = \\sum_{i=1}^{m_1} \\alpha_i \\cdot \\varphi \\left( \\sum_{j=1}^{m_0} w_{ij} \\cdot x_j + b_i \\right)\n\nas an approximate realization of the function f; that is,\n | F(\\mathbf{x}) - f(\\mathbf{x})| < \\epsilon\nfor all x \\in I_m.\n\n\nThis theorem shows that for any input/output mapping function f in supervised learning, there exists a MLP with m_1 neurons in the hidden layer which is able to approximate it with a desired precision!\nThe universal approximation theorem only proves the existence of a shallow MLP with m_1 neurons in the hidden layer that can approximate any function, but it does not tell how to find this number. A rule of thumb to find this number is that the generalization error is empirically close to:\n \\epsilon = \\frac{\\text{VC}_{\\text{dim}}(\\text{MLP})}{N}\n\nwhere \\text{VC}_{\\text{dim}}(\\text{MLP}) is the total number of weights and biases in the model, and N the number of training samples.\nThe more neurons in the hidden layer, the better the training error, but the worse the generalization error (overfitting). The optimal number should be found with cross-validation methods. For most functions, the optimal number m_1 is high and becomes quickly computationally untractable. We need to go deep!\n\n\nBackpropagation on a deep neural network\n\nA MLP with more than one hidden layer is a deep neural network.\n\n\n\nDeep network extract progressively more complex features.\n\n\nBackpropagation still works if we have many hidden layers \\mathbf{h}_1, \\ldots, \\mathbf{h}_n:\n\n\n\nForward pass. Source: David Silver https://icml.cc/2016/tutorials/deep_rl_tutorial.pdf\n\n\nIf each layer is differentiable, i.e. one can compute its gradient \\frac{\\partial \\mathbf{h}_{k}}{\\partial \\mathbf{h}_{k-1}}, we can chain backwards each partial derivatives to know how to update each layer.\n\n\n\nBackpropagation pass. Source: David Silver https://icml.cc/2016/tutorials/deep_rl_tutorial.pdf\n\n\nBackpropagation is simply an efficient implementation of the chain rule: the partial derivatives are iteratively reused in the backwards phase.\nA fully connected layer transforms an input vector \\mathbf{h}_{k-1} into an output vector \\mathbf{h}_{k} using a weight matrix W^k, a bias vector \\mathbf{b}^k and a non-linear activation function f:\n\n    \\mathbf{h}_{k} = f(\\textbf{net}_{\\mathbf{h}^k}) = f(W^k \\, \\mathbf{h}_{k-1} + \\mathbf{b}^k)\n\nThe gradient of its output w.r.t the input \\mathbf{h}_{k-1} is (using the chain rule):\n\n\\frac{\\partial \\mathbf{h}_{k}}{\\partial \\mathbf{h}_{k-1}} = f'(\\textbf{net}_{\\mathbf{h}^k}) \\, W^k\n\nThe gradients of its output w.r.t the free parameters W^k and \\mathbf{b}_{k} are:\n\n\\frac{\\partial \\mathbf{h}_{k}}{\\partial W^{k}} =  f'(\\textbf{net}_{\\mathbf{h}^k}) \\, \\mathbf{h}_{k-1}\n\n\n\\frac{\\partial \\mathbf{h}_{k}}{\\partial \\mathbf{b}_{k}} =  f'(\\textbf{net}_{\\mathbf{h}^k})\n\nA fully connected layer \\mathbf{h}_{k} = f(W^k \\, \\mathbf{h}_{k-1} + \\mathbf{b}^k) receives the gradient of the loss function w.r.t. its output \\mathbf{h}_{k} from the layer above:\n\n    \\frac{\\partial \\mathcal{L}(\\theta)}{\\partial \\mathbf{h}_{k}}\n\nIt adds to this gradient its own contribution and transmits it to the previous layer:\n\n    \\frac{\\partial \\mathcal{L}(\\theta)}{\\partial \\mathbf{h}_{k-1}} = \\frac{\\partial \\mathcal{L}(\\theta)}{\\partial \\mathbf{h}_{k}} \\times \\frac{\\partial \\mathbf{h}_{k}}{\\partial \\mathbf{h}_{k-1}} = f'(\\textbf{net}_{\\mathbf{h}^k}) \\, (W^k)^T \\times \\frac{\\partial \\mathcal{L}(\\theta)}{\\partial \\mathbf{h}_{k}}\n\nIt then updates its parameters W^k and \\mathbf{b}_{k} with:\n\\begin{cases}\n\\dfrac{\\partial \\mathcal{L}(\\theta)}{\\partial W^{k}} = \\dfrac{\\partial \\mathcal{L}(\\theta)}{\\partial \\mathbf{h}_{k}}  \\times \\dfrac{\\partial \\mathbf{h}_{k}}{\\partial W^{k}} =  f'(\\textbf{net}_{\\mathbf{h}^k}) \\, \\dfrac{\\partial \\mathcal{L}(\\theta)}{\\partial \\mathbf{h}_{k}}  \\times \\mathbf{h}_{k-1}^T \\\\\n\\\\\n\\dfrac{\\partial \\mathcal{L}(\\theta)}{\\partial \\mathbf{b}_{k}}  = \\dfrac{\\partial \\mathcal{L}(\\theta)}{\\partial \\mathbf{h}_{k}}  \\times \\dfrac{\\partial \\mathbf{h}_{k}}{\\partial \\mathbf{b}_{k}} =  f'(\\textbf{net}_{\\mathbf{h}^k}) \\, \\dfrac{\\partial \\mathcal{L}(\\theta)}{\\partial \\mathbf{h}_{k}} \\\\\n\\end{cases}\n\n\n\n\n\n\n\nTraining a deep neural network with backpropagation\n\n\n\nA feedforward neural network is an acyclic graph of differentiable and parameterized layers.\n\n    \\mathbf{x} \\rightarrow \\mathbf{h}_1 \\rightarrow \\mathbf{h}_2 \\rightarrow \\ldots \\rightarrow  \\mathbf{h}_n \\rightarrow \\mathbf{y}\n\nThe backpropagation algorithm is used to assign the gradient of the loss function \\mathcal{L}(\\theta) to each layer using backward chaining:\n\n    \\frac{\\partial \\mathcal{L}(\\theta)}{\\partial \\mathbf{h}_{k-1}} = \\frac{\\partial \\mathcal{L}(\\theta)}{\\partial \\mathbf{h}_{k}} \\times \\frac{\\partial \\mathbf{h}_{k}}{\\partial \\mathbf{h}_{k-1}}\n\nStochastic gradient descent is then used to update the parameters of each layer:\n\n    \\Delta W^k = - \\eta \\, \\frac{\\partial \\mathcal{L}(\\theta)}{\\partial W^{k}} = - \\eta \\, \\frac{\\partial \\mathcal{L}(\\theta)}{\\partial \\mathbf{h}_{k}} \\times \\frac{\\partial \\mathbf{h}_{k}}{\\partial W^{k}}"
  },
  {
    "objectID": "notes/3.1-NeuralNetworks.html#example-of-a-shallow-network",
    "href": "notes/3.1-NeuralNetworks.html#example-of-a-shallow-network",
    "title": "Multi-layer perceptron",
    "section": "Example of a shallow network",
    "text": "Example of a shallow network\n\nLet’s try to solve this non-linear binary classification problem:\n\n\n\nNon-linearly separable dataset.\n\n\nWe can create a shallow MLP with:\n\nTwo input neurons x_1, x_2 for the two input variables.\nEnough hidden neurons (e.g. 20), with a sigmoid or ReLU activation function.\nOne output neuron with the logistic activation function.\nThe cross-entropy (negative log-likelihood) loss function.\n\n\n\n\nSimple MLP for the non-linearly separable dataset.\n\n\nWe train it on the input data using the backpropagation algorithm and the SGD optimizer.\n\n\n\nPrediction after each epoch of training.\n\n\nExperiment with this network live on https://playground.tensorflow.org/! You will implement this MLP with numpy during exercise 8.\n\n\n\n\n\n\nAutomatic differentiation Deep Learning frameworks\n\n\n\nCurrent:\n\nTensorflow https://www.tensorflow.org/ released by Google in 2015 is one of the two standard DL frameworks.\nKeras https://keras.io/ is a high-level Python API over tensorflow (but also theano, CNTK and MxNet) written by Francois Chollet.\nPyTorch http://pytorch.org by Facebook is the other standard framework.\n\nHistorical:\n\nTheano http://deeplearning.net/software/theano/ released by U Toronto in 2010 is the predecessor of tensorflow. Now abandoned.\nCaffe http://caffe.berkeleyvision.org/ by U Berkeley was long the standard library for convolutional networks.\nCNTK https://github.com/Microsoft/CNTK (Microsoft Cognitive Toolkit) is a free library by Microsoft!\nMxNet https://github.com/apache/incubator-mxnet from Apache became the DL framework at Amazon.\n\n\n\nLet’s implement the previous MLP using keras. We first need to generate the data using scikit-learn:\nimport sklearn.datasets\nX, t = sklearn.datasets.make_circles(n_samples=100, shuffle=True, noise=0.15, factor=0.3)\nWe then import tensorflow:\nimport tensorflow as tf\nThe neural network is called a Sequential model in keras:\nmodel = tf.keras.Sequential()\nCreating a NN is simply stacking layers in the model. The input layer is just a placeholder for the data:\nmodel.add( tf.keras.layers.Input(shape=(2, )) )\nThe hidden layer has 20 neurons, the ReLU activation and takes input from the previous layer:\nmodel.add(\n    tf.keras.layers.Dense(\n        20, # Number of hidden neurons\n        activation='relu' # Activation function\n    )\n)\nThe output layer has 1 neuron with the logistic/sigmoid activation function:\nmodel.add(\n    tf.keras.layers.Dense(\n        1, # Number of output neurons\n        activation='sigmoid' # Soft classification\n    )\n)\nWe now choose an optimizer (SGD) with a learning rate \\eta = 0.001:\noptimizer = tf.keras.optimizers.SGD(lr=0.001)\nWe choose a loss function (binary cross-entropy, aka negative log-likelihood):\nloss = tf.keras.losses.binary_crossentropy\nWe compile the model (important!) and tell it to track the accuracy of the model:\nmodel.compile(\n    loss=loss,\n    optimizer=optimizer, \n    metrics=tf.keras.metrics.categorical_accuracy\n)\nEt voilà! The network has been created.\nprint(model.summary())\nModel: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense (Dense)                (None, 20)                60        \n_________________________________________________________________\ndense_1 (Dense)              (None, 1)                 21        \n=================================================================\nTotal params: 81\nTrainable params: 81\nNon-trainable params: 0\n_________________________________________________________________\nNone\nWe now train the model on the data for 100 epochs using a batch size of 10 and wait for it to finish:\nmodel.fit(X, t, batch_size=10, nb_epoch=100)\nWith keras (and the other automatic differentiation frameworks), you only need to define the structure of the network. The rest (backpropagation, SGD) is done automatically. To make predictions on new data, just do:\nmodel.predict(X_test)\n\n\n\n\nLinnainmaa, S. (1970). The representation of the cumulative rounding error of an algorithm as a Taylor expansion of the local rounding errors.\n\n\nRumelhart, D. E., Hinton, G. E., and Williams, R. J. (1986). Learning representations by back-propagating errors. Nature 323, 533–536. doi:10.1038/323533a0.\n\n\nWerbos, P. J. (1982). Applications of advances in nonlinear sensitivity analysis. in System Modeling and Optimization: Proc. IFIP (Springer)."
  },
  {
    "objectID": "notes/3.2-DNN.html#why-deep-neural-networks",
    "href": "notes/3.2-DNN.html#why-deep-neural-networks",
    "title": "Modern neural networks",
    "section": "Why deep neural networks?",
    "text": "Why deep neural networks?\n\nThe universal approximation theorem (Cybenko, 1989) states that a shallow network can approximate any mapping function between inputs and outputs. However, if the mapping function is too complex, a shallow network may need too many hidden neurons.\nThe hidden neurons extract features in the input space: typical characteristics of the input which, when combined by the output neurons, allow to solve the classification task. Problem: the features are not hierarchically organized and cannot become complex enough.\nShallow networks can not work directly with raw images: noise, translation, rotation, scaling… One needs first to extract complex and useful features from the input images in order to classify them correctly.\nA MLP with more than one hidden layer is a deep neural network. The different layers extract increasingly complex features.\n\n\n\n\n\nIn practice, training a deep network is not as easy as the theory would suggest. Four main problems have to be solved:\n\nBad convergence: the loss function has many local minima.\n\nMomentum, adaptive optimizers, annealing…\n\nLong training time: deep networks use gradient descent-like optimizers, an iterative method whose speed depends on initialization.\n\nNormalized initialization, batch normalization…\n\nOverfitting: deep networks have a lot of free parameters, so they tend to learn by heart the training set.\n\nRegularisation, dropout, data augmentation, early-stopping…\n\nVanishing gradient: the first layers may not receive sufficient gradients early in training.\n\nReLU activation function, unsupervised pre-training, residual networks…"
  },
  {
    "objectID": "notes/3.2-DNN.html#bad-convergence",
    "href": "notes/3.2-DNN.html#bad-convergence",
    "title": "Modern neural networks",
    "section": "Bad convergence",
    "text": "Bad convergence\n\nThe loss function \\mathcal{L}(\\theta) of a deep neural network has usually not a single global minimum, but many local minima: irregular loss landscape.\n\n\n\nVisualizing the loss landscape of neural nets (Li et al., 2018).\n\n\nGradient descent gets stuck in local minima by design. One could perform different weight initializations, in order to find per chance an initial position close enough from the global minimum, but this is inefficient.\n\nOptimizers\n\nStochastic gradient descent\nWhat we actually want to minimize is the mathematical expectation of the square error (or any other loss) on the distribution of the data.\n\n    \\mathcal{L}(\\theta) = \\mathbb{E}_\\mathcal{D} (||\\textbf{t} - \\textbf{y}||^2)\n\nWe do not have access to the true distribution of the data, so we have to estimate it through sampling.\n\nBatch gradient descent estimates the loss function by sampling the whole training set:\n\n\n    \\mathcal{L}(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^N ||\\textbf{t}_i - \\textbf{y}_i||^2\n\nThe estimated gradient is then unbiased (exact) and has no variance. Batch GD gets stuck in local minima.\n\nOnline gradient descent estimates the loss function by sampling a single example:\n\n\n    \\mathcal{L}(\\theta) \\approx ||\\textbf{t}_i - \\textbf{y}_i||^2\n\nThe estimated gradient has a high variance (never right) but is unbiased on average. Online GD avoids local minima, but also global minima (unstable)…\n\nStochastic gradient descent samples minibatches of K ~ 100 examples to approximate the mathematical expectation.\n\n\n    \\mathcal{L}(\\theta) = E_\\mathcal{D} (||\\textbf{t} - \\textbf{y}||^2) \\approx \\frac{1}{K} \\sum_{i=1}^K ||\\textbf{t}_i - \\textbf{y}_i||^2\n\n\n    \\Delta \\theta = - \\eta \\, \\nabla_\\theta  \\, \\mathcal{L}(\\theta)\n\nThis sampled loss has a high variance: take another minibatch and the gradient of the loss function will likely be very different. If the batch size is big enough, the estimated gradient is wrong, but usable on average (unbiased). The high variance of the estimated gradient helps getting out of local minimum: because our estimation of the gradient is often wrong, we get out of the local minima although we should have stayed in it. The true gradient is 0 for a local minimum, but its sampled value may not, so the parameters will be updated and hopefully get out of the local minimum. Which batch size works the best for your data? You need to use cross-validation, but beware that big batch sizes increase memory consumption, what can be a problem on GPUs.\nAnother issue with stochastic gradient descent is that it uses the same learning rate for all parameters. In ravines (which are common around minima), some parameters (or directions) have a higher influence on the loss function than others.\n\n\n\nRavine in the loss function. Source: https://distill.pub/2017/momentum/.\n\n\nIn the example above, you may want to go faster in the “horizontal” direction than in the “vertical” one, although the gradient is very small in the horizontal direction. With a fixed high learning rate for all parameters, SGD would start oscillating for the steep parameters, while being still very slow for the flat ones. The high variance of the sampled gradient is detrimental to performance as it can lead to oscillations. Most modern optimizers have a parameter-dependent adaptive learning rate.\n\n\nSGD with momentum\nOne solution is to smooth the gradients over time (i.e. between minibatches), in order to avoid that one parameter is increased by one minibatch and decreased by the next one. The momentum method uses a moving average of the gradient (momentum step) to update the parameters:\n\n    v(\\theta) = \\alpha \\, v(\\theta) - (1 - \\alpha)  \\, \\nabla_\\theta  \\, \\mathcal{L}(\\theta)\n\n\n    \\Delta \\theta = \\eta \\,  v(\\theta)\n\n0 \\leq \\alpha < 1 controls how much of the gradient we use for the parameter update (usually around 0.9). \\alpha=0 is the vanilla SGD.\n\n\n\n\n\nWhen the gradient for a single parameter has always the same direction between successive examples, gradient descent accelerates (bigger steps). When its sign changes, the weight changes continue in the same direction for while, allowing to “jump” over small local minima if the speed is sufficient. If the gradient keeps being in the opposite direction, the weight changes will finally reverse their direction. SGD with momentum uses an adaptive learning rate: the learning is implictly higher when the gradient does not reverse its sign (the estimate “accelerates”).\n\n\n\nThe momentum dampens oscillations around ravines. Source: https://distill.pub/2017/momentum/.\n\n\nWith momentum, the flat parameters keep increasing their update speed, while the steep ones slow down. SGD with momentum gets rid of oscillations at higher learning rates. The momentum method benefits a lot from the variance of SGD: noisy gradients are used to escape local minima but are averaged around the global minimum.\n\n\n\n\n\n\nNote\n\n\n\nCheck the great visualization by Gabriel Goh on https://distill.pub/2017/momentum/.\n\n\n\n\nSGD with Nesterov momentum\n\n\n\nThe momentum method tends to oscillate around the global minimum. Source: https://ikocabiyik.com/blog/en/visualizing-ml-optimizers/.\n\n\nSGD with momentum tends to oscillate around the minimum. The Nesterov momentum corrects these oscillations by estimating the gradient after the momentum update:\n\n    v(\\theta) = \\alpha \\, v(\\theta) - (1 - \\alpha) \\, \\nabla_\\theta  \\, \\mathcal{L}(\\theta \\color{red}{+ \\alpha \\, v(\\theta)})\n\n\n    \\Delta \\theta = \\eta \\,  v(\\theta)\n\n\n\n\nDifference between the momentum and Nesterov momentum. Source: https://cs231n.github.io/neural-networks-3/\n\n\n\n\nRMSprop\nInstead of smoothing the gradient, what destroys information, one could adapt the learning rate to the curvature of the loss function:\n\nput the brakes on when the function is steep (high gradient).\naccelerate when the loss function is flat (plateau).\n\nRMSprop (Root Mean Square Propagation, proposed by Geoffrey Hinton in his lecture http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf) scales the learning rate by a running average of the squared gradient (second moment \\approx variance).\n\n    v(\\theta) = \\alpha \\, v(\\theta) + (1 - \\alpha) \\, (\\nabla_\\theta  \\, \\mathcal{L}(\\theta))^2\n\n\n    \\Delta \\theta = - \\frac{\\eta}{\\epsilon + \\sqrt{v(\\theta)}} \\, \\nabla_\\theta  \\, \\mathcal{L}(\\theta)\n\nIf the gradients vary a lot between two minibatches, the learning rate is reduced. If the gradients do not vary much, the learning rate is increased.\n\n\nAdam\nAdam (Adaptive Moment Estimation, (Kingma and Ba, 2014)) builds on the idea of RMSprop, but uses also a moving average of the gradient.\n\n    m(\\theta) = \\beta_1 m(\\theta) + (1 - \\beta_1) \\, \\nabla_\\theta  \\, \\mathcal{L}(\\theta)\n \n    v(\\theta) = \\beta_2 v(\\theta) + (1 - \\beta_2) \\, \\nabla_\\theta  \\, \\mathcal{L}(\\theta)^2\n \n    \\Delta \\theta = - \\eta \\, \\frac{m(\\theta)}{\\epsilon + \\sqrt{v(\\theta)}}\n\nIn short: Adam = RMSprop + momentum. Other possible optimizers: Adagrad, Adadelta, AdaMax, Nadam…\n\n\nComparison of modern optimizers\nThe different optimizers build on the idea of gradient descent and try to fix the main issues. They have different convergence properties, which can be seen in the figures below.\nIn practice, SGD with momentum allows to find better solutions (global minimum), but the meta-parameters are harder to find (need for cross-validation). Adam finds slightly poorer solutions, but the parameters \\beta_1, \\beta_2 and \\epsilon can usually be kept at default, so it is a good idea to start with it, find the NN architecture that solves the problem and then replace it with SGD+momentum to fine-tune the performance.\n\n\n\nSource: Alec Radford https://imgur.com/a/Hqolp\n\n\n\n\n\nSource: Alec Radford https://imgur.com/a/Hqolp.\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe different optimizers are available in keras, see https://keras.io/api/optimizers.\n\nSGD:\n\noptimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n\nSGD with Nesterov momentum:\n\noptimizer = tf.keras.optimizers.SGD(\n    learning_rate=0.01, momentum=0.9, \n    nesterov=True)\n\nRMSprop:\n\noptimizer = tf.keras.optimizers.RMSprop(\n    learning_rate=0.001, rho=0.9, \n    momentum=0.0, epsilon=1e-07\n)\n\nAdam:\n\noptimizer = tf.keras.optimizers.Adam(\n    learning_rate=0.001, beta_1=0.9, \n    beta_2=0.999, epsilon=1e-07)\n\n\n\n\n\nHyperparameters annealing\nFinding the optimal value for the hyperparameters (or metaparameters) of the network is not easy: learning rate \\eta, momentum \\alpha, etc.\n\n\n\nFinding the optimal learning rate is difficult. Source: https://cs231n.github.io/neural-networks-3/\n\n\nFor example, choosing \\eta too small leads to very slow learning. Choosing it too big can lead to oscillations and prevent convergence. A better strategy is to start with a big learning rate to “roughly” find the position of the global minimum and progressively decrease its value for a better convergence:\n\n    \\eta \\leftarrow (1 - \\beta) \\, \\eta\n\nThe decrease can be applied after each epoch. \\beta is called the decay rate, usually very small (10^{-6}). The method is called annealing or scheduling.\n\n\n\nExponentially-decaying learning rate. Source: https://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1\n\n\nA simple trick to find a good estimate of the learning rate (or its start/stop value) is to increase its value exponentially for each minibatch at the beginning of learning. The “good” region for the learning rate is the one where the validation loss decreases, but does not oscillate.\n\n\n\nEstimating the optimal range for the learning rate by increasing its value at the beginning of learning. Source https://towardsdatascience.com/advanced-topics-in-neural-networks-f27fbcc638ae.\n\n\n\n\nHyperparameter search\nEven with annealing, it is tricky to find the optimal value of the hyperparameters. The only option is to perform cross-validation, by varying the hyperparameters systematically and initializing the weights randomly every time. There are two basic strategies:\n\nGrid search: different values of each parameter are chosen linearly ([0.1, 0.2, \\ldots, 0.9]) or logarithmically ([10^{-6}, 10^{-5}, \\ldots, 10^{-1}]).\nRandom search: the value are randomly chosen each time from some distribution (uniform, normal, lognormal).\n\n\n\n\nGrid-search vs. random search. Source: http://cs231n.github.io/neural-networks-3/\n\n\nThe advantage of random search is that you can stop it anytime if you can not wait any longer. Grid search is very time-consuming, but easy to perform in parallel if you have clusters of CPUs or GPUs (data-parallel).\nA more advanced and efficient technique is Bayesian hyperparameter optimization, for example the Tree Parzen Estimator (TPE) algorithm. The idea is to build a probability model of the objective function and use it to select the most promising hyperparameters to evaluate in the true objective function. Roughly speaking, it focuses parameter sampling on the interesting regions.\nThe hyperopt Python library https://github.com/hyperopt/hyperopt is extremely simple to use:\nfrom hyperopt import fmin, tpe, hp, STATUS_OK\n\ndef objective(eta):\n    # Train model with:\n    optimizer = tf.keras.optimizers.SGD(eta)\n    return {'loss': test_loss, 'status': STATUS_OK }\n\nbest = fmin(objective,\n    space=hp.loguniform('eta', -6, -1),\n    algo=tpe.suggest,\n    max_evals=100)\n\nprint best"
  },
  {
    "objectID": "notes/3.2-DNN.html#long-training-times",
    "href": "notes/3.2-DNN.html#long-training-times",
    "title": "Modern neural networks",
    "section": "Long training times",
    "text": "Long training times\n\n\nImportance of normalization\n\n\n\n\n\nIf the data is not centered in the input space, the hyperplane (i.e. each neuron) may need a lot of iterations to “move” to the correct position using gradient descent. The initialization of the weights will matter a lot: if you start too far away from the solution, you will need many iterations.\nIf the data is normalized (zero mean, unit variance), the bias can be initialized to 0 and will converge much faster. Only the direction of the weight vector matters, not its norm, so it will be able to classify the data much faster.\n\n\n\nInput data normalization. Source: http://cs231n.github.io/neural-networks-2/\n\n\nIn practice, the input data X must be normalized before training, in order to improve the training time:\n\nMean removal or zero-centering:\n\n\n    X' = X - \\mathbb{E}(X)\n\n\nNormalization : mean removal + unit variance:\n\n\n    X' = \\frac{X - \\mathbb{E}(X)}{\\text{Std}(X)}\n\nWhitening goes one step further by first decorrelating the input dimensions (using Principal Component Analysis - PCA) and then scaling them so that the data lies in the unit sphere. It is better method than simple data normalization, but computationally expensive. When predicting on new data, do not forget to normalize/whiten them too!\n\n\n\nInput data whitening. Source: http://cs231n.github.io/neural-networks-2/\n\n\n\n\nBatch normalization\nA single layer can learn very fast if its inputs are normalized with zero mean and unit variance. This is easy to do for the first layer, as one only need to preprocess the inputs \\mathbf{x}, but not the others. The outputs of the first layer are not normalized anymore, so learning in the second layer will be slow.\n\n\n\n\n\nBatch normalization (Ioffe and Szegedy, 2015) allows each layer to normalize its inputs on a single minibatch:\n\n    X_\\mathcal{B}' = \\frac{X_\\mathcal{B} - E(X_\\mathcal{B})}{\\text{Std}(X_\\mathcal{B})}\n\nThe mean and variance will vary from one minibatch to another, but it does not matter. At the end of learning, the mean and variance over the whole training set is computed and stored. BN allows to more easily initialize the weights relative to the input strength and to use higher learning rates.\nThe Batch Normalization layer is usually placed between the FC layer and the activation function.It is differentiable w.r.t the input layer and the parameters, so backpropagation still works.\n\n\n\nA batch normalization layer should be introduced between the linear net activation and the activation function. Source: http://heimingx.cn/2016/08/18/cs231n-neural-networks-part-2-setting-up-the-Data-and-the-loss/\n\n\n\n\nWeight initialization\nWeight matrices are initialized randomly, but how they are initialized impacts performance a lot There are empirical rules to initialize the weights between two layers with N_{\\text{in}} and N_{\\text{out}} neurons.\n\nXavier: Uniform initialization (when using logistic or tanh, (Glorot and Bengio, 2010)):\n\n\n    W \\in \\mathcal{U}( - \\sqrt{\\frac{6}{N_{\\text{in}}+N_{\\text{out}}}} , \\sqrt{\\frac{6}{N_{\\text{in}}+N_{\\text{out}}}}  )\n\n\nHe: Gaussian initialization (when using ReLU or PReLU, (He et al., 2015)):\n\n\n    W \\in \\mathcal{N}( 0 , \\sqrt{\\frac{2}{N_{\\text{in}} }} )\n\n\nWhen using BN, the bias b can be initialized to 0.\nMost frameworks (tensorflow, pytorch) initialize the weights correctly for you, but you can also control it."
  },
  {
    "objectID": "notes/3.2-DNN.html#overfitting",
    "href": "notes/3.2-DNN.html#overfitting",
    "title": "Modern neural networks",
    "section": "Overfitting",
    "text": "Overfitting\n\nThe main problem with deep NN is overfitting. With increasing depth, the network has too many weights = free parameters, so its VC dimension is high.\n\n    \\epsilon = \\frac{\\text{VC}_\\text{dim}}{N}\n\nThe training error will be very small, but the generalization error high. The network learns the data, not the underlying function.\nWe need to put constraints on the weights to reduce the VC dimension.\n\nIf the weights move freely (i.e. can take any value), the VC dimension is equal to the number of free parameters.\nIf the weights cannot take any value they like, this implicitely reduces the VC dimension.\n\nIn linear classification, the weights were unconstrained: the norm of the weight vector can take any value, as only its direction is important.\nIntuition: The norm of the weight vector influences the speed of learning in linear classification. A weight update on a strong weight has less influence than on a weak weight:\n\n    W \\leftarrow W + \\Delta W = W - \\eta \\, \\frac{\\partial \\mathcal{l}(\\theta)}{\\partial W}\n\nas the gradient \\frac{\\partial \\mathcal{l}(\\theta)}{\\partial W} does not depend on the norm of the weights, only the output error.\n\nL2 and L1 Regularization\n\\mathcal{L}_2 regularization keeps the \\mathcal{L}_2 norm of the free parameters ||\\theta|| as small as possible during learning.\n\n    ||\\theta||^2 = w_1^2 + w_2^2 + \\dots + w_M^2\n\nEach neuron will use all its inputs with small weights, instead on specializing on a small part with high weights. Two things have to be minimized at the same time: the training loss and a penalty term representing the norm of the weights:\n\n    \\mathcal{L}(\\theta) = \\mathbb{E}_\\mathcal{D}( ||\\mathbf{t} - \\mathbf{y}||^2) + \\lambda \\, ||\\theta||^2\n\nThe regularization parameter \\lambda controls the strength of regularization:\n\nif \\lambda is small, there is only a small regularization, the weights can increase.\nif \\lambda is high, the weights will be kept very small, but they may not minimize the training loss.\n\nExample of the mse loss with \\mathcal{L}_2 regularization penalty term:\n\n    \\mathcal{L}(\\theta) = \\mathbb{E}_\\mathcal{D} [||\\mathbf{t} - \\mathbf{y}||^2] + \\lambda \\, ||\\theta||^2\n\nThe gradient of the new loss function is easy to find:\n\n    \\nabla_\\theta \\mathcal{L}(\\theta) = - 2 \\, (\\mathbf{t} - \\mathbf{y}) \\nabla_\\theta \\mathbf{y} + 2 \\, \\lambda \\, \\theta\n\nThe parameter updates become:\n\n    \\Delta \\theta = \\eta \\, (\\mathbf{t} - \\mathbf{y}) \\nabla_\\theta \\mathbf{y} - \\eta \\, \\lambda \\, \\theta\n\n\\mathcal{L}_2 regularization leads to weight decay: even if there is no output error, the weight will converge to 0. This forces the weight to constantly learn: it can not specialize on a particular example anymore (overfitting) and is forced to generalize.\n\\mathcal{L}_1 regularization penalizes the absolute value of the weights instead of their Euclidian norm:\n\\mathcal{L}(\\theta) = \\mathbb{E}_\\mathcal{D} [||\\mathbf{t} - \\mathbf{y}||^2] + \\lambda \\, |\\theta|\nIt leads to very sparse representations: a lot of neurons will be inactive, and only a few will represent the input.\n\n\nDropout\nRandomly dropping (inactivating) some neurons with a probability p between two input presentations reduces the number of free parameters available for each learning phase. Multiple smaller networks (smaller VC dimension) are in fact learned in parallel on different data, but they share some parameters. This dropout method forces the network to generalize (Srivastava et al., 2014). It is a form of regularization (mathematically equivalent to L2), now preferred in deep networks. p is usually around 0.5.\n\n\n\nEach new input \\mathbf{x} (or minibatch of inputs) is learned by a different neural network, but on average, the big neural network has learned the whole dataset without overfitting. Source: https://towardsdatascience.com/preventing-deep-neural-network-from-overfitting-953458db800a\n\n\n\n\nData augmentation\nThe best way to avoid overfitting is to use more data (with variability), but this is not always possible. A simple trick to have more data is data augmentation, i.e. modifying the inputs while keeping the output constant. For object recognition, it consists of applying various affine transformations (translation, rotation, scaling) on each input image, while keeping the label constant. This allows virtually infinite training sets.\n\n\n\nData augmentation. Source : https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\n\n\n\n\nEarly-Stopping\nEarly-stopping fights overfitting by monitoring the model’s performance on a validation set. A validation set is a set of examples that we never use for gradient descent, but which is also not a part of the test set. If the model’s performance ceases to improve sufficiently on the validation set, or even degrades with further optimization, we can either stop learning or modify some meta-parameters (learning rate, momentum, regularization…). The validation loss is usually lower than the training loss at the beginning of learning (underfitting), but becomes higher when the network overfits.\n\n\n\nEarly-stopping by checking the validation loss during training."
  },
  {
    "objectID": "notes/3.2-DNN.html#vanishing-gradient",
    "href": "notes/3.2-DNN.html#vanishing-gradient",
    "title": "Modern neural networks",
    "section": "Vanishing gradient",
    "text": "Vanishing gradient\n\n\nPrinciple\nContrary to what we could think, adding more layers to a DNN does not necessarily lead to a better performance, both on the training and test set. Here is the performance of neural networks with 20 or 56 layers on CIFAR-10:\n\n\n\nAdding more layers does not necessarily increase the training or test accuracy on CIFAR-10. Source: https://towardsdatascience.com/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\n\n\nThe main reason behind this is the vanishing gradient problem. The gradient of the loss function is repeatedly multiplied by a weight matrix W as it travels backwards in a deep network.\n\n    \\frac{\\partial \\mathbf{h}_{k}}{\\partial \\mathbf{h}_{k-1}} = f' (W^k \\, \\mathbf{h}_{k-1} + \\mathbf{b}^k) \\, W^k\n\nWhen it arrives in the first FC layer, the contribution of the weight matrices is comprised between:\n\n    (W_\\text{min})^d \\quad \\text{and} \\quad (W_\\text{max})^d\n\nwhere W_\\text{max} (resp. W_\\text{min}) is the weight matrix with the highest (resp. lowest) norm, and d is the depth of the network.\n\nIf |W_\\text{max}| < 1, then (W_\\text{max})^d is very small for high values of d : the gradient vanishes.\nIf |W_\\text{min}| > 1, then (W_\\text{min})^d is very high for high values of d : the gradient explodes.\n\nExploding gradients can be solved by gradient clipping, i.e. normalizing the backpropagated gradient if its norm exceeds a threshold.\n\n    || \\frac{\\partial \\mathcal{L}(\\theta)}{\\partial W^k}|| \\gets \\min(||\\frac{\\partial \\mathcal{L}(\\theta)}{\\partial W^k}||, K)\n\nVanishing gradients are still the current limitation of deep networks. The solutions include: ReLU activation functions, unsupervised pre-training, batch normalization, residual networks…\n\n\n\nVanishing gradient problem. Source: https://smartstuartkim.wordpress.com/2019/02/09/vanishing-gradient-problem/\n\n\n\n\nDerivative of the activation function\n\n\n\nThe derivative of the logistic function is 0 for extreme values, what blocks the backpropagation of the gradient.\n\n\nOld-school MLP used logistic or tanh transfer functions for the hidden neurons, but their gradient is zero for very high or low net activations. If a neuron is saturated, it won’t transmit the gradient backwards, so the vanishing gradient is even worse. Deep networks now typically use the ReLU (Maas et al., 2013) or PReLU activation functions to improve convergence.\n\n    f'(x) = \\begin{cases} 1 \\qquad \\text{if } x > 0 \\\\\n                          \\alpha \\qquad \\text{if } x \\leq 0 \\\\\n            \\end{cases}\n\nPReLU always backpropagates the gradient, so it helps fighting against vanishing gradient."
  },
  {
    "objectID": "notes/3.2-DNN.html#deep-neural-networks-in-practice",
    "href": "notes/3.2-DNN.html#deep-neural-networks-in-practice",
    "title": "Modern neural networks",
    "section": "Deep neural networks in practice",
    "text": "Deep neural networks in practice\nThe definition of a deep NN in keras can be as simple as:\nfrom tf.keras.models import Sequential\nfrom tf.keras.layers import Input, Dense, Dropout, Activation, BatchNormalization\nfrom tf.keras.optimizers import Adam\n\nmodel = Sequential()\n\nmodel.add(Input(784,))\n\nmodel.add(Dense(200))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(100)\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(units=10, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy',\n    optimizer=Adam(lr=0.01, decay=1e-6), \n    metrics=['accuracy'] \n)\nIf you want to successfully train a deep neural network, you should:\n\nUse as much data as possible, with data augmentation if needed.\nNormalize the inputs.\nUse batch normalization in every layer and at least ReLU.\nUse a good optimizer (SGD with momentum, Adam).\nRegularize learning (L2, L1, dropout).\nTrack overfitting on the validation set and use early-stopping.\nSearch for the best hyperparameters using grid search or hyperopt:\n\nLearning rate, schedule, momentum, dropout level, number of layers/neurons, transfer functions, etc.\n\n\n\n\n\n\nGlorot, X., and Bengio, Y. (2010). Understanding the difficulty of training deep feedforward neural networks. in AISTATS, 8.\n\n\nHe, K., Zhang, X., Ren, S., and Sun, J. (2015). Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification. http://arxiv.org/abs/1502.01852.\n\n\nIoffe, S., and Szegedy, C. (2015). Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. http://arxiv.org/abs/1502.03167.\n\n\nKingma, D., and Ba, J. (2014). Adam: A Method for Stochastic Optimization. in Proc. ICLR, 1–13. doi:10.1145/1830483.1830503.\n\n\nLi, H., Xu, Z., Taylor, G., Studer, C., and Goldstein, T. (2018). Visualizing the Loss Landscape of Neural Nets. http://arxiv.org/abs/1712.09913.\n\n\nMaas, A. L., Hannun, A. Y., and Ng, A. Y. (2013). Rectifier Nonlinearities Improve Neural Network Acoustic Models. in ICML, 6.\n\n\nSrivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. (2014). Dropout: A Simple Way to Prevent Neural Networks from Overfitting. Journal of Machine Learning Research 15, 1929–1958. http://jmlr.org/papers/v15/srivastava14a.html."
  },
  {
    "objectID": "notes/4.1-CNN.html#convolutional-neural-networks",
    "href": "notes/4.1-CNN.html#convolutional-neural-networks",
    "title": "Convolutional neural networks",
    "section": "Convolutional neural networks",
    "text": "Convolutional neural networks\n\n\nRationale\nThe different layers of a deep network extract increasingly complex features.\n\nedges \\rightarrow contours \\rightarrow shapes \\rightarrow objects\n\n\n\n\n\n\nUsing full images as inputs leads to an explosion of the number of weights to be learned: A moderately big 800 * 600 image has 480,000 pixels with RGB values. The number of dimensions of the input space is 800 * 600 * 3 = 1.44 million. Even if you take only 1000 neurons in the first hidden layer, you get 1.44 billion weights to learn, just for the first layer. To obtain a generalization error in the range of 10%, you would need at least 14 billion training examples…\n\\epsilon \\approx \\frac{\\text{VC}_\\text{dim}}{N}\n\n\n\nFully-connected layers require a lot of weights in images.\n\n\nEarly features (edges) are usually local, there is no need to learn weights from the whole image. Natural images are stationary: the statistics of the pixel in a small patch are the same, regardless the position on the image. Idea: One only needs to extract features locally and share the weights between the different locations. This is a convolution operation: a filter/kernel is applied on small patches and slided over the whole image.\n\n\n\nConvolutional layers share weights along the image dimensions.\n\n\n\n\nThe convolutional layer\n\n\n\nPrinciple of a convolution. Source: https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53\n\n\nIn a convolutional layer, d filters are defined with very small sizes (3x3, 5x5…). Each filter is convoluted over the input image (or the previous layer) to create a feature map. The set of d feature maps becomes a new 3D structure: a tensor.\n\\mathbf{h}_k = W_k \\ast \\mathbf{h}_{k-1} + \\mathbf{b}_k\n\n\n\nConvolutional layer. Source: http://cs231n.github.io/convolutional-networks/\n\n\nIf the input image is 32x32x3, the resulting tensor will be 32x32xd. The convolutional layer has only very few parameters: each feature map has 3x3x3 values in the filter plus a bias, i.e. 28 parameters. As in image processing, a padding method must be chosen (what to do when a pixel is outside the image).\n\n\n\nConvolution with stride 1. Source: https://github.com/vdumoulin/conv_arithmetic\n\n\n\n\nMax-pooling layer\nThe number of elements in a convolutional layer is still too high. We need to reduce the spatial dimension of a convolutional layer by downsampling it. For each feature, a max-pooling layer takes the maximum value of a feature for each subregion of the image (generally 2x2).Mean-pooling layers are also possible, but they are not used anymore. Pooling allows translation invariance: the same input pattern will be detected whatever its position in the input image.\n\n\n\nPooling layer. Source: http://cs231n.github.io/convolutional-networks/\n\n\n\n\n\nPooling layer. Source: http://cs231n.github.io/convolutional-networks/\n\n\n\n\nConvolution with strides\nConvolution with strides (Springenberg et al., 2015) is an alternative to max-pooling layers. The convolution simply “jumps” one pixel when sliding over the image (stride 2). This results in a smaller feature map, using much less operations than a convolution with stride 1 followed by max-pooling, for the same performance. They are particularly useful for generative models (VAE, GAN, etc).\n\n\n\nConvolution with stride 2. Source: https://github.com/vdumoulin/conv_arithmetic\n\n\n\n\nDilated convolutions\nA dilated convolution is a convolution with holes (à trous). The filter has a bigger spatial extent than its number of values.\n\n\n\nConvolution à trous. Source: https://github.com/vdumoulin/conv_arithmetic\n\n\n\n\nBackpropagation through a convolutional layer\nBut how can we do backpropagation through a convolutional layer?\n\n\n\nForward convolution. Source: https://medium.com/@mayank.utexas/backpropagation-for-convolution-with-strides-8137e4fc2710\n\n\nIn the example above, the four neurons of the feature map will receive a gradient from the upper layers. How can we use it to learn the filter values and pass the gradient to the lower layers?\nThe answer is simply by convolving the output gradients with the flipped filter!\n\n\n\nBackward convolution. Source: https://medium.com/@mayank.utexas/backpropagation-for-convolution-with-strides-8137e4fc2710\n\n\nThe filter just has to be flipped (180^o symmetry) before the convolution.\n\n\n\nFlipping the filter. Source: https://medium.com/@mayank.utexas/backpropagation-for-convolution-with-strides-8137e4fc2710\n\n\nThe convolution operation is differentiable, so we can apply backpropagation and learn the filters.\n\\mathbf{h}_k = W_k \\ast \\mathbf{h}_{k-1} + \\mathbf{b}_k\n\\frac{\\partial \\mathcal{L}(\\theta)}{\\partial \\mathbf{h}_{k-1}} = W_k^F \\ast \\frac{\\partial \\mathcal{L}(\\theta)}{\\partial \\mathbf{h}_{k}}\n\n\nBackpropagation through a max-pooling layer\nWe can also use backpropagation through a max-pooling layer. We need to remember which location was the winning location in order to backpropagate the gradient. A max-pooling layer has no parameter, we do not need to learn anything, just to pass the gradient backwards.\n\n\n\nBackpropagation through a max-pooling layer. Source: https://mukulrathi.com/demystifying-deep-learning/conv-net-backpropagation-maths-intuition-derivation/\n\n\n\n\nConvolutional Neural Networks\nA convolutional neural network (CNN) is a cascade of convolution and pooling operations, extracting layer by layer increasingly complex features. The spatial dimensions decrease after each pooling operation, but the number of extracted features increases after each convolution. One usually stops when the spatial dimensions are around 7x7. The last layers are fully connected (classical MLP). Training a CNN uses backpropagation all along: the convolution and pooling operations are differentiable.\n\n\n\nConvolutional Neural Network. (LeCun et al., 1998).\n\n\n\n\n\n\n\n\nImplementing a CNN in keras\n\n\n\nConvolutional and max-pooling layers are regular objects in keras/tensorflow/pytorch/etc. You do not need to care about their implementation, they are designed to run fast on GPUs. You have to apply to the CNN all the usual tricks: optimizers, dropout, batch normalization, etc.\nmodel = Sequential()\nmodel.add(Input(X_train.shape[1:]))\n\nmodel.add(Conv2D(32, (3, 3), padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(Conv2D(32, (3, 3)))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(64, (3, 3), padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(Conv2D(64, (3, 3)))L\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\nmodel.add(Dense(512))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes))\nmodel.add(Activation('softmax'))\n\nopt = RMSprop(\n    lr=0.0001,\n    decay=1e-6\n)\n\nmodel.compile(\n    loss='categorical_crossentropy',\n    optimizer=opt,\n    metrics=['accuracy']\n)"
  },
  {
    "objectID": "notes/4.1-CNN.html#some-famous-convolutional-networks",
    "href": "notes/4.1-CNN.html#some-famous-convolutional-networks",
    "title": "Convolutional neural networks",
    "section": "Some famous convolutional networks",
    "text": "Some famous convolutional networks\n\n\nNeocognitron\nThe Neocognitron (Fukushima, 1980 (Fukushima, 1980)) was actually the first CNN able to recognize handwritten digits. Training is not based on backpropagation, but a set of biologically realistic learning rules (Add-if-silent, margined WTA). Inspired by the human visual system.\n\n\n\nNeocognitron. (Fukushima, 1980). Source: https://uplLoad.wikimedia.org/wikipedia/uk/4/42/Neocognitron.jpg\n\n\n\n\nLeNet\nLeNet (1998, Yann LeCun at AT&T labs (LeCun et al., 1998)) was one of the first CNN able to learn from raw data using backpropagation. It has two convolutional layers, two mean-pooling layers, two fully-connected layers and an output layer. It uses tanh as the activation function and works on CPU only. Used for handwriting recognition (for example ZIP codes).\n\n\n\nLeNet5. (LeCun et al., 1998).\n\n\n\n\nAlexNet\nAlexNet (2012, Toronto University (Krizhevsky et al., 2012)) started the DL revolution by winning ImageNet 2012. I has a similar architecture to LeNet, but is trained on two GPUs using augmented data. It uses ReLU, max-pooling, dropout, SGD with momentum, L2 regularization.\n\n\n\nAlexnet (Krizhevsky et al., 2012).\n\n\n\n\nVGG-16\nVGG-16 (2014, Visual Geometry Group, Oxford (Simonyan and Zisserman, 2015)) placed second at ImageNet 2014. It went much deeper than AlexNet with 16 parameterized layers (a VGG-19 version is also available with 19 layers). Its main novelty is that two convolutions are made successively before the max-pooling, implicitly increasing the receptive field (2 consecutive 3x3 filters cover 5x5 pixels). Drawback: 140M parameters (mostly from the last convolutional layer to the first fully connected) quickly fill up the memory of the GPU.\n\n\n\nVGG-16 (Simonyan and Zisserman, 2015).\n\n\n\n\nGoogLeNet - Inception v1\nGoogLeNet (2014, Google Brain (Szegedy et al., 2015)) used Inception modules (Network-in-Network) to further complexify each stage It won ImageNet 2014 with 22 layers. Dropout, SGD with Nesterov momentum.\n\n\n\nGoogLeNet (Szegedy et al., 2015).\n\n\nInside GoogleNet, each Inception module learns features at different resolutions using convolutions and max poolings of different sizes. 1x1 convolutions are shared MLPS: they transform a (w, h, d_1) tensor into (w, h, d_2) pixel per pixel. The resulting feature maps are concatenated along the feature dimension and passed to the next module.\n\n\n\nInception module (Szegedy et al., 2015).\n\n\nThree softmax layers predict the classes at different levels of the network. The combined loss is:\n\\mathcal{L}(\\theta) = \\mathbb{E}_\\mathcal{D} [- \\mathbf{t} \\, \\log \\mathbf{y}_1 - \\mathbf{t} \\, \\log \\mathbf{y}_2 - \\mathbf{t} \\, \\log \\mathbf{y}_3]\nOnly the deeper softmax layer matters for the prediction. The additional losses improve convergence by fight vanishing gradients: the early layers get useful gradients from the lower softmax layers.\nSeveral variants of GoogleNet have been later proposed: Inception v2, v3, InceptionResNet, Xception… Xception (Chollet, 2017) has currently the best top-1 accuracy on ImageNet: 126 layers, 22M parameters (88 MB). Pretrained weights are available in keras:\ntf.keras.applications.Xception(include_top=True, weights=\"imagenet\")\n\n\n\nInception v3 (Chollet, 2017). Source: https://cloud.google.com/tpu/docs/inception-v3-advanced\n\n\n\n\nResNet\nResNet (2015, Microsoft (He et al., 2015)) won ImageNet 2015. Instead of learning to transform an input directly with \\mathbf{h}_n = f_W(\\mathbf{h}_{n-1}), a residual layer learns to represent the residual between the output and the input:\n\n    \\mathbf{h}_n = f_W(\\mathbf{h}_{n-1}) + \\mathbf{h}_{n-1}  \\quad \\rightarrow \\quad f_W(\\mathbf{h}_{n-1}) = \\mathbf{h}_n - \\mathbf{h}_{n-1}\n\n\n\n\nResidual layer with skip connections (He et al., 2015).\n\n\nThese skip connections allow the network to decide how deep it has to be. If the layer is not needed, the residual layer learns to output 0.\n\n\n\nResNet (He et al., 2015).\n\n\nSkip connections help overcome the vanishing gradients problem, as the contribution of bypassed layers to the backpropagated gradient is 1.\n\\mathbf{h}_n = f_W(\\mathbf{h}_{n-1}) + \\mathbf{h}_{n-1}\n\\frac{\\partial \\mathbf{h}_n}{\\partial \\mathbf{h}_{n-1}} = \\frac{\\partial f_W(\\mathbf{h}_{n-1})}{\\partial \\mathbf{h}_{n-1}} + 1\nThe norm of the gradient stays roughly around one, limiting vanishing. Skip connections even can bypass whole blocks of layers. ResNet can have many layers without vanishing gradients. The most popular variants are:\n\nResNet-50.\nResNet-101.\nResNet-152.\n\nIt was the first network to make an heavy use of batch normalization.\n\n\n\nResidual block (He et al., 2015).\n\n\n\n\nHighNets: Highway networks\nHighway networks (IDSIA (Srivastava et al., 2015)) are residual networks which also learn to balance inputs with feature extraction:\n\n    \\mathbf{h}_n = T_{W'} \\, f_W(h_{n-1}) + (1 -  T_{W'}) \\, h_{n-1}\n\nThe balance between the primary pathway and the skip pathway adapts to the task. It has been used up to 1000 layers and improved state-of-the-art accuracy on MNIST and CIFAR-10.\n\n\n\nHighway network (Srivastava et al., 2015).\n\n\n\n\nDenseNets: Dense networks\nDense networks (Cornell University & Facebook AI (Huang et al., 2018)) are residual networks that can learn bypasses between any layer of the network (up to 5). It has 100 layers altogether and improved state-of-the-art accuracy on five major benchmarks.\n\n\n\nDense network (Huang et al., 2018).\n\n\n\n\nModel zoos\nThese famous models are described in their respective papers, you could reimplement them and train them on ImageNet. Fortunately, their code is often released on Github by the authors or reimplemented by others. Most frameworks maintain model zoos of the most popular networks. Some models also have pretrained weights available, mostly on ImageNet. Very useful for transfer learning (see later).\n\nOverview website:\n\nhttps://modelzoo.co\n\nCaffe:\n\nhttps://github.com/BVLC/caffe/wiki/Model-Zoo\n\nTensorflow:\n\nhttps://github.com/tensorflow/models\n\nPytorch:\n\nhttps://pytorch.org/docs/stable/torchvision/models.html\n\nPapers with code:\n\nhttps://paperswithcode.com/\nSeveral criteria have to be considered when choosing an architecture:\n\nAccuracy on ImageNet.\nNumber of parameters (RAM consumption).\nSpeed (flops).\n\n\n\n\nSpeed-accuracy trade-off of state-of-the-art CNNs. Source: https://dataconomy.com/2017/04/history-neural-networks"
  },
  {
    "objectID": "notes/4.1-CNN.html#applications-of-cnn",
    "href": "notes/4.1-CNN.html#applications-of-cnn",
    "title": "Convolutional neural networks",
    "section": "Applications of CNN",
    "text": "Applications of CNN\n\n\nObject recognition\nObject recognition has become very easy thanks to CNNs. In object recognition, each image is associated to a label. With huge datasets like ImageNet (14 millions images), a CNN can learn to recognize 1000 classes of objects with a better accuracy than humans. Just get enough examples of an object and it can be recognized.\n\n\n\nObject recognition on ImageNet. Source (Krizhevsky et al., 2012).\n\n\n\n\nFacial recognition\nFacebook used 4.4 million annotated faces from 4030 users to train DeepFace (Taigman et al., 2014). Accuracy of 97.35% for recognizing faces, on par with humans. Used now to recognize new faces from single examples (transfer learning, one-shot learning).\n\n\n\nDeepFace. Source (Taigman et al., 2014).\n\n\n\n\nPose estimation\nPoseNet (Kendall et al., 2016) is a Inception-based CNN able to predict 3D information from 2D images. It can be for example the calibration matrix of a camera, 3D coordinates of joints or facial features. There is a free tensorflow.js implementation that can be used in the browser.\n\n\n\nPoseNet (Kendall et al., 2016). Source: https://blog.tensorflow.org/2019/01/tensorflow-lite-now-faster-with-mobile.html\n\n\n\n\nSpeech recognition\nTo perform speech recognition, one could treat speech signals like images: one direction is time, the other are frequencies (e.g. mel spectrum). A CNN can learn to associate phonemes to the corresponding signal. DeepSpeech (Hannun et al., 2014) from Baidu is one of the state-of-the-art approaches. Convolutional networks can be used on any signals where early features are local. It uses additionally recurrent networks, which we will see later.\n\n\n\nDeepSpeech. Source (Hannun et al., 2014).\n\n\n\n\nSentiment analysis\nIt is also possible to apply convolutions on text. Sentiment analysis assigns a positive or negative judgment to sentences. Each word is represented by a vector of values (word2vec). The convolutional layer can slide over all over words to find out the sentiment of the sentence.\n\n\n\nSentiment analysis. Source (Kim, 2014).\n\n\n\n\nWavenet : text-to-speech synthesis\nText-To-Speech (TTS) is also possible using CNNs. Google Home relies on Wavenet (Oord et al., 2016), a complex CNN using dilated convolutions to grasp long-term dependencies.\n\n\n\nWavenet (Oord et al., 2016). Source: https://deepmind.com/blog/wavenet-generative-model-raw-audio/\n\n\n\n\n\nDilated convolutions in Wavenet (Oord et al., 2016). Source: https://deepmind.com/blog/wavenet-generative-model-raw-audio/"
  },
  {
    "objectID": "notes/4.1-CNN.html#transfer-learning",
    "href": "notes/4.1-CNN.html#transfer-learning",
    "title": "Convolutional neural networks",
    "section": "Transfer learning",
    "text": "Transfer learning\n\nMyth: ones needs at least one million labeled examples to use deep learning. This is true if you train the CNN end-to-end with randomly initialized weights. But there are alternatives:\n\nUnsupervised learning (autoencoders) may help extract useful representations using only images.\nTransfer learning allows to re-use weights obtained from a related task/domain.\n\n\n\n\nTransfer learning. Source: http://imatge-upc.github.io/telecombcn-2016-dlcv\n\n\nTake a classical network (VGG-16, Inception, ResNet, etc.) trained on ImageNet (if your task is object recognition).\nOff-the-shelf\n\nCut the network before the last layer and use directly the high-level feature representation.\nUse a shallow classifier directly on these representations (not obligatorily NN).\n\n\n\n\nOff-the-shelf transfer learning. Source: http://imatge-upc.github.io/telecombcn-2016-dlcv\n\n\nFine-tuning\n\nUse the trained weights as initial weight values and re-train the network on your data (often only the last layers, the early ones are frozen).\n\n\n\n\nFine-tuned transfer learning. Source: http://imatge-upc.github.io/telecombcn-2016-dlcv\n\n\n\n\n\n\n\n\nExample of transfer learning\n\n\n\n\n\n\n\n\nMicrosoft wanted a system to automatically detect snow leopards into the wild, but there were not enough labelled images to train a deep network end-to-end. They used a pretrained ResNet50 as a feature extractor for a simple logistic regression classifier.\nSource: https://blogs.technet.microsoft.com/machinelearning/2017/06/27/saving-snow-leopards-with-deep-learning-and-computer-vision-on-spark/\n\n\n\n\n\n\n\n\nTransfer learning in keras\n\n\n\nKeras provides pre-trained CNNs that can be used as feature extractors:\nfrom tf.keras.applications.vgg16 import VGG16\n\n# Download VGG without the FC layers\nmodel = VGG16(include_top=False, \n              input_shape=(300, 300, 3))\n\n# Freeze learning in VGG16\nfor layer in model.layers:\n    layer.trainable = False\n\n# Add a fresh MLP on top\nflat1 = Flatten()(model.layers[-1].output)\nclass1 = Dense(1024, activation='relu')(flat1)\noutput = Dense(10, activation='softmax')(class1)\n\n# New model\nmodel = Model(\n    inputs=model.inputs, outputs=output\n)\nSee https://keras.io/api/applications/ for the full list of pretrained networks."
  },
  {
    "objectID": "notes/4.1-CNN.html#ensemble-learning",
    "href": "notes/4.1-CNN.html#ensemble-learning",
    "title": "Convolutional neural networks",
    "section": "Ensemble learning",
    "text": "Ensemble learning\n\nSince 2016, only ensembles of existing networks win the competitions.\n\n\n\nTop networks on the ImageNet 2016 competition. Source http://image-net.org/challenges/LSVRC/2016/results\n\n\nEnsemble learning is the process of combining multiple independent classifiers together, in order to obtain a better performance. As long the individual classifiers do not make mistakes for the same examples, a simple majority vote might be enough to get better approximations.\n\n\n\nEnsemble of pre-trained CNNs. Source https://flyyufelix.github.io/2017/04/16/kaggle-nature-conservancy.html\n\n\nLet’s consider we have three independent binary classifiers, each with an accuracy of 70% (P = 0.7 of being correct). When using a majority vote, we get the following cases:\n\nall three models are correct:\nP = 0.7 * 0.7 * 0.7 = 0.3492\ntwo models are correct\nP = (0.7 * 0.7 * 0.3) + (0.7 * 0.3 * 0.7) + (0.3 * 0.7 * 0.7) = 0.4409\ntwo models are wrong\nP = (0.3 * 0.3 * 0.7) + (0.3 * 0.7 * 0.3) + (0.7 * 0.3 * 0.3) = 0.189\nall three models are wrong\nP = 0.3 * 0.3 * 0.3 = 0.027\n\nThe majority vote is correct with a probability of P = 0.3492 + 0.4409 = 0.78 ! The individual learners only have to be slightly better than chance, but they must be as independent as possible.\n\nBagging\nBagging methods (bootstrap aggregation) trains multiple classifiers on randomly sampled subsets of the data. A random forest is for example a bagging method for decision trees, where the data and features are sampled.. One can use majority vote, unweighted average, weighted average or even a meta-learner to form the final decision.\n\n\n\nBagging. Source: http://www.sciencedirect.com/science/article/pii/S0957417409008781\n\n\n\n\nBoosting\nBagging algorithms aim to reduce the complexity of models that overfit the training data. Boosting is an approach to increase the complexity of models that suffer from high bias, that is, models that underfit the training data. Algorithms: Adaboost, XGBoost (gradient boosting)…\n\n\n\nBoosting. Source: https://www.analyticsvidhya.com/blog/2015/11/quick-introduction-boosting-algorithms-machine-learning/\n\n\nBoosting is not very useful with deep networks (overfitting), but there are some approaches like SelfieBoost (https://arxiv.org/pdf/1411.3436.pdf).\n\n\nStacking\nStacking is an ensemble learning technique that combines multiple models via a meta-classifier. The meta-model is trained on the outputs of the basic models as features. Winning approach of ImageNet 2016 and 2017. See https://blog.statsbot.co/ensemble-learning-d1dcd548e936\n\n\n\nStacking. Source: doi:10.1371/journal.pone.0024386.g005\n\n\n\n\n\n\nChollet, F. (2017). Xception: Deep Learning with Depthwise Separable Convolutions. http://arxiv.org/abs/1610.02357.\n\n\nFukushima, K. (1980). Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position. Biol. Cybernetics 36, 193–202. doi:10.1007/BF00344251.\n\n\nHannun, A., Case, C., Casper, J., Catanzaro, B., Diamos, G., Elsen, E., et al. (2014). Deep Speech: Scaling up end-to-end speech recognition. http://arxiv.org/abs/1412.5567.\n\n\nHe, K., Zhang, X., Ren, S., and Sun, J. (2015). Deep Residual Learning for Image Recognition. http://arxiv.org/abs/1512.03385.\n\n\nHuang, G., Liu, Z., van der Maaten, L., and Weinberger, K. Q. (2018). Densely Connected Convolutional Networks. http://arxiv.org/abs/1608.06993.\n\n\nKendall, A., Grimes, M., and Cipolla, R. (2016). PoseNet: A Convolutional Network for Real-Time 6-DOF Camera Relocalization. http://arxiv.org/abs/1505.07427.\n\n\nKim, Y. (2014). Convolutional Neural Networks for Sentence Classification. http://arxiv.org/abs/1408.5882.\n\n\nKrizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). ImageNet Classification with Deep Convolutional Neural Networks. in Advances in Neural Information Processing Systems (NIPS) https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf.\n\n\nLeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. (1998). Gradient Based Learning Applied to Document Recognition. Proceedings of the IEEE 86, 2278–2324. doi:10.1109/5.726791.\n\n\nOord, A. van den, Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves, A., et al. (2016). WaveNet: A Generative Model for Raw Audio. http://arxiv.org/abs/1609.03499.\n\n\nSimonyan, K., and Zisserman, A. (2015). Very Deep Convolutional Networks for Large-Scale Image Recognition. International Conference on Learning Representations (ICRL), 1–14. doi:10.1016/j.infsof.2008.09.005.\n\n\nSpringenberg, J. T., Dosovitskiy, A., Brox, T., and Riedmiller, M. (2015). Striving for Simplicity: The All Convolutional Net. http://arxiv.org/abs/1412.6806.\n\n\nSrivastava, R. K., Greff, K., and Schmidhuber, J. (2015). Highway Networks. http://arxiv.org/abs/1505.00387.\n\n\nSzegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna, Z. (2015). Rethinking the Inception Architecture for Computer Vision. http://arxiv.org/abs/1512.00567.\n\n\nTaigman, Y., Yang, M., Ranzato, M., and Wolf, L. (2014). DeepFace: Closing the Gap to Human-Level Performance in Face Verification. in 2014 IEEE Conference on Computer Vision and Pattern Recognition (Columbus, OH, USA: IEEE), 1701–1708. doi:10.1109/CVPR.2014.220."
  },
  {
    "objectID": "notes/4.2-ObjectDetection.html#object-detection",
    "href": "notes/4.2-ObjectDetection.html#object-detection",
    "title": "Object detection",
    "section": "Object detection",
    "text": "Object detection\n\nContrary to object classification/recognition which assigns a single label to an image, object detection requires to both classify object and report their position and size on the image (bounding box).\n\n\n\nObject recognition vs. detection. Source: https://blog.athelas.com/a-brief-history-of-cnns-in-image-segmentation-from-r-cnn-to-mask-r-cnn-34ea83205de4\n\n\nA naive and very expensive method is to use a trained CNN as a high-level filter. The CNN is trained on small images and convolved on bigger images. The output is a heatmap of the probability that a particular object is present.\n\n\n\nUsing a pretrained CNN to generate heatmaps. Source: https://blog.athelas.com/a-brief-history-of-cnns-in-image-segmentation-from-r-cnn-to-mask-r-cnn-34ea83205de4\n\n\nObject detection is both a:\n\nClassification problem, as one has to recognize an object.\nRegression problem, as one has to predict the coordinates (x, y, w, h) of the bounding box.\n\n\n\n\nObject detection is both a classification and regression task. Source: https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e\n\n\nThe main datasets for object detection are the PASCAL Visual Object Classes Challenge (20 classes, ~10K images, ~25K annotated objects, http://host.robots.ox.ac.uk/pascal/VOC/voc2008/) and the MS COCO dataset (Common Objects in COntext, 330k images, 80 labels, http://cocodataset.org)"
  },
  {
    "objectID": "notes/4.2-ObjectDetection.html#r-cnn-regions-with-cnn-features",
    "href": "notes/4.2-ObjectDetection.html#r-cnn-regions-with-cnn-features",
    "title": "Object detection",
    "section": "R-CNN : Regions with CNN features",
    "text": "R-CNN : Regions with CNN features\nR-CNN (Girshick et al., 2014) was one of the first CNN-based architectures allowing object detection.\n\n\n\nR-CNN (Girshick et al., 2014).\n\n\nIt is a pipeline of 4 steps:\n\nBottom-up region proposals by searching bounding boxes based on pixel info (selective search https://ivi.fnwi.uva.nl/isis/publications/2013/UijlingsIJCV2013/UijlingsIJCV2013.pdf).\nFeature extraction using a pre-trained CNN (AlexNet).\nClassification using a SVM (object or not; if yes, which one?)\nIf an object is found, linear regression on the region proposal to generate tighter bounding box coordinates.\n\nEach region proposal is processed by the CNN, followed by a SVM and a bounding box regressor.\n\n\n\nR-CNN (Girshick et al., 2014). Source: https://courses.cs.washington.edu/courses/cse590v/14au/cse590v_wk1_rcnn.pdf\n\n\nThe CNN is pre-trained on ImageNet and fine-tuned on Pascal VOC (transfer learning).\n\n\n\nR-CNN (Girshick et al., 2014). Source: https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e"
  },
  {
    "objectID": "notes/4.2-ObjectDetection.html#fast-r-cnn",
    "href": "notes/4.2-ObjectDetection.html#fast-r-cnn",
    "title": "Object detection",
    "section": "Fast R-CNN",
    "text": "Fast R-CNN\nThe main drawback of R-CNN is that each of the 2000 region proposals have to go through the CNN: extremely slow. The idea behind Fast R-CNN (Girshick, 2015) is to extract region proposals in higher feature maps and to use transfer learning.\n\n\n\nFast R-CNN (Girshick, 2015).\n\n\nThe network first processes the whole image with several convolutional and max pooling layers to produce a feature map. Each object proposal is projected to the feature map, where a region of interest (RoI) pooling layer extracts a fixed-length feature vector. Each feature vector is fed into a sequence of FC layers that finally branch into two sibling output layers:\n\na softmax probability estimate over the K classes plus a catch-all “background” class.\na regression layer that outputs four real-valued numbers for each class.\n\nThe loss function to minimize is a composition of different losses and penalty terms:\n\n    \\mathcal{L}(\\theta) = \\lambda_1 \\, \\mathcal{L}_\\text{classification}(\\theta) + \\lambda_2 \\, \\mathcal{L}_\\text{regression}(\\theta) + \\lambda_3 \\, \\mathcal{L}_\\text{regularization}(\\theta)"
  },
  {
    "objectID": "notes/4.2-ObjectDetection.html#faster-r-cnn",
    "href": "notes/4.2-ObjectDetection.html#faster-r-cnn",
    "title": "Object detection",
    "section": "Faster R-CNN",
    "text": "Faster R-CNN\nBoth R-CNN and Fast R-CNN use selective search to find out the region proposals: slow and time-consuming. Faster R-CNN (Ren et al., 2016) introduces an object detection algorithm that lets the network learn the region proposals. The image is passed through a pretrained CNN to obtain a convolutional feature map. A separate network is used to predict the region proposals. The predicted region proposals are then reshaped using a RoI (region-of-interest) pooling layer which is then used to classify the object and predict the bounding box.\n\n\n\nFaster R-CNN (Ren et al., 2016)."
  },
  {
    "objectID": "notes/4.2-ObjectDetection.html#yolo",
    "href": "notes/4.2-ObjectDetection.html#yolo",
    "title": "Object detection",
    "section": "YOLO",
    "text": "YOLO\n\n(Fast(er)) R-CNN perform classification for each region proposal sequentially: slow. YOLO (You Only Look Once) (Redmon and Farhadi, 2016) applies a single neural network to the full image to predict all possible boxes and the corresponding classes. YOLO divides the image into a SxS grid of cells.\n\n\n\nYOLO (Redmon and Farhadi, 2016).\n\n\nEach grid cell predicts a single object, with the corresponding C class probabilities (softmax). It also predicts the coordinates of B possible bounding boxes (x, y, w, h) as well as a box confidence score. The SxSxB predicted boxes are then pooled together to form the final prediction.\nIn the figure below, the yellow box predicts the presence of a person (the class) as well as a candidate bounding box (it may be bigger than the grid cell itself).\n\n\n\nEach cell predicts a class (e.g. person) and the (x, y, w, h) coordinates of the bounding box. Source: https://medium.com/@jonathan_hui/real-time-object-detection-with-yolo-yolov2-28b1b93e2088\n\n\nIn the original YOLO implementation, each grid cell proposes 2 bounding boxes:\n\n\n\nEach cell predicts two bounding boxes per object. Source: https://medium.com/@jonathan_hui/real-time-object-detection-with-yolo-yolov2-28b1b93e2088\n\n\nEach grid cell predicts a probability for each of the 20 classes, two bounding boxes (4 coordinates per bounding box) and their confidence scores. This makes C + B * 5 = 30 values to predict for each cell.\n\n\n\nEach cell outputs 30 values: 20 for the classes and 5 for each bounding box, including the confidence score. Source: https://medium.com/@jonathan_hui/real-time-object-detection-with-yolo-yolov2-28b1b93e2088\n\n\n\nArchitecture of the CNN\nYOLO uses a CNN with 24 convolutional layers and 4 max-pooling layers to obtain a 7x7 grid. The last convolution layer outputs a tensor with shape (7, 7, 1024). The tensor is then flattened and passed through 2 fully connected layers. The output is a tensor of shape (7, 7, 30), i.e. 7x7 grid cells, 20 classes and 2 boundary box predictions per cell.\n\n\n\nArchitecture of the CNN used in YOLO (Redmon and Farhadi, 2016).\n\n\n\n\nConfidence score\nThe 7x7 grid cells predict 2 bounding boxes each: maximum of 98 bounding boxes on the whole image. Only the bounding boxes with the highest class confidence score are kept.\n\n    \\text{class confidence score = box confidence score * class probability}\n\nIn practice, the class confidence score should be above 0.25.\n\n\n\nOnly the bounding boxes with the highest class confidence scores are kept among the 98 possible ones. Source: (Redmon and Farhadi, 2016).\n\n\n\n\nIntersection over Union (IoU)\nTo ensure specialization, only one bounding box per grid cell should be responsible for detecting an object. During learning, we select the bounding box with the biggest overlap with the object. This can be measured by the Intersection over the Union (IoU).\n\n\n\nThe Intersection over Union (IoU) measures the overlap between bounding boxes. Source: https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/\n\n\n\n\n\nThe Intersection over Union (IoU) measures the overlap between bounding boxes. Source: https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/\n\n\n\n\nLoss functions\nThe output of the network is a 7x7x30 tensor, representing for each cell:\n\nthe probability that an object of a given class is present.\nthe position of two bounding boxes.\nthe confidence that the proposed bounding boxes correspond to a real object (the IoU).\n\nWe are going to combine three different loss functions:\n\nThe categorization loss: each cell should predict the correct class.\nThe localization loss: error between the predicted boundary box and the ground truth for each object.\nThe confidence loss: do the predicted bounding boxes correspond to real objects?\n\nClassification loss\nThe classification loss is the mse between:\n\n\\hat{p}_i(c): the one-hot encoded class c of the object present under each cell i, and\np_i(c): the predicted class probabilities of cell i.\n\n\n    \\mathcal{L}_\\text{classification}(\\theta) =  \\sum_{i=0}^{S^2} \\mathbb{1}_i^\\text{obj} \\sum_{c \\in \\text{classes}} (p_i(c) - \\hat{p}_i(c))^2\n\nwhere \\mathbb{1}_i^\\text{obj} is 1 when there actually is an object behind the cell i, 0 otherwise (background).\nThey could also have used the cross-entropy loss, but the output layer is not a regular softmax layer. Using mse is also more compatible with the other losses.\nLocalization loss\nFor all bounding boxes matching a real object, we want to minimize the mse between:\n\n(\\hat{x}_i, \\hat{y}_i, \\hat{w}_i, \\hat{h}_i): the coordinates of the ground truth bounding box, and\n(x_i, y_i, w_i, h_i): the coordinates of the predicted bounding box.\n\n\n    \\mathcal{L}_\\text{localization}(\\theta) = \\sum_{i=0}^{S^2} \\sum_{j=0}^{B} \\mathbb{1}_{ij}^\\text{obj} [ (x_i - \\hat{x}_i)^2 + (y_i - \\hat{y}_i)^2]\n     + \\sum_{i=0}^{S^2} \\sum_{j=0}^{B} \\mathbb{1}_{ij}^\\text{obj} [ (\\sqrt{w_i} - \\sqrt{\\hat{w}_i})^2 + (\\sqrt{h_i} - \\sqrt{\\hat{h}_i})^2]\n\nwhere \\mathbb{1}_{ij}^\\text{obj} is 1 when the bounding box j of cell i “matches” with an object (IoU). The root square of the width and height of the bounding boxes is used. This allows to penalize more the errors on small boxes than on big boxes.\nConfidence loss\nFinally, we need to learn the confidence score of each bounding box, by minimizing the mse between:\n\nC_i: the predicted confidence score of cell i, and\n\\hat{C}_i: the IoU between the ground truth bounding box and the predicted one.\n\n\n    \\mathcal{L}_\\text{confidence}(\\theta) = \\sum_{i=0}^{S^2} \\sum_{j=0}^{B} \\mathbb{1}_{ij}^\\text{obj} (C_{ij} - \\hat{C}_{ij})^2  \n     + \\lambda^\\text{noobj} \\, \\sum_{i=0}^{S^2} \\sum_{j=0}^{B} \\mathbb{1}_{ij}^\\text{noobj} (C_{ij} - \\hat{C}_{ij})^2\n\nTwo cases are considered:\n\nThere was a real object at that location (\\mathbb{1}_{ij}^\\text{obj} = 1): the confidences should be updated fully.\nThere was no real object (\\mathbb{1}_{ij}^\\text{noobj} = 1): the confidences should only be moderately updated (\\lambda^\\text{noobj} = 0.5)\n\nThis is to deal with class imbalance: there are much more cells on the background than on real objects.\nPut together, the loss function to minimize is:\n\n\\begin{align}\n    \\mathcal{L}(\\theta) & = \\mathcal{L}_\\text{classification}(\\theta) + \\lambda_\\text{coord} \\, \\mathcal{L}_\\text{localization}(\\theta) + \\mathcal{L}_\\text{confidence}(\\theta) \\\\\n              & = \\sum_{i=0}^{S^2} \\mathbb{1}_i^\\text{obj} \\sum_{c \\in \\text{classes}} (p_i(c) - \\hat{p}_i(c))^2 \\\\\n              & + \\lambda_\\text{coord} \\, \\sum_{i=0}^{S^2} \\sum_{j=0}^{B} \\mathbb{1}_{ij}^\\text{obj} [ (x_i - \\hat{x}_i)^2 + (y_i - \\hat{y}_i)^2] \\\\\n              & + \\lambda_\\text{coord} \\, \\sum_{i=0}^{S^2} \\sum_{j=0}^{B} \\mathbb{1}_{ij}^\\text{obj} [ (\\sqrt{w_i} - \\sqrt{\\hat{w}_i})^2 + (\\sqrt{h_i} - \\sqrt{\\hat{h}_i})^2] \\\\\n              & + \\sum_{i=0}^{S^2} \\sum_{j=0}^{B} \\mathbb{1}_{ij}^\\text{obj} (C_{ij} - \\hat{C}_{ij})^2  \\\\\n              & + \\lambda^\\text{noobj} \\, \\sum_{i=0}^{S^2} \\sum_{j=0}^{B} \\mathbb{1}_{ij}^\\text{noobj} (C_{ij} - \\hat{C}_{ij})^2 \\\\\n\\end{align}\n\n\n\nYOLO trained on PASCAL VOC\nYOLO was trained on PASCAL VOC (natural images) but generalizes well to other datasets (paintings…). YOLO runs in real-time (60 fps) on a NVIDIA Titan X. Faster and more accurate versions of YOLO have been developed: YOLO9000 (Redmon et al., 2016), YOLOv3 (Redmon and Farhadi, 2018), YOLOv5 (https://github.com/ultralytics/yolov5)…\n\n\n\nPerformance of YOLO compared to the state of the art. Source: (Redmon and Farhadi, 2016).\n\n\nRefer to the website of the authors for additional information: https://pjreddie.com/darknet/yolo/"
  },
  {
    "objectID": "notes/4.2-ObjectDetection.html#ssd",
    "href": "notes/4.2-ObjectDetection.html#ssd",
    "title": "Object detection",
    "section": "SSD",
    "text": "SSD\nThe idea of SSD (Single-Shot Detector, (Liu et al., 2016)) is similar to YOLO, but:\n\nfaster\nmore accurate\nnot limited to 98 objects per scene\nmulti-scale\n\nContrary to YOLO, all convolutional layers are used to predict a bounding box, not just the final tensor: skip connections. This allows to detect boxes at multiple scales (pyramid).\n\n\n\nSingle-Shot Detector, (Liu et al., 2016)."
  },
  {
    "objectID": "notes/4.2-ObjectDetection.html#d-object-detection",
    "href": "notes/4.2-ObjectDetection.html#d-object-detection",
    "title": "Object detection",
    "section": "3D object detection",
    "text": "3D object detection\nIt is also possible to use depth information (e.g. from a Kinect) as an additional channel of the R-CNN. The depth information provides more information on the structure of the object, allowing to disambiguate certain situations (segmentation).\n\n\n\nLearning Rich Features from RGB-D Images for Object Detection, (Gupta et al., 2014).\n\n\nLidar point clouds can also be used for detecting objects, for example VoxelNet (Zhou and Tuzel, 2017) trained in the KITTI dataset.\n\n\n\nVoxelNet (Zhou and Tuzel, 2017).\n\n\n\n\n\nVoxelNet (Zhou and Tuzel, 2017). Source: https://medium.com/@SmartLabAI/3d-object-detection-from-lidar-data-with-deep-learning-95f6d400399a\n\n\n\n\n\n\n\n\nAdditional resources on object detection\n\n\n\nhttps://medium.com/comet-app/review-of-deep-learning-algorithms-for-object-detection-c1f3d437b852\nhttps://medium.com/@smallfishbigsea/faster-r-cnn-explained-864d4fb7e3f8\nhttps://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e\nhttps://medium.com/@jonathan_hui/real-time-object-detection-with-yolo-yolov2-28b1b93e2088\nhttps://medium.com/@jonathan_hui/ssd-object-detection-single-shot-multibox-detector-for-real-time-processing-9bd8deac0e06\nhttps://towardsdatascience.com/lidar-3d-object-detection-methods-f34cf3227aea\n\n\n\n\n\n\nGirshick, R. (2015). Fast R-CNN. http://arxiv.org/abs/1504.08083.\n\n\nGirshick, R., Donahue, J., Darrell, T., and Malik, J. (2014). Rich feature hierarchies for accurate object detection and semantic segmentation. http://arxiv.org/abs/1311.2524.\n\n\nGupta, S., Girshick, R., Arbeláez, P., and Malik, J. (2014). Learning Rich Features from RGB-D Images for Object Detection and Segmentation. http://arxiv.org/abs/1407.5736.\n\n\nLiu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C.-Y., et al. (2016). SSD: Single Shot MultiBox Detector. 9905, 21–37. doi:10.1007/978-3-319-46448-0_2.\n\n\nRedmon, J., Divvala, S., Girshick, R., and Farhadi, A. (2016). You Only Look Once: Unified, Real-Time Object Detection. http://arxiv.org/abs/1506.02640.\n\n\nRedmon, J., and Farhadi, A. (2016). YOLO9000: Better, Faster, Stronger. http://arxiv.org/abs/1612.08242.\n\n\nRedmon, J., and Farhadi, A. (2018). YOLOv3: An Incremental Improvement. http://arxiv.org/abs/1804.02767.\n\n\nRen, S., He, K., Girshick, R., and Sun, J. (2016). Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks. http://arxiv.org/abs/1506.01497.\n\n\nZhou, Y., and Tuzel, O. (2017). VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection. http://arxiv.org/abs/1711.06396."
  },
  {
    "objectID": "notes/4.3-SemanticSegmentation.html#semantic-segmentation",
    "href": "notes/4.3-SemanticSegmentation.html#semantic-segmentation",
    "title": "Semantic segmentation",
    "section": "Semantic segmentation",
    "text": "Semantic segmentation\nSemantic segmentation is a class of segmentation methods where you use knowledge about the identity of objects to partition the image pixel-per-pixel.\n\n\n\nSemantic segmentation. Source : https://medium.com/nanonets/how-to-do-image-segmentation-using-deep-learning-c673cc5862ef\n\n\nClassical segmentation methods only rely on the similarity between neighboring pixels, they do not use class information. The output of a semantic segmentation is another image, where each pixel represents the class.\nThe classes can be binary, for example foreground/background, person/not, etc. Semantic segmentation networks are used for example in Youtube stories to add virtual backgrounds (background matting).\n\n\n\nVirtual backgrounds. Source: https://ai.googleblog.com/2018/03/mobile-real-time-video-segmentation.html\n\n\nClothes can be segmented to allow for virtual try-ons.\n\n\n\nVirtual try-ons. Source: (Wang et al., 2018).\n\n\nThere are many datasets freely available, but annotating such data is very painful, expensive and error-prone.\n\nPASCAL VOC 2012 Segmentation Competition\nCOCO 2018 Stuff Segmentation Task\nBDD100K: A Large-scale Diverse Driving Video Database\nCambridge-driving Labeled Video Database (CamVid)\nCityscapes Dataset\nMapillary Vistas Dataset\nApolloScape Scene Parsing\nKITTI pixel-level semantic segmentation\n\n\n\n\nSemantic segmentation on the KITTI dataset. Source: http://www.cvlibs.net/datasets/kitti/\n\n\n\nOutput encoding\nEach pixel of the input image is associated to a label (as in classification).\n\n\n\nSemantic labels. Source : https://medium.com/nanonets/how-to-do-image-segmentation-using-deep-learning-c673cc5862ef\n\n\nA one-hot encoding of the segmented image is therefore a tensor:\n\n\n\nOne-hot encoding of semantic labels. Source : https://medium.com/nanonets/how-to-do-image-segmentation-using-deep-learning-c673cc5862ef\n\n\n\n\nFully convolutional networks\nA fully convolutional network only has convolutional layers and learns to predict the output tensor. The last layer has a pixel-wise softmax activation. We minimize the pixel-wise cross-entropy loss\n\\mathcal{L}(\\theta) = \\mathbb{E}_\\mathcal{D} [- \\sum_\\text{pixels} \\sum_\\text{classes} t_i \\, \\log y_i]\n\n\n\nFully convolutional network. Source : http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture11.pdf\n\n\nDownside: the image size is preserved throughout the network: computationally expensive. It is therefore difficult to increase the number of features in each convolutional layer."
  },
  {
    "objectID": "notes/4.3-SemanticSegmentation.html#segnet-segmentation-network",
    "href": "notes/4.3-SemanticSegmentation.html#segnet-segmentation-network",
    "title": "Semantic segmentation",
    "section": "SegNet: segmentation network",
    "text": "SegNet: segmentation network\nSegNet (Badrinarayanan et al., 2016) has an encoder-decoder architecture, with max-pooling to decrease the spatial resolution while increasing the number of features. But what is the inverse of max-pooling? Upsampling operation.\n\n\n\nSegNet (Badrinarayanan et al., 2016).\n\n\nNearest neighbor and Bed of nails would just make random decisions for the upsampling. In SegNet, max-unpooling uses the information of the corresponding max-pooling layer in the encoder to place pixels adequately.\n\n\n\nUpsampling methods. Source : http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture11.pdf\n\n\nAnother popular option in the followers of SegNet is the transposed convolution. The original feature map is upsampled by putting zeros between the values and a learned filter performs a regular convolution to produce an upsampled feature map. This works well when convolutions with stride are used in the encoder, but it is quite expensive computationally.\n\n\n\nTransposed convolution. Source : https://github.com/vdumoulin/conv_arithmetic\n\n\n\n\n\nTransposed convolution. Source : http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture11.pdf"
  },
  {
    "objectID": "notes/4.3-SemanticSegmentation.html#u-net",
    "href": "notes/4.3-SemanticSegmentation.html#u-net",
    "title": "Semantic segmentation",
    "section": "U-Net",
    "text": "U-Net\nThe problem of SegNet is that small details (small scales) are lost because of the max-pooling. the segmentation is not precise. The solution proposed by U-Net (Ronneberger et al., 2015) is to add skip connections (as in ResNet) between different levels of the encoder-decoder. The final segmentation depends both on:\n\nlarge-scale information computed in the middle of the encoder-decoder.\nsmall-scale information processed in the early layers of the encoder.\n\n\n\n\nU-Net (Ronneberger et al., 2015)."
  },
  {
    "objectID": "notes/4.3-SemanticSegmentation.html#mask-r-cnn",
    "href": "notes/4.3-SemanticSegmentation.html#mask-r-cnn",
    "title": "Semantic segmentation",
    "section": "Mask R-CNN",
    "text": "Mask R-CNN\nFor many applications, segmenting the background is useless. A two-stage approach can save computations. Mask R-CNN (He et al., 2018) uses faster R-CNN to extract bounding boxes around interesting objects, followed by the prediction of a mask to segment the object.\n\n\n\nMask R-CNN (He et al., 2018).\n\n\n\n\n\nMask R-CNN (He et al., 2018).\n\n\n\n\n\n\n\nBadrinarayanan, V., Kendall, A., and Cipolla, R. (2016). SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation. http://arxiv.org/abs/1511.00561.\n\n\nHe, K., Gkioxari, G., Dollár, P., and Girshick, R. (2018). Mask R-CNN. http://arxiv.org/abs/1703.06870.\n\n\nRonneberger, O., Fischer, P., and Brox, T. (2015). U-Net: Convolutional Networks for Biomedical Image Segmentation. http://arxiv.org/abs/1505.04597.\n\n\nWang, B., Zheng, H., Liang, X., Chen, Y., Lin, L., and Yang, M. (2018). Toward Characteristic-Preserving Image-based Virtual Try-On Network. http://arxiv.org/abs/1807.07688."
  },
  {
    "objectID": "notes/5.1-Autoencoders.html#autoencoders",
    "href": "notes/5.1-Autoencoders.html#autoencoders",
    "title": "Autoencoders",
    "section": "Autoencoders",
    "text": "Autoencoders\n\nSupervised learning algorithms need a lot of labeled data (with \\mathbf{t}) in order to learn classification/regression tasks, but labeled data is very expensive to obtain (experts, crowd sourcing). A “bad” algorithm trained with a lot of data will perform better than a “good” algorithm trained with few data.\n\n“It is not who has the best algorithm who wins, it is who has the most data.”\n\nUnlabeled data is only useful for unsupervised learning, but very cheap to obtain (camera, microphone, search engines). Can we combine efficiently both approaches? Self-taught learning or semi-supervised learning.\nAn autoencoder is a NN trying to learn the identity function f(\\mathbf{x}) = \\mathbf{x} using a different number of neurons in the hidden layer than in the input layer.\n\n\n\nArchitecture of a shallow autoencoder.\n\n\nAn autoencoder minimizes the reconstruction loss between the input \\mathbf{x} and the reconstruction \\mathbf{x'}, for example the mse between the two vectors:\n\n    \\mathcal{L}_\\text{reconstruction}(\\theta) = \\mathbb{E}_{\\mathbf{x} \\in \\mathcal{D}} [ ||\\mathbf{x'} - \\mathbf{x}||^2 ]\n\nAn autoencoder uses unsupervised learning: the output data used for learning is the same as the input data: No need for labels!\nBy forcing the projection of the input data on a feature space with less dimensions (latent space), the network has to extract relevant features from the training data: Dimensionality reduction, compression.\nIf the latent space has more dimensions than the input space, we need to constrain the autoencoder so that it does not simply learn the identity mapping. Below is an example of a sparse autoencoder trained on natural images (Olshausen and Field, 1997).\n\n\n\nSparse autoencoder trained on natural images (Olshausen and Field, 1997).\n\n\nInputs are taken from random natural images and cut in 10*10 patches. 100 features are extracted in the hidden layer. The autoencoder is said sparse because it uses L1-regularization to make sure that only a few neurons are active in the hidden layer for a particular image. The learned features look like what the first layer of a CNN would learn, except that there was no labels at all! Can we take advantage of this to pre-train a supervised network?"
  },
  {
    "objectID": "notes/5.1-Autoencoders.html#stacked-autoencoders",
    "href": "notes/5.1-Autoencoders.html#stacked-autoencoders",
    "title": "Autoencoders",
    "section": "Stacked autoencoders",
    "text": "Stacked autoencoders\nIn supervised learning, deep neural networks suffer from many problems: local minima, vanishing gradients, long training times… All these problems are due to the fact that the weights are randomly initialized at the beginning of training. Pretraining the weights using unsupervised learning allows to start already close to a good solution: the network will need less steps to converge, the gradients will vanish less and less data will be needed to learn a particular supervised task.\nLet’s try to learn a stacked autoencoder by learning progressively each feature vector.\n\n\n\nArchitecture of the stacked autoencoder. Source: http://ufldl.stanford.edu/wiki/index.php/Stacked_Autoencoders.\n\n\nUsing unlabeled data, train an autoencoder to extract first-order features, freeze the weights and remove the decoder.\n\n\n\nThe first layer is trained using an autoencoder on the inputs. Source: http://ufldl.stanford.edu/wiki/index.php/Stacked_Autoencoders.\n\n\nTrain another autoencoder on the same unlabeled data, but using the previous latent space as input/output.\n\n\n\nThe second layer is trained using an autoencoder on the first layer. Source: http://ufldl.stanford.edu/wiki/index.php/Stacked_Autoencoders.\n\n\nRepeat the operation as often as needed, and finish with a simple classifier using the labeled data.\n\n\n\nThe output layer is trained using supervised learning on the last hidden layer. Source: http://ufldl.stanford.edu/wiki/index.php/Stacked_Autoencoders.\n\n\nThis defines a stacked autoencoder, trained using Greedy layer-wise learning. Each layer progressively learns more and more complex features of the input data (edges - contour - forms - objects): feature extraction. This method allows to train a deep network on few labeled data: the network will not overfit, because the weights are already in the right region. It solves gradient vanishing, as the weights are already close to the optimal solution and will efficiently transmit the gradient backwards. One can keep the pre-trained weights fixed for the classification task or fine-tune all the weights as in a regular DNN."
  },
  {
    "objectID": "notes/5.1-Autoencoders.html#deep-autoencoders",
    "href": "notes/5.1-Autoencoders.html#deep-autoencoders",
    "title": "Autoencoders",
    "section": "Deep autoencoders",
    "text": "Deep autoencoders\n\n\nSemi-supervised learning\nAutoencoders are not restricted to a single hidden layer.\n\nThe encoder goes from the input space \\mathbf{x} to the latent space \\mathbf{z}.\n\n\n    \\mathbf{z} = g_\\phi(\\mathbf{x})\n\n\nThe decoder goes from the latent space \\mathbf{z} to the output space \\mathbf{x'}.\n\n\n    \\mathbf{x'} = f_\\theta(\\mathbf{z})\n\n\n\n\nDeep autoencoder. Source: https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html.\n\n\nThe latent space is a bottleneck layer of lower dimensionality, learning a compressed representation of the input which has to contain enough information in order to reconstruct the input. Both the encoder with weights \\phi and the decoder with weights \\theta try to minimize the reconstruction loss:\n\n\\mathcal{L}_\\text{reconstruction}(\\theta, \\phi) = \\mathbb{E}_{\\mathbf{x} \\in \\mathcal{D}} [ ||f_\\theta(g_\\phi(\\mathbf{x})) - \\mathbf{x}||^2 ]\n\nLearning is unsupervised: we only need input data.\nThe encoder and decoder can be anything: fully-connected, convolutional, recurrent, etc. When using convolutional layers, the decoder has to upsample the latent space: max-unpooling or transposed convolutions can be used as in segmentation networks.\n\n\n\nDeep convolutional autoencoder. Source: (Guo et al., 2017).\n\n\nIn semi-supervised or self-taught learning, we can first train an autoencoder on huge amounts of unlabeled data, and then use the latent representations as an input to a shallow classifier on a small supervised dataset.\n\n\n\nThe encoder of an unsupervised autoencoder can be used as a feature extractor for a classifier. Source: https://doi.org/10.1117/12.2303912.\n\n\nA linear classifier might even be enough if the latent space is well trained. The weights of the encoder can be fine-tuned with backpropagation, or remain fixed.\n\n\nDenoising autoencoders\nA denoising autoencoder (DAE, (Vincent et al., 2010)) is trained with noisy inputs (some pixels are dropped) but perfect desired outputs. It learns to suppress that noise.\n\n\n\nDenoising autoencoder. Source: https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html.\n\n\n\n\n\nDenoising autoencoder. Source: https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html.\n\n\n\n\nDeep clustering\nClustering algorithms (k-means, Gaussian Mixture Models, spectral clustering, etc) can be applied in the latent space to group data points into clusters. If you are lucky, the clusters may even correspond to classes.\n\n\n\nClustering can be applied on the latent representations. Source: doi:10.1007/978-3-030-32520-6_55."
  },
  {
    "objectID": "notes/5.1-Autoencoders.html#variational-autoencoders-vae",
    "href": "notes/5.1-Autoencoders.html#variational-autoencoders-vae",
    "title": "Autoencoders",
    "section": "Variational autoencoders (VAE)",
    "text": "Variational autoencoders (VAE)\n\n\nMotivation\nAutoencoders are deterministic: after learning, the same input \\mathbf{x} will generate the same latent code \\mathbf{z} and the same reconstruction \\mathbf{\\tilde{x}}. Sampling the latent space generally generates non-sense reconstructions, because an autoencoder only learns data samples, it does not learn the underlying probability distribution.\n\n\n\nDeterministic autoencoders do not regularize their latent space. Source: https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73.\n\n\nThe main problem of supervised learning is to get enough annotated data. Being able to generate new images similar to the training examples would be extremely useful (data augmentation).\nIn order for this to work, we need to regularize the latent space: Close points in the latent space should correspond to close images. “Classical” L1 or L2 regularization does not ensure the regularity of the latent space.\n\n\n\nVariational autoencoders do regularize their latent space. Source: https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73.\n\n\n\n\nArchitecture\nThe variational autoencoder (VAE) (Kingma and Welling, 2013) solves this problem by having the encoder represent the probability distribution q_\\phi(\\mathbf{z}|\\mathbf{x}) instead of a point \\mathbf{z} in the latent space.\nThis probability distribution is then sampled to obtain a vector \\mathbf{z} that will be passed to the decoder p_\\theta(\\mathbf{z}). The strong hypothesis is that the latent space follows a normal distribution with mean \\mathbf{\\mu_x} and variance \\mathbf{\\sigma_x}^2.\n\n    \\mathbf{z} \\sim \\mathcal{N}(\\mathbf{\\mu_x}, \\mathbf{\\sigma_x}^2)\n\nThe two vectors \\mathbf{\\mu_x} and \\mathbf{\\sigma_x}^2 are the outputs of the encoder.\n\n\n\n\n\n\nSampling from a normal distribution\n\n\n\n\n\n\nSampling from a normal distribution.\n\n\nThe normal distribution \\mathcal{N}(\\mu, \\sigma^2) is fully defined by its two parameters:\n\n\\mu is the mean of the distribution.\n\\sigma^2 is its variance.\n\nThe probability density function (pdf) of the normal distribution is defined by the Gaussian function:\n\n    f(x; \\mu, \\sigma) = \\frac{1}{\\sqrt{2\\,\\pi\\,\\sigma^2}} \\, e^{-\\displaystyle\\frac{(x - \\mu)^2}{2\\,\\sigma^2}}\n\nA sample x will likely be close to \\mu, with a deviation defined by \\sigma^2. It can be obtained using a sample of the standard normal distribution \\mathcal{N}(0, 1):\nx = \\mu + \\sigma \\, \\xi \\; \\; \\text{with} \\; \\xi \\sim \\mathcal{N}(0, 1)\n\n\nArchitecture of the VAE:\n\nThe encoder q_\\phi(\\mathbf{z}|\\mathbf{x}) outputs the parameters \\mathbf{\\mu_x} and \\mathbf{\\sigma_x}^2 of a normal distribution \\mathcal{N}(\\mathbf{\\mu_x}, \\mathbf{\\sigma_x}^2).\nWe sample one vector \\mathbf{z} from this distribution: \\mathbf{z} \\sim \\mathcal{N}(\\mathbf{\\mu_x}, \\mathbf{\\sigma_x}^2).\nThe decoder p_\\theta(\\mathbf{z}) reconstructs the input.\n\n\n\n\nArchitecture of a variational autoencoder. Source: https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73\n\n\nOpen questions:\n\nWhich loss should we use and how do we regularize?\nDoes backpropagation still work?\n\n\n\nLoss function of a VAE\nThe loss function used in a VAE is of the form:\n\n    \\mathcal{L}(\\theta, \\phi) = \\mathcal{L}_\\text{reconstruction}(\\theta, \\phi) + \\mathcal{L}_\\text{regularization}(\\phi)\n\nThe first term is the usual reconstruction loss of an autoencoder which depends on both the encoder and the decoder. One could simply compute the mse (summed over all pixels) between the input and the reconstruction:\n \\mathcal{L}_\\text{reconstruction}(\\theta, \\phi) = \\mathbb{E}_{\\mathbf{x} \\in \\mathcal{D}, \\mathbf{z} \\sim q_\\phi(\\mathbf{z}|\\mathbf{x})} [ ||p_\\theta(\\mathbf{z}) - \\mathbf{x}||^2 ]\nIn the expectation, \\mathbf{x} is sampled from the dataset \\mathcal{D} while \\mathbf{z} is sampled from the encoder q_\\phi(\\mathbf{z}|\\mathbf{x}). In (Kingma and Welling, 2013), pixels values are normalized between 0 and 1, the decoder uses the logistic activation function for its output layer and the binary cross-entropy loss function is used:\n \\mathcal{L}_\\text{reconstruction}(\\theta, \\phi) = \\mathbb{E}_{\\mathbf{x} \\in \\mathcal{D}, \\mathbf{z} \\sim q_\\phi(\\mathbf{z}|\\mathbf{x})} [ - \\log p_\\theta(\\mathbf{z})]\nThe justification comes from variational inference and evidence lower-bound optimization (ELBO) but is out of the scope of this lecture.\nThe second term is the regularization term for the latent space, which only depends on the encoder with weights \\phi:\n\n    \\mathcal{L}_\\text{regularization}(\\phi) = \\text{KL}(q_\\phi(\\mathbf{z}|\\mathbf{x}) || \\mathcal{N}(\\mathbf{0}, \\mathbf{1})) = \\text{KL}(\\mathcal{N}(\\mathbf{\\mu_x}, \\mathbf{\\sigma_x}^2) || \\mathcal{N}(\\mathbf{0}, \\mathbf{1}))\n\nIt is defined as the Kullback-Leibler divergence between the output of the encoder and the standard normal distribution \\mathcal{N}(\\mathbf{0}, \\mathbf{1}). Think of it as a statistical “distance” between the distribution q_\\phi(\\mathbf{z}|\\mathbf{x}) and the distribution \\mathcal{N}(\\mathbf{0}, \\mathbf{1}). The principle is not very different from L2-regularization, where we want the weights to be as close as possible from 0. Here we want the encoder to be as close as possible from \\mathcal{N}(\\mathbf{0}, \\mathbf{1}).\nWhy do we want the latent distributions to be close from \\mathcal{N}(\\mathbf{0}, \\mathbf{1}) for all inputs \\mathbf{x}? \n    \\mathcal{L}(\\theta, \\phi) = \\mathcal{L}_\\text{reconstruction}(\\theta, \\phi) + \\text{KL}(q_\\phi(\\mathbf{z}|\\mathbf{x}) || \\mathcal{N}(\\mathbf{0}, \\mathbf{1}))\n\nBy forcing the distributions to be close, we avoid “holes” in the latent space: we can move smoothly from one distribution to another without generating non-sense reconstructions.\n\n\n\nRegularizing the latent space by minimizing the KL with \\mathcal{N}(0,1) makes sure that we can smoothly travel in the latent space. Source: https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73\n\n\n\n\n\n\n\n\nNote\n\n\n\nTo make q_\\phi(\\mathbf{z}|\\mathbf{x}) close from \\mathcal{N}(\\mathbf{0}, \\mathbf{1}), one could minimize instead the Euclidian distance in the parameter space:\n\n    \\mathcal{L}(\\theta, \\phi) = \\mathcal{L}_\\text{reconstruction}(\\theta, \\phi) +  (||\\mathbf{\\mu_x}||^2 + ||\\mathbf{\\sigma_x} - 1||^2)\n\nHowever, this does not consider the overlap between the distributions. The two pairs of distributions below have the same distance between their means (0 and 1) and the same variance (1 and 10 respectively). The distributions on the left are very different from each other, but the distance in the parameter space is the same.\n\n\n\n\n\n\n\n\n\n\n\n\n\nKullback-Leibler divergence\n\n\n\nThe KL divergence between two random distributions X and Y measures the statistical distance between them. It describes, on average, how likely a sample from X could come from Y:\n\n    \\text{KL}(X ||Y) = \\mathbb{E}_{x \\sim X}[- \\log \\frac{P(Y=x)}{P(X=x)}]\n\n\n\n\n\n\nWhen the two distributions are equal almost anywhere, the KL divergence is 0. Otherwise it is positive. Minimizing the KL divergence between two distributions makes them close in the statistical sense.\n\n\nThe advantage of minimizing the KL of q_\\phi(\\mathbf{z}|\\mathbf{x}) with \\mathcal{N}(0, 1) is that the KL takes a closed form when the distributions are normal, i.e. there is no need to compute the expectation over all possible latent representations \\mathbf{z}:\n\n    \\mathcal{L}_\\text{regularization}(\\phi) = \\text{KL}(q_\\phi(\\mathbf{z}|\\mathbf{x}) || \\mathcal{N}(\\mathbf{0}, \\mathbf{1})) = \\mathbb{E}_{\\mathbf{x} \\in \\mathcal{D}, \\mathbf{z} \\sim q_\\phi(\\mathbf{z}|\\mathbf{x})}[- \\log \\frac{f_{0, 1}(\\mathbf{z}|\\mathbf{x})}{q_\\phi(\\mathbf{z}|\\mathbf{x})}]\n\nIf \\mathbf{\\mu_x} and \\mathbf{\\sigma_x} have K elements (dimension of the latent space), the KL can be expressed as:\n\n    \\mathcal{L}_\\text{regularization}(\\phi) = \\mathbb{E}_{\\mathbf{x} \\in \\mathcal{D}}[\\dfrac{1}{2} \\, \\sum_{k=1}^K (\\mathbf{\\sigma_x^2} + \\mathbf{\\mu_x}^2 -1 - \\log \\mathbf{\\sigma_x^2})]\n\nThe KL is very easy to differentiate w.r.t \\mathbf{\\mu_x} and \\mathbf{\\sigma_x}, i.e. w.r.t \\phi! In practice, the encoder predicts the vectors \\mathbf{\\mu_x} and \\Sigma_\\mathbf{x} = \\log \\mathbf{\\sigma_x^2}, so the loss becomes:\n\n    \\mathcal{L}_\\text{regularization}(\\phi) = \\dfrac{1}{2} \\, \\sum_{k=1}^K (\\exp \\Sigma_\\mathbf{x} + \\mathbf{\\mu_x}^2 - 1 - \\Sigma_\\mathbf{x})\n\nSee https://wiseodd.github.io/techblog/2016/12/10/variational-autoencoder/ for the proof.\nRegularization tends to create a “gradient” over the information encoded in the latent space. A point of the latent space sampled between the means of two encoded distributions should be decoded in an image in between the two training images.\n\n\n\nA regularized latent space makes sure that reconstructions always make sense. Source: https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73\n\n\n\n\nReparameterization trick\nThe second problem is that backpropagation does not work through the sampling operation. It is easy to backpropagate the gradient of the loss function through the decoder until the sample \\mathbf{z}. But how do you backpropagate to the outputs of the encoder: \\mathbf{\\mu_x} and \\mathbf{\\sigma_x}?\n\n\n\nArchitecture of a variational autoencoder. Source: https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73\n\n\nModifying slightly \\mathbf{\\mu_x} or \\mathbf{\\sigma_x} may not change at all the sample \\mathbf{z} \\sim \\mathcal{N}(\\mathbf{\\mu_x}, \\mathbf{\\sigma_x}^2), so you cannot estimate any gradient.\n\\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{\\mu_x}} = \\; ?\nBackpropagation does not work through a sampling operation, because it is not differentiable.\n\\mathbf{z} \\sim \\mathcal{N}(\\mathbf{\\mu_x}, \\mathbf{\\sigma_x}^2)\nThe reparameterization trick consists in taking a sample \\xi out of \\mathcal{N}(0, 1) and reconstruct \\mathbf{z} with:\n\\mathbf{z} = \\mathbf{\\mu_x} + \\mathbf{\\sigma_x} \\, \\xi \\qquad \\text{with} \\qquad \\xi \\sim \\mathcal{N}(0, 1)\n\n\n\nArchitecture of a variational autoencoder with the reparameterization trick. Source: https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73\n\n\nThe sampled value \\xi \\sim \\mathcal{N}(0, 1) becomes just another input to the neural network.\n\n\n\nArchitecture of a variational autoencoder with the reparameterization trick. Source: https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73\n\n\nIt allows to transform \\mathbf{\\mu_x} and \\mathbf{\\sigma_x} into a sample \\mathbf{z} of \\mathcal{N}(\\mathbf{\\mu_x}, \\mathbf{\\sigma_x}^2):\n\\mathbf{z} = \\mathbf{\\mu_x} + \\mathbf{\\sigma_x} \\, \\xi\nWe do not need to backpropagate through \\xi, as there is no parameter to learn! The neural network becomes differentiable end-to-end, backpropagation will work.\n\n\nSummary\nA variational autoencoder is an autoencoder where the latent space represents a probability distribution q_\\phi(\\mathbf{z} | \\mathbf{x}) using the mean \\mathbf{\\mu_x} and standard deviation \\mathbf{\\sigma_x} of a normal distribution. The latent space can be sampled to generate new images using the decoder p_\\theta(\\mathbf{z}). KL regularization and the reparameterization trick are essential to VAE.\n\n\\begin{aligned}\n    \\mathcal{L}(\\theta, \\phi) &= \\mathcal{L}_\\text{reconstruction}(\\theta, \\phi) + \\mathcal{L}_\\text{regularization}(\\phi) \\\\\n    &= \\mathbb{E}_{\\mathbf{x} \\in \\mathcal{D}, \\xi \\sim \\mathcal{N}(0, 1)} [ - \\log p_\\theta(\\mathbf{\\mu_x} + \\mathbf{\\sigma_x} \\, \\xi) + \\dfrac{1}{2} \\, \\sum_{k=1}^K (\\mathbf{\\sigma_x^2} + \\mathbf{\\mu_x}^2 -1 - \\log \\mathbf{\\sigma_x^2})] \\\\\n\\end{aligned}\n\n\n\n\nPrinciple of a VAE. Source: https://ijdykeman.github.io/ml/2016/12/21/cvae.html\n\n\nThe two main applications of VAEs in unsupervised learning are:\n\nDimensionality reduction: projecting high dimensional data (images) onto a smaller space, for example a 2D space for visualization.\nGenerative modeling: generating samples from the same distribution as the training data (data augmentation, deep fakes) by sampling on the manifold.\n\n\n\nAdvanced VAE\n\nDeepFake\n\nDeepFakes now became very easy, you can for example find DeepFace here: https://github.com/iperov/DeepFaceLab.\n\n\n\nDuring training of a deepfake, one encoder and two decoders learns to reproduce the face of each person. Source: https://www.alanzucconi.com/2018/03/14/understanding-the-technology-behind-deepfakes/\n\n\n\n\n\nWhen generating the deepfake, the decoder of person B is used on the latent representation of person A. Source: https://www.alanzucconi.com/2018/03/14/understanding-the-technology-behind-deepfakes/\n\n\n\n\n\\beta-VAE\nVAE does not use a regularization parameter to balance the reconstruction and regularization losses. What happens if you do?\n\n\\begin{aligned}\n    \\mathcal{L}(\\theta, \\phi) &= \\mathcal{L}_\\text{reconstruction}(\\theta, \\phi) + \\beta \\, \\mathcal{L}_\\text{regularization}(\\phi) \\\\\n    &= \\mathbb{E}_{\\mathbf{x} \\in \\mathcal{D}, \\xi \\sim \\mathcal{N}(0, 1)} [ - \\log p_\\theta(\\mathbf{\\mu_x} + \\mathbf{\\sigma_x} \\, \\xi) + \\dfrac{\\beta}{2} \\, \\sum_{k=1}^K (\\mathbf{\\sigma_x^2} + \\mathbf{\\mu_x}^2 -1 - \\log \\mathbf{\\sigma_x^2})] \\\\\n\\end{aligned}\n\nUsing \\beta > 1 puts emphasis on learning statistically independent latent factors.\nThe \\beta-VAE (Higgins et al., 2016) allows to disentangle the latent variables, i.e. manipulate them individually to vary only one aspect of the image (pose, color, gender, etc.).\n\n\n\n\\beta-VAE trained on CelebA allows to disentangle skin color, age/gender or saturation by manipulating individual latent variables. (Higgins et al., 2016).\n\n\n\n\n\n\n\n\nNote\n\n\n\nSee https://worldmodels.github.io/ for a live demo in the RL context.\n\n\n\n\nVQ-VAE\nDeepmind researchers proposed VQ-VAE-2 (Razavi et al., 2019), a hierarchical VAE using vector-quantized priors able to generate high-resolution images.\n\n\n\nVQ-VAE-2 (Razavi et al., 2019).\n\n\n\n\n\nFaces generated by VQ-VAE-2 (Razavi et al., 2019).\n\n\n\n\nConditional variational autoencoder (CVAE)\nWhat if we provide the labels to the encoder and the decoder during training?\n\n\n\nConditional VAE. Source: https://ijdykeman.github.io/ml/2016/12/21/cvae.html\n\n\nWhen trained with labels, the conditional variational autoencoder (CVAE (Sohn et al., 2015)) becomes able to sample many images of the same class.\n\n\n\nConditional VAE. Source: https://ijdykeman.github.io/ml/2016/12/21/cvae.html\n\n\nCVAE allows to sample as many samples of a given class as we want: data augmentation.\n\n\n\nMNIST digits generated by a Conditional VAE. Source: https://ijdykeman.github.io/ml/2016/12/21/cvae.html\n\n\nThe condition does not need to be a label, it can be a shape or another image (passed through another encoder).\n\n\n\nCVAE conditioned on shapes. Source: https://hci.iwr.uni-heidelberg.de/content/variational-u-net-conditional-appearance-and-shape-generation"
  },
  {
    "objectID": "notes/5.1-Autoencoders.html#variational-inference-optional",
    "href": "notes/5.1-Autoencoders.html#variational-inference-optional",
    "title": "Autoencoders",
    "section": "Variational inference (optional)",
    "text": "Variational inference (optional)\n\n\nLearning probability distributions from samples\nThe input data X comes from an unknown distribution P(X). The training set \\mathcal{D} is formed by samples of that distribution. Learning the distribution of the data means learning a parameterized distribution p_\\theta(X) that is as close as possible from the true distribution P(X). The parameterized distribution could be a family of known distributions (e.g. normal) or a neural network with a softmax output layer.\n\n\n\nDensity estimation. Source: https://machinelearningmastery.com/probability-density-estimation/\n\n\nThis means that we want to minimize the KL between the two distributions:\n\\min_\\theta \\, \\text{KL}(P(X) || p_\\theta(X)) = \\mathbb{E}_{x \\sim P(X)} [- \\log \\dfrac{p_\\theta(X=x)}{P(X=x)}]\nThe problem is that we do not know P(X) as it is what we want to learn, so we cannot estimate the KL directly.\nIn supervised learning, we are learning the conditional probability P(T | X) of the targets given the inputs, i.e. what is the probability of having the label T=t given the input X=x. A NN with a softmax output layer represents the parameterized distribution p_\\theta(T | X). The KL between the two distributions is:\n\\text{KL}(P(T | X) || p_\\theta(T | X)) = \\mathbb{E}_{x, t \\sim \\mathcal{D}} [- \\log \\dfrac{p_\\theta(T=t | X=x)}{P(T=t | X=x)}]\nWith the properties of the log, we know that the KL is the cross-entropy minus the entropy of the data:\n\\begin{aligned}\n\\text{KL}(P(T | X) || p_\\theta(T | X)) &= \\mathbb{E}_{x, t \\sim \\mathcal{D}} [- \\log p_\\theta(T=t | X=x)]  - \\mathbb{E}_{x, t \\sim \\mathcal{D}} [- \\log P(T=t | X=x)] \\\\\n&\\\\\n    & = H(P(T | X), p_\\theta(T |X)) - H(P(T|X)) \\\\\n\\end{aligned}\n\nWhen we minimize the KL by applying gradient descent on the parameters \\theta, only the cross-entropy will change, as the data does not depends on the model:\n\\begin{aligned}\n\\nabla_\\theta \\, \\text{KL}(P(T | X) || p_\\theta(T | X))  & = \\nabla_\\theta \\, H(P(T | X), p_\\theta(T |X)) - \\nabla_\\theta \\,  H(P(T|X)) \\\\\n    &\\\\\n     & = \\nabla_\\theta \\, H(P(T | X), p_\\theta(T |X)) \\\\\n     & \\\\\n     & = \\nabla_\\theta \\, \\mathbb{E}_{x, t \\sim \\mathcal{D}} [-  \\log p_\\theta(T=t | X=x) ]\\\\\n\\end{aligned}\n\nMinimizing the cross-entropy (negative log likelihood) of the model on the data is the same as minimizing the KL between the two distributions in supervised learning! We were actually minimizing the KL all along.\nWhen trying to learn the distribution P(X) of the data directly, we could use the same trick:\n\\nabla_\\theta \\, \\text{KL}(P(X) || p_\\theta(X)) = \\nabla_\\theta \\, H(P(X), p_\\theta(X))  = \\nabla_\\theta \\, \\mathbb{E}_{x \\sim X} [- \\log p_\\theta(X=x)]\ni.e. maximize the log-likelihood of the model on the data X. If we use N data samples to estimate the expectation, we notice that:\n\n\\mathbb{E}_{x \\sim X} [\\log p_\\theta(X=x)] \\approx \\dfrac{1}{N} \\, \\sum_{i=1}^N \\log p_\\theta(X=x_i) = \\dfrac{1}{N} \\, \\log \\prod_{i=1}^N p_\\theta(X=x_i) = \\dfrac{1}{N} \\, \\log L(\\theta)\n\nis indeed the log-likelihood of the model on the data that we maximized in maximum likelihood estimation.\nThe problem is that images are highly-dimensional (one dimension per pixel), so we would need astronomical numbers of samples to estimate the gradient (once): curse of dimensionality.\n\n\n\nCurse of dimensionality. Source: https://dibyaghosh.com/blog/probability/highdimensionalgeometry.html\n\n\nMLE does not work well in high-dimensional spaces. We need to work in a much lower-dimensional space.\n\n\nLatent space\nImages are not random samples of the pixel space: natural images are embedded in a much lower-dimensional space called a manifold. A manifold is a locally Euclidian topological space of lower dimension: The surface of the earth is locally flat and 2D, but globally spherical and 3D.\nIf we have a generative model telling us how a point on the manifold z maps to the image space (P(X | z)), we would only need to learn the distribution of the data in the lower-dimensional latent space.\n\n\n\nManifold. Source: https://en.wikipedia.org/wiki/Manifold\n\n\nThe low-dimensional latent variables z are the actual cause for the observations X. Given a sample z on the manifold, we can train a generative model p_\\theta(X | z) to recreate the input X. p_\\theta(X | z) is the decoder: given a latent representation z, what is the corresponding observation X?\n\n\n\nLatent variable. Source: https://blog.evjang.com/2016/08/variational-bayes.html\n\n\nIf we learn the distribution p_\\theta(z) of the manifold (latent space), we can infer the distribution of the data p_\\theta(X) using that model:\np_\\theta(X) = \\mathbb{E}_{z \\sim p_\\theta(z)} [p_\\theta(X | z)] = \\int_z p_\\theta(X | z) \\, p_\\theta(z) \\, dz\nProblem: we do not know p_\\theta(z), as the only data we see is X: z is called a latent variable because it explains the data but is hidden.\n\n\nVariational inference\nTo estimate p_\\theta(z), we could again marginalize over X:\np_\\theta(z) = \\mathbb{E}_{x \\sim p_\\theta(X)} [p_\\theta(z | x)] = \\int_x p_\\theta(z | x) \\, p_\\theta(x) \\, dx\np_\\theta(z | x) is the encoder: given an input x \\sim p_\\theta(X), what is its latent representation z? The Bayes rule tells us:\np_\\theta(z | x) = p_\\theta(x |z) \\, \\dfrac{p_\\theta(z)}{p_\\theta(x)}\nThe posterior probability (encoder) p_\\theta(z | X) depends on the model (decoder) p_\\theta(X|z), the prior (assumption) p_\\theta(z) and the evidence (data) p_\\theta(X).\nWe get:\np_\\theta(z) = \\mathbb{E}_{x \\sim p_\\theta(X)} [p_\\theta(x |z) \\, \\dfrac{p_\\theta(z)}{p_\\theta(x)}]\nThe posterior is untractable as it would require to integrate over all possible inputs x \\sim p_\\theta(X):\np_\\theta(z) = \\mathbb{E}_{x \\sim p_\\theta(X)} [p_\\theta(x |z) \\, \\dfrac{p_\\theta(z)}{p_\\theta(x)}] = \\int_x p_\\theta(x |z) \\, p_\\theta(z) \\, dx\nVariational inference proposes to approximate the true encoder p_\\theta(z | x) by another parameterized distribution q_\\phi(z|x).\n\n\n\nVariational inference approximates the encoder with another parameterized distribution. Source: https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html\n\n\nThe decoder p_\\theta(x |z) generates observations x from a latent representation x with parameters \\theta. The encoder q_\\phi(z|x) estimates the latent representation z of a generated observation x. It should approximate p_\\theta(z | x) with parameters \\phi.\nTo make q_\\phi(z| X) close from p_\\theta(z | X), we minimize their KL divergence:\n\\begin{aligned}\n\\text{KL}(q_\\phi(z|X) || p_\\theta(z | X) ) &= \\mathbb{E}_{z \\sim q_\\phi(z|X)} [- \\log \\dfrac{p_\\theta(z | X)}{q_\\phi(z|X)}]\\\\\n\\end{aligned}\n\nNote that we sample the latent representations from the learned encoder q_\\phi(z|X) (imagination). As p_\\theta(z | X) = p_\\theta(X |z) \\, \\dfrac{p_\\theta(z)}{p_\\theta(X)}, we get:\n\\begin{aligned}\n\\text{KL}(q_\\phi(z|X) || p_\\theta(z | X) ) &= \\mathbb{E}_{z \\sim q_\\phi(z|X)} [- \\log \\dfrac{p_\\theta(X | z) \\, p_\\theta(z)}{q_\\phi(z|X) \\, p_\\theta(X)}]\\\\\n&=\\mathbb{E}_{z \\sim q_\\phi(z|X)} [- \\log \\dfrac{p_\\theta(z)}{q_\\phi(z|X)}] - \\mathbb{E}_{z \\sim q_\\phi(z|X)} [- \\log p_\\theta(X)]  \\\\\n&+ \\mathbb{E}_{z \\sim q_\\phi(z|X)} [- \\log p_\\theta(X | z)]\\\\\n\\end{aligned}\n\np_\\theta(X) does not depend on z, so its expectation w.r.t z is constant:\n\\begin{aligned}\n\\text{KL}(q_\\phi(z|X) || p_\\theta(z | X) ) &= \\text{KL}(q_\\phi(z|X) || p_\\theta(z)) + \\log p_\\theta(X) + \\mathbb{E}_{z \\sim q_\\phi(z|X)} [- \\log p_\\theta(X | z)]\\\\\n\\end{aligned}\n\nWe rearrange the terms:\n\\begin{aligned}\n\\log p_\\theta(X) - \\text{KL}(q_\\phi(z|X) || p_\\theta(z | X) ) &=  - \\mathbb{E}_{z \\sim q_\\phi(z|X)} [- \\log p_\\theta(X | z)] - \\text{KL}(q_\\phi(z|X) || p_\\theta(z))\\\\\n\\end{aligned}\n\n\nTraining the encoder means that we minimize \\text{KL}(q_\\phi(z|X) || p_\\theta(z | X) ).\nTraining the decoder means that we maximize \\log p_\\theta(X) (log-likelihood of the model).\nTraining the encoder and decoder together means that we maximize:\n\n \\text{ELBO}(\\theta, \\phi) = \\log p_\\theta(X) - \\text{KL}(q_\\phi(z|X) || p_\\theta(z | X) )\nThe KL divergence is always positive or equal to 0, so we have:\n\\text{ELBO}(\\theta, \\phi) \\leq \\log p_\\theta(X)\nThis term is called the evidence lower bound (ELBO): by maximizing it, we also maximize the untractable evidence \\log p_\\theta(X), which is what we want to do. The trick is that the right-hand term of the equation gives us a tractable definition of the ELBO term:\n\\begin{aligned}\n\\text{ELBO}(\\theta, \\phi) &=  \\log p_\\theta(X) - \\text{KL}(q_\\phi(z|X) || p_\\theta(z | X) ) \\\\\n&\\\\\n&= - \\mathbb{E}_{z \\sim q_\\phi(z|X)} [- \\log p_\\theta(X | z)] - \\text{KL}(q_\\phi(z|X) || p_\\theta(z))\n\\end{aligned}\n\nWhat happens when we minimize the negative ELBO?\n \\mathcal{L}(\\theta, \\phi) = - \\text{ELBO}(\\theta, \\phi) = \\mathbb{E}_{z \\sim q_\\phi(z|X)} [- \\log p_\\theta(X | z)] + \\text{KL}(q_\\phi(z|X) || p_\\theta(z))\n\n\\mathbb{E}_{z \\sim q_\\phi(z|X)} [- \\log p_\\theta(X | z)] is the reconstruction loss of the decoder p_\\theta(X | z):\n\nGiven a sample z of the encoder q_\\phi(z|X), minimize the negative log-likelihood of the reconstruction p_\\theta(X | z).\n\n\\text{KL}(q_\\phi(z|X) || p_\\theta(z)) is the regularization loss for the encoder:\n\nThe latent distribution q_\\phi(z|X) should be too far from the prior p_\\theta(z).\n\n\nVariational autoencoders use \\mathcal{N}(0, 1) as a prior for the latent space, but any other prior could be used.\n\\begin{aligned}\n    \\mathcal{L}(\\theta, \\phi) &= \\mathcal{L}_\\text{reconstruction}(\\theta, \\phi) + \\mathcal{L}_\\text{regularization}(\\phi) \\\\\n    &\\\\\n    &= \\mathbb{E}_{\\mathbf{x} \\in \\mathcal{D}, \\mathbf{z} \\sim q_\\phi(\\mathbf{z}|\\mathbf{x})} [ - \\log p_\\theta(\\mathbf{z})] + \\text{KL}(q_\\phi(\\mathbf{z}|\\mathbf{x}) || \\mathcal{N}(\\mathbf{0}, \\mathbf{1}))\\\\\n\\end{aligned}\n\nThe reparameterization trick and the fact that the KL between normal distributions has a closed form allow us to use backpropagation end-to-end. The encoder q_\\phi(z|X) and decoder p_\\theta(X | z) are neural networks in a VAE, but other parametrized distributions can be used (e.g. in physics).\n\n\n\n\n\n\nSources\n\n\n\n\nhttps://wiseodd.github.io/techblog/2016/12/10/variational-autoencoder/\nhttps://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html\nhttps://blog.evjang.com/2016/08/variational-bayes.html\nhttps://jonathan-hui.medium.com/machine-learning-variational-inference-273d8e6480bb\n\n\n\n\n\n\n\nGuo, X., Liu, X., Zhu, E., and Yin, J. (2017). Deep Clustering with Convolutional Autoencoders. in Neural Information Processing Lecture Notes in Computer Science., eds. D. Liu, S. Xie, Y. Li, D. Zhao, and E.-S. M. El-Alfy (Cham: Springer International Publishing), 373–382. doi:10.1007/978-3-319-70096-0_39.\n\n\nHiggins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X., Botvinick, M., et al. (2016). Beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework. in ICLR 2017 https://openreview.net/forum?id=Sy2fzU9gl.\n\n\nKingma, D. P., and Welling, M. (2013). Auto-Encoding Variational Bayes. http://arxiv.org/abs/1312.6114.\n\n\nOlshausen, B. A., and Field, D. J. (1997). Sparse coding with an overcomplete basis set: A strategy employed by V1? Vision Research 37, 3311–3325. doi:10.1016/S0042-6989(97)00169-7.\n\n\nRazavi, A., Oord, A. van den, and Vinyals, O. (2019). Generating Diverse High-Fidelity Images with VQ-VAE-2. http://arxiv.org/abs/1906.00446.\n\n\nSohn, K., Lee, H., and Yan, X. (2015). “Learning Structured Output Representation using Deep Conditional Generative Models,” in Advances in Neural Information Processing Systems 28, eds. C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett (Curran Associates, Inc.), 3483–3491. http://papers.nips.cc/paper/5775-learning-structured-output-representation-using-deep-conditional-generative-models.pdf.\n\n\nVincent, P., Larochelle, H., Lajoie, I., Bengio, Y., and Manzagol, P.-A. (2010). Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion. Journal of Machine Learning Research, 38."
  },
  {
    "objectID": "notes/5.2-RBM.html#structure-of-a-rbm",
    "href": "notes/5.2-RBM.html#structure-of-a-rbm",
    "title": "Restricted Boltzmann machines (optional)",
    "section": "Structure of a RBM",
    "text": "Structure of a RBM\nAuto-encoders are not the only feature extractors that can be stacked.\nRestricted Boltzmann Machines (RBM, (Hinton et al., 2006)) are generative stochastic artificial neural networks that can learn a probability distribution of their inputs. Their neurons form a bipartite graph with two groups of reciprocally connected units:\n\nthe visible units \\mathbf{v} (the inputs)\nthe hidden units \\mathbf{h} (the features or latent space).\n\nConnections are bidirectional between \\mathbf{v} and \\mathbf{h}, but the neurons inside the two groups are independent from each other (restricted). The goal of learning is to find the weights allowing the network to explain best the input data.\n\n\n\nRestricted Boltzmann Machine (Hinton et al., 2006).\n\n\nRBMs are a form of autoencoder where the input \\rightarrow feature weight matrix is the same as the feature \\rightarrow output matrix. There are two steps:\n\nThe forward pass P(\\mathbf{h} | \\mathbf{x}) propagates the visible units activation to the hidden units.\nThe backward pass P(\\mathbf{x} | \\mathbf{h}) reconstructs the visible units from the the hidden units.\n\nIf the weight matrix is correctly chosen, the reconstructed input should “match” the original input: the data is explained.\n\n\n\nRBMs are autoencoders where the decoder uses the same weights as the encoder. Source: https://www.edureka.co/blog/restricted-boltzmann-machine-tutorial/\n\n\nThe visible and units are generally binary units (0 or 1), with a probability defined by the weights and biases and the logistic function:\n\n    P(h_j = 1 | \\mathbf{v}) = \\sigma( \\sum_i W_{ij} \\, v_i + c_j)\n\n\n    P(v_i = 1 | \\mathbf{h}) = \\sigma(\\sum_i W_{ji} \\, h_j + b_i )\n\nThe weight matrix W and the biases \\mathbf{b}, \\mathbf{c} are the parameters \\theta of a probability distribution over the activation of the visible and hidden units."
  },
  {
    "objectID": "notes/5.2-RBM.html#minimizing-the-free-energy",
    "href": "notes/5.2-RBM.html#minimizing-the-free-energy",
    "title": "Restricted Boltzmann machines (optional)",
    "section": "Minimizing the free energy",
    "text": "Minimizing the free energy\nThe goal is to find the parameters which explain best the data (visible units), i.e. the ones maximizing the log-likelihood of the model for the data (\\mathbf{v}_1, \\ldots, \\mathbf{v}_N). We use maximum likelihood estimation (MLE) to maximize the log-likelihood of the model:\n\n    \\max_{\\theta} \\,  \\mathcal{L}(\\theta) = \\mathbb{E}_{\\mathbf{v} \\sim \\mathcal{D}} [ \\log \\, P_\\theta(\\mathbf{v}) ]\n\nIn practice, MLE is not tractable in a RBM, as we cannot estimate the joint probability P(\\mathbf{v}, \\mathbf{h}) of \\mathbf{v} and \\mathbf{h} (too many combinations are possible).\n\n    P(\\mathbf{v}) = \\sum_\\mathbf{h} P(\\mathbf{v}, \\mathbf{h})\n\nThe main trick in energy-based models is to rewrite the probabilities using an energy function E(\\mathbf{v}, \\mathbf{h}):\n\n    P(\\mathbf{v}) = \\sum_\\mathbf{h} P(\\mathbf{v}, \\mathbf{h}) = \\dfrac{\\sum_\\mathbf{h} \\exp^{-E(\\mathbf{v}, \\mathbf{h})}}{\\sum_{\\mathbf{v}, \\mathbf{h}} \\exp^{-E(\\mathbf{v}, \\mathbf{h})}} = \\dfrac{1}{Z} \\sum_\\mathbf{h} \\exp^{-E(\\mathbf{v}, \\mathbf{h})}\n\nwhere:\n\nZ = \\sum_{\\mathbf{v}, \\mathbf{h}} \\exp^{-E(\\mathbf{v}, \\mathbf{h})} = \\sum_{\\mathbf{v}} P(\\mathbf{v}) \\, \\sum_{\\mathbf{h}} \\exp^{-E(\\mathbf{v}, \\mathbf{h})}\n\nis the partition function (a normalizing term).\nThe probabilities come from a Gibbs distribution (or Boltzmann distribution) parameterized by the energy of the system. This is equivalent to a simple softmax over the energy…\nHaving reformulated the probabilities in terms of energy:\n\n    P(\\mathbf{v}) = \\frac{1}{Z} \\sum_\\mathbf{h} \\exp^{-E(\\mathbf{v}, \\mathbf{h})}\n\nwe can introduce the free energy of the model for a sample \\mathbf{v} (how surprising is the input \\mathbf{v} for the model):\n\n    \\mathcal{F}(\\mathbf{v}) = - \\log \\, \\sum_\\mathbf{h} \\exp^{-E(\\mathbf{v}, \\mathbf{h})}\n\nThe log-likelihood of the model for a sample \\mathbf{v} of the training data (\\mathbf{v}_1, \\ldots, \\mathbf{v}_N) becomes:\n\\log \\, P(\\mathbf{v}) = \\log \\, \\frac{1}{Z} \\sum_\\mathbf{h} \\exp^{-E(\\mathbf{v}, \\mathbf{h})} = - \\mathcal{F}(\\mathbf{v}) + \\log Z = - \\mathcal{F}(\\mathbf{v}) + \\sum_{\\mathbf{v}} P(\\mathbf{v}) \\, \\mathcal{F}(\\mathbf{v})\nNote that the second term sums over all possible inputs \\mathbf{v}. Maximizing the log-likelihood of the model on the training data can be done using gradient ascent by following this gradient:\n\n    \\nabla_\\theta \\mathcal{L}(\\theta) = \\mathbb{E}_{\\mathbf{v}} [\\nabla_\\theta \\log \\, P(\\mathbf{v}_i)] = \\mathbb{E}_{\\mathbf{v}} [ - \\nabla_\\theta \\mathcal{F}(\\mathbf{v}) + \\sum_{\\mathbf{v}} P(\\mathbf{v}) \\nabla_\\theta  \\mathcal{F}(\\mathbf{v})]\n\nThe free energy for a RBM with binary neurons is fortunately known analytically:\n\n    \\mathcal{F}(\\mathbf{v}) = - \\sum_i b_i \\, v_i - \\sum_j \\log (1 + \\exp^{\\sum_i W_{ij} \\, v_i + c_j})\n\nso finding the gradient w.r.t \\theta = (W, \\mathbf{b}, \\mathbf{c}) of the first term on the r.h.s (the free energy of the sample) is easy:\n\n    \\nabla_\\theta \\log \\, P(\\mathbf{v}) = - \\nabla_\\theta \\mathcal{F}(\\mathbf{v}) + \\sum_{\\mathbf{v}} P(\\mathbf{v}) \\nabla_\\theta  \\mathcal{F}(\\mathbf{v})\n\nIn particular, the gradient w.r.t the matrix W is the outer product between \\mathbf{v} and P(\\mathbf{h} | \\mathbf{v}):\n\n    \\nabla_W \\mathcal{F}(\\mathbf{v}) = - \\mathbf{v} \\times P(\\mathbf{h} | \\mathbf{v})\n\nThe problem is the second term: we would need to integrate over all possible values of the inputs \\mathbf{v}, what is not tractable. We will therefore make an approximation using Gibbs sampling (a variant of Monte-Carlo Markov Chain sampling - MCMC) to estimate that second term.\n\n\n\nGibbs sampling. Source : https://towardsdatascience.com/deep-learning-meets-physics-restricted-boltzmann-machines-part-i-6df5c4918c15\n\n\nGibbs sampling consists of repeatedly applying the encoder P(\\mathbf{h} | \\mathbf{v}) and the decoder P(\\mathbf{v} | \\mathbf{h}) on the input.\n\nWe start by setting \\mathbf{v}_0 = \\mathbf{v} using a training sample.\nWe obtain \\mathbf{h}_0 by computing P(\\mathbf{h} | \\mathbf{v}_0) and sampling it.\nWe obtain \\mathbf{v}_1 y computing P(\\mathbf{v} | \\mathbf{h}_0) and sampling it.\n…\nWe obtain \\mathbf{v}_k y computing P(\\mathbf{v} | \\mathbf{h}_{k-1}) and sampling it.\n\nAfter enough iterations k, we should have a good estimate of P(\\mathbf{v}, \\mathbf{h}). The k iterations have generated enough reconstructions of \\mathbf{v} to cover the distribution of \\mathbf{v}.\nWe set \\mathbf{v}_0 = \\mathbf{v} on a training sample and let Gibbs sampling iterate for k iterations until we obtain \\mathbf{v}_k = \\mathbf{v}^*. Contrastive divergence (CD-k, (Hinton and Salakhutdinov, 2006)) shows that the gradient of the log-likelihood can be approximated by:\n\n\\begin{align}\n\\nabla_W \\log \\, P(\\mathbf{v}) & = - \\nabla_W \\mathcal{F}(\\mathbf{v}) + \\sum_{\\mathbf{v}} P(\\mathbf{v}) \\nabla_W  \\mathcal{F}(\\mathbf{v}) \\\\\n    & \\approx \\mathbf{v} \\times P(\\mathbf{h} | \\mathbf{v}) - \\mathbf{v}^* \\times P(\\mathbf{h} | \\mathbf{v}^*) \\\\\n\\end{align}\n\nThe gradient of the log-likelihood is the difference between the initial explanation of \\mathbf{v} by the model, and its explanation after k iterations (relaxation). If the model is good, the reconstruction \\mathbf{v}^* is the same as the input \\mathbf{v}, so the gradient is zero. An input \\mathbf{v} is likely under the RBM model if it is able to reconstruct it, i.e. when it is not surprising (the free energy is low). In practice, k=1 gives surprisingly good results, but RBMs are very painful to train (hyperparameters)…"
  },
  {
    "objectID": "notes/5.2-RBM.html#deep-belief-networks-stacked-rbms",
    "href": "notes/5.2-RBM.html#deep-belief-networks-stacked-rbms",
    "title": "Restricted Boltzmann machines (optional)",
    "section": "Deep Belief Networks = stacked RBMs",
    "text": "Deep Belief Networks = stacked RBMs\nA Deep Belief Network (DBN) is a simple stack of RBMS, trained using greedy layer-wise learning. The “bottom” parts of the DBM become unidirectional when learning the top part.\n\n\n\nDeep belief network (Hinton and Salakhutdinov, 2006).\n\n\nAndrew Ng and colleagues (Google, Stanford, (Le, 2013)) trained a deep belief network on color images (200x200) taken from 10 million random unlabeled Youtube videos. Each layer was trained greedily. They also used a couple of other tricks (receptive fields, contrast normalization). Training was distributed over 1000 machines (16.000 cores) and lasted for three days. There was absolutely no task: the network just had to watch youtube videos. After learning, they visualized what the neurons had learned.\n\n\n\nDeep Belief Network (Le, 2013).\n\n\nAfter training, some neurons had learned to respond uniquely to faces, or to cats, without ever having been instructed to. The network can then be fine-tuned for classification tasks, improving the pre-AlexNet state-of-the-art on ImageNet by 70%.\n\n\n\nDeep Belief Network (Le, 2013).\n\n\n\n\n\n\nHinton, G. E., Osindero, S., and Teh, Y.-W. (2006). A fast learning algorithm for deep belief nets. Neural Comput. 18, 1527–1554. doi:10.1162/neco.2006.18.7.1527.\n\n\nHinton, G. E., and Salakhutdinov, R. R. (2006). Reducing the Dimensionality of Data with Neural Networks. Science 313, 504–507. doi:10.1126/science.1127647.\n\n\nLe, Q. V. (2013). Building high-level features using large scale unsupervised learning. in 2013 IEEE International Conference on Acoustics, Speech and Signal Processing (Vancouver, BC, Canada: IEEE), 8595–8598. doi:10.1109/ICASSP.2013.6639343."
  },
  {
    "objectID": "notes/5.3-GAN.html#generative-adversarial-networks",
    "href": "notes/5.3-GAN.html#generative-adversarial-networks",
    "title": "Generative adversarial networks",
    "section": "Generative adversarial networks",
    "text": "Generative adversarial networks\n\n\nGenerative models\nAn autoencoder learns to first encode inputs in a latent space and then use a generative model to model the data distribution.\n\\mathcal{L}_\\text{autoencoder}(\\theta, \\phi) = \\mathbb{E}_{\\mathbf{x} \\in \\mathcal{D}, \\mathbf{z} \\sim q_\\phi(\\mathbf{z}|\\mathbf{x})} [ - \\log p_\\theta(\\mathbf{z})]\nCouldn’t we learn a decoder using random noise as input but still learning the distribution of the data?\n\\mathcal{L}_\\text{GAN}(\\theta, \\phi) = \\mathbb{E}_{\\mathbf{z} \\sim \\mathcal{N}(0, 1)} [ - \\log p_\\theta(\\mathbf{z}) ]\nAfter all, this is how random numbers are generated: a uniform distribution of pseudo-random numbers is transformed into samples of another distribution using a mathematical formula.\n\n\n\nRandom numbers are generated using a standard distribution as source of randomness. Source: https://towardsdatascience.com/understanding-generative-adversarial-networks-gans-cd6e4651a29\n\n\nThe problem is how to estimate the discrepancy between the true distribution and the generated distribution when we only have samples. The Maximum Mean Discrepancy (MMD) approach allows to do that, but does not work very well in highly-dimensional spaces.\n\n\n\nThe generative sample should learn to minimize the statistical distance between the true distribution and the parameterized distribution using samples. Source: https://towardsdatascience.com/understanding-generative-adversarial-networks-gans-cd6e4651a29\n\n\n\n\nArchitecture of a GAN\nThe Generative Adversarial Network (GAN, (Goodfellow et al., 2014)) is a smart way of providing a loss function to the generative model. It is composed of two parts:\n\nThe Generator (or decoder) produces an image based on latent variables sampled from some random distribution (e.g. uniform or normal).\nThe Discriminator has to recognize real images from generated ones.\n\n\n\n\nArchitecture of a GAN. The generator only sees noisy latent representations and outputs a reconstruction. The discriminator gets alternatively real or generated inputs and predicts whether it is real or fake. Source: https://www.oreilly.com/library/view/java-deep-learning/9781788997454/60579068-af4b-4bbf-83f1-e988fbe3b226.xhtml\n\n\nThe generator and the discriminator are in competition with each other. The discriminator uses pure supervised learning: we know if the input is real or generated (binary classification) and train the discriminator accordingly. The generator tries to fool the discriminator, without ever seeing the data!\n\n\n\nPrinciple of a GAN. Source: https://towardsdatascience.com/understanding-generative-adversarial-networks-gans-cd6e4651a29\n\n\n\n\nGAN loss\nLet’s define x \\sim P_\\text{data}(x) as a real image from the dataset and G(z) as an image generated by the generator, where z \\sim P_z(z) is a random input. The output of the discriminator is a single sigmoid neuron:\n\nD(x) = 1 for real images.\nD(G(z)) = 0 for generated images\n\nThe discriminator wants both D(x) and 1-D(G(z)) to be close from 1, so the goal of the discriminator is to minimize the negative log-likelihood (cross-entropy) of classifying correctly the data:\n\n    \\mathcal{L}(D) = \\mathbb{E}_{x \\sim P_\\text{data}(x)} [ - \\log D(x)] + \\mathbb{E}_{z \\sim P_z(z)} [ - \\log(1 - D(G(z)))]\n\nIt is similar to logistic regression: x belongs to the positive class, G(z) to the negative class.\nThe goal of the generator is to maximize the negative log-likelihood of the discriminator being correct on the generated images, i.e. fool it:\n\n    \\mathcal{J}(G) = \\mathbb{E}_{z \\sim P_z(z)} [ - \\log(1 - D(G(z)))]\n\nThe generator tries to maximize what the discriminator tries to minimize.\nPutting both objectives together, we obtain the following minimax problem:\n\n    \\min_G \\max_D \\, \\mathcal{V}(D, G) = \\mathbb{E}_{x \\sim P_\\text{data}(x)} [\\log D(x)] + \\mathbb{E}_{z \\sim P_z(z)} [\\log(1 - D(G(z)))]\n\nD and G compete on the same objective function: one tries to maximize it, the other to minimize it. Note that the generator G never sees the data x: all it gets is a backpropagated gradient through the discriminator:\n\\nabla_{G(z)} \\, \\mathcal{V}(D, G) = \\nabla_{D(G(z))} \\, \\mathcal{V}(D, G) \\times \\nabla_{G(z)} \\, D(G(z))\nIt informs the generator which pixels are the most responsible for an eventual bad decision of the discriminator.\nThis objective function can be optimized when the generator uses gradient descent and the discriminator gradient ascent: just apply a minus sign on the weight updates!\n\n    \\min_G \\max_D V(D, G) = \\mathbb{E}_{x \\sim P_\\text{data}(x)} [\\log D(x)] + \\mathbb{E}_{z \\sim P_z(z)} [\\log(1 - D(G(z)))]\n\nBoth can therefore use the usual backpropagation algorithm to adapt their parameters. The discriminator and the generator should reach a Nash equilibrium: they try to beat each other, but both become better over time.\n\n\n\nThe generator and discriminator loss functions reach an equilibrium, it is quite hard to tell when the network has converged. Source: Research project - Vivek Bakul Maru - TU Chemnitz\n\n\n\n\nVariants\nDCGAN (Radford et al., 2015) is the convolutional version of GAN, using transposed convolutions in the generator and concolutions with stride in the discriminator.\n\n\n\nDCGAN (Radford et al., 2015).\n\n\n\n\n\nResults of DCGAN (Radford et al., 2015).\n\n\nGAN are quite sensible to train: the discriminator should not become too good too early, otherwise there is no usable gradient for the generator. In practice, one updates the generator more often than the discriminator. There has been many improvements on GANs to stabilizes training (see (Salimans et al., 2016)):\n\nWasserstein GAN (relying on the Wasserstein distance instead of the log-likelihood) (Arjovsky et al., 2017).\nf-GAN (relying on any f-divergence) (Nowozin et al., 2016).\n\nBut the generator often collapses, i.e. outputs always the same image regarless the input noise. Hyperparameter tuning is very difficult.\nStyleGAN2 from NVIDIA (Karras et al., 2020) is one of the most realistic GAN variant. Check its generated faces at https://thispersondoesnotexist.com/."
  },
  {
    "objectID": "notes/5.3-GAN.html#conditional-gans",
    "href": "notes/5.3-GAN.html#conditional-gans",
    "title": "Generative adversarial networks",
    "section": "Conditional GANs",
    "text": "Conditional GANs\n\n\ncGAN\nThe generator can also get additional deterministic information to the latent space, not only the random vector z. One can for example provide the label (class) in the context of supervised learning, allowing to generate many new examples of each class: data augmentation using a conditional GAN (Mirza and Osindero, 2014). One could also provide the output of a pre-trained CNN (ResNet) to condition on images.\n\n\n\ncGAN (Mirza and Osindero, 2014).\n\n\n\n\n\ncGAN conditioned on text (Reed et al., 2016).\n\n\n\n\n\ncGAN conditioned on text (Reed et al., 2016).\n\n\n\n\npix2pix\ncGAN can be extended to have an autoencoder-like architecture, allowing to generate images from images. pix2pix (Isola et al., 2018) is trained on pairs of similar images in different domains. The conversion from one domain to another is easy in one direction, but we want to learn the opposite.\n\n\n\npix2pix (Isola et al., 2018).\n\n\nThe goal of the generator is to convert for example a black-and-white image into a colorized one. It is a deep convolutional autoencoder, with convolutions with strides and transposed convolutions (SegNet-like).\n\n\n\npix2pix generator. Source: https://affinelayer.com/pix2pix/.\n\n\n\n\n\nBlocks of the pix2pix generator. Source: https://affinelayer.com/pix2pix/.\n\n\nIn practice, it has a U-Net architecture with skip connections to generate fine details.\n\n\n\npix2pix generator with skip connections. Source: https://affinelayer.com/pix2pix/.\n\n\nThe discriminator takes a pair of images as input: input/target or input/generated. It does not output a single value real/fake, but a 30x30 “image” telling how real or fake is the corresponding patch of the unknown image. Patches correspond to overlapping 70x70 regions of the 256x256 input image. This type of discriminator is called a PatchGAN.\n\n\n\npix2pix discriminator. Source: https://affinelayer.com/pix2pix/.\n\n\n\n\n\npix2pix discriminator. Source: https://affinelayer.com/pix2pix/.\n\n\nThe discriminator is trained like in a regular GAN by alternating input/target or input/generated pairs.\n\n\n\npix2pix discriminator training. Source: https://affinelayer.com/pix2pix/.\n\n\nThe generator is trained by maximizing the GAN loss (using gradients backpropagated through the discriminator) but also by minimizing the L1 distance between the generated image and the target (supervised learning).\n\n    \\min_G \\max_D V(D, G) = V_\\text{GAN}(D, G) + \\lambda \\, \\mathbb{E}_\\mathcal{D} [|T - G|]\n\n\n\n\npix2pix generator training. Source: https://affinelayer.com/pix2pix/.\n\n\n\n\nCycleGAN : Neural Style Transfer\nThe drawback of pix2pix is that you need paired examples of each domain, which is sometimes difficult to obtain. In style transfer, we are interested in converting images using unpaired datasets, for example realistic photographies and paintings. CycleGAN (Zhu et al., 2020) is a GAN architecture for neural style transfer.\n\n\n\nNeural style transfer requires unpaired domains (Zhu et al., 2020).\n\n\n\n\n\nNeural style transfer. Source: https://hardikbansal.github.io/CycleGANBlog/\n\n\nLet’s suppose that we want to transform domain A (horses) into domain B (zebras) or the other way around. The problem is that the two datasets are not paired, so we cannot provide targets to pix2pix (supervised learning). If we just select any zebra target for a horse input, pix2pix would learn to generate zebras that do not correspond to the input horse (the shape may be lost). How about we train a second GAN to generate the target?\n\n\n\nNeural style transfer between horses and zebras. Source: https://towardsdatascience.com/gender-swap-and-cyclegan-in-tensorflow-2-0-359fe74ab7ff\n\n\nCycle A2B2A\n\nThe A2B generator generates a sample of B from an image of A.\nThe B discriminator allows to train A2B using real images of B.\nThe B2A generator generates a sample of A from the output of A2B, which can be used to minimize the L1-reconstruction loss (shape-preserving).\n\n\n\n\nCycle A2B2A. Source: https://towardsdatascience.com/gender-swap-and-cyclegan-in-tensorflow-2-0-359fe74ab7ff\n\n\nCycle B2A2B\nIn the B2A2B cycle, the domains are reversed, what allows to train the A discriminator.\n\n\n\nCycle B2A2B. Source: https://towardsdatascience.com/gender-swap-and-cyclegan-in-tensorflow-2-0-359fe74ab7ff\n\n\nThis cycle is repeated throughout training, allowing to train both GANS concurrently.\n\n\n\nCycleGAN. Source: https://github.com/junyanz/CycleGAN.\n\n\n\n\n\nCycleGAN. Source: https://github.com/junyanz/CycleGAN.\n\n\n\n\n\nCycleGAN. Source: https://github.com/junyanz/CycleGAN.\n\n\n\n\n\n\n\nArjovsky, M., Chintala, S., and Bottou, L. (2017). Wasserstein GAN. http://arxiv.org/abs/1701.07875.\n\n\nGoodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., et al. (2014). Generative Adversarial Networks. http://arxiv.org/abs/1406.2661.\n\n\nIsola, P., Zhu, J.-Y., Zhou, T., and Efros, A. A. (2018). Image-to-Image Translation with Conditional Adversarial Networks. http://arxiv.org/abs/1611.07004.\n\n\nKarras, T., Laine, S., Aittala, M., Hellsten, J., Lehtinen, J., and Aila, T. (2020). Analyzing and Improving the Image Quality of StyleGAN. http://arxiv.org/abs/1912.04958.\n\n\nMirza, M., and Osindero, S. (2014). Conditional Generative Adversarial Nets. http://arxiv.org/abs/1411.1784.\n\n\nNowozin, S., Cseke, B., and Tomioka, R. (2016). F-GAN: Training Generative Neural Samplers using Variational Divergence Minimization. http://arxiv.org/abs/1606.00709.\n\n\nRadford, A., Metz, L., and Chintala, S. (2015). Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks. http://arxiv.org/abs/1511.06434.\n\n\nReed, S., Akata, Z., Yan, X., Logeswaran, L., Schiele, B., and Lee, H. (2016). Generative Adversarial Text to Image Synthesis. http://arxiv.org/abs/1605.05396.\n\n\nSalimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A., and Chen, X. (2016). Improved Techniques for Training GANs. http://arxiv.org/abs/1606.03498.\n\n\nZhu, Y., Gao, T., Fan, L., Huang, S., Edmonds, M., Liu, H., et al. (2020). Dark, Beyond Deep: A Paradigm Shift to Cognitive AI with Humanlike Common Sense. Engineering. doi:10.1016/j.eng.2020.01.011."
  },
  {
    "objectID": "notes/6.1-RNN.html#recurrent-neural-networks",
    "href": "notes/6.1-RNN.html#recurrent-neural-networks",
    "title": "Recurrent neural networks",
    "section": "Recurrent neural networks",
    "text": "Recurrent neural networks\n\n\nProblem with feedforward neural networks\nFeedforward neural networks learn to associate an input vector to an output.\n\\mathbf{y} = F_\\theta(\\mathbf{x})\nIf you present a sequence of inputs \\mathbf{x}_0, \\mathbf{x}_1, \\ldots, \\mathbf{x}_t to a feedforward network, the outputs will be independent from each other:\n\\mathbf{y}_0 = F_\\theta(\\mathbf{x}_0) \\mathbf{y}_1 = F_\\theta(\\mathbf{x}_1) \\dots \\mathbf{y}_t = F_\\theta(\\mathbf{x}_t)\nMany problems depend on time series, such as predicting the future of a time series by knowing its past values:\nx_{t+1} = F_\\theta(x_0, x_1, \\ldots, x_t)\nExample: weather prediction, financial prediction, predictive maintenance, natural language processing, video analysis…\nA naive solution is to aggregate (concatenate) inputs over a sufficiently long window and use it as a new input vector for the feedforward network.\n\\mathbf{X} = \\begin{bmatrix}\\mathbf{x}_{t-T} & \\mathbf{x}_{t-T+1} & \\ldots & \\mathbf{x}_t \\\\ \\end{bmatrix}\n\\mathbf{y}_t = F_\\theta(\\mathbf{X})\n\nProblem 1: How long should the window be?\nProblem 2: Having more input dimensions increases dramatically the complexity of the classifier (VC dimension), hence the number of training examples required to avoid overfitting.\n\n\n\nRecurrent neural network\nA recurrent neural network (RNN) uses it previous output as an additional input (context). All vectors have a time index t denoting the time at which this vector was computed.\nThe input vector at time t is \\mathbf{x}_t, the output vector is \\mathbf{h}_t:\n\n    \\mathbf{h}_t = \\sigma(W_x \\times \\mathbf{x}_t + W_h \\times \\mathbf{h}_{t-1} + \\mathbf{b})\n\n\\sigma is a transfer function, usually logistic or tanh. The input \\mathbf{x}_t and previous output \\mathbf{h}_{t-1} are multiplied by learnable weights:\n\nW_x is the input weight matrix.\nW_h is the recurrent weight matrix.\n\n\n\n\nRecurrent neural network. Source: http://colah.github.io/posts/2015-08-Understanding-LSTMs.\n\n\nOne can unroll a recurrent network: the output \\mathbf{h}_t depends on the whole history of inputs from \\mathbf{x}_0 to \\mathbf{x}_t.\n\n\\begin{aligned}\n    \\mathbf{h}_t & = \\sigma(W_x \\times \\mathbf{x}_t + W_h \\times \\mathbf{h}_{t-1} + \\mathbf{b}) \\\\\n                 &\\\\\n                 & = \\sigma(W_x \\times \\mathbf{x}_t + W_h \\times \\sigma(W_x \\times \\mathbf{x}_{t-1} + W_h \\times \\mathbf{h}_{t-2} + \\mathbf{b})  + \\mathbf{b}) \\\\\n                 &\\\\\n                 & = f_{W_x, W_h, \\mathbf{b}} (\\mathbf{x}_0, \\mathbf{x}_1, \\dots,\\mathbf{x}_t) \\\\\n\\end{aligned}\n\nA RNN is considered as part of deep learning, as there are many layers of weights between the first input \\mathbf{x}_0 and the output \\mathbf{h}_t. The only difference with a DNN is that the weights W_x and W_h are reused at each time step.\n\n\n\nRecurrent neural network unrolled. Source: http://colah.github.io/posts/2015-08-Understanding-LSTMs.\n\n\n\n\nBPTT: Backpropagation through time\nThe function between the history of inputs and the output at time t is differentiable: we can simply apply gradient descent to find the weights! This variant of backpropagation is called Backpropagation Through Time (BPTT). Once the loss between \\mathbf{h}_t and its desired value is computed, one applies the chain rule to find out how to modify the weights W_x and W_h using the history (\\mathbf{x}_0, \\mathbf{x}_1, \\ldots, \\mathbf{x}_t).\nLet’s compute the gradient accumulated between \\mathbf{h}_{t-1} and \\mathbf{h}_{t}:\n\n\\begin{aligned}\n    \\mathbf{h}_{t} & = \\sigma(W_x \\times \\mathbf{x}_{t} + W_h \\times \\mathbf{h}_{t-1} + \\mathbf{b}) \\\\\n\\end{aligned}\n\nAs for feedforward networks, the gradient of the loss function is decomposed into two parts:\n\n    \\frac{\\partial \\mathcal{L}(W_x, W_h)}{\\partial W_x} =\n    \\frac{\\partial \\mathcal{L}(W_x, W_h)}{\\partial \\mathbf{h}_t} \\times\n    \\frac{\\partial \\mathbf{h}_t}{\\partial W_x}\n\n\n    \\frac{\\partial \\mathcal{L}(W_x, W_h)}{\\partial W_h} =\n    \\frac{\\partial \\mathcal{L}(W_x, W_h)}{\\partial \\mathbf{h}_t} \\times\n    \\frac{\\partial \\mathbf{h}_t}{\\partial W_h}\n\nThe first part only depends on the loss function (mse, cross-entropy):\n\n    \\frac{\\partial \\mathcal{L}(W_x, W_h)}{\\partial \\mathbf{h}_t} = - (\\mathbf{t}_{t}- \\mathbf{h}_{t})\n\nThe second part depends on the RNN itself:\n\n\\begin{aligned}\n    \\mathbf{h}_{t} & = \\sigma(W_x \\times \\mathbf{x}_{t} + W_h \\times \\mathbf{h}_{t-1} + \\mathbf{b}) \\\\\n\\end{aligned}\n\nThe gradients w.r.t the two weight matrices are given by this recursive relationship (product rule):\n\n\\begin{aligned}\n    \\frac{\\partial \\mathbf{h}_t}{\\partial W_x} & = \\mathbf{h'}_{t} \\times (\\mathbf{x}_t + W_h \\times \\frac{\\partial \\mathbf{h}_{t-1}}{\\partial W_x})\\\\\n    & \\\\\n    \\frac{\\partial \\mathbf{h}_t}{\\partial W_h} & = \\mathbf{h'}_{t} \\times (\\mathbf{h}_{t-1} + W_h \\times \\frac{\\partial \\mathbf{h}_{t-1}}{\\partial W_h})\\\\\n\\end{aligned}\n\nThe derivative of the transfer function is noted \\mathbf{h'}_{t}:\n\n    \\mathbf{h'}_{t} = \\begin{cases}\n        \\mathbf{h}_{t} \\, (1 - \\mathbf{h}_{t}) \\quad \\text{ for logistic}\\\\\n        (1 - \\mathbf{h}_{t}^2) \\quad \\text{ for tanh.}\\\\\n    \\end{cases}\n\nIf we unroll the gradient, we obtain:\n\n\\begin{aligned}\n    \\frac{\\partial \\mathbf{h}_t}{\\partial W_x} & = \\mathbf{h'}_{t} \\, (\\mathbf{x}_t + W_h \\times \\mathbf{h'}_{t-1} \\, (\\mathbf{x}_{t-1} + W_h \\times \\mathbf{h'}_{t-2} \\, (\\mathbf{x}_{t-2} + W_h \\times \\ldots (\\mathbf{x}_0))))\\\\\n    & \\\\\n    \\frac{\\partial \\mathbf{h}_t}{\\partial W_h} & = \\mathbf{h'}_{t} \\, (\\mathbf{h}_{t-1} + W_h \\times \\mathbf{h'}_{t-1} \\, (\\mathbf{h}_{t-2} + W_h \\times \\mathbf{h'}_{t-2} \\, \\ldots (\\mathbf{h}_{0})))\\\\\n\\end{aligned}\n\nWhen updating the weights at time t, we need to store in memory:\n\nthe complete history of inputs \\mathbf{x}_0, \\mathbf{x}_1, … \\mathbf{x}_t.\nthe complete history of outputs \\mathbf{h}_0, \\mathbf{h}_1, … \\mathbf{h}_t.\nthe complete history of derivatives \\mathbf{h'}_0, \\mathbf{h'}_1, … \\mathbf{h'}_t.\n\nbefore computing the gradients iteratively, starting from time t and accumulating gradients backwards in time until t=0. Each step backwards in time adds a bit to the gradient used to update the weights.\nIn practice, going back to t=0 at each time step requires too many computations, which may not be needed. Truncated BPTT only updates the gradients up to T steps before: the gradients are computed backwards from t to t-T. The partial derivative in t-T-1 is considered 0. This limits the horizon of BPTT: dependencies longer than T will not be learned, so it has to be chosen carefully for the task. T becomes yet another hyperparameter of your algorithm…\n\n\n\nTruncated backpropagation through time. Source: https://r2rt.com/styles-of-truncated-backpropagation.html.\n\n\n\n\nVanishing gradients\nBPTT is able to find short-term dependencies between inputs and outputs: perceiving the inputs \\mathbf{x}_0 and \\mathbf{x}_1 allows to respond correctly at t = 3.\n\n\n\nRNN can learn short-term dependencies. Source: http://colah.github.io/posts/2015-08-Understanding-LSTMs.\n\n\nBut it fails to detect long-term dependencies because of:\n\nthe truncated horizon T (for computational reasons).\nthe vanishing gradient problem (Hochreiter, 1991).\n\n\n\n\nRNN cannot learn long-term dependencies. Source: http://colah.github.io/posts/2015-08-Understanding-LSTMs.\n\n\nLet’s look at the gradient w.r.t to the input weights:\n\n\\begin{aligned}\n    \\frac{\\partial \\mathbf{h}_t}{\\partial W_x} & = \\mathbf{h'}_{t} \\, (\\mathbf{x}_t + W_h \\times \\frac{\\partial \\mathbf{h}_{t-1}}{\\partial W_x})\\\\\n    & \\\\\n\\end{aligned}\n\nAt each iteration backwards in time, the gradients are multiplied by W_h. If you search how \\frac{\\partial \\mathbf{h}_t}{\\partial W_x} depends on \\mathbf{x}_0, you obtain something like:\n\n\\begin{aligned}\n    \\frac{\\partial \\mathbf{h}_t}{\\partial W_x} & \\approx \\prod_{k=0}^t \\mathbf{h'}_{k} \\, ((W_h)^t \\, \\mathbf{x}_0 + \\dots) \\\\\n\\end{aligned}\n\nIf |W_h| > 1, |(W_h)^t| increases exponentially with t: the gradient explodes. If |W_h| < 1, |(W_h)^t| decreases exponentially with t: the gradient vanishes.\nExploding gradients are relatively easy to deal with: one just clips the norm of the gradient to a maximal value.\n\n    || \\frac{\\partial \\mathcal{L}(W_x, W_h)}{\\partial W_x}|| \\gets \\min(||\\frac{\\partial \\mathcal{L}(W_x, W_h)}{\\partial W_x}||, T)\n\nBut there is no solution to the vanishing gradient problem for regular RNNs: the gradient fades over time (backwards) and no long-term dependency can be learned. This is the same problem as for feedforward deep networks: a RNN is just a deep network rolled over itself. Its depth (number of layers) corresponds to the maximal number of steps back in time. In order to limit vanishing gradients and learn long-term dependencies, one has to use a more complex structure for the layer. This is the idea behind long short-term memory (LSTM) networks."
  },
  {
    "objectID": "notes/6.1-RNN.html#long-short-term-memory-networks---lstm",
    "href": "notes/6.1-RNN.html#long-short-term-memory-networks---lstm",
    "title": "Recurrent neural networks",
    "section": "Long short-term memory networks - LSTM",
    "text": "Long short-term memory networks - LSTM\n\n\n\n\n\n\n\nNote\n\n\n\nAll figures in this section are taken from this great blog post by Christopher Olah, which is worth a read:\nhttp://colah.github.io/posts/2015-08-Understanding-LSTMs\n\n\n\n\n\nRNN layer. Source: http://colah.github.io/posts/2015-08-Understanding-LSTMs.\n\n\n\n\n\nLSTM layer. Source: http://colah.github.io/posts/2015-08-Understanding-LSTMs.\n\n\nA LSTM layer (Hochreiter and Schmidhuber, 1997) is a RNN layer with the ability to control what it memorizes. In addition to the input \\mathbf{x}_t and output \\mathbf{h}_t, it also has a state \\mathbf{C}_t which is maintained over time. The state is the memory of the layer (sometimes called context). It also contains three multiplicative gates:\n\nThe input gate controls which inputs should enter the memory.\n\nare they worth remembering?\n\nThe forget gate controls which memory should be forgotten.\n\ndo I still need them?\n\nThe output gate controls which part of the memory should be used to produce the output.\n\nshould I respond now? Do I have enough information?\n\n\nThe state \\mathbf{C}_t can be seen as an accumulator integrating inputs (and previous outputs) over time. The gates learn to open and close through learnable weights.\n\nState conveyor belt\n\n\n\nState conveyor belt. Source: http://colah.github.io/posts/2015-08-Understanding-LSTMs.\n\n\nBy default, the cell state \\mathbf{C}_t stays the same over time (conveyor belt). It can have the same number of dimensions as the output \\mathbf{h}_t, but does not have to. Its content can be erased by multiplying it with a vector of 0s, or preserved by multiplying it by a vector of 1s. We can use a sigmoid to achieve this:\n\n\n\nElement-wise multiplication with a vector using using the logistic/sigmoid function. Source: http://colah.github.io/posts/2015-08-Understanding-LSTMs.\n\n\n\n\nForget gate\n\n\n\nForget gate. Source: http://colah.github.io/posts/2015-08-Understanding-LSTMs.\n\n\nForget weights W_f and a sigmoid function are used to decide if the state should be preserved or not.\n\n    \\mathbf{f}_t = \\sigma(W_f \\times [\\mathbf{h}_{t-1}; \\mathbf{x}_t] + \\mathbf{b}_f)\n\n[\\mathbf{h}_{t-1}; \\mathbf{x}_t] is simply the concatenation of the two vectors \\mathbf{h}_{t-1} and \\mathbf{x}_t. \\mathbf{f}_t is a vector of values between 0 and 1, one per dimension of the cell state \\mathbf{C}_t.\n\n\nInput gate\n\n\n\nInput gate. Source: http://colah.github.io/posts/2015-08-Understanding-LSTMs.\n\n\nSimilarly, the input gate uses a sigmoid function to decide if the state should be updated or not.\n\n    \\mathbf{i}_t = \\sigma(W_i \\times [\\mathbf{h}_{t-1}; \\mathbf{x}_t] + \\mathbf{b}_i)\n\nAs for RNNs, the input \\mathbf{x}_t and previous output \\mathbf{h}_{t-1} are combined to produce a candidate state \\tilde{\\mathbf{C}}_t using the tanh transfer function.\n\n    \\tilde{\\mathbf{C}}_t = \\text{tanh}(W_C \\times [\\mathbf{h}_{t-1}; \\mathbf{x}_t] + \\mathbf{b}_c)\n\n\n\nCandidate state\n\n\n\nCandidate state. Source: http://colah.github.io/posts/2015-08-Understanding-LSTMs.\n\n\nThe new state \\mathbf{C}_t is computed as a part of the previous state \\mathbf{C}_{t-1} (element-wise multiplication with the forget gate \\mathbf{f}_t) plus a part of the candidate state \\tilde{\\mathbf{C}}_t (element-wise multiplication with the input gate \\mathbf{i}_t).\n\n    \\mathbf{C}_t = \\mathbf{f}_t \\odot \\mathbf{C}_{t-1} + \\mathbf{i}_t \\odot \\tilde{\\mathbf{C}}_t\n\nDepending on the gates, the new state can be equal to the previous state (gates closed), the candidate state (gates opened) or a mixture of both.\n\n\nOutput gate\n\n\n\nOutput gate. Source: http://colah.github.io/posts/2015-08-Understanding-LSTMs.\n\n\nThe output gate decides which part of the new state will be used for the output.\n\n    \\mathbf{o}_t = \\sigma(W_o \\times [\\mathbf{h}_{t-1}; \\mathbf{x}_t] + \\mathbf{b}_o)\n\nThe output not only influences the decision, but also how the gates will updated at the next step.\n\n    \\mathbf{h}_t = \\mathbf{o}_t \\odot \\text{tanh} (\\mathbf{C}_t)\n\n\n\nLSTM layer\nThe function between \\mathbf{x}_t and \\mathbf{h}_t is quite complicated, with many different weights, but everything is differentiable: BPTT can be applied.\n\n\n\nLSTM layer. Source: http://colah.github.io/posts/2015-08-Understanding-LSTMs.\n\n\nEquations:\n\nForget gate\n\n\\mathbf{f}_t = \\sigma(W_f \\times [\\mathbf{h}_{t-1}; \\mathbf{x}_t] + \\mathbf{b}_f)\n\nInput gate\n\n\\mathbf{i}_t = \\sigma(W_i \\times [\\mathbf{h}_{t-1}; \\mathbf{x}_t] + \\mathbf{b}_i)\n\nOutput gate\n\n\\mathbf{o}_t = \\sigma(W_o \\times [\\mathbf{h}_{t-1}; \\mathbf{x}_t] + \\mathbf{b}_o)\n\nCandidate state\n\n\\tilde{\\mathbf{C}}_t = \\text{tanh}(W_C \\times [\\mathbf{h}_{t-1}; \\mathbf{x}_t] + \\mathbf{b}_c)\n\nNew state\n\n\\mathbf{C}_t = \\mathbf{f}_t \\odot \\mathbf{C}_{t-1} + \\mathbf{i}_t \\odot \\tilde{\\mathbf{C}}_t\n\nOutput\n\n\\mathbf{h}_t = \\mathbf{o}_t \\odot \\text{tanh} (\\mathbf{C}_t)\n\n\nVanishing gradients\nHow do LSTM solve the vanishing gradient problem? Not all inputs are remembered by the LSTM: the input gate controls what comes in. If only \\mathbf{x}_0 and \\mathbf{x}_1 are needed to produce \\mathbf{h}_{t+1}, they will be the only ones stored in the state, the other inputs are ignored.\nIf the state stays constant between t=1 and t, the gradient of the error will not vanish when backpropagating from t to t=1, because nothing happens!\n\n    \\mathbf{C}_t = \\mathbf{C}_{t-1} \\rightarrow \\frac{\\partial \\mathbf{C}_t}{\\partial \\mathbf{C}_{t-1}} = 1\n\nThe gradient is multiplied by exactly one when the gates are closed.\nLSTM are particularly good at learning long-term dependencies, because the gates protect the cell from vanishing gradients. Its problem is how to find out which inputs (e.g. \\mathbf{x}_0 and \\mathbf{x}_1) should enter or leave the state memory.\nTruncated BPTT is used to train all weights: the weights for the candidate state (as for RNN), and the weights of the three gates. LSTM are also subject to overfitting. Regularization (including dropout) can be used. The weights (also for the gates) can be convolutional. The gates also have a bias, which can be fixed (but hard to find). LSTM layers can be stacked to detect dependencies at different scales (deep LSTM network).\n\n\nPeephole connections\n\n\n\nPeephole connections. Source: http://colah.github.io/posts/2015-08-Understanding-LSTMs.\n\n\nA popular variant of LSTM adds peephole connections (Gers and Schmidhuber, 2000), where the three gates have additionally access to the state \\mathbf{C}_{t-1}.\n\\begin{align}\n    \\mathbf{f}_t &= \\sigma(W_f \\times [\\mathbf{C}_{t-1}; \\mathbf{h}_{t-1}; \\mathbf{x}_t] + \\mathbf{b}_f) \\\\\n    &\\\\\n    \\mathbf{i}_t &= \\sigma(W_i \\times [\\mathbf{C}_{t-1}; \\mathbf{h}_{t-1}; \\mathbf{x}_t] + \\mathbf{b}_i) \\\\\n    &\\\\\n    \\mathbf{o}_t &= \\sigma(W_o \\times [\\mathbf{C}_{t}; \\mathbf{h}_{t-1}; \\mathbf{x}_t] + \\mathbf{b}_o) \\\\\n\\end{align}\nIt usually works better, but adds more weights.\n\n\nGRU: Gated Recurrent Unit\n\n\n\nGated recurrent unit (GRU). Source: http://colah.github.io/posts/2015-08-Understanding-LSTMs.\n\n\nAnother variant is called the Gated Recurrent Unit (GRU) (Chung et al., 2014). It uses directly the output \\mathbf{h}_t as a state, and the forget and input gates are merged into a single gate \\mathbf{r}_t.\n\\begin{align}\n    \\mathbf{z}_t &= \\sigma(W_z \\times [\\mathbf{h}_{t-1}; \\mathbf{x}_t]) \\\\\n    &\\\\\n    \\mathbf{r}_t &= \\sigma(W_r \\times [\\mathbf{h}_{t-1}; \\mathbf{x}_t]) \\\\\n    &\\\\\n    \\tilde{\\mathbf{h}}_t &= \\text{tanh} (W_h \\times [\\mathbf{r}_t \\odot \\mathbf{h}_{t-1}; \\mathbf{x}_t])\\\\\n    & \\\\\n    \\mathbf{h}_t &= (1 - \\mathbf{z}_t) \\odot \\mathbf{h}_{t-1} + \\mathbf{z}_t \\odot \\tilde{\\mathbf{h}}_t\\\\\n\\end{align}\nIt does not even need biases (mostly useless in LSTMs anyway). Much simpler to train as the LSTM, and almost as powerful.\n\n\nBidirectional LSTM\nA bidirectional LSTM learns to predict the output in two directions:\n\nThe feedforward line learns using the past context (classical LSTM).\nThe backforward line learns using the future context (inputs are reversed).\n\nThe two state vectors are then concatenated at each time step to produce the output. Only possible offline, as the future inputs must be known. Works better than LSTM on many problems, but slower.\n\n\n\nBidirectional LSTM. Source: http://www.paddlepaddle.org/doc/demo/sentiment_analysis/sentiment_analysis.html.\n\n\n\n\n\n\nChung, J., Gulcehre, C., Cho, K., and Bengio, Y. (2014). Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling. http://arxiv.org/abs/1412.3555.\n\n\nGers, F. A., and Schmidhuber, J. (2000). Recurrent nets that time and count. in Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks. IJCNN 2000. Neural Computing: New Challenges and Perspectives for the New Millennium, 189–194 vol.3. doi:10.1109/IJCNN.2000.861302.\n\n\nHochreiter, S. (1991). Untersuchungen zu dynamischen neuronalen Netzen. http://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf.\n\n\nHochreiter, S., and Schmidhuber, J. (1997). Long short-term memory. Neural computation 9, 1735–80. https://www.ncbi.nlm.nih.gov/pubmed/9377276."
  },
  {
    "objectID": "notes/6.2-NLP.html#word2vec",
    "href": "notes/6.2-NLP.html#word2vec",
    "title": "Natural Language Processing",
    "section": "word2vec",
    "text": "word2vec\n\nThe most famous application of RNNs is Natural Language Processing (NLP): text understanding, translation, etc… Each word of a sentence has to be represented as a vector \\mathbf{x}_t in order to be fed to a LSTM. Which representation should we use?\nThe naive solution is to use one-hot encoding, one element of the vector corresponding to one word of the dictionary.\n\n\n\nOne-hot encoding of words. Source: https://cdn-images-1.medium.com/max/1600/1*ULfyiWPKgWceCqyZeDTl0g.png.\n\n\nOne-hot encoding is not a good representation for words:\n\nThe vector size will depend on the number of words of the language:\n\nEnglish: 171,476 (Oxford English Dictionary), 470,000 (Merriam-Webster)… 20,000 in practice.\nFrench: 270,000 (TILF).\nGerman: 200,000 (Duden).\nChinese: 370,000 (Hanyu Da Cidian).\nKorean: 1,100,373 (Woori Mal Saem)\n\nSemantically related words have completely different representations (“endure” and “tolerate”).\nThe representation is extremely sparse (a lot of useless zeros).\n\n\n\n\nAudio and image data are dense, i.e. each input dimension carries information. One-hot encoded sentences are sparse. Source: https://www.tensorflow.org/tutorials/representation/word2vec.\n\n\nword2vec (Mikolov et al., 2013) learns word embeddings by trying to predict the current word based on the context (CBOW, continuous bag-of-words) or the context based on the current word (skip-gram). See https://code.google.com/archive/p/word2vec/ and https://www.tensorflow.org/tutorials/representation/word2vec for more information.\nIt uses a three-layer autoencoder-like NN, where the hidden layer (latent space) will learn to represent the one-hot encoded words in a dense manner.\n\n\n\nWord2vec is an autoencoder trained to reproduce the context of a word using one-hot encoded vectors. Source: https://jaxenter.com/deep-learning-search-word2vec-147782.html.\n\n\nword2vec has three parameters:\n\nthe vocabulary size: number of words in the dictionary.\nthe embedding size: number of neurons in the hidden layer.\nthe context size: number of surrounding words to predict.\n\nIt is trained on huge datasets of sentences (e.g. Wikipedia). After learning, the hidden layer represents an embedding vector, which is a dense and compressed representation of each possible word (dimensionality reduction). Semantically close words (“endure” and “tolerate”) tend to appear in similar contexts, so their embedded representations will be close (Euclidian distance). One can even perform arithmetic operations on these vectors!\n\nqueen = king + woman - man\n\n\n\n\nArithmetic operations on word2vec representations. Source: https://www.tensorflow.org/tutorials/representation/word2vec."
  },
  {
    "objectID": "notes/6.2-NLP.html#applications-of-rnns",
    "href": "notes/6.2-NLP.html#applications-of-rnns",
    "title": "Natural Language Processing",
    "section": "Applications of RNNs",
    "text": "Applications of RNNs\n\n\nClassification of LSTM architectures\nSeveral architectures are possible using recurrent neural networks:\n\n\n\nArithmetic operations on word2vec representations. Source: http://karpathy.github.io/2015/05/21/rnn-effectiveness/.\n\n\n\nOne to One: classical feedforward network.\n\nImage \\rightarrow Label.\n\nOne to Many: single input, many outputs.\n\nImage \\rightarrow Text.\n\nMany to One: sequence of inputs, single output.\n\nVideo / Text \\rightarrow Label.\n\nMany to Many: sequence to sequence.\n\nText \\rightarrow Text.\nVideo \\rightarrow Text.\n\n\n\n\nImage caption generation\nShow and Tell (Vinyals et al., 2015) uses the last FC layer of a CNN to feed a LSTM layer and generate words. The pretrained CNN (VGG16, ResNet50) is used as a feature extractor. Each word of the sentence is encoded/decoded using word2vec. The output of the LSTM at time t becomes its new input at time t+1.\n\n\n\nShow and Tell architecture. Source: (Sathe et al., 2022).\n\n\nShow, attend and tell (Xu et al., 2015) uses attention to focus on specific parts of the image when generating the sentence.\n\n\n\nShow, attend and tell (Xu et al., 2015).\n\n\n\n\n\nCaption generation using Show, attend and tell (Xu et al., 2015).\n\n\n\n\nNext character/word prediction\nCharacters or words are fed one by one into a LSTM. The desired output is the next character or word in the text.\nExample:\n\nInputs: To, be, or, not, to\nOutput: be\n\nThe text below was generated by a LSTM having read the entire writings of William Shakespeare, learning to predict the next letter (see http://karpathy.github.io/2015/05/21/rnn-effectiveness/). Each generated character is used as the next input.\nPANDARUS:\nAlas, I think he shall be come approached and the day\nWhen little srain would be attain'd into being never fed,\nAnd who is but a chain and subjects of his death,\nI should not sleep.\n\nSecond Senator:\nThey are away this miseries, produced upon my soul,\nBreaking and strongly should be buried, when I perish\nThe earth and thoughts of many states.\n\nDUKE VINCENTIO:\nWell, your wit is in the care of side and that.\n\nSecond Lord:\nThey would be ruled after this chamber, and\nmy fair nues begun out of the fact, to be conveyed,\nWhose noble souls I'll have the heart of the wars.\n\nClown:\nCome, sir, I will make did behold your worship.\n\n\n\n\n\n\nSunspring SciFi movie\n\n\n\n\nMore info: http://www.thereforefilms.com/sunspring.html\n\n\n\n\nSentiment analysis\nSentiment analysis consists of attributing a value (positive or negative) to a text. A 1D convolutional layers “slides” over the text, each word being encoded using word2vec. The bidirectional LSTM computes a state vector for the complete text. A classifier (fully connected layer) learns to predict the sentiment of the text (positive/negative).\n\n\n\nSentiment analysis using bidirectional LSTMs. Source: https://offbit.github.io/how-to-read/.\n\n\n\n\nQuestion answering / Scene understanding\nA LSTM can learn to associate an image (static) plus a question (sequence) with the answer (sequence). The image is abstracted by a CNN pretrained for object recognition.\n\n\n\nQuestion answering. Source: (Malinowski et al., 2015).\n\n\n\n\nseq2seq\nThe state vector obtained at the end of a sequence can be reused as an initial state for another LSTM. The goal of the encoder is to find a compressed representation of a sequence of inputs. The goal of the decoder is to generate a sequence from that representation. Sequence-to-sequence (seq2seq (Sutskever et al., 2014)) models are recurrent autoencoders.\n\n\n\nseq2seq architecture. Source: (Sutskever et al., 2014).\n\n\nThe encoder learns for example to encode each word of a sentence in French. The decoder learns to associate the final state vector to the corresponding English sentence. seq2seq allows automatic text translation between many languages given enough data. Modern translation tools are based on seq2seq, but with attention.\n\n\n\n\nMalinowski, M., Rohrbach, M., and Fritz, M. (2015). Ask Your Neurons: A Neural-based Approach to Answering Questions about Images. http://arxiv.org/abs/1505.01121.\n\n\nMikolov, T., Chen, K., Corrado, G., and Dean, J. (2013). Efficient Estimation of Word Representations in Vector Space. http://arxiv.org/abs/1301.3781.\n\n\nSathe, S., Shinde, S., Chorge, S., Thakare, S., and Kulkarni, L. (2022). Overview of Image Caption Generators and Its Applications. in Proceeding of International Conference on Computational Science and Applications Algorithms for Intelligent Systems., eds. S. Bhalla, M. Bedekar, R. Phalnikar, and S. Sirsikar (Singapore: Springer Nature), 105–110. doi:10.1007/978-981-19-0863-7_8.\n\n\nSutskever, I., Vinyals, O., and Le, Q. V. (2014). Sequence to Sequence Learning with Neural Networks. http://arxiv.org/abs/1409.3215.\n\n\nVinyals, O., Toshev, A., Bengio, S., and Erhan, D. (2015). Show and Tell: A Neural Image Caption Generator. http://arxiv.org/abs/1411.4555.\n\n\nXu, K., Ba, J. L., Kiros, R., Cho, K., Courville, A., Salakhutdinov, R., et al. (2015). Show, Attend and Tell: Neural Image Caption Generation with Visual Attention. in Proceedings of the 32nd International Conference on Machine Learning - Volume 37 ICML’15. (JMLR.org), 2048–2057. http://dl.acm.org/citation.cfm?id=3045118.3045336."
  },
  {
    "objectID": "notes/6.3-Attention.html#rnns-with-attention",
    "href": "notes/6.3-Attention.html#rnns-with-attention",
    "title": "Attentional neural networks",
    "section": "RNNs with Attention",
    "text": "RNNs with Attention\n\nThe problem with the seq2seq architecture is that it compresses the complete input sentence into a single state vector.\n\nFor long sequences, the beginning of the sentence may not be present in the final state vector:\n\nTruncated BPTT, vanishing gradients.\nWhen predicting the last word, the beginning of the paragraph might not be necessary.\n\nConsequence: there is not enough information in the state vector to start translating. A solution would be to concatenate the state vectors of all steps of the encoder and pass them to the decoder.\n\n\nProblem 1: it would make a lot of elements in the state vector of the decoder (which should be constant).\nProblem 2: the state vector of the decoder would depend on the length of the input sequence.\n\nAttentional mechanisms (Bahdanau et al., 2016) let the decoder decide (by learning) which state vectors it needs to generate each word at each step.\nThe attentional context vector of the decoder A^\\text{decoder}_t at time t is a weighted average of all state vectors C^\\text{encoder}_i of the encoder.\nA^\\text{decoder}_t = \\sum_{i=0}^T a_i \\, C^\\text{encoder}_i\n\nThe coefficients a_i are called the attention scores : how much attention is the decoder paying to each of the encoder’s state vectors? The attention scores a_i are computed as a softmax over the scores e_i (in order to sum to 1):\na_i = \\frac{\\exp e_i}{\\sum_j \\exp e_j} \\Rightarrow A^\\text{decoder}_t = \\sum_{i=0}^T \\frac{\\exp e_i}{\\sum_j \\exp e_j} \\, C^\\text{encoder}_i\n\nThe score e_i is computed using:\n\nthe previous output of the decoder \\mathbf{h}^\\text{decoder}_{t-1}.\nthe corresponding state vector C^\\text{encoder}_i of the encoder at step i.\nattentional weights W_a.\n\ne_i = \\text{tanh}(W_a \\, [\\mathbf{h}^\\text{decoder}_{t-1}; C^\\text{encoder}_i])\nEverything is differentiable, these attentional weights can be learned with BPTT.\nThe attentional context vector A^\\text{decoder}_t is concatenated with the previous output \\mathbf{h}^\\text{decoder}_{t-1} and used as the next input \\mathbf{x}^\\text{decoder}_t of the decoder:\n\\mathbf{x}^\\text{decoder}_t = [\\mathbf{h}^\\text{decoder}_{t-1} ; A^\\text{decoder}_t]\n\n\n\n\nSeq2seq architecture with attention (Bahdanau et al., 2016). Source: https://towardsdatascience.com/day-1-2-attention-seq2seq-models-65df3f49e263.\n\n\nThe attention scores or alignment scores a_i are useful to interpret what happened. They show which words in the original sentence are the most important to generate the next word.\n\n\n\nAlignment scores during translation. Source: https://towardsdatascience.com/day-1-2-attention-seq2seq-models-65df3f49e263.\n\n\nAttentional mechanisms are now central to NLP. The whole history of encoder states is passed to the decoder, which learns to decide which part is the most important using attention. This solves the bottleneck of seq2seq architectures, at the cost of much more operations. They require to use fixed-length sequences (generally 50 words).\n\n\n\nComparison of seq2seq and seq2seq with attention. Source: https://towardsdatascience.com/day-1-2-attention-seq2seq-models-65df3f49e263.\n\n\nGoogle Neural Machine Translation (GNMT (Wu et al., 2016)) uses an attentional recurrent NN, with bidirectional GRUs, 8 recurrent layers on 8 GPUs for both the encoder and decoder.\n\n\n\nGoogle Neural Machine Translation (GNMT (Wu et al., 2016))\n\n\n\n\n\n\nBahdanau, D., Cho, K., and Bengio, Y. (2016). Neural Machine Translation by Jointly Learning to Align and Translate. http://arxiv.org/abs/1409.0473.\n\n\nWu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., Macherey, W., et al. (2016). Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation. https://arxiv.org/abs/1609.08144v2."
  },
  {
    "objectID": "notes/7.1-Transformers.html#transformers",
    "href": "notes/7.1-Transformers.html#transformers",
    "title": "Transformers",
    "section": "Transformers",
    "text": "Transformers\n\n\nArchitecture\nAttentional mechanisms are so powerful that recurrent networks are not even needed anymore. Transformer networks (Vaswani et al., 2017) use self-attention in a purely feedforward architecture and outperform recurrent architectures. They are used in Google BERT and OpenAI GPT-2/3 for text understanding (e.g. search engine queries).\n\n\n\nArchitecture of the Transformer. Source: http://jalammar.github.io/illustrated-transformer/\n\n\nTransformer networks use an encoder-decoder architecture, each with 6 stacked layers.\n\n\n\nEncoder-decoder structure of the Transformer. Source: http://jalammar.github.io/illustrated-transformer/\n\n\nEach layer of the encoder processes the n words of the input sentence in parallel. Word embeddings (as in word2vec) of dimension 512 are used as inputs (but learned end-to-end).\n\n\n\nEncoder layer. Source: http://jalammar.github.io/illustrated-transformer/\n\n\nTwo operations are performed on each word embedding \\mathbf{x}_i:\n\nself-attention vector \\mathbf{z}_i depending on the other words.\na regular feedforward layer to obtain a new representation \\mathbf{r}_i (shared among all words).\n\n\n\n\nEncoder layer. Source: http://jalammar.github.io/illustrated-transformer/\n\n\n\n\nSelf-attention\nThe first step of self-attention is to compute for each word three vectors of length d_k = 64 from the embeddings \\mathbf{x}_i or previous representations \\mathbf{r}_i (d = 512).\n\nThe query \\mathbf{q}_i using W^Q.\nThe key \\mathbf{k}_i using W^K.\nThe value \\mathbf{v}_i using W^V.\n\n\n\n\nSelf-attention. Source: http://jalammar.github.io/illustrated-transformer/\n\n\nThis operation can be done in parallel over all words:\n\n\n\nQuery, key and value matrices. Source: http://jalammar.github.io/illustrated-transformer/\n\n\nWhy query / key / value? This a concept inspired from recommendation systems / databases. A Python dictionary is a set of key / value entries:\ntel = {\n    'jack': 4098, \n    'sape': 4139\n}\nThe query would ask the dictionary to iterate over all entries and return the value associated to the key equal or close to the query.\ntel['jacky'] # 4098\nThis would be some sort of fuzzy dictionary.\nIn attentional RNNs, the attention scores were used by each word generated by the decoder to decide which input word is relevant.\nIf we apply the same idea to the same sentence (self-attention), the attention score tells how much words of the same sentence are related to each other (context).\n\nThe animal didn’t cross the street because it was too tired.\n\nThe goal is to learn a representation for the word it that contains information about the animal, not the street.\n\n\n\nSource: https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html\n\n\nEach word \\mathbf{x}_i of the sentence generates its query \\mathbf{q}_i, key \\mathbf{k}_i and value \\mathbf{v}_i.\nFor all other words \\mathbf{x}_j, we compute the match between the query \\mathbf{q}_i and the keys \\mathbf{k}_j with a dot product:\ne_{i, j} = \\mathbf{q}_i^T \\, \\mathbf{k}_j\nWe normalize the scores by dividing by \\sqrt{d_k} = 8 and apply a softmax:\na_{i, j} = \\text{softmax}(\\dfrac{\\mathbf{q}_i^T \\, \\mathbf{k}_j}{\\sqrt{d_k}})\nThe new representation \\mathbf{z}_i of the word \\mathbf{x}_i is a weighted sum of the values of all other words, weighted by the attention score:\n\\mathbf{z}_i = \\sum_{j} a_{i, j} \\, \\mathbf{v}_j\n\n\n\nSummary of self-attention on two words. Source: http://jalammar.github.io/illustrated-transformer/\n\n\nIf we concatenate the word embeddings into a n\\times d matrix X, self-attention only implies matrix multiplications and a row-based softmax:\n\n\\begin{cases}\n    Q = X \\times W^Q \\\\\n    K = X \\times W^K \\\\\n    V = X \\times W^V \\\\\n    Z = \\text{softmax}(\\dfrac{Q \\times K^T}{\\sqrt{d_k}}) \\times V \\\\\n\\end{cases}\n\n\n\n\nSelf-attention. Source: http://jalammar.github.io/illustrated-transformer/\n\n\nNote 1: everything is differentiable, backpropagation will work.\nNote 2: the weight matrices do not depend on the length n of the sentence.\n\n\nMulti-headed self-attention\nIn the sentence The animal didn’t cross the street because it was too tired., the new representation for the word it will hopefully contain features of the word animal after training.\nBut what if the sentence was The animal didn’t cross the street because it was too wide.? The representation of it should be linked to street in that context. This is not possible with a single set of matrices W^Q, W^K and W^V, as they would average every possible context and end up being useless.\n\n\n\nSource: https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html\n\n\nThe solution is to use multiple attention heads (h=8) with their own matrices W^Q_k, W^K_k and W^V_k.\n\n\n\nMultiple attention heads. Source: http://jalammar.github.io/illustrated-transformer/\n\n\nEach attention head will output a vector \\mathbf{z}_i of size d=512 for each word. How do we combine them?\n\n\n\nMultiple attention heads. Source: http://jalammar.github.io/illustrated-transformer/\n\n\nThe proposed solution is based on ensemble learning (stacking): let another matrix W^O decide which attention head to trust… 8 \\times 512 rows, 512 columns.\n\n\n\nMultiple attention heads. Source: http://jalammar.github.io/illustrated-transformer/\n\n\n\n\n\nSummary of self-attention. Source: http://jalammar.github.io/illustrated-transformer/\n\n\nEach attention head learns a different context:\n\nit refers to animal.\nit refers to street.\netc.\n\nThe original transformer paper in 2017 used 8 attention heads. OpenAI’s GPT-3 uses 96 attention heads…\n\n\nEncoder layer\nMulti-headed self-attention produces a vector \\mathbf{z}_i for each word of the sentence. A regular feedforward MLP transforms it into a new representation \\mathbf{r}_i.\n\none input layer with 512 neurons.\none hidden layer with 2048 neurons and a ReLU activation function.\none output layer with 512 neurons.\n\nThe same NN is applied on all words, it does not depend on the length n of the sentence.\n\n\n\nEncoder layer. Source: http://jalammar.github.io/illustrated-transformer/\n\n\n\n\nPositional encoding\nAs each word is processed in parallel, the order of the words in the sentence is lost.\n\nstreet was animal tired the the because it cross too didn’t\n\nWe need to explicitly provide that information in the input using positional encoding.\nA simple method would be to append an index i = 1, 2, \\ldots, n to the word embeddings, but it is not very robust.\n\n\n\nPositional encoding. Source: http://jalammar.github.io/illustrated-transformer/\n\n\nIf the elements of the 512-d embeddings are numbers between 0 and 1, concatenating an integer between 1 and n will unbalance the dimensions. Normalizing that integer between 0 and 1 requires to know n in advance, this introduces a maximal sentence length…\nHow about we append the binary code of that integer?\n\n\n\nPositional encoding. Source: https://kazemnejad.com/blog/transformer_architecture_positional_encoding/\n\n\nSounds good, we have numbers between 0 and 1, and we just need to use enough bits to encode very long sentences. However, representing a binary value (0 or 1) with a 64 bits floating number is a waste of computational resources.\nWe can notice that the bits of the integers oscillate at various frequencies:\n\nthe lower bit oscillates every number.\nthe bit before oscillates every two numbers.\netc.\n\nWe could also represent the position of a word using sine and cosine functions at different frequencies (Fourier basis). We create a vector, where each element oscillates at increasing frequencies. The “code” for each position in the sentence is unique.\n\n\n\nPositional encoding. Source: https://kazemnejad.com/blog/transformer_architecture_positional_encoding/\n\n\nIn practice, a 512-d vector is created using sine and cosine functions.\n\n    \\begin{cases}\n        t(\\text{pos}, 2i) = \\sin(\\dfrac{\\text{pos}}{10000^{2 i / 512}})\\\\\n        t(\\text{pos}, 2i + 1) = \\cos(\\dfrac{\\text{pos}}{10000^{2 i / 512}})\\\\\n    \\end{cases}\n\n\n\n\nPositional encoding. Source: http://jalammar.github.io/illustrated-transformer/\n\n\nThe positional encoding vector is added element-wise to the embedding, not concatenated!\n\\mathbf{x}_{i} = \\mathbf{x}^\\text{embed}_{i} + \\mathbf{t}_i\n\n\nLayer normalization\nThe last tricks of the encoder layers are:\n\nskip connections (residual layer)\nlayer normalization\n\nThe input X is added to the output of the multi-headed self-attention and normalized (zero mean, unit variance).\nLayer normalization (Ba et al., 2016) is an alternative to batch normalization, where the mean and variance are computed over single vectors, not over a minibatch:\n\\mathbf{z} \\leftarrow \\dfrac{\\mathbf{z} - \\mu}{\\sigma}\nwith \\mu = \\dfrac{1}{d} \\displaystyle\\sum_{i=1}^d z_i and \\sigma = \\dfrac{1}{d} \\displaystyle\\sum_{i=1}^d (z_i - \\mu)^2.\nThe feedforward network also uses a skip connection and layer normalization.\n\n\n\nEncoder layer with layer normalization and skip connections. Source: http://jalammar.github.io/illustrated-transformer/\n\n\nWe can now stack 6 (or more, 96 in GPT-3) of these encoder layers and use the final representation of each word as an input to the decoder.\n\n\n\nThe encoder is a stack of encoder layers. Source: http://jalammar.github.io/illustrated-transformer/\n\n\n\n\nDecoder\nIn the first step of decoding, the final representations of the encoder are used as query and value vectors of the decoder to produce the first word. The input to the decoder is a “start of sentence” symbol.\n\n\n\nEncoding of a sentence. Source: http://jalammar.github.io/illustrated-transformer/\n\n\nThe decoder is autoregressive: it outputs words one at a time, using the previously generated words as an input.\n\n\n\nAutoregressive generation of words. Source: http://jalammar.github.io/illustrated-transformer/\n\n\nEach decoder layer has two multi-head attention sub-layers:\n\nA self-attention sub-layer with query/key/values coming from the generated sentence.\nAn encoder-decoder attention sub-layer, with the query coming from the generated sentence and the key/value from the encoder.\n\nThe encoder-decoder attention is the regular attentional mechanism used in seq2seq architectures. Apart from this additional sub-layer, the same residual connection and layer normalization mechanisms are used.\n\n\n\nTransformer architecture. Source (Vaswani et al., 2017).\n\n\nWhen the sentence has been fully generated (up to the <eos> symbol), masked self-attention has to applied in order for a word in the middle of the sentence to not “see” the solution in the input when learning. Learning occurs on minibatches of sentences, not on single words.\n\n\n\nMasked self-attention. Source: https://jalammar.github.io/illustrated-gpt2/\n\n\nThe output of the decoder is a simple softmax classification layer, predicting the one-hot encoding of the word using a vocabulary (vocab_size=25000).\n\n\n\nWord production from the output. Source: http://jalammar.github.io/illustrated-transformer/\n\n\n\n\nResults\nThe transformer is trained on the WMT datasets:\n\nEnglish-French: 36M sentences, 32000 unique words.\nEnglish-German: 4.5M sentences, 37000 unique words.\n\nCross-entropy loss, Adam optimizer with scheduling, dropout. Training took 3.5 days on 8 P100 GPUs. The sentences can have different lengths, as the decoder is autoregressive. The transformer network beat the state-of-the-art performance in translation with less computations and without any RNN.\n\n\n\nPerformance of the Transformer on NLP tasks. Source (Vaswani et al., 2017)."
  },
  {
    "objectID": "notes/7.1-Transformers.html#self-supervised-transformers",
    "href": "notes/7.1-Transformers.html#self-supervised-transformers",
    "title": "Transformers",
    "section": "Self-supervised Transformers",
    "text": "Self-supervised Transformers\n\nThe Transformer is considered as the AlexNet moment of natural language processing (NLP). However, it is limited to supervised learning of sentence-based translation.\nTwo families of architectures have been developed from that idea to perform all NLP tasks using unsupervised pretraining or self-supervised training:\n\nBERT (Bidirectional Encoder Representations from Transformers) from Google (Devlin et al., 2019).\nGPT (Generative Pre-trained Transformer) from OpenAI https://openai.com/blog/better-language-models/.\n\n\n\n\nSource: https://jalammar.github.io/illustrated-gpt2/\n\n\n\nBERT\nBERT (Devlin et al., 2019) only uses the encoder of the transformer (12 layers, 12 attention heads, d = 768). BERT is pretrained on two different unsupervised tasks before being fine-tuned on supervised tasks.\n\nTask 1: Masked language model. Sentences from BooksCorpus and Wikipedia (3.3G words) are presented to BERT during pre-training, with 15% of the words masked. The goal is to predict the masked words from the final representations using a shallow FNN.\n\n\n\n\nMasked language model. Source: https://jalammar.github.io/illustrated-bert/\n\n\n\nTask 2: Next sentence prediction. Two sentences are presented to BERT. The goal is to predict from the first representation whether the second sentence should follow the first.\n\n\n\n\nNext sentence prediction. Source: https://jalammar.github.io/illustrated-bert/\n\n\nOnce BERT is pretrained, one can use transfer learning with or without fine-tuning from the high-level representations to perform:\n\nsentiment analysis / spam detection\nquestion answering\n\n\n\n\nSentiment analysis with BERT. Source: https://jalammar.github.io/illustrated-bert/\n\n\n\n\n\nTransfer learning with BERT. Source: https://jalammar.github.io/illustrated-bert/\n\n\n\n\nGPT\nGPT is an autoregressive language model learning to predict the next word using only the transformer’s decoder.\n\n\n\nGPT is the decoder of the Transformer. Source: https://jalammar.github.io/illustrated-gpt2/\n\n\nAutoregression mimicks a LSTM that would output words one at a time.\n\n\n\nAutoregression. Source: https://jalammar.github.io/illustrated-gpt2/\n\n\nGPT-2 comes in various sizes, with increasing performance. GPT-3 is even bigger, with 175 billion parameters and a much larger training corpus.\n\n\n\nGPT sizes. Source: https://jalammar.github.io/illustrated-gpt2/\n\n\nGPT can be fine-tuned (transfer learning) to perform machine translation.\n\n\n\nMachine translation with GPT. Source: https://jalammar.github.io/illustrated-gpt2/\n\n\nGPT can be fine-tuned to summarize Wikipedia articles.\n\n\n\nWikipedia summarization with GPT. Source: https://jalammar.github.io/illustrated-gpt2/\n\n\n\n\n\nWikipedia summarization with GPT. Source: https://jalammar.github.io/illustrated-gpt2/\n\n\n\n\n\n\n\n\nNote\n\n\n\nTry transformers at https://huggingface.co/\npip install transformers\n\n\nGithub and OpenAI trained a GPT-3-like architecture on the available open source code. Copilot is able to “autocomplete” the code based on a simple comment/docstring.\n\n\n\nGithub Copilot. https://copilot.github.com/\n\n\nAll NLP tasks (translation, sentence classification, text generation) are now done using transformer-like architectures (BERT, GPT), unsupervisedly pre-trained on huge corpuses. BERT can be used for feature extraction, while GPT is more generative. Transformer architectures seem to scale: more parameters = better performance. Is there a limit?\nThe price to pay is that these models are very expensive to train (training one instance of GPT-3 costs 12M$) and to use (GPT-3 is only accessible with an API). Many attempts have been made to reduce the size of these models while keeping a satisfying performance.\n\nDistilBERT, RoBERTa, BART, T5, XLNet…\n\nSee https://medium.com/mlearning-ai/recent-language-models-9fcf1b5f17f5."
  },
  {
    "objectID": "notes/7.1-Transformers.html#vision-transformers",
    "href": "notes/7.1-Transformers.html#vision-transformers",
    "title": "Transformers",
    "section": "Vision transformers",
    "text": "Vision transformers\n\n\nViT\nThe transformer architecture can also be applied to computer vision, by splitting images into a sequence of small patches (16x16). The sequence of vectors can then be classified by the output of the transformer using labels.\n\n\n\nVision Tranformer (ViT). Source: https://ai.googleblog.com/2020/12/transformers-for-image-recognition-at.html\n\n\nThe Vision Transformer (ViT, (Dosovitskiy et al., 2021)) outperforms state-of-the-art CNNs while requiring less computations.\n\n\n\nViT performance. Source: (Dosovitskiy et al., 2021)\n\n\n\n\n\nViT performance. Source: (Dosovitskiy et al., 2021)\n\n\n\n\nSiT\nViT only works on big supervised datasets (ImageNet). Can we benefit from self-supervised learning as in BERT or GPT? The Self-supervised Vision Transformer (SiT) (Atito et al., 2021) has an denoising autoencoder-like structure, reconstructing corrupted patches autoregressively.\n\n\n\nSiT architecture (Atito et al., 2021).\n\n\nSelf-supervised learning is possible through from data augmentation techniques. Various corruptions (masking, replacing, color distortion, blurring) are applied to the input image, but SiT must reconstruct the original image (denoising autoencoder).\n\n\n\nData augmentation for SiT (Atito et al., 2021).\n\n\nAn auxiliary rotation loss forces SiT to predict the orientation of the image (e.g. 30°). Another auxiliary contrastive loss ensures that high-level representations are different for different images.\n\n\n\nPerformance of SiT (Atito et al., 2021).\n\n\n\n\nDINO\nA recent approach for self-supervised learning has been proposed by Facebook AI researchers using self-distillation (Self-distillation with no labels - DINO, (Caron et al., 2021)). The images are split into global and local patches at different scales. Global patches contain label-related information (whole objects) while local patches contain finer details.\n\n\n\nGlobal and local views for DINO. Source: https://towardsdatascience.com/on-dino-self-distillation-with-no-labels-c29e9365e382\n\n\nThe idea of self-distillation in DINO is to use two similar ViT networks to classify the patches. The teacher network gets the global views as an input, while the student network get both the local and global ones. Both have a MLP head to predict the softmax probabilities, but do not use any labels.\n\n\n\nSelf-distillation in DINO. (Caron et al., 2021).\n\n\nThe student tries to imitate the output of the teacher, by minimizing the cross-entropy (or KL divergence) between the two probability distributions. The teacher slowly integrates the weights of the student (momentum or exponentially moving average ema):\n\\theta_\\text{teacher} \\leftarrow \\beta \\, \\theta_\\text{teacher} + (1 - \\beta) \\, \\theta_\\text{student}\n\n\n\nDINO training. Source: https://ai.facebook.com/blog/dino-paws-computer-vision-with-self-supervised-transformers-and-10x-more-efficient-training/\n\n\nThe predicted classes do not matter when pre-training, as there is no ground truth. The only thing that matters is the high-level representation of an image before the softmax output, which can be used for transfer learning. Self-distillation forces the representations to be meaningful at both the global and local scales, as the teacher gets global views. ImageNet classes are already separated in the high-level representations: a simple kNN (k-nearest neighbour) classifier achieves 74.5% accuracy (vs. 79.3% for a supervised ResNet50).\n\n\n\nt-SNE visualization of the feature representations in DINO. Source: https://ai.facebook.com/blog/dino-paws-computer-vision-with-self-supervised-transformers-and-10x-more-efficient-training/\n\n\nMore interestingly, by looking at the self-attention layers, one can obtain saliency maps that perform object segmentation without ever having been trained to!"
  },
  {
    "objectID": "notes/7.1-Transformers.html#time-series-transformers",
    "href": "notes/7.1-Transformers.html#time-series-transformers",
    "title": "Transformers",
    "section": "Time series transformers",
    "text": "Time series transformers\nTransformers can also be used for time-series classification or forecasting instead of RNNs. Example: weather forecasting, market prices, etc.\n\n\n\nTime series forecasting. (Wu et al., 2020)\n\n\n\n\n\nTime series transformer. (Wu et al., 2020)\n\n\n\n\n\n\nAtito, S., Awais, M., and Kittler, J. (2021). SiT: Self-supervised vIsion Transformer. http://arxiv.org/abs/2104.03602.\n\n\nBa, J. L., Kiros, J. R., and Hinton, G. E. (2016). Layer Normalization. http://arxiv.org/abs/1607.06450.\n\n\nCaron, M., Touvron, H., Misra, I., Jégou, H., Mairal, J., Bojanowski, P., et al. (2021). Emerging Properties in Self-Supervised Vision Transformers. http://arxiv.org/abs/2104.14294.\n\n\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. http://arxiv.org/abs/1810.04805.\n\n\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., et al. (2021). An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. http://arxiv.org/abs/2010.11929.\n\n\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., et al. (2017). Attention Is All You Need. http://arxiv.org/abs/1706.03762.\n\n\nWu, N., Green, B., Ben, X., and O’Banion, S. (2020). Deep Transformer Models for Time Series Forecasting: The Influenza Prevalence Case. http://arxiv.org/abs/2001.08317."
  },
  {
    "objectID": "notes/7.2-ContrastiveLearning.html",
    "href": "notes/7.2-ContrastiveLearning.html",
    "title": "Contrastive Learning",
    "section": "",
    "text": "Coming soon…"
  },
  {
    "objectID": "notes/8.1-Limits.html#the-ai-hype-and-general-artificial-intelligence",
    "href": "notes/8.1-Limits.html#the-ai-hype-and-general-artificial-intelligence",
    "title": "Limits of deep learning",
    "section": "The AI hype and general artificial intelligence",
    "text": "The AI hype and general artificial intelligence\n\n\n“Intuition, insight, and learning are no longer exclusive possessions of human beings: any large high-speed computer can be programed to exhibit them also.”\nHerbert Simon, MIT, Nobel Prize, Turing award, 1958.\n\nIt is not the first time in history that the field of Artificial Intelligence pretends to be just a few years away from a true general artificial intelligence.\n\nBased on the progress allowed by deep learning, recent declarations are of the same essence:\n\n“If a typical person can do a mental task with less than one second of thought, we can probably automate it using AI either now or in the near future.”\nAndrew Ng, Stanford University, Google Brain / Baidu, 2016.\n\n\n“The development of full artificial intelligence could spell the end of the human race… It would take off on its own, and re-design itself at an ever increasing rate. Humans, who are limited by slow biological evolution, couldn’t compete, and would be superseded.”\nStephen Hawking, Cambridge University, 2014.\n\n\n“Artificial intelligence will reach human levels by around 2029. Follow that out further to, say, 2045, we will have multiplied the intelligence, the human biological machine intelligence of our civilization a billion-fold.”\nRay Kurzweil, Google, 2017.\n\nThe reasoning is that if technological progress continues at its current rate, it will increase exponentially. Artificial Intelligence will soon reach the human intelligence level: this is the singularity. Past that point, super artificial intelligence will be infinitely more intelligent than humans. Skynet syndrome: Will machines still need us after the singularity? Kurzweil and colleagues argue for transhumanity, i.e. the augmentation of human intelligence by super AI.\nThe singularity hypothesis relies on an exponential increase of computational power. Moore’s law (the number of transistors in a dense integrated circuit doubles about every two years) is the only known physical process following an exponential curve, and it is coming to an end.\n\n\n\nMoore’s law. Source: https://www.alleywatch.com/2017/03/preparing-end-moores-law/\n\n\nBut is scientific knowledge exponentially increasing?\n\n“Max Planck said, ‘Science progresses one funeral at a time.’ The future depends on some graduate student who is deeply suspicious of everything I have said”\nGeoffrey Hinton, Univ. Toronto, 2017.\n\nIs the current deep learning approach taking all the light, at the expense of more promising approaches? Science progresses with breakthroughs, which are by definition unpredictable. Serendipity (luck + curiosity) is at the heart of scientific discoveries (gravity, microwaves, etc)."
  },
  {
    "objectID": "notes/8.1-Limits.html#current-limitations-of-deep-learning",
    "href": "notes/8.1-Limits.html#current-limitations-of-deep-learning",
    "title": "Limits of deep learning",
    "section": "Current limitations of deep learning",
    "text": "Current limitations of deep learning\n\n\nData is never infinite\nDeep networks are very powerful and complex models, which tend to overfit (bad interpolation). They learn their parameters from the training data only:\n \nDatasets for deep learning are typically huge:\n\nImageNet (14 million images)\nOpenImages (9 million images)\nMachine Translation of Various Languages (30 million sentences)\nLibrispeech (1000 hours of speech)\n…\n\nThe deeper your network, the more powerful, but the more data it needs to be useful. Solutions: data augmentation, transfer learning, unsupervised pre-training…\nDeep Reinforcement Learning has the same sample complexity problem: it needs many trial-and-errors to find a correct behavior. DQN and its variants need 200 million frames to learn to play Atari games: 38 days of uninterrupted human playing… On December 18th 2018, Google Deepmind defeated the human team “Mana” on Starcraft II, a much more complex game than Go for computers.\n\n“The AlphaStar league was run for 14 days, using 16 TPUs for each agent. During training, each agent experienced up to 200 years of real-time StarCraft play.”\nSource: https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/\n\n\n\nComputational power and energy\n\n\n\nThe computational power needed by deep architectures has increased exponentially in the last decade. Source : https://openai.com/blog/ai-and-compute/\n\n\nThe computational power needed by deep networks increases exponentially: more layers, more parameters, more data, more everything. Training modern deep networks is now out of reach of most universities / companies. GPT-3 (OpenAI) was trained on 500B words (Wikipedia, Common Crawl) and has 175B parameters. Training it on a single V100 would take 355 years and cost 4.6 M$ in the cloud.\nInference times (making a prediction after training) become prohibitive: it is hard to use deep networks on low-budget hardware such as smartphones or embedded hardware (FPGA, DSP), computations must be deported to the cloud. Can’t we make the networks smaller after training?\n\nQuantization\nNN require single or double-precision floating numbers (32 or 64 bits) to represent weights during learning, as small learning rates are used (e.g. 10^{-5}) to add very small quantities to them. After learning, do we need such a high precision? 2.378898437534897932 \\approx 2.4\nQuantization consists of transforming the weights into 8-bits integers or even 1 or 2 bits (binary networks) without losing (too much accuracy). Frameworks such as Tensorflow Lite, TensorRT or PyTorch allow to automatically apply quantization on pretrained networks and embed, or even to use Quantization-aware training (QAT). See https://arxiv.org/pdf/2004.09602.pdf for a review.\n\n\n\nPrinciple of quantization. Source: https://medium.com/@kaustavtamuly/compressing-and-accelerating-high-dimensional-neural-networks-6b501983c0c8\n\n\n\n\nPruning\nAnother technique to reduce inference times by making the networks smaller is pruning: removing weights, filters, neurons or even layers that are not necessary after learning.\nNN need a lot of weights/neurons to find the solution (training), but not obligatorily to implement it. Several metrics or techniques can be used to decide whether or not to keep parameters:\n\nthresholds\nredundancy\ncontribution to loss\n\nSome methods iteratively re-train the network after pruning, leading to reductions up to 90%. See https://link.springer.com/article/10.1007/s10462-020-09816-7 for a review.\n\n\n\nPruning. Source: https://towardsdatascience.com/pruning-deep-neural-network-56cae1ec5505\n\n\n\n\nModel distillation\nIn model distillation (Hinton et al., 2015), the deep teacher learns to perform classification on the hard one-hot encoded labels. Its knowledge can be transferred (distilled) to a shallower network. The shallow student learns to perform regression on the logits \\mathbf{z} of the softmax output of the teacher, which is easier and leads to the same accuracy!\n\n    y_j = P(\\text{class = j} | \\mathbf{x}) = \\mathcal{S}(z_j) = \\frac{\\exp(z_j)}{\\sum_k \\exp(z_k)}\n\nLogits carry information about the similarity between classes: cats are closer to dogs than to cars. See (Gou et al., 2020) for a review.\n\n\n\nModel distillation (Hinton et al., 2015). Source: (Gou et al., 2020)\n\n\n\n\n\nData is biased\nDeep networks only learn from data, so if the data is wrong or biased, the predictions will reproduce it.\n\nScenario 1: you use AI to sort CVs based on how well your previous employees performed.\n\nIf you only hired white middle-aged men in the past, the AI will discard all the others. (Amazon)\n\nScenario 2: you use AI to predict whether an individual is likely to commit a crime based on population statistics.\n\nBlack people are 37% of the incarcerated population in the US, but only 12% of the population. Black people will be overly tagged as potential criminals. (DoJ)\n\nScenario 3: You train your speech recognition system on male American voices.\n\nYou will not recognize female voices or foreign accents well (everybody).\n\nScenario 4: You create an AI chatbot on twitter, “Tay.ai”, learning from conversations with the twitter crowd.\n\nThe chatbot became in hours a horrible sexist, racist, homophobic monster (Microsoft).\n\n\nAI bias is currently taken very seriously by the major players. Sources: https://www.fastcompany.com/40536485/now-is-the-time-to-act-to-stop-bias-in-ai, https://www.weforum.org/agenda/2019/01/to-eliminate-human-bias-from-ai-we-need-to-rethink-our-approach/\n\n\nAdversarial attacks\nOne major problem of deep networks is that they are easy to fool.\n\n\n\nAdversarial attacks. Source: https://blog.openai.com/adversarial-example-research\n\n\nInstead of searching for the weights which produce the right output for a given image (training), you search for the image that produces a different output for a given set of trained weights (adversarial attacks, (Goodfellow et al., 2015)). It turns out that a minimal change on the input image is enough to completely change the output of a trained network. Using neural networks everywhere (self-driving cars, biometric recognition) poses serious security issues which are unsolved as of now. Many different attacks and defenses are currently investigated https://arxiv.org/pdf/1712.07107.pdf.\nLet’s suppose we have a network trained to recognize cats from dogs using the loss function \\mathcal{L}(\\theta). As an attacker, you want to find a cat-like image \\mathbf{x}' that makes the network answer dog. You define an adversarial loss making the network want to answer dog for a cat image:\n\n    \\mathcal{L}_\\text{adversarial}(\\mathbf{x}) = \\mathbb{E}_{\\mathbf{x} \\in  \\text{cat}} ||\\text{dog} - \\mathbf{y}(\\mathbf{x})||^2\n\nStarting from a cat image \\mathbf{x}, you can apply gradient descent on the image space to minimize the adversarial loss:\n\n    \\Delta \\mathbf{x} = - \\eta \\, \\frac{\\partial \\mathcal{L}_\\text{adversarial}(\\mathbf{x})}{\\partial \\mathbf{x}}\n\nOne should add a constraint on \\Delta \\mathbf{x} to keep it small (Lagrange optimization). You only need access to the output \\mathbf{y} to attack the network, not its weights (blackbox attack)\nAdversarial attacks work even when printed on paper.\n\n\n\nAdversarial attacks on printed images. Source: https://blog.openai.com/adversarial-example-research\n\n\n\n\n\nAdversarial attacks on object detection. Source: https://blog.openai.com/adversarial-example-research\n\n\nThey also work in real life: a couple of stickers are enough to have this stop sign recognized as a speed limit sign by an autonomous car…\n\n\n\nReal-world adversarial attacks. Source: https://arxiv.org/abs/1707.08945\n\n\nFace identification is a major issue:\n\n\n\nFacial adversarial attacks. When adding adversarial glasses to Reese Witherspoon, the face detector recognizes her as Russel Crowe. Source: https://arxiv.org/abs/1707.08945\n\n\n\n\nLearning is mostly offline\nNN are prone to catastrophic forgetting: if you learn A then B, you forget A.\n\n\n\nCatastrophic forgetting.\n\n\nThe only solution is to mix A and B during training (stochastic gradient descent). Online learning or lifelong learning is very difficult: you can’t adapt a NN once it has learned. Currently a hot topic of research, but not working yet.\n\n\nOne task at a time\nThe fact that computers can be better than humans on single tasks should not be worrying: The program written by Jim Slagle for his PhD thesis with Marvin Minsky was already better than MIT students at calculus in 1961.\nDeep networks are still highly specialized, they do either:\n\nComputer Vision\nSpeech processing\nNatural Language Processing\nMotor Control\n\nbut not two at the same time. Some may be able to play different games at the same time (DQN, AlphaZero) but it stays in the same domain. The ability to perform different tasks at the same time is a criteria for general intelligence. See Gato (Reed et al., 2022) and MIA (Abramson et al., 2022) from Deepmind.\n\n\nExplainable / interpretable AI\nDeep networks do not learns concepts such as cats, dogs, paddles or walls: they merely learn correlations between images and labels. Comparative (animal) psychology sometimes call this phenomenon overattribution. We want AI to be intelligent, so we attribute it intelligent features. The only way to verify this is to have deep networks verbalize their decisions (not there yet).\nResearch on interpretability (XAI, explainable AI) may allow to better understand and trust how deep networks take decisions. Neural networks are black box models: they are able to learn many things, but one does not know how. Can we really trust their decisions? This is particularly important for safety-critical applications (self-driving cars, nuclear plants, etc).\nLayer-wise relevance propagation (Binder et al., 2016) allows to visualize which part of the input is most resposnible for the prediction. It is a form of backpropagation, but from the prediction \\mathbf{y} to the input \\mathbf{x}, instead of from the loss function \\mathcal{L}(\\theta) to the parameters \\theta. See http://www.heatmapping.org/ for explanations and code.\n\n\n\nLayer-wise relevance propagation. Source: (Binder et al., 2016).\n\n\nThe results are sometimes surprising (Lapuschkin et al., 2019). Horse images in Pascal VOC all have a tag in the bottom left. The CNN has learned to detect that tag, not the horse…\n\n\n\nClever Hans effect: the answer is correct, but for bad reasons… Source: (Lapuschkin et al., 2019)."
  },
  {
    "objectID": "notes/8.1-Limits.html#what-deep-learning-might-never-be-able-to-do",
    "href": "notes/8.1-Limits.html#what-deep-learning-might-never-be-able-to-do",
    "title": "Limits of deep learning",
    "section": "What deep learning might never be able to do",
    "text": "What deep learning might never be able to do\n\n\nNo real generalization\nDeep networks can be forced to interpolate with enough data (generalization), but cannot extrapolate. For example, CNNs do not generalize to different viewpoints, unless you add them to the training data:\n\n\n\nNN cann only generalize to rotated images if they have seen examples during training. Source: http://imatge-upc.github.io/telecombcn-2016-dlcv\n\n\n\n\nLack of abstraction\n\nA schmister is a sister over the age of 10 but under the age of 21.\nDo you have a schmister?\n\nDeep learning currently lacks a mechanism for learning abstractions through explicit, verbal definition. They would need to experience thousands of sentences with schmister before they can use it.\n\n“Indeed even 7-month old infants can do so, acquiring learned abstract language-like rules from a small number of unlabeled examples, in just two minutes (Marcus, Vijayan, Bandi Rao, & Vishton, 1999).”\nMarcus, G. (2018). Deep Learning: A Critical Appraisal. arXiv:1801.00631.\n\n\n\nLack of common sense\n\nI stuck a pin in a carrot; when I pulled the pin out, it had a hole.\nWhat has a hole, the carrot or the pin?\n\nDL models do not have a model of physics: if the task (and the data) do not contain physics, it won’t learn it. DL finds correlations between the inputs and the outputs, but not the causation. Using gigantic datasets as in GPT-3 might give the illusion of reasoning, but it sometimes fails on surprisingly simple tasks. DL has no theory of mind: when playing against humans (Go), it does not bother inferring the opponent’s mental state, it just plays his game.\n\n\n\nCorrelations are everywhere, but not causation.\n\n\nNo DL model to date has been able to show causal reasoning (or at least in a generic way). Other AI approaches are better at causal reasoning (hierarchical Bayesian computing, probabilistic graphical models), but they do not mix well with deep learning yet.\n\n\nGame fallacy\nDeep learning has only been successful on relatively “easy” tasks until now. Games like Chess or Go are easy for AI, as the rules are simple, fixed and deterministic. Things get much more complicated when you go in the real-world: think of where the Robocup is.\nMoravec’s paradox (https://blog.piekniewski.info/2016/11/15/ai-and-the-ludic-fallacy/):\n\nIt is comparatively easy to make computers exhibit adult level performance on intelligence tests or playing checkers, and difficult or impossible to give them the skills of a one-year-old when it comes to perception and mobility.\n\n\n\n\n\nEmbodiment\nIntelligence and cognition require a body to interact with the world. The brain is not an isolated number cruncher. The body valuates the world: it provides needs, goals, emotions. It can even be a co-processor of the brain: gut feelings. Emotions are totally absent from the current AI approach. Goals are set externally: so-called AIs do not form their own goals (desirable?). Deep Reinforcement Learning is a first small step in that direction.\n\n\nConclusion on the limits of deep learning\nDeep learning methods are very powerful and have not reached yet their full potential for technological applications. However, there are fundamental reasons why deep learning methods may not reach general intelligence. End-to-end learning with backpropagation works very well, but what if it was the problem?\n\nMy view is throw it all away and start again.\nGeoffrey Hinton on backpropagation.\n\nThe only intelligent system we know is the brain. By taking inspiration from how the brain works, instead of stupidly minimizing loss functions, we may be able to reach human intelligence.\n\n\n\n\n\n\nSome references on the limitations of deep learning\n\n\n\n\nMarcus, G. (2018). Deep Learning: A Critical Appraisal. arXiv:1801.00631. Available at: http://arxiv.org/abs/1801.00631.\nMarcus, G. (2018). In defense of skepticism about deep learning. Available at: https://medium.com/@GaryMarcus/in-defense-of-skepticism-about-deep-learning-6e8bfd5ae0f1.\nPiekniewski, F. (2018). AI winter is well on its way. Piekniewski’s blog. Available at: https://blog.piekniewski.info/2018/05/28/ai-winter-is-well-on-its-way.\nBrooks, R. (2019). Predictions Scorecard, 2019 January 01. Available at: http://rodneybrooks.com/predictions-scorecard-2019-january-01.\nRichbourg, R. (2018). ‘It’s Either a Panda or a Gibbon’: AI Winters and the Limits of Deep Learning. Available at: https://warontherocks.com/2018/05/its-either-a-panda-or-a-gibbon-ai-winters-and-the-limits-of-deep-learning.\nSeif, G. (2019). Is Deep Learning Already Hitting its Limitations? Available at: https://towardsdatascience.com/is-deep-learning-already-hitting-its-limitations-c81826082ac3.\nHawkins, J (2015). The Terminator Is Not Coming. The Future Will Thank Us. Available at: https://www.recode.net/2015/3/2/11559576/the-terminator-is-not-coming-the-future-will-thank-us.\n\n\n\n\n\n\n\nAbramson, J., Ahuja, A., Brussee, A., Carnevale, F., Cassin, M., Fischer, F., et al. (2022). Creating Multimodal Interactive Agents with Imitation and Self-Supervised Learning. doi:10.48550/arXiv.2112.03763.\n\n\nBinder, A., Montavon, G., Bach, S., Müller, K.-R., and Samek, W. (2016). Layer-wise Relevance Propagation for Neural Networks with Local Renormalization Layers. http://arxiv.org/abs/1604.00825.\n\n\nGoodfellow, I. J., Shlens, J., and Szegedy, C. (2015). Explaining and Harnessing Adversarial Examples. http://arxiv.org/abs/1412.6572.\n\n\nGou, J., Yu, B., Maybank, S. J., and Tao, D. (2020). Knowledge Distillation: A Survey. http://arxiv.org/abs/2006.05525.\n\n\nHinton, G., Vinyals, O., and Dean, J. (2015). Distilling the Knowledge in a Neural Network. http://arxiv.org/abs/1503.02531.\n\n\nLapuschkin, S., Wäldchen, S., Binder, A., Montavon, G., Samek, W., and Müller, K.-R. (2019). Unmasking Clever Hans predictors and assessing what machines really learn. Nature Communications 10, 1096. doi:10.1038/s41467-019-08987-4.\n\n\nReed, S., Zolna, K., Parisotto, E., Colmenarejo, S. G., Novikov, A., Barth-Maron, G., et al. (2022). A Generalist Agent. http://arxiv.org/abs/2205.06175."
  },
  {
    "objectID": "notes/8.2-Beyond.html#towards-biological-deep-learning",
    "href": "notes/8.2-Beyond.html#towards-biological-deep-learning",
    "title": "Beyond deep Learning",
    "section": "Towards biological deep learning?",
    "text": "Towards biological deep learning?\n\n\nThe credit assignment problem\nThe credit assignment problem is the issue of knowing which part of the brain is responsible when something goes wrong (or well) so that it can learn from it.\n\n\n\nCredit assignement problem. Source: https://simons.berkeley.edu/sites/default/files/docs/9574/backpropagationanddeeplearninginthebrain-timothylillicrap.pdf\n\n\nBackpropagation solves the credit assignment problem by transmitting the error gradient backwards through the weights (\\sim synapses).\n\\Delta W_0 = \\eta \\, (\\mathbf{t} - \\mathbf{y}) \\times W_1 \\times \\mathbf{x}^T\n\n\n\nBackpropagation solves the credit assignement problem by transmitting the error backwards. Source: https://simons.berkeley.edu/sites/default/files/docs/9574/backpropagationanddeeplearninginthebrain-timothylillicrap.pdf\n\n\nBut information only goes in one direction in the brain: from the presynaptic neuron to the postsynaptic one. A synapse does know not the weight of other synapses and cannot transmit anything backwards. Backpropagation is not biologically plausible in its current formulation.\n\n\nFeedback alignment\nAn alternative mechanism consists of backpropagating the error through another set of feedback weights (feedback alignment, (Lillicrap et al., 2016)). Feedback connections are ubiquitous in the brain, especially in the neocortex. The feedback weights do not need to learn: they can stay random but still transmit useful gradients. The mechanism only works for small networks on MNIST for now.\n\n\n\nFeedback alignment. Source: (Lillicrap et al., 2016).\n\n\n\n\nDeep learning architectures are way too simple and unidirectional\nDeep learning architectures are mostly unidirectional, from the input to the output, without feedback connections. The brain is totally differently organized: a big “mess” of interconnected areas processing everything in parallel. The figure on the left is only for vision, and only for the cerebral cortex: the thalamus, basal ganglia, hippocampus, cerebellum, etc, create additional shortcuts. Is the complex structure of the brain just a side effect of evolution, or is it the only possible solution? Inductive bias: the choice of the architecture constrains the functions it can perform / learn.\n\n\n\nOrganization of the visual system. Felleman, D. J., and Van Essen, D. C. (1991). Distributed hierarchical processing in the primate cerebral cortex. Cereb. Cortex 1, 1–47. doi:10.1093/cercor/1.1.1\n\n\n\n\nBiological neurons have dynamics\nThe artificial neuron has no dynamics, it is a simple mathematical function:\n\n    y = f( \\sum_{i=1}^d w_i \\, x_i + b)\n\nIf you do not change the inputs to an artificial neuron, its output won’t change. Time does not exist, even in a LSTM: the only temporal variable is the frequency at which inputs are set.\nBiological neurons have dynamics:\n\nThey adapt their firing rate to constant inputs.\nThey continue firing after an input disappears.\nThey fire even in the absence of inputs (tonic).\n\nThese dynamics are essential to information processing in the brain.\n\n\nRecurrent dynamics and emergence of functions\nRecurrent networks of dynamical neurons can exhibit very complex dynamics. Biological neural networks evolve at the edge of chaos, i.e. in a highly non-linear regime while still being deterministic. This allows the emergence of complex functions: the whole is more than the sum of its parts.\n\n\n\nReservoir computing.\n\n\n\n\nSelf-organization\nThere are two complementary approaches to unsupervised learning:\n\nthe statistical approach, which tries to extract the most relevant information from the distribution of unlabeled data (autoencoders, etc).\nself-organization, which tries to understand the principles of organization of natural systems and use them to create efficient algorithms.\n\nSelf-organization is a generic process relying on four basic principles: locality of computations, learning, competition and cooperation.\nSelf-organization is observed in a wide range of natural processes:\n\nPhysics: formation of crystals, star formation, chemical reactions…\nBiology: folding of proteins, social insects, flocking behavior, brain functioning, Gaia hypothesis…\nSocial science: critical mass, group thinking, herd behavior…\n\nA self-organizing system is composed of elementary units (particles, cells, neurons, organs, individuals…) which all perform similar deterministic functions (rule of behavior) on a small part of the available information.\nThere is no central supervisor or coordinator that knows everything and tells each unit what to do: they have their own rule of behavior and apply it to the information they receive. The units are able to adapt their behavior to the available information: principle of localized learning. There is no explicit loss function specifying what the system should do: emergence.\n\n\n\nConway’s game of life. Source: https://www.jakubkonka.com/2015/03/15/game-of-life.html\n\n\nThe rules of Conway’s Game of Life (1970) are extremely simple:\n\nA cell is either dead or alive.\nA living cell with less than 1 neighbor dies.\nA living cell with more than 4 neighbors dies.\nA dead cell with 3 neighbors relives.\n\nDespite this simplicity, GoL can exhibit very complex patterns (fractals, spaceships, pulsars). The GoL is an example of self-organizing cellular automata https://en.wikipedia.org/wiki/Conway%27s_Game_of_Life.\n\n\nKey differences between deep networks and the brain\n\nNo backpropagation in the brain, at least in its current form.\nInformation processing is local to each neuron and synapse.\nHighly recurrent architecture (feedback connections).\nNeurons have non-linear dynamics, especially as populations (edge of chaos).\nEmergence of functions: the whole is more than the sum of its parts\nSelf-organization. There is no explicit loss function to minimize: the only task of the brain is to ensure survival of the organism (homeostasis).\n\n\n\n\n\nLillicrap, T. P., Cownden, D., Tweed, D. B., and Akerman, C. J. (2016). Random synaptic feedback weights support error backpropagation for deep learning. Nat Commun 7, 1–10. doi:10.1038/ncomms13276."
  },
  {
    "objectID": "exercises/Content.html#introduction-to-python",
    "href": "exercises/Content.html#introduction-to-python",
    "title": "List of exercises",
    "section": "Introduction to Python",
    "text": "Introduction to Python\nThis exercise is an introduction to Python for absolute beginners. If you already know Python, you can safely skip it.\nNotebook: download .ipynb or run on colab.\nSolution: download .ipynb or run on colab.\nPresentation\n\nCommented solution"
  },
  {
    "objectID": "exercises/Content.html#numpy-and-matplotlib",
    "href": "exercises/Content.html#numpy-and-matplotlib",
    "title": "List of exercises",
    "section": "Numpy and Matplotlib",
    "text": "Numpy and Matplotlib\nThe goal of this exercise is to present the basics of the numerical library numpy as well as the visualization library matplotlib.\nNotebook: download .ipynb or run on colab.\nSolution: download .ipynb or run on colab.\nPresentation\n\nCommented solution"
  },
  {
    "objectID": "exercises/Content.html#linear-regression",
    "href": "exercises/Content.html#linear-regression",
    "title": "List of exercises",
    "section": "Linear regression",
    "text": "Linear regression\nThe goal of this exercise is to study linear regression in batch and online versions, as well as to learn to use scikit-learn.\nNotebook: download .ipynb or run on colab.\nSolution: download .ipynb or run on colab.\nPresentation\n\nCommented solution"
  },
  {
    "objectID": "exercises/Content.html#multiple-linear-regression",
    "href": "exercises/Content.html#multiple-linear-regression",
    "title": "List of exercises",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\nThe goal of this exercise is to study multiple linear regression on the Boston Housing Dataset, with L1 and L2 regularization.\nNotebook: download .ipynb or run on colab.\nSolution: download .ipynb or run on colab.\nPresentation\n\nCommented solution"
  },
  {
    "objectID": "exercises/Content.html#cross-validation",
    "href": "exercises/Content.html#cross-validation",
    "title": "List of exercises",
    "section": "Cross-validation",
    "text": "Cross-validation\nThe goal of this exercise is to study the interest of cross-validation (simple and k-fold) on a small polynomial regression problem.\nNotebook: download .ipynb or run on colab.\nSolution: download .ipynb or run on colab.\nPresentation\n\nCommented solution"
  },
  {
    "objectID": "exercises/Content.html#linear-classification",
    "href": "exercises/Content.html#linear-classification",
    "title": "List of exercises",
    "section": "Linear classification",
    "text": "Linear classification\nThe goal of this exercise is to investigate hard and soft binary classification.\nNotebook: download .ipynb or run on colab.\nSolution: download .ipynb or run on colab.\nPresentation\n\nCommented solution"
  },
  {
    "objectID": "exercises/Content.html#softmax-classifier",
    "href": "exercises/Content.html#softmax-classifier",
    "title": "List of exercises",
    "section": "Softmax classifier",
    "text": "Softmax classifier\nThe goal of this exercise is to investigate the linear softmax classifier on single digits.\nNotebook: download .ipynb or run on colab.\nSolution: download .ipynb or run on colab.\nPresentation\n\nCommented solution"
  },
  {
    "objectID": "exercises/Content.html#multi-layer-perceptron",
    "href": "exercises/Content.html#multi-layer-perceptron",
    "title": "List of exercises",
    "section": "Multi-layer perceptron",
    "text": "Multi-layer perceptron\nThe goal of this exercise is to implement a shallow MLP and the backpropagation algorithm, as well as to investigate several improvements to improve convergence.\nNotebook: download .ipynb or run on colab.\nSolution: download .ipynb or run on colab.\nPresentation\n\nCommented solution\nPart 1\n\nPart 2"
  },
  {
    "objectID": "exercises/Content.html#mnist-classification-using-keras",
    "href": "exercises/Content.html#mnist-classification-using-keras",
    "title": "List of exercises",
    "section": "MNIST classification using keras",
    "text": "MNIST classification using keras\nThe goal of this exercise to to discover the keras API of tensorflow and apply it to the MNIST dataset.\nNotebook: download .ipynb or run on colab.\nSolution: download .ipynb or run on colab.\nPresentation\n\nCommented solution"
  },
  {
    "objectID": "exercises/Content.html#convolutional-neural-networks",
    "href": "exercises/Content.html#convolutional-neural-networks",
    "title": "List of exercises",
    "section": "Convolutional neural networks",
    "text": "Convolutional neural networks\nThe goal of this exercise is to implement a CNN using keras and to visualize the learned feature maps.\nNotebook: download .ipynb or run on colab.\nSolution: download .ipynb or run on colab.\nPresentation\n\nCommented solution"
  },
  {
    "objectID": "exercises/Content.html#transfer-learning",
    "href": "exercises/Content.html#transfer-learning",
    "title": "List of exercises",
    "section": "Transfer learning",
    "text": "Transfer learning\nThe goal of this exercise is to investigate data augmentation and transfer learning on a small dataset of cats and dogs.\nNotebook: download .ipynb or run on colab.\nSolution: download .ipynb or run on colab.\nPresentation\n\nCommented solution"
  },
  {
    "objectID": "exercises/Content.html#variational-autoencoder",
    "href": "exercises/Content.html#variational-autoencoder",
    "title": "List of exercises",
    "section": "Variational autoencoder",
    "text": "Variational autoencoder\nThe goal of this exercise is to implement a variational autoencoder on the MNIST dataset.\nNotebook: download .ipynb or run on colab.\nSolution: download .ipynb or run on colab.\nPresentation\n\nCommented solution"
  },
  {
    "objectID": "exercises/Content.html#recurrent-neural-networks",
    "href": "exercises/Content.html#recurrent-neural-networks",
    "title": "List of exercises",
    "section": "Recurrent neural networks",
    "text": "Recurrent neural networks\nThe goal of this exercise is to use LSTM layers to perform sentiment analysis and time series prediction.\nNotebook: download .ipynb or run on colab.\nSolution: download .ipynb or run on colab.\nPresentation\n\nCommented solution"
  },
  {
    "objectID": "exercises/Installation.html#anaconda",
    "href": "exercises/Installation.html#anaconda",
    "title": "Python installation",
    "section": "Anaconda",
    "text": "Anaconda\n\nInstalling Anaconda\nPython should be already installed if you use Linux, a very old version if you use MacOS, and probably nothing under Windows. Moreover, Python 2.7 became obsolete in December 2019 but is still the default on some distributions.\nFor these reasons, we strongly recommend installing Python 3 using the Anaconda distribution, or even better the community-driven fork Miniforge:\nhttps://github.com/conda-forge/miniforge\nAnaconda offers all the major Python packages in one place, with a focus on data science and machine learning. To install it, simply download the installer / script for your OS and follow the instructions. Beware, the installation takes quite a lot of space on the disk (around 1 GB), so choose the installation path wisely.\n\n\nInstalling packages\nTo install packages (for example numpy), you just have to type in a terminal:\nconda install numpy\nRefer to the docs (https://docs.anaconda.com/anaconda/) to know more.\nIf you prefer your local Python installation, or if a package is not available or outdated with Anaconda, the pip utility allows to also install virtually any Python package:\npip install numpy\n\n\nVirtual environments\nIt is a good idea to isolate the required packages from the rest of your Python installation, otherwise conflicts between package versions may arise.\nVirtual environments allow to create an isolated Python distribution for a project. The Python ecosystem offers many tools for that:\n\nvenv, the default Python 3 module.\nvirtualenv\npyenv\npipenv\n\nAs we advise to use Anaconda, we focus here on conda environments, but the logic is always the same.\nTo create a conda environment with the name neurocomputing using Python 3.9, type in a terminal:\nconda create --name neurocomputing python=3.9\nYou should see that it installs a bunch of basic packages along python:\n(base) ~/ conda create --name neurocomputing python=3.9\nCollecting package metadata (current_repodata.json): done\nSolving environment: done\n\n## Package Plan ##\n\n  environment location: /Users/vitay/Applications/miniforge3/envs/neurocomputing\n\n  added / updated specs:\n    - python=3.9\n\n\nThe following NEW packages will be INSTALLED:\n\n  bzip2              conda-forge/osx-arm64::bzip2-1.0.8-h3422bc3_4 None\n  ca-certificates    conda-forge/osx-arm64::ca-certificates-2022.9.24-h4653dfc_0 None\n  libffi             conda-forge/osx-arm64::libffi-3.4.2-h3422bc3_5 None\n  libsqlite          conda-forge/osx-arm64::libsqlite-3.39.4-h76d750c_0 None\n  libzlib            conda-forge/osx-arm64::libzlib-1.2.12-h03a7124_4 None\n  ncurses            conda-forge/osx-arm64::ncurses-6.3-h07bb92c_1 None\n  openssl            conda-forge/osx-arm64::openssl-3.0.5-h03a7124_2 None\n  pip                conda-forge/noarch::pip-22.2.2-pyhd8ed1ab_0 None\n  python             conda-forge/osx-arm64::python-3.9.13-h96fcbfb_0_cpython None\n  readline           conda-forge/osx-arm64::readline-8.1.2-h46ed386_0 None\n  setuptools         conda-forge/noarch::setuptools-65.4.1-pyhd8ed1ab_0 None\n  sqlite             conda-forge/osx-arm64::sqlite-3.39.4-h2229b38_0 None\n  tk                 conda-forge/osx-arm64::tk-8.6.12-he1e0b03_0 None\n  tzdata             conda-forge/noarch::tzdata-2022d-h191b570_0 None\n  wheel              conda-forge/noarch::wheel-0.37.1-pyhd8ed1ab_0 None\n  xz                 conda-forge/osx-arm64::xz-5.2.6-h57fd34a_0 None\n\n\nProceed ([y]/n)?\n\nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: done\n#\n# To activate this environment, use\n#\n#     $ conda activate neurocomputing\n#\n# To deactivate an active environment, use\n#\n#     $ conda deactivate\n\nRetrieving notices: ...working... done\nAs indicated at the end of the message, you need to activate the environment to use its packages:\nconda activate neurocomputing\nWhen you are done, you can deactivate it, or simply close the terminal.\n\n\n\n\n\n\nNote\n\n\n\nYou need to activate the environment every time you start an exercise or open a new terminal!\n\n\nYou can then install all the required packages to their latest versions, alternating between conda and pip:\nconda install numpy matplotlib jupyterlab scikit-learn\npip install tensorflow\n\n\n\n\n\n\nHint\n\n\n\nIf you installed the regular Anaconda and not miniforge, we strongly advise to force using the conda forge channel:\nconda install -c conda-forge numpy matplotlib jupyterlab scikit-learn\n\n\nAlternatively, you can use one of the following files and install everything in one shot:\n\nconda-linux.yml for Linux and (possibly) Windows.\nconda-macos.yml for MacOS arm64 (M1). Untested on Intel-based macs.\n\nconda env create -f conda-linux.yml\nconda env create -f conda-macos.yml\n\n\n\n\n\n\nNote\n\n\n\nIf you have a CUDA-capable NVIDIA graphical card, follow these instructions to install tensorflow:\nhttps://www.tensorflow.org/install/pip\n\n\n\n\nUsing notebooks\nWhen the installation is complete, you just need to download the Jupyter notebooks (.ipynb) on this page, activate your environment, and type:\njupyter lab name_of_the_notebook.ipynb\nto open a browser tab with the notebook."
  },
  {
    "objectID": "exercises/Installation.html#colab",
    "href": "exercises/Installation.html#colab",
    "title": "Python installation",
    "section": "Colab",
    "text": "Colab\nAnother option is to run the notebooks in the cloud, for example on Google Colab:\nhttps://colab.research.google.com/\nColab has all major ML packages already installed, so you do not have to care about anything. Under conditions, you can also use a GPU for free (but for maximally 24 hours in a row).\nA link to run the notebooks on colab is provided in the list of exercises. Note that you will need a Google account (a dedicated one is fine if you are concerned about privacy).\nIf you want to save your progress, make a copy of the notebook in your Google drive when you open the link, or download the notebook at the end."
  },
  {
    "objectID": "exercises/1-Python-solution.html#working-with-python",
    "href": "exercises/1-Python-solution.html#working-with-python",
    "title": "Introduction To Python",
    "section": "Working With Python",
    "text": "Working With Python\nThere are basically three ways to program in Python: the interpreter for small commands, scripts for longer programs and notebooks (as here) for interactive programming.\n\nPython Interpreter\nTo start the Python interpreter, simply type its name in a terminal under Linux:\nuser@machine ~ $ python\nPython 3.7.4 (default, Jul 16 2019, 07:12:58) \n[GCC 9.1.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> \nYou can then type anything at the prompt, for example a print statement:\n>>> print(\"Hello World!\")\nHello World!\nTo exit Python call the exit() function (or Ctrl+d):\n>>> exit()\n\n\nScripts\nInstead of using the interpreter, you can run scripts which can be executed sequentially. Simply edit a text file called MyScript.py containing for example:\n# MyScript.py\n# Implements the Hello World example.\n\ntext = 'Hello World!' # define a string variable\n\nprint(text)\nThe # character is used for comments. Execute this script by typing in a Terminal:\npython MyScript.py\nAs it is a scripted language, each instruction in the script is executed from the beginning to the end, except for the declared functions or classes which can be used later.\n\n\nJupyter Notebooks\nA third recent (but very useful) option is to use Jupyter notebooks (formerly IPython notebooks).\nJupyter notebooks allow you to edit Python code in your browser (but also Julia, R, Scala…) and run it locally.\nTo launch a Jupyter notebook, type in a terminal:\njupyter notebook\nand create a new notebook (Python 3)\nWhen a Jupyter notebook already exists (here 1-Python.ipynb), you can also start it directly:\njupyter notebook 1-Python.ipynb\nAlternatively, Jupyter lab has a more modern UI, but is still in beta.\nThe main particularity of notebooks is that code is not executed sequentially from the beginning to the end, but only when a cell is explicitly run with Ctrl + Enter (the active cell stays the same) or Shift + Enter (the next cell becomes active).\nTo edit a cell, select it and press Enter (or double-click).\nQ: In the next cell, run the Hello World! example:\n\ntext = 'Hello World!'\nprint(text)\n\nHello World!\n\n\nThere are three types of cells:\n\nPython cells allow to execute Python code (the default)\nMarkdown cells which allow to document the code nicely (code, equations), like the current one.\nRaw cell are passed to nbconvert directly, it allows you to generate html or pdf versions of your notebook (not used here).\n\nBeware that the order of execution of the cells matters!\nQ: In the next three cells, put the following commands:\n\ntext = \"Text A\"\ntext = \"Text B\"\nprint(text)\n\nand run them in different orders (e.g. 1, 2, 3, 1, 3)\n\ntext = \"Text A\"\n\n\ntext = \"Text B\"\n\n\nprint(text)\n\nText B\n\n\nExecuting a cell can therefore influence cells before and after it. If you want to run the notebook sequentially, select Kernel/Restart & Run all.\nTake a moment to explore the options in the menu (Insert cells, Run cells, Download as Python, etc)."
  },
  {
    "objectID": "exercises/1-Python-solution.html#basics-in-python",
    "href": "exercises/1-Python-solution.html#basics-in-python",
    "title": "Introduction To Python",
    "section": "Basics In Python",
    "text": "Basics In Python\n\nPrint Statement\nIn Python 3, the print() function is a regular function:\nprint(value1, value2, ...)\nYou can give it as many arguments as you want (of whatever type), they will be printed one after another separated by spaces.\nQ: Try to print “Hello World!” using two different strings “Hello” and “World!”:\n\ntext1 = 'Hello'\ntext2 = \"World!\"\n\nprint(text1, text2)\n\nHello World!\n\n\n\n\nData Types\nAs Python is an interpreted language, variables can be assigned without specifying their type: it will be inferred at execution time.\nThe only thing that counts is how you initialize them and which operations you perform on them.\na = 42          # Integer\nb = 3.14159     # Double precision float\nc = 'My string' # String\nd = False       # Boolean\ne = a > b       # Boolean\nQ: Print these variables as well as their type:\nprint(type(a))\n\na = 42          # Integer\nb = 3.14159     # Double precision float\nc = 'My string' # String\nd = False       # Boolean\ne = a > b       # Boolean\n\nprint('Value of a is', a,', Type of a is:', type(a))\nprint('Value of b is', b,', Type of b is:', type(b))\nprint('Value of c is', c,', Type of c is:', type(c))\nprint('Value of d is', d,', Type of d is:', type(d))\nprint('Value of e is', e,', Type of e is:', type(e))\n\nValue of a is 42 , Type of a is: <class 'int'>\nValue of b is 3.14159 , Type of b is: <class 'float'>\nValue of c is My string , Type of c is: <class 'str'>\nValue of d is False , Type of d is: <class 'bool'>\nValue of e is True , Type of e is: <class 'bool'>\n\n\n\n\nAssignment Statement And Operators\n\nAssignment Statement\nThe assignment can be done for a single variable, or for a tuple of variables separated by commas:\nm = 5 + 7\n\nx, y = 10, 20\n\na, b, c, d = 5, 'Text', None, x==y\nQ: Try these assignments and print the values of the variables.\n\nm = 5 + 7\nx, y = 10, 20\na, b, c, d = 5, 'Text', None, x==y\n\nprint(m, x, y, a, b, c, d,)\n\n12 10 20 5 Text None False\n\n\n\n\nOperators\nMost usual operators are available:\n+ , - , * , ** , / , // , %\n== , != , <> , > , >= , < , <=\nand , or , not\nQ: Try them and comment on their behaviour. Observe in particular what happens when you add an integer and a float.\n\nx = 3 + 5.\nprint(x, type(x))\n\n8.0 <class 'float'>\n\n\nQ: Notice how integer division is handled by python 3 by dividing an integer by either an integer or a float:\n\nprint(5/2)\nprint(5/2.)\n\n2.5\n2.5\n\n\n\n\n\nStrings\nA string in Python can be surrounded by either single or double quotes (no difference as long as they match). Three double quotes allow to print new lines directly (equivalent of \\n in C).\nQ: Use the function print() to see the results of the following statements:\na = 'abc'\n\nb = \"def\"\n\nc = \"\"\"aaa\nbbb\nccc\"\"\"\n\nd = \"xxx'yyy\"\n\ne = 'mmm\"nnn'\n\nf = \"aaa\\nbbb\\nccc\"\n\na = 'abc'\nb = \"def\"\nc = \"\"\"aaa\nbbb\nccc\"\"\"\nd = \"xxx'yyy\"\ne = 'mmm\"nnn'\nf = \"aaa\\nbbb\\nccc\"\n\nprint(a)\nprint(b)\nprint(c)\nprint(d)\nprint(e)\nprint(f)\n\nabc\ndef\naaa\nbbb\nccc\nxxx'yyy\nmmm\"nnn\naaa\nbbb\nccc\n\n\n\n\nLists\nPython knows a number of compound data types, used to group together other values. The most versatile is the list, which can be written as a list of comma-separated values (items) between square brackets []. List items need not all to have the same type.\na = ['spam', 'eggs', 100, 1234]\nQ: Define a list of various variables and print it:\n\na = ['spam', 'eggs', 100, 1234]\n\nprint(a)\n\n['spam', 'eggs', 100, 1234]\n\n\nThe number of items in a list is available through the len() function applied to the list:\nlen(a)\nQ: Apply len() on the list, as well as on a string:\n\nprint(\"Length of the list:\", len(a))\nprint(\"Length of the word spam:\", len('spam'))\n\nLength of the list: 4\nLength of the word spam: 4\n\n\nTo access the elements of the list, indexing and slicing can be used.\n\nAs in C, indices start at 0, so a[0] is the first element of the list, a[3] is its fourth element.\nNegative indices start from the end of the list, so a[-1] is the last element, a[-2] the last but one, etc.\nSlices return a list containing a subset of elements, with the form a[start:stop], stop being excluded. a[1:3] returns the second and third elements. WHen omitted, start is 0 (a[:2] returns the two first elements) and stop is the length of the list (a[1:] has all elements of a except the first one).\n\nQ: Experiment with indexing and slicing on your list.\n\nprint(a)\nprint(\"a[0]\", a[0])\nprint(\"a[3]\", a[3])\nprint(\"a[-1]\", a[-1])\nprint(\"a[1:3]\", a[1:3])\n\n['spam', 'eggs', 100, 1234]\na[0] spam\na[3] 1234\na[-1] 1234\na[1:3] ['eggs', 100]\n\n\nCopying lists can cause some problems:\na = [1,2,3] # Initial list\n\nb = a # \"Copy\" the list by reference \n\na[0] = 9 # Change one item of the initial list\nQ: Now print a and b. What happens?\n\na = [1,2,3] # Initial list\n\nb = a # \"Copy\" the list by reference \n\na[0] = 9 # Change one item of the initial list\n\nprint('a :', a)\nprint('b :', b)\n\na : [9, 2, 3]\nb : [9, 2, 3]\n\n\nA: B = A does not make a copy of the content of A, but of its reference (pointer). So a and b both points at the same object.\nThe solution is to use the built-in copy() method of lists:\nb = a.copy()\nQ: Try it and observe the difference.\n\na = [1, 2, 3]\nb = a.copy()\na[0] = 9\n\nprint(a)\nprint(b)\n\n[9, 2, 3]\n[1, 2, 3]\n\n\nLists are objects, with a lot of different built-in methods (type help(list) in the interpreter or in a cell):\n\na.append(x): Add an item to the end of the list.\na.extend(L): Extend the list by appending all the items in the given list.\na.insert(i, x): Insert an item at a given position.\na.remove(x): Remove the first item from the list whose value is x.\na.pop(i): Remove the item at the given position in the list, and return it.\na.index(x): Return the index in the list of the first item whose value is x.\na.count(x): Return the number of times x appears in the list.\na.sort(): Sort the items of the list, in place.\na.reverse(): Reverse the elements of the list, in place.\n\nQ: Try out quickly these methods, in particular append() which we will use quite often.\n\na = [1, 2, 3]\n\na.append(4)\n\nprint(a)\n\n[1, 2, 3, 4]\n\n\n\n\nDictionaries\nAnother useful data type built into Python is the dictionary. Unlike lists, which are indexed by a range of numbers from 0 to length -1, dictionaries are indexed by keys, which can be any immutable type; strings and numbers can always be keys.\nDictionaries can be defined by curly braces {} instead of square brackets. The content is defined by key:item pairs, the item can be of any type:\ntel = {\n    'jack': 4098, \n    'sape': 4139\n}\nTo retrieve an item, simply use the key:\ntel_jack = tel['jack']\nTo add an entry to the dictionary, just use the key and assign a value to the item. It automatically extends the dictionary (warning, it can be dangerous!).\ntel['guido'] = 4127\nQ: Create a dictionary and elements to it.\n\ntel = {'jack': 4098, 'sape': 4139}\ntel_jack = tel['jack']\ntel['guido'] = 4127\n\nprint(tel)\nprint(tel_jack)\n\n{'jack': 4098, 'sape': 4139, 'guido': 4127}\n4098\n\n\nThe keys() method of a dictionary object returns a list of all the keys used in the dictionary, in the order in which you added the keys (if you want it sorted, just apply the sorted() function on it).\na = tel.keys()\nb = sorted(tel.keys())\nvalues() does the same for the value of the items:\nc = tel.values()\nQ: Do it on your dictionary.\n\na = tel.keys()\nb = sorted(a)\nc = tel.values()\n\nprint(a)\nprint(b)\nprint(c)\n\ndict_keys(['jack', 'sape', 'guido'])\n['guido', 'jack', 'sape']\ndict_values([4098, 4139, 4127])\n\n\n\n\nIf Statement\nPerhaps the most well-known conditional statement type is the if statement. For example:\nif x < 0 :\n    print('x =', x, 'is negative')\nelif x == 0:\n    print('x =', x, 'is zero')\nelse:\n    print('x =', x, 'is positive')\nQ: Give a value to the variable x and see what this statement does.\n\nx = 5\n\nif x < 0 :\n    print('x =', x, 'is negative')\nelif x == 0:\n    print('x =', x, 'is zero')\nelse:\n    print('x =', x, 'is positive')\n\nx = 5 is positive\n\n\nImportant! The main particularity of the Python syntax is that the scope of the different structures (functions, for, if, while, etc…) is defined by the indentation, not by curly braces {}. As long as the code stays at the same level, it is in the same scope:\nif x < 0 :\n    print('x =', x, 'is negative')\n    x = -x\n    print('x =', x, 'is now positive')\nelif x == 0:\n    print('x =', x, 'is zero')\nelse:\n    print('x =', x, 'is positive')\nA reasonable choice is to use four spaces for the indentation instead of tabs (configure your text editor if you are not using Jupyter).\nWhen the scope is terminated, you need to come back at exactly the same level of indentation. Try this misaligned structure and observe what happens:\nif x < 0 :\n    print('x =', x, 'is negative')\n elif x == 0:\n    print('x =', x, 'is zero')\n else:\n    print('x =', x, 'is positive')\nJupyter is nice enough to highlight it for you, but not all text editors do that…\n\nif x < 0 :\n    print('x =', x, 'is negative')\n elif x == 0:\n    print('x =', x, 'is zero')\n else:\n    print('x =', x, 'is positive')\n\nIndentationError: unindent does not match any outer indentation level (<tokenize>, line 3)\n\n\nIn a if statement, there can be zero or more elif parts. What to do when the condition is true should be indented. The keyword \"elif\" is a shortened form of \"else if\", and is useful to avoid excessive indentation. An if ... elif ... elif ... sequence is a substitute for the switch or case statements found in other languages.\nThe elif and else statements are optional. You can also only use the if statement alone:\na = [1, 2, 0]\nhas_zero = False\nif 0 in a:\n    has_zero = True\nNote the use of the in keyword to know if an element exists in a list.\n\n\nFor loops\nThe for statement in Python differs a bit from what you may be used to in C, Java or Pascal.\nRather than always iterating over an arithmetic progression of numbers (like in Pascal), or giving the user the ability to define both the iteration step and halting condition (as C), Python’s for statement iterates over the items of any sequence (a list or a string), in the order they appear in the sequence.\nlist_words = ['cat', 'window', 'defenestrate']\n\nfor word in list_words:\n    print(word, len(word))\nQ: Iterate over the list you created previously and print each element.\n\na = ['spam', 'eggs', 100, 1234]\n\nfor el in a:\n    print(el)\n\nspam\neggs\n100\n1234\n\n\nIf you do need to iterate over a sequence of numbers, the built-in function range() comes in handy. It generates lists containing arithmetic progressions:\nfor i in range(5):\n    print(i)\nQ: Try it.\n\nfor i in range(5):\n    print(i)\n\n0\n1\n2\n3\n4\n\n\nrange(N) generates a list of N number starting from 0 until N-1.\nIt is possible to specify a start value (0 by default), an end value (excluded) and even a step:\nrange(5, 10)\nrange(5, 10, 2)\nQ: Print the second and fourth elements of your list (['spam', 'eggs', 100, 1234]) using range().\n\nfor i in range(1, 4, 2):\n    print(a[i])\n\neggs\n1234\n\n\nTo iterate over all the indices of a list (0, 1, 2, etc), you can combine range() and len() as follows:\nfor idx in range(len(a)):\nThe enumerate() function allows to get at the same time the index and the content:\nfor i, val in enumerate(a):\n    print(i, val)\n\nfor i, val in enumerate(a):\n    print(i, val)\n\n0 spam\n1 eggs\n2 100\n3 1234\n\n\nTo get iteratively the keys and items of a dictionary, use the items() method of dictionary:\nfor key, val in tel.items():\nQ: Print one by one all keys and values of your dictionary.\n\ntel = {'jack': 4098, 'sape': 4139, 'guido': 4127}\n\nfor name, number in tel.items():\n    print(name, number)\n\njack 4098\nsape 4139\nguido 4127\n\n\n\n\nFunctions\nAs in most procedural languages, you can define functions. Functions are defined by the keyword def. Only the parameters of the function are specified (without type), not the return values.\nThe content of the function has to be incremented as with for loops.\nReturn values can be specified with the return keywork. It is possible to return several values at the same time, separated by commas.\ndef say_hello_to(first, second):\n    question = 'Hello, I am '+ first + '!'\n    answer = 'Hello '+ first + '! I am ' + second + '!'\n    return question, answer\nTo call that function, pass the arguments that you need and retrieve the retruned values separated by commas.\nquestion, answer = say_hello_to('Jack', 'Gill')\nQ: Test it with different names as arguments.\n\ndef say_hello_to(first, second):\n    question = 'Hello, I am '+ first + '!'\n    answer = 'Hello '+ first + '! I am ' + second + '!'\n    return question, answer\n\nquestion, answer = say_hello_to('Jack', 'Gill')\n\nprint(question)\nprint(answer)\n\nHello, I am Jack!\nHello Jack! I am Gill!\n\n\nQ: Redefine the tel dictionary {'jack': 4098, 'sape': 4139, 'guido': 4127} if needed, and create a function that returns a list of names whose number is higher than 4100.\n\ndef filter_dict(tel):\n    answer = []\n    for name, number in tel.items():\n        if number >= 4100:\n            answer.append(name)\n    return answer\n\ntel = {'jack': 4098, 'sape': 4139, 'guido': 4127}\nnames = filter_dict(tel)\nprint(names)\n\n['sape', 'guido']\n\n\nFunctions can take several arguments (with default values or not). The name of the argument can be specified during the call, so their order won’t matter.\nQ: Try these different calls to the say_hello_to() function:\nquestion, answer = say_hello_to('Jack', 'Gill')\nquestion, answer = say_hello_to(first='Jack', second='Gill')\nquestion, answer = say_hello_to(second='Jack', first='Gill')\n\nquestion, answer = say_hello_to('Jack', 'Gill')\nprint(question)\nprint(answer)\nquestion, answer = say_hello_to(first='Jack', second='Gill')\nprint(question)\nprint(answer)\nquestion, answer = say_hello_to(second='Jack', first='Gill')\nprint(question)\nprint(answer)\n\nHello, I am Jack!\nHello Jack! I am Gill!\nHello, I am Jack!\nHello Jack! I am Gill!\nHello, I am Gill!\nHello Gill! I am Jack!\n\n\nDefault values can be specified for the last arguments, for example:\ndef add (a, b=1):\n    return a + b\n\nx = add(2, 3) # returns 5\ny = add(2) # returns 3\nz = add(a=4) # returns 5\nQ: Modify say_hello_to() so that the second argument is your own name by default.\n\ndef say_hello_to(first, second=\"Julien\"):\n    question = 'Hello, I am '+ first + '!'\n    answer = 'Hello '+ first + '! I am ' + second + '!'\n    return question, answer\n\nquestion, answer = say_hello_to('Jack', 'Gill')\nprint(question)\nprint(answer)\n\nquestion, answer = say_hello_to('Jack')\nprint(question)\nprint(answer)\n\nHello, I am Jack!\nHello Jack! I am Gill!\nHello, I am Jack!\nHello Jack! I am Julien!\n\n\n\n\nClasses\nClasses are structures allowing to:\n\nStore information in an object.\nApply functions on this information.\n\nIn a nutshell:\nclass Foo:\n    \n    def __init__(self, val):\n        self.val = val\n        \n    def print(self):\n        print(self.val)\n   \n    def set_val(self, val):\n        self.val = val\n        self.print()\n        \na = Foo(42)\na.print()\nThis defines the class Foo. The first (obligatory) method of the class is the constructor __init__. This determines how the instance a will be instantiated after the call to a = Foo(42). The first argument is self, which represents the current instance of the class. We can specify other arguments to the constructor (here val), which can be processed or stored.\nHere we store val as an attribute of the class Foo with self.val. It is data that will be specific to each created object: if you create b = Foo(\"deep learning\"), the attribute self.val will have different values between the two instances. As always in Python, the type does not matter, it can be a float, a string, a numpy array, another object…\nAttributes are accessible from each object as:\nx = a.val\nYou can set its value by:\na.val = 12\nClasses can define methods that can manipulate class attributes like any regular function. The first argument must always be self. With the self object, you can access all attributes (or other methods) of the instance.\nWith our toy class, a.set_val(34) does exactly the same as a.val = 34, or a.print() is the same as print(a.val).\nFor C++/Java experts: attributes and methods are always public in Python. If you want to make an attribute private, preprend its name with an underscore, e.g. self._val. It will then not be part of the API of the class (but can be read or written publicly anyway…).\nQ: Play around with this basic class, create different objects with different attributes, print them, change them, etc.\n\nclass Foo:\n    \n    def __init__(self, val):\n        self.val = val\n    \n    def print(self):\n        print(self.val)\n    \n    def set_val(self, val):\n        self.val = val\n        self.print()\n\na = Foo(42)\na.print()\nprint(a.val)\na.set_val(32)\na.print()\n\n42\n42\n32\n32\n\n\nA major concept in object-oriented programming (OOP) is class inheritance. We will not use it much in these exercises, but let’s talk shortly about it.\nInheriting a class is creating a new class that inherits from the attributes and methods of another class (a kind of “copy” of the definition of the class). You can then add new attributes or methods, or overload existing ones.\nExample:\nclass Bar(Foo):\n    def add(self, val):\n        self.val += val\n    def print(self):\n        print(\"val =\", self.val)\nBar is a child class of Foo. It inherits all attributes and methods, including __init__, print and set_val. It creates a new method add and overloads print: the old definition of print in Foo does not exist anymore for instances of the Bar class (but does for instances of the Foo class). The constructor can also be overloaded, for example to add new arguments:\nclass Bar(Foo):\n    def __init__(self, val, val2):\n        self.val2 = val2\n        super().__init__(val)\n    def add(self, val):\n        self.val += val\n    def print(self):\n        print(\"val =\", self.val)\nsuper().__init__(val) calls the constructor of the Foo class (the “super” class of bar), so it sets the value of self.val.\nQ: Play around with inheritance to understand the concept.\n\nclass Bar(Foo):\n    def __init__(self, val, val2):\n        self.val2 = val2\n        super().__init__(val)\n    def add(self, val):\n        self.val += val\n    def print(self):\n        print(\"val =\", self.val)\n        \nb = Bar(12, 23)\nb.add(30)\nb.print()\n\nval = 42"
  },
  {
    "objectID": "exercises/1-Python-solution.html#exercise",
    "href": "exercises/1-Python-solution.html#exercise",
    "title": "Introduction To Python",
    "section": "Exercise",
    "text": "Exercise\nIn cryptography, a Caesar cipher is a very simple encryption technique in which each letter in the plain text is replaced by a letter some fixed number of positions down the alphabet. For example, with a shift of 3, A would be replaced by D, B would become E, and so on. The method is named after Julius Caesar, who used it to communicate with his generals. ROT-13 (“rotate by 13 places”) is a widely used example of a Caesar cipher where the shift is 13. In Python, the key for ROT-13 may be represented by means of the following dictionary:\n\ncode = {'a':'n', 'b':'o', 'c':'p', 'd':'q', 'e':'r', 'f':'s',\n        'g':'t', 'h':'u', 'i':'v', 'j':'w', 'k':'x', 'l':'y',\n        'm':'z', 'n':'a', 'o':'b', 'p':'c', 'q':'d', 'r':'e',\n        's':'f', 't':'g', 'u':'h', 'v':'i', 'w':'j', 'x':'k',\n        'y':'l', 'z':'m', 'A':'N', 'B':'O', 'C':'P', 'D':'Q',\n        'E':'R', 'F':'S', 'G':'T', 'H':'U', 'I':'V', 'J':'W',\n        'K':'X', 'L':'Y', 'M':'Z', 'N':'A', 'O':'B', 'P':'C',\n        'Q':'D', 'R':'E', 'S':'F', 'T':'G', 'U':'H', 'V':'I', \n        'W':'J', 'X':'K', 'Y':'L', 'Z':'M'}\n\nQ: Your task in this final exercise is to implement an encoder/decoder of ROT-13. Once you’re done, you will be able to read the following secret message:\nJnvg, jung qbrf vg unir gb qb jvgu qrrc yrneavat??\nThe idea is to write a decode() function taking the message and the code dictionary as inputs, and returning the decoded message. It should iterate over all letters of the message and replace them with the decoded letter. If the letter is not in the dictionary (e.g. punctuation), keep it as it is.\n\n# Method to decode a message\ndef decode(msg, code):\n    result = \"\"\n    for letter in msg:\n        if letter in code.keys():\n            result += code[letter]\n        else:\n            result += letter\n    return result\n\n# Message to decode\nmsg = \"Jnvg, jung qbrf vg unir gb qb jvgu qrrc yrneavat??\"\n\n# Decode the message\ndecoded = decode(msg, code)\nprint(decoded)"
  },
  {
    "objectID": "exercises/2-Numpy-solution.html#numpy",
    "href": "exercises/2-Numpy-solution.html#numpy",
    "title": "Numpy and Matplotlib",
    "section": "Numpy",
    "text": "Numpy\nNumPy is a linear algebra library in Python, with computationally expensive methods written in FORTRAN for speed.\n\nThe reference manual is at https://numpy.org/doc/stable/.\nA nice tutorial can be found at https://numpy.org/doc/stable/user/quickstart.html\nor: https://cs231n.github.io/python-numpy-tutorial/\nIf you already know Matlab, a comparison is at https://numpy.org/doc/stable/user/numpy-for-matlab-users.html\n\n\nImporting libraries\nTo import a library in Python, you only need to use the keyword import at the beginning of your script / notebook (or more exactly, before you use it).\nimport numpy\nThink of it as the equivalent of #include <numpy.h> in C/C++ (if you know Java, you will not be shocked). You can then use the functions and objects provided by the library using the namespace of the library:\nx = numpy.array([1, 2, 3])\nIf you do not want to type numpy. everytime, and if you are not afraid that numpy redefines any important function, you can also simply import every definition declared by the library in your current namespace with:\nfrom numpy import *\nand use the objects directly:\nx = array([1, 2, 3])\nHowever, it is good practice to give an alias to the library when its name is too long (numpy is still okay, but think of matplotlib…):\nimport numpy as np \nYou can then use the objects like this:\nx = np.array([1, 2, 3])\nRemember that you can get help on any NumPy function:\nhelp(np.array)\nhelp(np.ndarray.transpose)\n\nimport numpy as np\n\n\n\nVectors and matrices\nThe basic object in NumPy is an array with d-dimensions (1D = vector, 2D = matrix, 3D or more = tensor). They can store either integers or floats, using various precisions.\nIn order to create a vector of three floats, you simply have to build an array() object by providing a list of floats as input:\nA = np.array( [ 1., 2., 3.] )\nMatrices should be initialized with a list of lists. For a 3x4 matrix of 8 bits unsigned integers, it is:\nB = np.array( [ \n    [ 1, 2, 3, 4],\n    [ 5, 6, 7, 8],\n    [ 4, 3, 2, 1] \n  ] , dtype=np.uint8)\nMost of the time, you won’t care about the type (the default floating-point precision is what you want for machine learning), but if you need it, you can always specify it with the parameter dtype={int32, uint16, float64, ...}. Note that even if you pass integers to the array (np.array( [ 1, 2, 3] )), they will be converted to floats by default.\nThe following attributes of an array can be accessed:\n\nA.shape : returns the shape of the vector (n,) or matrix (m, n).\nA.size : returns the total number of elements in the array.\nA.ndim : returns the number of dimensions of the array (vector: 1, matrix:2).\nA.dtype.name : returns the type of data stored in the array (int32, uint16, float64…).\n\nQ: Define the two arrays A and B from above and print those attributes. Modify the arrays (number of elements, type) and observe how they change.\nHint: you can print an array just like any other Python object.\n\nA = np.array( [ 1., 2., 3.] )\n\nprint(A)\nprint('Shape of A is :', A.shape)\nprint('Size of A is :', A.size)\nprint('Number of dimensions of A is :', A.ndim)\nprint('Type of elements in A is :', A.dtype.name)\n\nB = np.array( [ \n    [ 1, 2, 3, 4],\n    [ 5, 6, 7, 8],\n    [ 4, 3, 2, 1] \n] , dtype=np.uint8)\n\nprint(B)\nprint('Shape of B is :', B.shape)\nprint('Size of B is :', B.size)\nprint('Number of dimensions of B is :', B.ndim)\nprint('Type of elements in B is :', B.dtype.name)\n\n[1. 2. 3.]\nShape of A is : (3,)\nSize of A is : 3\nNumber of dimensions of A is : 1\nType of elements in A is : float64\n[[1 2 3 4]\n [5 6 7 8]\n [4 3 2 1]]\nShape of B is : (3, 4)\nSize of B is : 12\nNumber of dimensions of B is : 2\nType of elements in B is : uint8\n\n\nInternally, the values are stored sequentially as a vector, even if your array has more than one dimension. The apparent shape is just used for mathematical operations. You can reshape a matrix very easily with the reshape() method:\nB = np.array( [ \n    [ 1, 2, 3, 4],\n    [ 5, 6, 7, 8],\n    [ 4, 3, 2, 1] \n]) # B has 3 rows, 4 columns\n\nC = B.reshape((6, 2)) # C has 6 rows, 2 columns\nThe only thing to respect is that the total number of elements must be the same. Beware also of the order in which the elements will be put.\nQ: Create a vector with 8 elements and reshape it into a 2x4 matrix.\n\nB = np.array( [1, 2, 3, 4, 5, 6, 7, 8])\n\nC = B.reshape((2, 4))\nprint(C)\n\n[[1 2 3 4]\n [5 6 7 8]]\n\n\n\n\nInitialization of an array\nProviding a list of values to array() would be tedious for large arrays. Numpy offers constructors that allow to construct simply most vectors or matrices.\nnp.zeros(shape) creates an array of shape shape filled with zeros. Note: if you give a single integer for the shape, it will be interpreted as a vector of shape (d,).\nnp.ones(shape) creates an array of shape shape filled with ones.\nnp.full(shape, val) creates an array of shape shape filled with val.\nnp.eye(n) creates a diagonal matrix of shape (n, n).\nnp.arange(a, b) creates a vector of integers whose value linearly increase from a to b (excluded).\nnp.linspace(a, b, n) creates a vector of n values evenly distributed between a and b (included).\nQ: Create and print:\n\na 2x3 matrix filled with zeros.\na vector of 12 elements initialized to 3.14.\na vector of 11 elements whose value linearly increases from 0.0 to 10.0.\na vector of 11 elements whose value linearly increases from 10 to 20.\n\n\nA = np.zeros((2,3))  \nB = np.full(12, 3.14)  # 3.14 * np.ones(12) would also work\nC = np.linspace(0.0, 10.0, 11) \nD = np.arange(10, 21)\n\n\n\nRandom distributions\nIn many cases, it is useful to initialize a vector or matrix with random values. Random number generators (rng) allows to draw numbers from any probability distribution (uniform, normal, etc.) using pseudo-random methods.\nIn numpy versions before 1.16, the numpy.random module had direct methods allowing to initialize arrays:\nA = np.random.uniform(-1.0, 1.0, (10, 10)) # a 10x10 matrix with values uniformly taken between -1 and 1\nSince numpy 1.16, this method has been deprecated in favor of a more explicit initialization of the underlying rng:\nrng = np.random.default_rng()\nA = rng.uniform(-1.0, 1.0, (10, 10))\nThe advantages of this new method (reproducibility, parallel seeds) will not matter for these exercises, but let’s take good habits already.\nThe generator has many built-in methods, covering virtually any useful probability distribution. Read the documentation of the random generator:\nhttps://numpy.org/doc/stable/reference/random/generator.html\nQ: Create:\n\nA vector of 20 elements following a normal distribution with mean 2.0 and standard devation 3.0.\nA 10x10 matrix whose elements come from the exponential distribution with \\beta = 2.\nA vector of 10 integers randomly chosen between 1 and 100 (hint: involves arange and rng.choice).\n\n\nrng = np.random.default_rng()\nA = rng.normal(2.0, 3.0, 20)\nB = rng.exponential(2.0, (10, 10))\nC = rng.choice(np.arange(1, 101), 10)\n\n\n\nManipulation of matrices: indices, slices\nTo access a particular element of a matrix, you can use the usual Python list style (the first element has a rank of 0), once per dimension:\nA = np.array(\n    [ \n        [ 1,  2,  3,  4],\n        [ 5,  6,  7,  8],\n        [ 9, 10, 11, 12]\n    ]\n)\n\nx = A[0, 2] # The element on the first row and third column\nFor matrices, the first index represents the rows, the second the columns. [0, 2] represents the element at the first row, third column.\nQ: Define this matrix and replace the element 12 by a zero using indices:\n\nA = np.array(\n    [ \n        [ 1,  2,  3,  4],\n        [ 5,  6,  7,  8],\n        [ 9, 10, 11, 12]\n    ]\n)\n\nA[2, 3] = 0.\nprint(A)\n\n[[ 1  2  3  4]\n [ 5  6  7  8]\n [ 9 10 11  0]]\n\n\nIt is possible to access complete row or columns of a matrix using slices. The : symbol is a shortcut for “everything”:\nb = A[:, 2] # third column\nc = A[0, :] # first row\nQ: Set the fourth column of A to 1.\n\nA[:, 3] = 1.\n\nprint(A)\n\n[[ 1  2  3  1]\n [ 5  6  7  1]\n [ 9 10 11  1]]\n\n\nAs for python lists, you can specify a range start:stop to get only a subset of a row/column (beware, stop is excluded):\nd = A[0, 1:3] # second and third elements of the first row\ne = A[1, :2] # first and second elements of the second row\nYou can use boolean arrays to retrieve indices:\nA = np.array( \n    [ [ -2,  2,  1, -4],\n      [  3, -1, -5, -3] ])\n\nnegatives = A < 0 # Boolean array where each element is True when the condition is met.\nA[negatives] = 0 # All negative elements of A (where the boolean matrix is True) will be set to 0\nA simpler way to write it is:\nA[A < 0] = 0\nQ: print A, negatives and A again after the assignment:\n\nA = np.array( \n    [ [ -2,  2,  1, -4],\n      [  3, -1, -5, -3] ])\nnegatives = A < 0\nA[negatives] = 0\n\nprint(negatives)\nprint(A)\n\n[[ True False False  True]\n [False  True  True  True]]\n[[0 2 1 0]\n [3 0 0 0]]\n\n\n\n\nBasic linear algebra\nLet’s first define some matrices:\n\nA = np.array( [ [ 1, 2, 3, 4],\n                [ 5, 6, 7, 8] ])\n\nB = np.array( [ [ 1, 2],\n                [ 3, 4],\n                [ 5, 6],\n                [ 7, 8] ])\n\nC = np.array( [ [  1,  2,  3,  4],\n                [  5,  6,  7,  8],\n                [  9,  0,  1,  1],\n                [ 13,  7,  2,  6] ])\n\n\nTranspose a matrix\nA matrix can be transposed with the transpose() method or the .T shortcut:\nD = A.transpose() \nE = A.T # equivalent\nQ: Try it:\n\nD = A.transpose() \nE = A.T\n\nprint(A)\nprint(D)\n\n[[1 2 3 4]\n [5 6 7 8]]\n[[1 5]\n [2 6]\n [3 7]\n [4 8]]\n\n\ntranspose() does not change A, it only returns a transposed copy. To transpose A definitely, you have to use the assigment A = A.T\n\n\nMultiply two matrices\nThere are two manners to multiply matrices:\n\nelement-wise: Two arrays of exactly the same shape can be multiplied element-wise by using the * operator:\n\nD = A * B\n\nalgebrically: To perform a matrix multiplication, you have to use the dot() method. Beware: the dimensions must match! (m, n) * (n, p) = (m, p)\n\nE = np.dot(A,  B)\nQ: Use the matrices A and B previously defined and multiply them element-wise and algebrically. You may have to transpose one of them.\n\nD = A * B.T\nE = A.T * B\n\nprint(D)\nprint(E)\n\nF = np.dot(A, B)\nG = np.dot(B, A)\n\nprint(F)\nprint(G)\n\n[[ 1  6 15 28]\n [10 24 42 64]]\n[[ 1 10]\n [ 6 24]\n [15 42]\n [28 64]]\n[[ 50  60]\n [114 140]]\n[[11 14 17 20]\n [23 30 37 44]\n [35 46 57 68]\n [47 62 77 92]]\n\n\n\n\nMultiplying a matrix with a vector\n* and np.dot also apply on matrix-vector multiplications \\mathbf{y} = A \\times \\mathbf{x} or vector-vector multiplications.\nQ: Define a vector \\mathbf{x} with four elements and multiply it with the matrix A using * and np.dot. What do you obtain? Try the same by multiplying the vector \\mathbf{x} and itself.\n\nx = np.array([1, 2, 3, 4])\n\ny = np.dot(A, x)\nz = A*x\n\np = x*x\nq = np.dot(x, x)\n\nprint(y)\nprint(z)\nprint(p)\nprint(q)\n\n[30 70]\n[[ 1  4  9 16]\n [ 5 12 21 32]]\n[ 1  4  9 16]\n30\n\n\nA: the element-wise multiplies each column of the matrix by the corresponding element of the vector. np.dot works as expected. The same happens for vector-vector multiplications: element-wise for *, dot-product for np.dot (hence the name of the method).\n\n\nInverting a matrix\nInverting a Matrix (when possible) can be done using the inv() method whitch is defined in the linalg submodule of NumPy.\ninv_C = np.linalg.inv(C)\nQ:\n\nInvert C and print the result.\nMultiply C with its inverse and print the result. What do observe? Why is Numpy called a numerical computation library?\n\n\ninv_C = np.linalg.inv(C)\n\nprint(inv_C)\nprint(np.dot(C,inv_C))\n\n[[-0.0467033   0.00274725  0.0989011   0.01098901]\n [-0.62362637  0.27197802 -0.20879121  0.08791209]\n [-0.61263736  0.4478022   0.12087912 -0.20879121]\n [ 1.03296703 -0.47252747 -0.01098901  0.10989011]]\n[[ 1.00000000e+00  0.00000000e+00 -8.32667268e-17  5.55111512e-17]\n [ 0.00000000e+00  1.00000000e+00 -2.77555756e-17 -1.11022302e-16]\n [ 2.22044605e-16 -1.11022302e-16  1.00000000e+00 -8.32667268e-17]\n [ 0.00000000e+00 -2.22044605e-16  1.11022302e-16  1.00000000e+00]]\n\n\nA: Some elements which should be 0 have a very small value. This is due to numerical precision issues. Numpy does not make symbolic computations like Mathematica or sympy, it deals with numbers up to a certain precision.\n\n\n\nSumming elements\nOne can sum the elements of a matrix globally, row-wise or column-wise:\n# Globally\nS1 = np.sum(A)\n\n# Per column\nS2 = np.sum(A, axis=0) \n\n# Per row\nS3 = np.sum(A, axis=1) \nQ: Try them:\n\n# Globally\nS1 = np.sum(A)\n\n# Per column\nS2 = np.sum(A, axis=0) \n\n# Per row\nS3 = np.sum(A, axis=1) \n\nprint(A)\nprint(S1)\nprint(S2)\nprint(S3)\n\n[[1 2 3 4]\n [5 6 7 8]]\n36\n[ 6  8 10 12]\n[10 26]\n\n\nYou also have access to the minimum (np.min()), maximum (np.max()), mean (np.mean()) of an array, also per row/column.\nQ: Try them out:\n\nprint(np.min(A))\nprint(np.max(A))\nprint(np.mean(A))\n\nprint(np.min(A, axis=0))\nprint(np.max(A, axis=0))\nprint(np.mean(A, axis=0))\n\nprint(np.min(A, axis=1))\nprint(np.max(A, axis=1))\nprint(np.mean(A, axis=1))\n\n1\n8\n4.5\n[1 2 3 4]\n[5 6 7 8]\n[3. 4. 5. 6.]\n[1 5]\n[4 8]\n[2.5 6.5]\n\n\n\n\nMathematical operations\nYou can apply any usual mathematical operations (cos, sin, exp, etc…) on each element of a matrix (element-wise):\nD = np.exp(A)\nE = np.cos(A)\nF = np.log(A)\nG = (A+3) * np.cos(A-2)\nQ: Try it.\n\nD = np.exp(A)\nE = np.cos(A)\nF = np.log(A)\nG = (A+3) * np.cos(A-2)\n\nprint(D)\nprint(E)\nprint(F)\nprint(G)\n\n[[2.71828183e+00 7.38905610e+00 2.00855369e+01 5.45981500e+01]\n [1.48413159e+02 4.03428793e+02 1.09663316e+03 2.98095799e+03]]\n[[ 0.54030231 -0.41614684 -0.9899925  -0.65364362]\n [ 0.28366219  0.96017029  0.75390225 -0.14550003]]\n[[0.         0.69314718 1.09861229 1.38629436]\n [1.60943791 1.79175947 1.94591015 2.07944154]]\n[[ 2.16120922  5.          3.24181384 -2.91302786]\n [-7.91993997 -5.88279259  2.83662185 10.56187315]]"
  },
  {
    "objectID": "exercises/2-Numpy-solution.html#matplotlib",
    "href": "exercises/2-Numpy-solution.html#matplotlib",
    "title": "Numpy and Matplotlib",
    "section": "Matplotlib",
    "text": "Matplotlib\nMatplotlib is a python 2D plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments across platforms.\n\nReference: http://matplotlib.org\nTutorial by N. Rougier: http://www.labri.fr/perso/nrougier/teaching/matplotlib\n\nThis is the default historical visualization library in Python, which anybody should know, but not the nicest. If you are interested in having better visualizations, have a look at:\n\nseaborn https://seaborn.pydata.org/\nggplot2 https://ggplot2.tidyverse.org/\nbokeh https://docs.bokeh.org/\nplotly https://plotly.com/python/\n\nWe will nevertheless stick to matplotlib in these exercises.\nThe pyplot module is the most famous, as it has a similar interface to Matlab. It is customary to use the plt namescape for it:\n\nimport matplotlib.pyplot as plt\n\n\nplt.plot()\nThe plt.plot() command allows to make simple line drawings:\n\nx = np.linspace(0., 10., 100)\ny = x**2 + 1.\n\nplt.figure()\nplt.plot(x, y)\nplt.show()\n\n\n\n\n\n\n\n\nplot() takes two vectors x and y as inputs (they must have the same size) and plots them against each other. It is standard to define the x-axis with np.linspace() if you just want to plot a function. 100 points is usually a good choice, but you can experiments with less points.\nThe call to plt.show() is obligatory at the end to display the window when using a script (very common mistake to forget it!). It is not needed in Jupyter notebooks as it is implicitly called, but let’s take the habit anyway.\nThe call to plt.figure() is also optional, as a new figure is created when you call plt.plot() for the first time.\nQ: Create a third vector z (e.g. z = -x**2 + 2) and plot it against x right after y (i.e. between plt.plot(x, y) and plt.show()). What happens?\n\nx = np.linspace(0., 10., 100)\ny = x**2 + 1.\nz = -x**2 + 2.\n\nplt.figure()\nplt.plot(x, y)\nplt.plot(x, z)\nplt.show()\n\n\n\n\n\n\n\n\nQ: Now call plt.figure() again between the two plots. What happens?\n\nx = np.linspace(0., 10., 100)\ny = x**2 + 1.\nz = -x**2 + 2.\n\nplt.figure()\nplt.plot(x, y)\nplt.figure()\nplt.plot(x, z)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBy default, the plot is quite empty. This is fine when experimenting in a notebook, but not when incorporating the figures in your thesis. You can make a plot look better by adding a title, labels on the axes, etc.\nplt.title('My title')\nplt.xlabel('x-axis')\nplt.ylabel('y-axis')\nQ: Make the previous plots nicer by adding legends and axes.\nHint: if you know LateX equations, you can insert simple formulas in the title or axes by using two dollar signs $$.\n\nx = np.linspace(0., 10., 100)\ny = x**2 + 1.\nz = -x**2 + 2.\n\nplt.figure()\nplt.plot(x, y)\nplt.title(\"Function $x^2 + 1$\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\n\nplt.figure()\nplt.plot(x, z)\nplt.title(\"Function $-x^2 + 2$\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf you make multiple plots on the same figure by calling plt.plot() multiple times, you can add a label to each plot to create a legend with plt.legend():\nplt.plot(x, y, label='y')\nplt.plot(x, z, label='z')\nplt.legend()\n\nx = np.linspace(0., 10., 100)\ny = x**2 + 1.\nz = -x**2 + 2.\n\nplt.figure()\nplt.plot(x, y, label=\"$x^2 + 1$\")\nplt.plot(x, z, label=\"$-x^2 + 2$\")\nplt.title(\"Functions $x^2 + 1$ and $-x^2 + 2$\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nAnother advantage of declaring a figure is that you can modify its size (which is very small in a notebook by default) with the figsize argument in inches:\nplt.figure(figsize=(16, 10))\nQ: Experiment with figure sizes.\n\nx = np.linspace(0., 10., 100)\ny = x**2 + 1.\nz = -x**2 + 2.\n\nplt.figure(figsize=(12, 10))\nplt.plot(x, y, label=\"$x^2 + 1$\")\nplt.plot(x, z, label=\"$-x^2 + 2$\")\nplt.title(\"Functions $x^2 + 1$ and $-x^2 + 2$\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nSide-by-side plots\nTo make separate plots in the same figure, you can use plt.subplot(abc).\nThe function takes three digits a, b, c as input (e.g. 221 or 122) where:\n\na is the number of rows.\nb is the number of columns.\nc is the index (starting at 1) of the current subplot.\n\nHere is a dummy example of a 2x2 grid of plots:\nplt.subplot(221)\nplt.plot(x, y)\n\nplt.subplot(222)\nplt.plot(x, z)\n\nplt.subplot(223)\nplt.plot(y, x)\n\nplt.subplot(224)\nplt.plot(z, x)\nQ: Try it.\n\nx = np.linspace(0., 10., 100)\ny = x**2 + 1.\nz = -x**2 + 2.\n\nplt.figure(figsize=(12, 10))\nplt.subplot(221)\nplt.plot(x, y)\nplt.xlabel('x')\nplt.ylabel('y')\n\nplt.subplot(222)\nplt.plot(x, z)\nplt.xlabel('x')\nplt.ylabel('z')\n\nplt.subplot(223)\nplt.plot(y, x)\nplt.xlabel('y')\nplt.ylabel('x')\n\nplt.subplot(224)\nplt.plot(z, x)\nplt.xlabel('z')\nplt.ylabel('x')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nplt.imshow()\nMatrices can be displayed using plt.imshow(). You can choose the color code with the cmap argument (e.g. gray or hot).\nplt.imshow(A, cmap=plt.cm.hot, interpolation='nearest')\nplt.colorbar()\nplt.colorbar() allows to show a vertical bar indicating the color code.\nThe interpolation method can also be selected for small matrices ('nearest by default, but you can choose interpolation=\"bicubic\" for a smoother display).\n(0, 0) is at the top-left of the image, the first axis is vertical. Change it with the origin parameter.\nQ: Create a 10x10 matrix (e.g. randomly) and plot it. Try different color maps (https://matplotlib.org/3.1.0/tutorials/colors/colormaps.html and interpolation methods.\n\nA = rng.uniform(0., 1., (10, 10))\n\nplt.figure()\nplt.imshow(A, cmap=plt.cm.hot, interpolation='nearest')\nplt.colorbar()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nplt.scatter()\nIf you want to display dots instead of of lines or pixels, plt.scatter takes two vectors of same size and plots them against each other:\nplt.scatter(x, y)\nQ: Create two vectors with 100 elements and make a scatter plot.\n\nx = rng.uniform(0., 1., 100)\ny = rng.uniform(0., 1., 100)\n\nplt.figure()\nplt.scatter(x, y)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nplt.hist()\nHistograms can be useful to visualize the distribution of some data. If z is a vector of values, the histogram is simply:\nplt.hist(z, bins=20)\nThe number of bins is 10 by default, but you can of course change it.\nQ: Draw 1000 values from a normal distribution of your choice and make an histogram.\n\nz = rng.normal(10., 2.0, 1000)\n\nplt.figure()\nplt.hist(z, bins=20)\nplt.show()"
  },
  {
    "objectID": "exercises/3-LinearRegression-solution.html#least-mean-squares",
    "href": "exercises/3-LinearRegression-solution.html#least-mean-squares",
    "title": "Linear regression",
    "section": "Least mean squares",
    "text": "Least mean squares\nTo generate the data for the exercise, we will use the scikit-learn library https://scikit-learn.org. It provides a huge selection of already implemented machine learning algorithms for classification, regression or clustering.\nIf you use Colab, scikit-learn should already be installed. Otherwise, install it with either conda or pip (you may need to restart this notebook afterwards):\nconda install scikit-learn\n# or:\npip install scikit-learn\nWe will use the method sklearn.datasets.make_regression to generate the data. The documentation of this method is available at https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html.\nThe following cell imports the method:\n\nfrom sklearn.datasets import make_regression\n\nWe can now generate the data. We start with the simplest case where the inputs have only one dimension. We will generate 100 samples (x_i, t_i) linked by a linear relationship and some noise.\nThe following code generates the data:\n\nN = 100\nX, t = make_regression(n_samples=N, n_features=1, noise=15.0)\n\nn_samples is the number of samples generates, n_features is the number of input variables and noise quantifies how the points deviate from the linear relationship.\nQ: Print the shape of the arrays X and t to better understand what is generated. Visualize the dataset using matplotlib (plt.scatter). Vary the value of the noise argument in the previous cell and visualize the data again.\n\nprint(X.shape)\nprint(t.shape)\n\nplt.figure(figsize=(10, 5))\nplt.scatter(X, t)\nplt.xlabel(\"x\")\nplt.ylabel(\"t\")\nplt.show()\n\n(100, 1)\n(100,)\n\n\n\n\n\n\n\n\n\nNow is the time to implement the LMS algorithm with numpy.\nRemember the LMS algorithm from the course:\n\nw=0 \\quad;\\quad b=0\nfor M epochs:\n\ndw=0 \\quad;\\quad db=0\nfor each sample (x_i, t_i):\n\ny_i = w \\, x_i + b\ndw = dw + (t_i - y_i) \\, x_i\ndb = db + (t_i - y_i)\n\n\\Delta w = \\eta \\, \\frac{1}{N} dw\n\\Delta b = \\eta \\, \\frac{1}{N} db\n\n\nOur linear model y = w \\, x + b predicts outputs for an input x. The error t-y between the prediction and the data is used to adapt the weight w and the bias b at the end of each epoch.\nQ: Implement the LMS algorithm and apply it to the generated data. The Python code that you will write is almost a line-by-line translation of the pseudo-code above. You will use a learning rate eta = 0.1 at first, but you will vary this value later. Start by running a single epoch, as it will be easier to debug it, and then increase the number of epochs to 100. Print the value of the weight and bias at the end.\n\nw = 0\nb = 0\n\neta = 0.1\n\nfor epoch in range(100):\n    dw = 0\n    db = 0.0\n    \n    for i in range(N):\n        # Prediction\n        y = w * X[i] + b\n        \n        # LMS\n        dw += (t[i] - y) * X[i]\n        db += (t[i] - y)\n        \n    # Parameter updates\n    w += eta * dw / N\n    b += eta * db / N\n    \nprint(w, b)\n\n[35.129774] [0.43380744]\n\n\nQ: Visualize the quality of the fit by superposing the learned model to the data with matplotlib.\nTip: you can get the extreme values of the xaxis with X.min() and X.max(). To visualize the model, you just need to plot a line between the points (X.min(), w*X.min()+b) and (X.max(), w*X.max()+b).\n\nplt.figure(figsize=(10, 5))\n\nplt.scatter(X, t)\n\nx_axis = [X.min(), X.max()]\nplt.plot(x_axis, w*x_axis + b)\n\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.show()\n\n\n\n\n\n\n\n\nAnother option is to predict a value for all inputs and plot this vector y against the desired values t.\nQ: Make a scatter plot where t is the x-axis and y = w\\, x + b is the y-axis. How should the points be arranged in the ideal case? Also plot what this ideal relationship should be.\n\ny = w * X + b\n\nplt.figure(figsize=(10, 5))\n\nplt.scatter(t, y)\n\nx_axis = [t.min(), t.max()]\nplt.plot(x_axis, x_axis)\n\nplt.xlabel(\"t\")\nplt.ylabel(\"y\")\nplt.show()\n\n\n\n\n\n\n\n\nA: The points (t, y) should be on a line with slope 1 and intercept 0 (i.e. t=y).\nA much better method to analyse the result of the learning algorithm is to track the mean squared error (mse) after each epoch, i.e. the loss function which we actually want to minimize. The MSE is defined as:\n\\text{mse} = \\frac{1}{N} \\, \\sum_{i=1}^N (t_i - y_i)^2\nQ: Modify your LMS algorithm (either directly or copy it in the next cell) to track the mse after each epoch. After each epoch, append the mse on the training set to a list (initially empty) and plot it at the end. How does the mse evolve? Which value does it get in the end? Why? How many epochs do you actually need?\n\nw = 0\nb = 0\n\neta = 0.1\n\nlosses = []\nfor epoch in range(100):\n    dw = 0\n    db = 0.0\n    mse = 0.0\n    \n    for i in range(N):\n        # Prediction\n        y = w * X[i] + b\n        \n        # LMS\n        dw += (t[i] - y) * X[i]\n        db += (t[i] - y)\n        \n        # mse\n        mse += (t[i] - y)**2\n    \n    # Parameter updates\n    w += eta*dw/N\n    b += eta*db/N\n    losses.append(mse/N)\n    \nprint('mse:', mse/N)\n    \nplt.figure(figsize=(10, 5))\nplt.plot(losses)\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"mse\")\nplt.show()\n\nmse: [255.93046608]\n\n\n\n\n\n\n\n\n\nA: The mse decreases exponentially with the epochs. It never reaches 0 because of the noise in the data (try setting the noise argument in the data generator to 0 and check that the mse reaches 0). 30 or 40 epochs seem sufficient to solve the problem, nothing happens afterwards.\nLet’s now study the influence of the learning rate eta=0.1 seemed to work, but is it the best value?\nQ: Iterate over multiple values of eta using a logarithmic scale and plot the final mse after 100 epochs as a function of the learning rate. Conclude.\nHint: the logarithmic scale means that you will try values such as 10^{-5}, 10^{-4}, 10^{-3}, etc. until 1.0. In Python, you can either write explictly 0.0001 or use the notation 1e-4. For the plot, use np.log10(eta) to only display the exponent on the X-axis.\n\nlosses = []\n\netas = [1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1.0]\n\nfor eta in etas:\n    w = 0\n    b = 0\n    \n    for epoch in range(100):\n        dw = 0\n        db = 0.0\n        mse = 0.0\n\n        for i in range(N):\n            # Prediction\n            y = w*X[i] + b\n            \n            # LMS\n            dw += (t[i] - y)*X[i]\n            db += (t[i] - y)\n            \n            # mse\n            mse += (t[i] - y)**2\n\n        # Parameter updates\n        w += eta*dw/N\n        b += eta*db/N\n    \n    losses.append(mse/N)\n    \nprint(losses)\n\nplt.figure(figsize=(10, 5))\nplt.plot(np.log10(etas), losses)\nplt.xlabel(\"log(eta)\")\nplt.ylabel(\"mse\")\nplt.show()\n\n[array([2502.38125727]), array([2498.82326114]), array([2463.56399936]), array([2141.11879663]), array([670.57755267]), array([276.42556726]), array([276.42533428])]\n\n\n\n\n\n\n\n\n\nA: With small values of the learning rate, LMS does not have time to converge within 100 epochs. A learning rate of 1 is actually possible in this simple problem. Try setting eta to 2.0 and observe what happens."
  },
  {
    "objectID": "exercises/3-LinearRegression-solution.html#scikit-learn",
    "href": "exercises/3-LinearRegression-solution.html#scikit-learn",
    "title": "Linear regression",
    "section": "Scikit-learn",
    "text": "Scikit-learn\nThe code that you have written is functional, but extremely slow, as you use for loops in Python. For so little data samples, it does not make a difference, but if you had millions of samples, this would start to be a problem.\nThe solution is to use optimized implementations of the algorithms, running in C++ or FORTRAN under the hood. We will use here the LMS algorithm provided by scikit-learn as you have already installed it and it is very simple to use. Note that one could use tensorflow too, but that would be killing a fly with a sledgehammer.\nscikit-learn provides a LinearRegression object that implements LMS. The documentation is at: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html.\nYou simply import it with:\nfrom sklearn.linear_model import LinearRegression\nYou create the object with:\nreg = LinearRegression()\nreg is now an object with different methods (fit(), predict()) that accept any kind of data and performs linear regression.\nTo train the model on the data (X, t), simply use:\nreg.fit(X, t)\nThe parameters of the model are obtained with reg.coef_ for w and reg.intercept_ for b.\nYou can predict outputs for new inputs using:\ny = reg.predict(X)\nQ: Apply linear regression on the data using scikit-learn. Check the model parameters after learning and compare them to what you obtained previously. Print the mse and make a plot comparing the predictions with the data.\n\nfrom sklearn.linear_model import LinearRegression\n\n# Linear regression\nreg = LinearRegression()\nreg.fit(X, t)\n\nprint(reg.coef_, reg.intercept_)\n\n# Prediction\ny = reg.predict(X)\n\n# mse\nmse = np.mean((t - y)**2)\nprint('mse:', mse)\n\nplt.figure(figsize=(10, 5))\nplt.scatter(t, y)\nx_axis = [t.min(), t.max()]\nplt.plot(x_axis, x_axis)\nplt.xlabel(\"t\")\nplt.ylabel(\"y\")\nplt.show()\n\n[50.61163981] -2.15821943523261\nmse: 276.42533428232537\n\n\n\n\n\n\n\n\n\nA: No need to select the right learning rate, fast and error-free."
  },
  {
    "objectID": "exercises/3-LinearRegression-solution.html#delta-learning-rule",
    "href": "exercises/3-LinearRegression-solution.html#delta-learning-rule",
    "title": "Linear regression",
    "section": "Delta learning rule",
    "text": "Delta learning rule\nLet’s now implement the online version of LMS, the delta learning rule. The only difference is that the parameter updates are applied immediately after each example is evaluated, not at the end of training.\n\nw=0 \\quad;\\quad b=0\nfor M epochs:\n\nfor each sample (x_i, t_i):\n\ny_i = w \\, x_i + b\n\\Delta w = \\eta \\, (t_i - y_i ) \\, x_i\n\\Delta b = \\eta \\, (t_i - y_i)\n\n\n\nQ: Implement the delta learning rule for the regression problem with eta = 0.1. Plot the evolution of the mse and compare it to LMS.\n\nw = 0\nb = 0\n\neta = 0.1\n\nlosses = []\nfor epoch in range(100):\n    \n    mse = 0.0\n    \n    for i in range(N):\n        # Prediction\n        y = w * X[i] + b\n        \n        # Delta learning rule\n        w += eta * (t[i] - y) * X[i]\n        b += eta * (t[i] - y)\n        \n        # mse\n        mse += (t[i] - y)**2\n\n    losses.append(mse/N)\n    \nprint('mse:', mse/N)\n    \nplt.figure(figsize=(10, 5))\nplt.plot(losses)\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"mse\")\nplt.show() \n\nmse: [317.20325456]\n\n\n\n\n\n\n\n\n\n\ny = w * X + b\n\nplt.figure(figsize=(10, 5))\n\nplt.scatter(t, y)\n\nx_axis = [t.min(), t.max()]\nplt.plot(x_axis, x_axis)\n\nplt.xlabel(\"t\")\nplt.ylabel(\"y\")\nplt.show()\n\n\n\n\n\n\n\n\nA: The delta learning rule converges faster, but the final mse is higher. This is due to the learning rate as shown in the next question.\nQ: Vary the learning rate logarithmically as for LMS and conclude.\n\nlosses = []\n\netas = [1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\n\nfor eta in etas:\n    \n    w = 0\n    b = 0\n\n    for epoch in range(100):\n    \n        mse = 0.0\n    \n        for i in range(N):\n            # Prediction\n            y = w * X[i] + b\n        \n            # Delta learning rule\n            w += eta * (t[i] - y) * X[i]\n            b += eta * (t[i] - y)\n        \n            # mse\n            mse += (t[i] - y)**2\n\n    losses.append(mse/N)\n    \nprint(losses)\n\nplt.figure(figsize=(10, 5))\nplt.plot(np.log10(etas), losses)\nplt.xlabel(\"eta\")\nplt.ylabel(\"mse\")\nplt.show()\n\n[array([2463.37614216]), array([2139.66406069]), array([670.31330378]), array([276.95458754]), array([281.52657112]), array([317.20325456])]\n\n\n\n\n\n\n\n\n\nA: The optimal value of eta is now 0.001, higher learning rates lead to worse mse. One explanation is that the true learning rate of LMS is eta/N = 0.001, not eta=0.1. When using 0.001 for the learning, the delta learning rule behaves exactly like LMS:\n\nw = 0\nb = 0\n\neta = 0.001\n\nlosses = []\nfor epoch in range(100):\n    \n    mse = 0.0\n    \n    for i in range(N):\n        # Prediction\n        y = w * X[i] + b\n        \n        # Delta learning rule\n        w += eta * (t[i] - y) * X[i]\n        b += eta * (t[i] - y)\n        \n        # mse\n        mse += (t[i] - y)**2\n\n    losses.append(mse/N)\n    \nprint('mse:', mse/N)\n    \nplt.figure(figsize=(10, 5))\nplt.plot(losses)\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"mse\")\nplt.show() \n\nmse: [276.95458754]"
  },
  {
    "objectID": "exercises/4-MLR-solution.html#linear-regression",
    "href": "exercises/4-MLR-solution.html#linear-regression",
    "title": "Multiple linear regression",
    "section": "Linear regression",
    "text": "Linear regression\nQ: Apply MLR on the California data using the same LinearRegression method of scikit-learn as last time. Print the mse, plot how the predictions predict the price for each feature, and plot the prediction y against the true value t for each sample as in the last exercise. Does it work well?\nYou will also plot the weights of the model (reg.coef_) and conclude on the relative importance of the different features: which feature has the stronger weight and why?\n\nfrom sklearn.linear_model import LinearRegression\n\n# Linear regression\nreg = LinearRegression()\nreg.fit(X, t)\n\n# Prediction\ny = reg.predict(X)\n\n# mse\nmse = np.mean((t - y)**2)\nprint(\"MSE:\", mse)\n\nplt.figure(figsize=(8, 6))\nplt.scatter(t, y)\nplt.plot([t.min(), t.max()], [t.min(), t.max()], c=\"red\")\nplt.xlabel(\"t\")\nplt.ylabel(\"y\")\n\nplt.figure(figsize=(12, 15))\nfor i in range(8):\n    plt.subplot(4, 2 , i+1)\n    plt.scatter(X[:, i], t)\n    plt.scatter(X[:, i], y)\n    plt.title(dataset.feature_names[i])\nplt.show()\n\nplt.figure(figsize=(12, 5))\nplt.plot(np.abs(reg.coef_))\nplt.xlabel(\"Feature\")\nplt.ylabel(\"Weight\")\nplt.show()\n\nMSE: 0.5243209861846072\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA: Feature 3 (AveBedrms, average number of bedrooms per household) got a very strong weight, although it is not a very good predictor of the price. The main reason is that the features are not normalized: AveBedrms varies between 0 an 135, while population varies between 0 and 35000. The weight on population does not need to be very high to influence the price prediction, although it might be very important.\nA good practice in machine learning is to normalize the inputs, i.e. to make sure that the features have a mean of 0 and a standard deviation of 1. The formula is:\nX^\\text{normalized} = \\dfrac{X - \\mathbb{E}[X]}{\\text{std}(X)}\ni.e. you compute the mean and standard deviation of each column of X and apply the formula on each column.\nQ: Normalize the dataset. Make sure that the new mean and std is correct.\nTip: X.mean(axis=0) and X.std(axis=0) should be useful.\n\nX_normalized = (X - X.mean(axis=0))/X.std(axis=0)\n\nprint(\"Old mean:\", X.mean(axis=0))\nprint(\"New mean:\", X_normalized.mean(axis=0))\nprint(\"Old std:\", X.std(axis=0))\nprint(\"New std:\", X_normalized.std(axis=0))\n\nOld mean: [ 3.87067100e+00  2.86394864e+01  5.42899974e+00  1.09667515e+00\n  1.42547674e+03  3.07065516e+00  3.56318614e+01 -1.19569704e+02]\nNew mean: [ 6.60969987e-17  5.50808322e-18  6.60969987e-17 -1.06030602e-16\n -1.10161664e-17  3.44255201e-18 -1.07958431e-15 -8.52651283e-15]\nOld std: [1.89977569e+00 1.25852527e+01 2.47411320e+00 4.73899376e-01\n 1.13243469e+03 1.03857980e+01 2.13590065e+00 2.00348319e+00]\nNew std: [1. 1. 1. 1. 1. 1. 1. 1.]\n\n\nQ: Apply MLR again on X^\\text{normalized}, print the mse and visualize the weights. What has changed?\n\n# Linear regression\nreg = LinearRegression()\nreg.fit(X_normalized, t)\n\n# Prediction\ny = reg.predict(X_normalized)\n\n# mse\nmse = np.mean((t - y)**2)\nprint(\"mse:\", mse)\n\nplt.figure(figsize=(8, 6))\nplt.scatter(t, y)\nplt.plot([t.min(), t.max()], [t.min(), t.max()], c=\"red\")\nplt.xlabel(\"t\")\nplt.ylabel(\"y\")\n\nplt.figure(figsize=(12, 15))\nfor i in range(8):\n    plt.subplot(4, 2 , i+1)\n    plt.scatter(X_normalized[:, i], t)\n    plt.scatter(X_normalized[:, i], y)\n    plt.title(dataset.feature_names[i])\nplt.show()\n\nplt.figure(figsize=(12, 5))\nplt.plot(np.abs(reg.coef_))\nplt.xlabel(\"Feature\")\nplt.ylabel(\"Weight\")\nplt.show()\n\nmse: 0.5243209861846072\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA: The mse does not change (it may in more complex problems), but the weights are more interpretable. Now strong weights in absolute value mean that the features are very predictive of the price. In particular, the feature populations"
  },
  {
    "objectID": "exercises/4-MLR-solution.html#regularized-regression",
    "href": "exercises/4-MLR-solution.html#regularized-regression",
    "title": "Multiple linear regression",
    "section": "Regularized regression",
    "text": "Regularized regression\nNow is time to investigate regularization: 1. MLR with L2 regularization is called Ridge regression 2. MLR with L1 regularization is called Lasso regression\nFortunately, scikit-learn provides these methods with a similar interface to LinearRegression. The Ridge and Lasso objects take an additional argument alpha which represents the regularization parameter:\nreg = Ridge(alpha=1.0)\nreg = Lasso(alpha=1.0)\n\nfrom sklearn.linear_model import Ridge, Lasso\n\nQ: Apply Ridge and Lasso regression on the scaled data, vary the regularization parameter to understand its function and comment on the results. In particular, vary the regularization parameter for LASSO and identify the features which are the most predictive of the price. Does it make sense?\n\nreg = Ridge(alpha=10.0)\n\nreg.fit(X_normalized, t)\ny = reg.predict(X_normalized)\nmse = np.mean((t - y)**2)\nprint(mse)\n\nprint(reg.coef_)\n\nplt.figure(figsize=(8, 6))\nplt.scatter(t, y)\nplt.plot([t.min(), t.max()], [t.min(), t.max()], c=\"red\")\nplt.xlabel(\"t\")\nplt.ylabel(\"y\")\n\nplt.figure(figsize=(12, 15))\nfor i in range(8):\n    plt.subplot(4, 2 , i+1)\n    plt.scatter(X_normalized[:, i], t)\n    plt.scatter(X_normalized[:, i], y)\n    plt.title(dataset.feature_names[i])\nplt.show()\n\nplt.figure(figsize=(12, 5))\nplt.plot(np.abs(reg.coef_))\nplt.xlabel(\"Feature\")\nplt.ylabel(\"Weight\")\nplt.show()\n\n0.5243267377872187\n[ 0.8293461   0.11939823 -0.26422311  0.30398067 -0.00427544 -0.03936068\n -0.8937389  -0.86433656]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nreg = Lasso(alpha=0.1)\n\nreg.fit(X_normalized, t)\ny = reg.predict(X_normalized)\nmse = np.mean((t - y)**2)\nprint(mse)\n\nprint(reg.coef_)\n\nplt.figure(figsize=(8, 6))\nplt.scatter(t, y)\nplt.plot([t.min(), t.max()], [t.min(), t.max()], c=\"red\")\nplt.xlabel(\"t\")\nplt.ylabel(\"y\")\n\nplt.figure(figsize=(12, 15))\nfor i in range(8):\n    plt.subplot(4, 2 , i+1)\n    plt.scatter(X_normalized[:, i], t)\n    plt.scatter(X_normalized[:, i], y)\n    plt.title(dataset.feature_names[i])\nplt.show()\n\nplt.figure(figsize=(12, 5))\nplt.plot(np.abs(reg.coef_))\nplt.xlabel(\"Feature\")\nplt.ylabel(\"Weight\")\nplt.show()\n\n0.6741415809786988\n[ 0.70571337  0.10601099 -0.         -0.         -0.         -0.\n -0.01121267 -0.        ]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA: Ridge regression does not have a big effect for this data. By increasing the regularization parameter in Lasso regression, the MSE worsens slightly, but the number of non-zero weights becomes very small. With alpha=1.0, all weights are set to 0… With alpha=0.1, there are only 3 non-zero parameters: the corresponding features are the most predictive of the prices. You can identify them with:\n\nfor i, w in enumerate(reg.coef_):\n    if w != 0.0:\n        print(dataset.feature_names[i], w)\n\nMedInc 0.705713372959335\nHouseAge 0.10601099216033393\nLatitude -0.011212667061584703"
  },
  {
    "objectID": "exercises/5-Crossvalidation-solution.html#polynomial-regression",
    "href": "exercises/5-Crossvalidation-solution.html#polynomial-regression",
    "title": "Cross-validation and polynomial regression",
    "section": "Polynomial regression",
    "text": "Polynomial regression\nPolynomial regression consists of fitting some data (x, y) to a n-order polynomial of the form:\n\ny = f(x) = b + w_1 \\cdot x + w_2 \\cdot x^2 + ... + w_n \\cdot x^n\n\nBy rewriting the unidimensional input x into the following vector:\n\n\\mathbf{x} = \\begin{bmatrix} x & x^2 & ... & x^n \\end{bmatrix}^T\n\nand the weight vector as:\n\n\\mathbf{w} = \\begin{bmatrix} w_1 & w_2 & ... & w_n \\end{bmatrix}^T\n\nthe problem can be reduced to linear regression:\n\ny = \\langle \\mathbf{w} \\cdot \\mathbf{x} \\rangle + b\n\nand we can apply the delta learning rule to find \\mathbf{w} and b:\n\n\\Delta \\mathbf{w} =  \\eta \\, (t_i - y_i ) \\, \\mathbf{x_i}\n \n\\Delta b =  \\eta \\cdot (t_i - y_i )\n\nA first method to perform polynomial regression would be to adapt the code you wrote in the last exercise session for linear regression. However, you saw that properly setting the correct learning rate can be quite tricky.\nThe solution retained for this exercise is to use the built-in functions of Numpy which can already perform polynomial regression in an optimized and proved-sure manner (Note: NumPy does not use gradient descent, but rather directly minimizes the error-function by inversing the Gram matrix).\nw = np.polyfit(X, t, deg)\nThis function takes the inputs X, the desired outputs t and the desired degree of the polynomial deg, performs the polynomial regression and returns the adequate set of weights (beware: the higher-order coefficient comes first, the bias is last).\nOnce the weights are obtained, one can use them to predict the value of an example with the function:\ny = np.polyval(w, X)\nNote: if you prefer to use scikit-learn, check https://scikit-learn.org/stable/auto_examples/linear_model/plot_polynomial_interpolation.html but see https://towardsdatascience.com/polynomial-regression-with-scikit-learn-what-you-should-know-bed9d3296f2 for why it may be a bad idea.\nLet’s start by importing the usual stuff and create a dataset of 16 samples generated using the function x \\, \\sin x plus some noise:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Just to avoid the annoying warnings, please ignore\ndef warn(*args, **kwargs):\n    pass\nimport warnings\nwarnings.warn = warn\n \ndef create_dataset(N, noise):\n    \"Creates a dataset of N points generated from x*sin(x) plus some noise.\"\n    \n    x = np.linspace(0, 10, 300)\n    rng = np.random.default_rng()\n    rng.shuffle(x)\n    x = np.sort(x[:N])\n    t = x * np.sin(x) + noise*rng.uniform(-1.0, 1.0, N)\n    \n    return x, t\n\nN = 16\nX, t = create_dataset(N, noise=0.2)\n\n\nx = np.linspace(0, 10, 100)\n\nplt.figure(figsize=(10, 6))\nplt.plot(x, x*np.sin(x), label=\"Ground truth\")\nplt.scatter(X, t)\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nQ: Apply the np.polyfit() function on the data and visualize the result for different degrees of the polynomial (from 1 to 20 or even more). What do you observe? Find a polynomial degree which clearly overfits.\n\ndeg = 10\n\n# Polynomial regression\nw = np.polyfit(X, t, deg)\n\n# Inference on the training set\ny = np.polyval(w, x)\n\nplt.figure(figsize=(10, 6))\nplt.plot(x, x*np.sin(x), label=\"Ground truth\")\nplt.plot(x, y, label=\"Polynome\")\nplt.scatter(X, t)\nplt.ylim((-10., 10.))\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nQ: Plot the mean square error on the training set for all polynomial regressions from 1 to 20. How does the training error evolve when the degree of the polynomial is increased? What is the risk by taking the hypothesis with the smallest training error?\n\ntraining_mse = []\n\ndegrees = range(1, 21)\n\nfor deg in degrees:\n    \n    w = np.polyfit(X, t, deg)\n    y = np.polyval(w, X)\n    \n    mse = np.mean((t-y)**2)\n    \n    training_mse.append(mse)\n    \n    print(\"Degree\", deg, \": training error\", mse)\n    \nplt.figure(figsize=(10, 6))\nplt.plot(degrees, training_mse)\nplt.xlabel(\"Order of the polynomial\")\nplt.ylabel(\"Training mse\")\nplt.show()\n\nDegree 1 : training error 12.040390187866535\nDegree 2 : training error 10.875215511427697\nDegree 3 : training error 9.72130992005902\nDegree 4 : training error 1.013275849205464\nDegree 5 : training error 0.6554057884023228\nDegree 6 : training error 0.10045095302800439\nDegree 7 : training error 0.014213994513393955\nDegree 8 : training error 0.0034403058897094084\nDegree 9 : training error 0.0034237572576303627\nDegree 10 : training error 0.0034237252284956216\nDegree 11 : training error 0.002368908807055285\nDegree 12 : training error 0.0015870603333565426\nDegree 13 : training error 0.00104481788854725\nDegree 14 : training error 1.49961699852604e-08\nDegree 15 : training error 2.512531648830143e-15\nDegree 16 : training error 4.444342304484706e-15\nDegree 17 : training error 5.355915227708014e-16\nDegree 18 : training error 5.833350451392072e-16\nDegree 19 : training error 4.220566952479118e-16\nDegree 20 : training error 1.1832546556704793e-16\n\n\n\n\n\n\n\n\n\nA: The more complex the model, the smaller the training error."
  },
  {
    "objectID": "exercises/5-Crossvalidation-solution.html#simple-hold-out-cross-validation",
    "href": "exercises/5-Crossvalidation-solution.html#simple-hold-out-cross-validation",
    "title": "Cross-validation and polynomial regression",
    "section": "Simple hold-out cross-validation",
    "text": "Simple hold-out cross-validation\nYou will now apply simple hold-out cross-validation to find the optimal degree for the polynomial regression. You will need to separate the data set into a training set S_{\\text{train}} (70% of the data) and a test set S_{\\text{test}} (the remaining 30%).\nThe data (X, t) could be easily split into two sets of arrays using slices of indices, as the data is already randomized:\nN_train = int(0.7*N)\nX_train, t_train = X[:N_train], t[:N_train]\nX_test, t_test = X[N_train:], t[N_train:]\nA much more generic approach is to use the library scikit-learn (https://www.scikit-learn.org), which provides a method able to split any dataset randomly.\nYou can import the method train_test_split() from its module:\n\nfrom sklearn.model_selection import train_test_split\n\nThe doc of the function is available at: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html.\nQ: Use scikit-learn to split the data into the corresponding training and test sets. Train each polynomial from degree 1 to 20 on S_{\\text{train}} and plot the generalization error on S_{\\text{test}}. Which degree of the polynomial gives the minimal empirical error? Why? Run the cross-validation split multiple times. Do you always obtain the same optimal degree?\n\nX_train, X_test, t_train, t_test = train_test_split(X, t, test_size=0.3)\n\ndegrees = range(1, 21)\n\ntest_mse = []\n\nfor deg in degrees:\n    # Train on the training set\n    w = np.polyfit(X_train, t_train, deg)\n    \n    # Test on the test set\n    y_test = np.polyval(w, X_test)\n    mse = np.mean((t_test-y_test)**2)\n    test_mse.append(mse)\n    \n    print(\"Degree\", deg, \": empirical error\", mse)\n    \nplt.figure(figsize=(10, 6))\nplt.plot(degrees, test_mse)\nplt.xlabel(\"Order of the polynomial\")\nplt.ylabel(\"Test mse\")\nplt.show()\n\nDegree 1 : empirical error 9.227259124556571\nDegree 2 : empirical error 7.121999332462833\nDegree 3 : empirical error 7.800338997175413\nDegree 4 : empirical error 2.923789248256516\nDegree 5 : empirical error 1.9088688058251706\nDegree 6 : empirical error 0.8374409236089155\nDegree 7 : empirical error 0.1014285554051753\nDegree 8 : empirical error 0.3225130019507089\nDegree 9 : empirical error 3.634336627210941\nDegree 10 : empirical error 182.85222170544995\nDegree 11 : empirical error 108.29728063704897\nDegree 12 : empirical error 79.12104828992105\nDegree 13 : empirical error 75.20358313836738\nDegree 14 : empirical error 90.44257331101254\nDegree 15 : empirical error 125.77278918485517\nDegree 16 : empirical error 187.00252014852725\nDegree 17 : empirical error 284.62431607035285\nDegree 18 : empirical error 434.6137740816647\nDegree 19 : empirical error 659.8631554481153\nDegree 20 : empirical error 992.1188629796101\n\n\n\n\n\n\n\n\n\nA: Depending on the split, the optimal degree is around 6 or 7. This variability is because some samples of the test set might better fit with a given polynomial, depending on how the split was made."
  },
  {
    "objectID": "exercises/5-Crossvalidation-solution.html#k-fold-cross-validation",
    "href": "exercises/5-Crossvalidation-solution.html#k-fold-cross-validation",
    "title": "Cross-validation and polynomial regression",
    "section": "k-fold cross-validation",
    "text": "k-fold cross-validation\nAs we only have 16 samples to learn from, it is quite annoying to “lose” 5 of them for the test set. Here we can afford to use k-fold cross-validation, where the cross-validation split is performed k times:\n\nThe dataset is split into k subsets of equal size (if possible).\nEach subset is iteratively used as the test set, while the k-1 other ones are used as a training set.\nThe final empirical error is the average of the mse on all subsets.\n\nIt would be possible to make the splits using indices too, but it is much easier to use scikit-learn once again. You can import the KFold class like this:\n\nfrom sklearn.model_selection import KFold\n\nk = 4\nkf = KFold(n_splits=k, shuffle=True)\n\nn_splits corresponds to k: how many times the dataset is split. We can take k=4 for example (4 subsets of 4 samples).\nQ: Check the doc of KFold (https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html). Print the indices of the examples of the training and test sets for each iteration of the algorithm. Change the value of k to understand how it works.\n\nfor train_index, test_index in kf.split(X, t):\n    print(\"Train:\", train_index)\n    print(\"Test:\", test_index)\n    print('-------')\n\nTrain: [ 1  2  3  4  5  6  7  8  9 10 13 15]\nTest: [ 0 11 12 14]\n-------\nTrain: [ 0  2  4  5  7  8  9 10 11 12 14 15]\nTest: [ 1  3  6 13]\n-------\nTrain: [ 0  1  2  3  5  6  7  8 11 12 13 14]\nTest: [ 4  9 10 15]\n-------\nTrain: [ 0  1  3  4  6  9 10 11 12 13 14 15]\nTest: [2 5 7 8]\n-------\n\n\nQ: Apply k-fold cross-validation on the polynomial regression problem. Which polynomial degree is the best? Run the split multiple times: does the best polynomial degree change?\n\nk = 4\nkf = KFold(n_splits=k, shuffle=True)\n\ndegrees = range(1, 21)\n\ntest_mse = []\n\nfor train_index, test_index in kf.split(X, t):\n    \n    split_mse = []\n    \n    for deg in degrees:\n        \n        w = np.polyfit(X[train_index], t[train_index], deg)\n        y = np.polyval(w, X[test_index])\n        \n        mse = np.mean((t[test_index] - y)**2)\n        split_mse.append(mse)\n    \n    test_mse.append(split_mse)\n        \ntest_mse = np.mean(test_mse, axis=0)\n\nfor deg, mse in zip(degrees, test_mse):\n    print(deg, mse)\n\nplt.figure(figsize=(10, 6))\nplt.plot(degrees, test_mse)\nplt.xlabel(\"Degree of the polynome\")\nplt.ylabel(\"k-fold cross-validated mse\")\nplt.show()\n\n1 14.373254672725508\n2 32.64780689331275\n3 57.437214035571806\n4 2.0628672303945312\n5 48.64866507586964\n6 12.48275298341056\n7 54.942484536366294\n8 3.45332197162841\n9 229.5520298425848\n10 1441.4049703344376\n11 104.14494006648448\n12 15.321973716914645\n13 63.57807742564708\n14 396.8565006894557\n15 1211.4900029370529\n16 2711.1739306435934\n17 5003.070625815316\n18 7908.869099904825\n19 10708.239704364289\n20 11952.837698754893\n\n\n\n\n\n\n\n\n\nA: A polynomial of order between 6 and 8 still seems the best, but it varies from run to run (as well as on the data, you may observe something different).\nQ: Change k to N. How stable are the results between two runs?\n\nk = N\nkf = KFold(n_splits=k, shuffle=True)\n\ndegrees = range(1, 21)\n\ntest_mse = []\n\nfor train_index, test_index in kf.split(X, t):\n    \n    split_mse = []\n    \n    for deg in degrees:\n        \n        w = np.polyfit(X[train_index], t[train_index], deg)\n        y = np.polyval(w, X[test_index])\n        \n        mse = np.mean((t[test_index] - y)**2)\n        split_mse.append(mse)\n    \n    test_mse.append(split_mse)\n        \ntest_mse = np.mean(test_mse, axis=0)\n\nfor deg, mse in zip(degrees, test_mse):\n    print(deg, mse)\n\nplt.figure(figsize=(10, 6))\nplt.plot(degrees, test_mse)\nplt.xlabel(\"Degree of the polynome\")\nplt.ylabel(\"k-fold cross-validated mse\")\nplt.show()\n\n1 15.098319479975594\n2 17.552207001911373\n3 18.676985531565293\n4 1.9252033758472606\n5 2.5834884434006\n6 0.5085078629713802\n7 0.18595843931801115\n8 0.026693779886147913\n9 0.15529428559686834\n10 2.9763791625175444\n11 4.2610890696345605\n12 224.0411629243045\n13 376.5448377988029\n14 0.02667071664931066\n15 34.16017293924377\n16 195.02547279346703\n17 607.7556163128706\n18 1471.281491746452\n19 3078.178648457924\n20 5823.739259283549\n\n\n\n\n\n\n\n\n\nA: With k=N (one sample in the test set every time), the results are more stable. It is called leave-one-out cross-validation (LOOCV). It is the best cross-validation you can make in terms of bias (you use almost all your data to learn), but it is very expensive (you have to retrain your algorithm for each sample) and the empirical error has a high variance.\nQ: Regenerate the data with a noise equal to 0.0 and re-run all experiments. What does it change?\nA: Without noise, higher-order polynomials are a better fit. This is because the x \\, \\sin x can be “naturally” approximated by polynomials (Taylor series)."
  },
  {
    "objectID": "exercises/6-LinearClassification-solution.html#hard-linear-classification-perceptron-algorithm",
    "href": "exercises/6-LinearClassification-solution.html#hard-linear-classification-perceptron-algorithm",
    "title": "Linear classification",
    "section": "Hard linear classification: perceptron algorithm",
    "text": "Hard linear classification: perceptron algorithm\n\nLinearly separable data\nWe start by generating a binary linear classification dataset with 100 examples (50 in each class). You do not need to read the code generating the data, we just randomly select 100 point in [0, 1]^2 and assign a label depending on their position.\n\ndef create_dataset(n_samples):\n    \n    rng = np.random.default_rng()\n    \n    X = rng.uniform(0.0, 1.0, (n_samples, 2))\n    \n    t = np.array([1 if X[i, 0] + X[i, 1] > 1. else -1 for i in range(n_samples)])\n    \n    return X, t\n\nX, t = create_dataset(100)\n\nLet’s visualize the training set now in the following cell. Samples of the positive class (t=1) will be represented by blue points, examples of the negative class (t=-1) by orange ones.\n\nplt.figure(figsize=(10, 6))\nplt.scatter(X[t==1, 0], X[t==1, 1])\nplt.scatter(X[t==-1, 0], X[t==-1, 1])\nplt.show()\n\n\n\n\n\n\n\n\nYou will now implement the online version of the Perceptron algorithm to classify this data set.\nAs a reminder from the course, we will use an hyperplane (\\mathbf{w}, b) to predict whether an input \\mathbf{x}_i belongs to the positive class (+1) or negative class (-1) using the following function:\n\n    y_i =  \\text{sign}( \\langle \\mathbf{w} . \\mathbf{x}_i \\rangle + b)\n\nOur goal is to minimize the mean square error (mse) of the hyperplane on the training set:\n\n    L(\\mathbf{w}, b) = \\frac{1}{N} \\, \\sum_{i=1}^N (t_i - y_i)^2\n\nBy applying gradient descent on this loss function, we obtain the delta learning rule:\n\n    \\Delta \\mathbf{w} = \\eta \\, \\sum_{i=1}^N (t_i - y_i) \\, \\mathbf{x}_i\n\n\n    \\Delta b = \\eta \\, \\sum_{i=1}^N (t_i - y_i)\n\nThe online version of the Perceptron is given by the following algorithm:\n\\text{Initialize the weight vector } \\mathbf{w} \\text{ and the bias } b.\n\\textbf{for } M \\text{epochs:}\n\\qquad \\textbf{forall } \\text{examples } (\\mathbf{x}_i, t_i) :\n\\qquad \\qquad y_i = \\text{sign}( \\langle \\mathbf{w} . \\mathbf{x}_i \\rangle + b)\n\\qquad \\qquad \\mathbf{w} \\gets \\mathbf{w} + \\eta \\, (t_i - y_i) \\, \\mathbf{x}_i\n\\qquad \\qquad b \\gets b + \\eta \\, (t_i - y_i)\nQ: Implement the algorithm based on the linear regression algorithm of exercise 3. The only difference is that the weight vector is now a vector… You will need to use np.dot. Use 20 epochs and a learning rate of 0.01 at first, but you can vary it later. Initialize the weight vector and the bias to 0. Make a plot of the mse during training.\n\n# Parameters\neta = 0.01\nnb_epochs = 20\nN = len(t)\n\n# Initialize the weight vector and bias\nw = np.zeros(2)\nb = 0.\n\n# Perceptron algorithm\nlosses = []\n\nfor epoch in range(nb_epochs):\n    loss = 0\n    \n    # Iterate over all training examples\n    for i in range(N):\n        # Prediction\n        y_i = np.sign(np.dot(w, X[i, :]) + b)\n        # Update the weight\n        w += eta * (t[i] - y_i) * X[i, :]\n        # Update the bias\n        b += eta * (t[i] - y_i) \n        # Loss mse\n        loss += (t[i] - y_i)**2\n        \n    # Append \n    losses.append(loss/N)\n    \nplt.figure(figsize=(10, 6))\nplt.plot(losses)\nplt.show()\n\n\n\n\n\n\n\n\nQ: Visualize the hyperplane. If we call x_0 and x_1 the two coordinates of the inputs, the equation of the hyperplane is:\nw_0 \\, x_0 + w_1 \\, x_1 + b = 0\nwhich takes the form:\nx_1 = - (w_0 \\, x_0 + b) / w_1\nYou just need to draw a line between the two extremities of the hyperplane, for example between 0 and 1 or between X[:, 0].min() and X[:, 0].max().\n\nplt.figure(figsize=(10, 6)) \n\n# Data\nplt.scatter(X[t==1, 0], X[t==1, 1])\nplt.scatter(X[t==-1, 0], X[t==-1, 1])\n\n# Model\nx_min, x_max = X[:, 0].min(), X[:, 0].max()\nplt.plot([x_min, x_max], [-(x_min*w[0] + b)/w[1], -(x_max*w[0] + b)/w[1]])\n\n\n\n\n\n\n\n\nBefore going further, let’s track the evolution of the classification error during learning, defined as the fraction of incorrectly classified examples during one epoch:\n\n    \\epsilon = \\frac{\\text{Number of misclassifications}}{\\text{Number of samples}}\n\nQ: Modify your algorithm to compute the training error and the mse loss for each epoch. How do the training error and loss evolve during learning? Do you really need both?\nTips: When iterating over each training example, you will need to increment a counter for misclassifications when your prediction y_i is different from t[i] (use != for “not equal”).\n\n# Parameters\neta = 0.01\nnb_epochs = 20\nN = len(t)\n\n# Initialize the weight vector and bias\nw = np.zeros(2)\nb = 0.\n\n# Perceptron algorithm\nerrors = []\nlosses = []\nfor epoch in range(nb_epochs):\n    loss = 0\n    error = 0\n    \n    # Iterate over all training examples\n    for i in range(N):\n        # Prediction\n        y_i = np.sign(np.dot(w, X[i, :]) + b)\n        \n        # Update the weight\n        w += eta * (t[i] - y_i) * X[i, :]\n        \n        # Update the bias\n        b += eta * (t[i] - y_i) \n        \n        # Loss mse\n        loss += (t[i] - y_i)**2\n        \n        # Count misclassifications\n        if t[i] != y_i :\n            error += 1\n        \n    # Append \n    losses.append(loss/N)\n    errors.append(error/N)\n    \nprint(\"Error:\", error/N)  \n\nplt.figure(figsize=(10, 6)) \nplt.scatter(X[t==1, 0], X[t==1, 1])\nplt.scatter(X[t==-1, 0], X[t==-1, 1])\nx_min, x_max = X[:, 0].min(), X[:, 1].max()\nplt.plot([x_min, x_max], [-(x_min*w[0] + b)/w[1], -(x_max*w[0] + b)/w[1]])\n    \nplt.figure(figsize=(10, 6))\nplt.plot(losses, label=\"Loss\")\nplt.plot(errors, label=\"Error\")\nplt.legend()\nplt.show()\n\nError: 0.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA: The training error and the loss vary together, by definition. The scale of the loss is somewhat arbitrary, while the error corresponds to what we want. If you have to choose, visualize the error.\nBoth go down during learning (hopefully), but there are oscillations: this is due to the online version of the Perceptron, which is not stable.\nQ: Now is the time to play with the hyperparameters:\n\nVary the learning rate eta between extreme values (from 0.000001 to 100.0).\nIncrease the number of epochs nb_epochs.\n\nWhat does it change?\n\n# Parameters\neta = 0.0001\nnb_epochs = 200\nN = len(t)\n\n# Initialize the weight vector and bias\nw = np.zeros(2)\nb = 0.\n\n# Perceptron algorithm\nerrors = []\nlosses = []\nfor epoch in range(nb_epochs):\n    loss = 0\n    error = 0\n    \n    # Iterate over all training examples\n    for i in range(N):\n        # Prediction\n        y_i = np.sign(np.dot(w, X[i, :]) + b)\n        # Update the weight\n        w += eta * (t[i] - y_i) * X[i, :]\n        # Update the bias\n        b += eta * (t[i] - y_i) \n        # Loss mse\n        loss += (t[i] - y_i)**2\n        # Count misclassifications\n        if t[i] != y_i :\n            error += 1\n        \n    # Append \n    losses.append(loss/N)\n    errors.append(error/N)\n    \nprint(\"Error:\", error/N)  \n\nplt.figure(figsize=(10, 6)) \nplt.scatter(X[t==1, 0], X[t==1, 1])\nplt.scatter(X[t==-1, 0], X[t==-1, 1])\nx_min, x_max = X[:, 0].min(), X[:, 1].max()\nplt.plot([x_min, x_max], [-(x_min*w[0] + b)/w[1], -(x_max*w[0] + b)/w[1]])\n    \nplt.figure(figsize=(10, 6))\nplt.plot(losses, label=\"Loss\")\nplt.plot(errors, label=\"Error\")\nplt.legend()\nplt.show()\n\nError: 0.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA: The learning rate does not change much… We will see why in the next question.\nAfter a minimum of epochs, the result is stable, because there are no errors anymore: t_i - y_i is always 0, so the weight vector and the bias are not changed anymore.\nQ: Change the initial value of the weight vector \\mathbf{w} to something different from 0 (e.g. [1, -1], [-5, 5], etc). What does it change? Vary the learning rate again and conclude on the importance of weight initialization.\n\n# Parameters\neta = 0.01\nnb_epochs = 20\nN = len(t)\n\n# Initialize the weight vector and bias\nw = np.array([-10., 10.])\nb = 0.\n\n# Perceptron algorithm\nerrors = []\nlosses = []\nfor epoch in range(nb_epochs):\n    loss = 0\n    error = 0\n    \n    # Iterate over all training examples\n    for i in range(N):\n        # Prediction\n        y_i = np.sign(np.dot(w, X[i, :]) + b)\n        # Update the weight\n        w += eta * (t[i] - y_i) * X[i, :]\n        # Update the bias\n        b += eta * (t[i] - y_i) \n        # Loss mse\n        loss += (t[i] - y_i)**2\n        # Count misclassifications\n        if t[i] != y_i :\n            error += 1\n        \n    # Append \n    losses.append(loss/N)\n    errors.append(error/N)\n    \nprint(\"Error:\", error/N)  \n\nplt.figure(figsize=(10, 6)) \nplt.scatter(X[t==1, 0], X[t==1, 1])\nplt.scatter(X[t==-1, 0], X[t==-1, 1])\nx_min, x_max = X[:, 0].min(), X[:, 1].max()\nplt.plot([x_min, x_max], [-(x_min*w[0] + b)/w[1], -(x_max*w[0] + b)/w[1]])\n    \nplt.figure(figsize=(10, 6))\nplt.plot(losses, label=\"Loss\")\nplt.plot(errors, label=\"Error\")\nplt.legend()\nplt.show()\n\nError: 0.33\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA: Contrary to the case where the weight vector is initially 0, the learning rate had a big influence on the learning speed when the initial weight vector is non-zero. It is even worse with big weights ([-10, 10]).\nThe explanation lies on the incremental nature of the perceptron learning rule:\n\n    \\mathbf{w} \\leftarrow \\mathbf{w} + \\eta \\, (t - y) \\, \\mathbf{x}\n\nIf \\mathbf{w} is initially small, the first couple of errors will bring \\mathbf{w} in the right direction very fast. As its norm does not matter for the classification, only the bias will have to grow incrementally.\nIf \\mathbf{w} is initially big, you will need a lot of iterations to bring it in the right direction, as you only add small errors at each time step. The learning rate will then influence a lot the speed of convergence.\n\n\nNon-linearly separable data\nThe generated dataset was obviously linearly separable, because you found a linear hyperplane able to classify it… Let’s now see what happens when you apply your algorithm on a non-linearly separable dataset. It is basically the same method as before, except that we add one outlier at the end.\n\ndef create_dataset(n_samples):\n    \n    rng = np.random.default_rng()\n    X = rng.uniform(0.0, 1.0, (n_samples-1, 2))\n    t = np.array([1 if X[i, 0] + X[i, 1] > 1. else -1 for i in range(n_samples-1)])\n    \n    # Outlier\n    X = np.append(X, np.array([0.1, 0.1]).reshape((1, 2)), axis=0)\n    t = np.append(t, [1])\n    return X, t\n\nX, t = create_dataset(100)\n\n\nplt.figure(figsize=(10, 6))\nplt.scatter(X[t==1, 0], X[t==1, 1])\nplt.scatter(X[t==-1, 0], X[t==-1, 1])\nplt.show()\n\n\n\n\n\n\n\n\nQ: Apply your online Perceptron algorithm (with default values: eta = 0.1, nb_epochs = 20,\\mathbf{w} and b initialized to 0) to the non-linear data. At the end of learning, compute the final error on the training set. What do you observe? Is it a satisfying result? Does it get better when you change the learning rate or weight initialization?\n\n# Parameters\neta = 0.1\nnb_epochs = 100\nN = len(t)\n\n# Initialize the weight vector and bias\nw = np.zeros(2)\nb = 0.\n\n# Perceptron algorithm\nerrors = []\nlosses = []\nfor epoch in range(nb_epochs):\n    loss = 0\n    error = 0\n    \n    # Iterate over all training examples\n    for i in range(N):\n        # Prediction\n        y_i = np.sign(np.dot(w, X[i, :]) + b)\n        \n        # Update the weight\n        w += eta * (t[i] - y_i) * X[i, :] \n        \n        # Update the bias\n        b += eta * (t[i] - y_i) \n        \n        # Loss mse\n        loss += (t[i] - y_i)**2\n        \n        # Count misclassifications\n        if t[i] != y_i :\n            error += 1\n        \n    # Append \n    losses.append(loss/N)\n    errors.append(error/N)\n\nerror = 0\nfor i in range(N):\n    y_i = np.sign(np.dot(w, X[i, :]) + b)\n    if t[i] != y_i :\n        error += 1\nprint(\"Final error:\", error/N)    \n    \nplt.figure(figsize=(10, 6)) \nplt.scatter(X[t==1, 0], X[t==1, 1])\nplt.scatter(X[t==-1, 0], X[t==-1, 1])\nx_min, x_max = X[:, 0].min(), X[:, 1].max()\nplt.plot([x_min, x_max], [-(x_min*w[0] + b)/w[1], -(x_max*w[0] + b)/w[1]])\n    \nplt.figure(figsize=(10, 6))\nplt.plot(losses, label=\"Loss\")\nplt.plot(errors, label=\"Error\")\nplt.legend()\nplt.show()\n\nFinal error: 0.13\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA: The outlier heavily attracts the hyperplane and changing the hyperparameters does not improve the situation. That’s bad."
  },
  {
    "objectID": "exercises/6-LinearClassification-solution.html#soft-linear-classification-logistic-regression",
    "href": "exercises/6-LinearClassification-solution.html#soft-linear-classification-logistic-regression",
    "title": "Linear classification",
    "section": "Soft linear classification: logistic regression",
    "text": "Soft linear classification: logistic regression\nLet’s now see whether logistic regression helps us with outliers. The following cell implements the logistic function:\n\\sigma(x) = \\dfrac{1}{1 + e^{-x}}\n\ndef logistic(x):\n    return 1. / (1 + np.exp(-x))\n\nIn logistic regression, the prediction y = \\sigma(w \\, x + b) represents the probability of belonging to the positive class. If y>0.5, we can say that the example belongs to the positive class.\nAs seen in the course, there is absolutely no difference in the learning algorithm apart from using the logistic function instead of the sign. One thing to take care of, though, is that the targets t_i should be 0 and 1 in the logistic regression algorithm, while they are -1 and 1 in the current vector \\mathbf{t}. The following cell transforms the array t to match the output of the logistic function.\n\nt[t==-1] = 0\n\nQ: Implement the logistic regression algorithm on the non-linear data. When computing the error, you will need to predict the class based on the probability y. What do you observe? Conclude.\n\n# Parameters\neta = 0.1\nnb_epochs = 100\nN = len(t)\n\n# Initialize the weight vector and bias\nw = np.zeros(2)\nb = 0.\n\n# Perceptron algorithm\nerrors = []\nlosses = []\nfor epoch in range(nb_epochs):\n    loss = 0\n    error = 0\n    \n    # Iterate over all training examples\n    for i in range(N):\n        # Probability\n        y_i = logistic(np.dot(w, X[i, :]) + b)\n        \n        # Predicted class\n        c_i = 1.0 if y_i > 0.5 else 0.0\n        \n        # Update the weight\n        w += eta * (t[i] - y_i) * X[i, :] \n        \n        # Update the bias\n        b += eta * (t[i] - y_i) \n        \n        # Loss mse\n        loss += (t[i] - y_i)**2\n        \n        # Count misclassifications\n        if t[i] != c_i :\n            error += 1\n        \n    # Append \n    losses.append(loss/N)\n    errors.append(error/N)\n\n# Final error\nerror = 0\nfor i in range(N):\n    y_i = logistic(np.dot(w, X[i, :]) + b)\n    c_i = 1.0 if y_i > 0.5 else 0.0\n    if t[i] != c_i :\n        error += 1\nprint(\"Final error:\", error/N)    \n    \nplt.figure(figsize=(10, 6)) \nplt.scatter(X[t==1, 0], X[t==1, 1])\nplt.scatter(X[t==0, 0], X[t==0, 1])\nx_min, x_max = X[:, 0].min(), X[:, 1].max()\nplt.plot([x_min, x_max], [-(x_min*w[0] + b)/w[1], -(x_max*w[0] + b)/w[1]])\n    \nplt.figure(figsize=(10, 6))\nplt.plot(losses, label=\"Loss\")\nplt.plot(errors, label=\"Error\")\nplt.legend()\nplt.show()\n\nFinal error: 0.02\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA: As logistic regression does not use “hard” boundaries like the perceptron, but soft ones, it is much more robust to outliers. As we have in addition the confidence score, the conclusion is that one should never use the perceptron algorithm but logistic regression for binary classification.\n\nx_1, x_2 = np.meshgrid(np.linspace(0, 1, 100), np.linspace(0, 1, 100))\n\ny = logistic(w[0] * x_1 + w[1] * x_2 + b)\n\nfig = plt.figure(figsize=(10, 8))\nax = fig.add_subplot(1, 1, 1)\n\ns = ax.contourf(x_1, x_2, y, 10, cmap='RdGy', alpha=0.2, vmin=0., vmax=1.0)\ncb = fig.colorbar(s)\n\nplt.scatter(X[t==1, 0], X[t==1, 1])\nplt.scatter(X[t==0, 0], X[t==0, 1])\nx_min, x_max = X[:, 0].min(), X[:, 1].max()\nplt.plot([x_min, x_max], [-(x_min*w[0] + b)/w[1], -(x_max*w[0] + b)/w[1]], 'k')\nplt.show()"
  },
  {
    "objectID": "exercises/7-SoftmaxClassifier-solution.html#loading-the-data",
    "href": "exercises/7-SoftmaxClassifier-solution.html#loading-the-data",
    "title": "Softmax classification",
    "section": "Loading the data",
    "text": "Loading the data\nLet’s now import the digits dataset provided by scikit-learn:\nhttps://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html\nIt contains 1797 small (8x8) black and white images of digits between 0 and 9.\nThe two following cells load the data and visualize 16 images chosen randomly.\n\nfrom sklearn.datasets import load_digits\ndigits = load_digits()\n\nN, w, h = digits.images.shape\nd = w * h # number of pixels\nc = len(digits.target_names) # number of classes\n\n\nrng = np.random.default_rng()\nindices = rng.choice(N, 16)\nplt.figure(figsize=(16, 16))\nfor i in range(16):\n    plt.subplot(4, 4, i+1)\n    plt.imshow(digits.images[indices[i], :], cmap=\"gray\")\n    plt.title(\"Label: \"+ str(digits.target[indices[i]]))\nplt.show()\n\n\n\n\n\n\n\n\nDigits are indeed to be recognized, the hope being that they are linearly separable and we can apply a softmax classifier directly on the pixels.\nThe only problem is that each image is a 8x8 matrix, while we want vectors for our model. Fortunately, that is very easy with reshape:\n\nX = digits.images.reshape((N, d))\nprint(X.shape)\n\n(1797, 64)\n\n\nLet’s know have a look at the targets, i.e. the ground truth / labels of each digit:\n\nlabels = digits.target\nprint(labels)\nprint(labels.shape)\n\n[0 1 2 ... 8 9 8]\n(1797,)\n\n\nEach label is an integer between 0 and 9, while our softmax classifier expects a one-hot-encoded vector of 10 classes, with only one non-zero element, for example for digit 3:\n[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\nTo do the conversion, we can once again use a built-in method of scikit-learn:\n\nfrom sklearn.preprocessing import OneHotEncoder\n\nt = OneHotEncoder().fit_transform(labels.reshape(-1, 1)).toarray()\n\nprint(t)\nprint(t.shape)\n\n[[1. 0. 0. ... 0. 0. 0.]\n [0. 1. 0. ... 0. 0. 0.]\n [0. 0. 1. ... 0. 0. 0.]\n ...\n [0. 0. 0. ... 0. 1. 0.]\n [0. 0. 0. ... 0. 0. 1.]\n [0. 0. 0. ... 0. 1. 0.]]\n(1797, 10)\n\n\nQ: Split the data into a training set X_train, t_train and a test set X_test, t_test using scikit-learn (e.g. with a ratio 70/30).\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, t_train, t_test = train_test_split(X, t, test_size=0.3)\n\nN_train = X_train.shape[0]\nN_test = X_test.shape[0]\n\nprint(N_train, \"training samples,\", N_test, \"test samples.\")\n\n1257 training samples, 540 test samples."
  },
  {
    "objectID": "exercises/7-SoftmaxClassifier-solution.html#softmax-linear-classifier",
    "href": "exercises/7-SoftmaxClassifier-solution.html#softmax-linear-classifier",
    "title": "Softmax classification",
    "section": "Softmax linear classifier",
    "text": "Softmax linear classifier\nLet’s remember the structure of the softmax linear classifier: the input vector \\mathbf{x} is transformed into a logit score vector \\mathbf{z} using a weight matrix W and a bias vector \\mathbf{b}:\n\n    \\mathbf{z} = W \\times \\mathbf{x} + \\mathbf{b}\n\nThis logit score has one element per class, so the weight matrix must have a size (c, d), where c is the number of classes (10) and d is the number of dimensions of the input space (64). The bias vector has 10 elements (one per class).\nThe logit score is turned into probabilities using the softmax operator:\n\n    y_j = P(\\text{class = j}) = \\frac{\\exp(z_j)}{\\sum_k \\exp(z_k)}\n\nThe following Python function allows to turn any vector \\mathbf{z} (numpy array) into softmax probabilities:\n\ndef softmax(z):\n    e = np.exp(z - z.max())\n    return e/np.sum(e)\n\nQ: Experiment with the softmax() to understand its function. Pass it different numpy arrays (e.g. [-1, 0, 2]) and print or plot the corresponding probabilities.\n\nz = np.array([-1, 0, 2])\ns = softmax(z)\nprint(s)\n\n[0.04201007 0.1141952  0.84379473]\n\n\nThe loss function to use is the cross-entropy or negative log-likelihood, defined for a single example as:\n\n    \\mathcal{l}(W, \\mathbf{b}) =   - \\mathbf{t} \\cdot \\log \\mathbf{y} = - \\log y_j\n\nwhere \\mathbf{t} is a one-hot encoding of the class of the example and j is the index of the corresponding class.\nAfter doing the derivations, we obtain the following learning rules for W and \\mathbf{b} to minimize the loss function:\n\n    \\Delta W = \\eta \\, (\\mathbf{t} - \\mathbf{y}) \\, \\mathbf{x}^T\n\n\n    \\Delta \\mathbf{b} = \\eta \\, (\\mathbf{t} - \\mathbf{y})\n\nNote that because W is a (c, d) matrix, \\Delta W too. (\\mathbf{t} - \\mathbf{y}) \\, \\mathbf{x}^T is therefore the outer product between the error vector \\mathbf{t} - \\mathbf{y} (c elements) and the input vector \\mathbf{x} (d elements)."
  },
  {
    "objectID": "exercises/7-SoftmaxClassifier-solution.html#implementation",
    "href": "exercises/7-SoftmaxClassifier-solution.html#implementation",
    "title": "Softmax classification",
    "section": "Implementation",
    "text": "Implementation\nYou will now modify your implementation of the online Perceptron algorithm from last week.\nSome things to keep in mind:\n\nW must now be defined as a (c, d) matrix (numpy array) and b as a vector with c elements. Both can be initialized to 0.\nWhen computing the logit score \\mathbf{z} = W \\times \\mathbf{x} + \\mathbf{b}, remember that W is now a matrix, so its position will matter in the dot product np.dot.\nUse the softmax() function defined above on the whole vector instead of np.sign() or logistic to get the prediction \\mathbf{y}.\nFor \\Delta W, you will need the outer product between the vectors \\mathbf{t} - \\mathbf{y}_\\text{train} and \\mathbf{x}_\\text{train}. Check the doc for np.outer().\nThe one-hot encoding of the class of the example i is now a vector with 10 elements t_train[i, :]. You can get the index of the corresponding class by looking at the position of its maximum with t_train[i, :].argmax().\nSimilarly, the predicted class by the model can be identified by the class with the maximum probability: y.argmax().\nDo not forget to record and plot the evolution of the training error and loss. Compute the test error and loss at the end of each epoch, and plot them together with the training error/loss.\nPick the right learning rate and number of epochs.\n\nQ: Let’s go.\n\n# Parameters\neta = 0.001\nnb_epochs = 100\n\n# Initialize the weight matrix and bias vector\nW = np.zeros((c, d))\nb = np.zeros(c)\n\n# Perceptron algorithm\ntraining_errors = []\ntraining_losses = []\ntest_errors = []\ntest_losses = []\n\nfor epoch in range(nb_epochs):\n    error = 0\n    loss = 0.\n    \n    # Iterate over all training examples\n    for i in range(N_train):\n        \n        # Prediction\n        z = np.dot(W, X_train[i, :]) + b\n        \n        # Probability\n        y = softmax(z)\n        \n        # Update the weight\n        W += eta * np.outer((t_train[i, :] - y), X_train[i, :])\n        \n        # Update the bias\n        b += eta * (t_train[i, :] - y) \n        \n        # Increment the error if the maximum probability is different from the class\n        if y.argmax() != t_train[i, :].argmax():\n            error += 1\n        \n        # Accumulate the loss\n        loss -= np.log(y[t_train[i, :].argmax()])\n        \n    training_errors.append(error/N_train)\n    training_losses.append(loss/N_train)\n\n    # Test error\n    error = 0\n    loss = 0.\n\n    # Iterate over all test examples\n    for i in range(N_test):\n\n        # Prediction\n        z = np.dot(W, X_test[i, :]) + b\n\n        # Probability\n        y = softmax(z)\n\n        # Increment the error if the maximum probability is different from the class\n        if y.argmax() != t_test[i, :].argmax():\n            error += 1\n\n        # Accumulate the loss\n        loss -= np.log(y[t_test[i, :].argmax()])\n\n    test_errors.append(error/N_test)\n    test_losses.append(loss/N_test)\n\nprint(\"Final training error:\", training_errors[-1])\nprint(\"Final training loss:\", training_losses[-1])\n\nprint(\"Final test error:\", test_errors[-1])\nprint(\"Final test loss:\", test_errors[-1])\n\nplt.figure(figsize=(15, 6))\nplt.subplot(121)\nplt.plot(training_errors, label=\"training\")\nplt.plot(test_errors, label=\"validation\")\nplt.legend()\nplt.title(\"Error\")\nplt.subplot(122)\nplt.plot(training_losses, label=\"training\")\nplt.plot(test_losses, label=\"validation\")\nplt.legend()\nplt.title(\"Cross-entropy loss\")\nplt.show()\n\nFinal training error: 0.0\nFinal training loss: 0.005522075921611576\nFinal test error: 0.04259259259259259\nFinal test loss: 0.04259259259259259\n\n\n\n\n\n\n\n\n\nQ: What is the final training error and loss of the model? After how many epochs do you get a perfect classification? Why do they evolve like this?\nHint: you may need to avoid plotting the error/loss during the first 20 epochs or so to observe the effect.\nA: The training error reaches quickly 0, but the loss still decreases for a number of epochs. This is because the probability vector y is a correct prediction for the class (the maximum probability corresponds to the correct class) but not with a probability of 1 yet. The loss continues to evolve even when there is no error, as it wants to bring the probability vector as close as possible to a binary vector.\nThis is the main difference with the mean square error (mse) loss function: as soon as there are no errors, the mse loss becomes 0 and learning stops. In classification problems, one should therefore track the loss function, not the training error.\nQ: Compare the evolution of the training and test errors during training. What happens?\nThe test error is higher than the final training error, as the examples were not used for training. This is a classical sign of overfitting, although the model is linear. Regularization may help.\nIf you let the network learn for more epochs, the validation error may even start to go up: you need early stopping.\nQ: The following cell samples 12 misclassified images from the test and shows the predicted class together with the ground truth. What do you think?\n\nmisclassified = []\n\nfor i in range(N_test):\n    pred = softmax(np.dot(W, X_test[i, :]) + b).argmax()\n    if pred != t_test[i, :].argmax():\n        misclassified.append([X_test[i, :].reshape((8, 8)), t_test[i, :].argmax(), pred])\n        if len(misclassified) > 12: break\n        \n        \nplt.figure(figsize=(16, 12))\nfor i in range(12):\n    if i < len(misclassified):\n        X, t, pred = misclassified[i]\n        plt.subplot(3, 4, i+1)\n        plt.imshow(X, cmap=\"gray\")\n        plt.title(\"Label \" + str(t) + \" ; Prediction \" + str(pred))\n    \nplt.show()\n\n\n\n\n\n\n\n\nA: For some misclassified images, the mistakes are quite understandable, so the classifier did a quite good job. Real-world data are never clean, there always are some bad annotations. It is therefore important to use methods that are robust to outliers, such as soft classifiers."
  },
  {
    "objectID": "exercises/8-MLP-solution.html#structure-of-the-mlp",
    "href": "exercises/8-MLP-solution.html#structure-of-the-mlp",
    "title": "Multi-layer Perceptron",
    "section": "Structure of the MLP",
    "text": "Structure of the MLP\nIn this exercise, we will consider a MLP for non-linear binary classification composed of 2 input neurons \\mathbf{x}, one output neuron y and K hidden neurons in a single hidden layer (\\mathbf{h}).\n\nThe output neuron is a vector \\mathbf{y} with one element that sums its inputs with K weights W^2 and a bias \\mathbf{b}^2.\n\\mathbf{y} = \\sigma( W^2 \\times \\mathbf{h} + \\mathbf{b}^2)\nIt uses the logistic transfer function:\n\\sigma(x) = \\dfrac{1}{1 + \\exp -x}\nAs in logistic regression for linear classification, we will interpret y as the probability that the input \\mathbf{x} belongs to the positive class.\nW^2 is a 1 \\times K matrix (we could interpret it as a vector, but this will make the computations easier) and \\mathbf{b}^2 is a vector with one element.\nEach of the K hidden neurons receives 2 weights from the input layer, what gives a K \\times 2 weight matrix W^1, and K biases in the vector \\mathbf{b}^1. They will also use the logistic activation function at first:\n\\mathbf{h} = \\sigma(W^1 \\times \\mathbf{x} + \\mathbf{b}^1)\nThe goal is to implement the backpropagation algorithm by comparing the desired output t with the prediction y:\n\nThe output error is a vector with one element:\n\n\\delta =  (\\mathbf{t} - \\mathbf{y})\n\nThe backpropagated error is a vector with K elements:\n\n\\delta_\\text{hidden} = \\sigma'(W^1 \\times \\mathbf{x} + \\mathbf{b}^1) \\, W_2^T \\times \\delta\n(W^2 is a 1 \\times K matrix, so W_2^T \\times \\delta is a K \\times 1 vector. The vector \\sigma'(W^1 \\times \\mathbf{x} + \\mathbf{b}^1) is multiplied element-wise.)\n\nParameter updates follow the delta learning rule:\n\n\\Delta W^1 = \\eta \\,  \\delta_\\text{hidden} \\times \\mathbf{x}^T\n\\Delta \\mathbf{b}^1 = \\eta \\, \\delta_\\text{hidden} \n\\Delta W^2 = \\eta \\, \\delta \\, \\mathbf{h}^T\n\\Delta \\mathbf{b}^2 = \\eta \\, \\delta\nNotice the transpose operators to obtain the correct shapes. You will remember that the derivative of the logistic function is given by:\n\\sigma'(x)= \\sigma(x) \\, (1- \\sigma(x))\nQ: Why do not we use the derivative of the transfer function of the output neuron when computing the output error \\delta?\nA: As in logistic regression, we will interpret the output of the network as the probability of belonging to the positive class. We therefore use (implicitly) the cross-entropy / negative log-likelihood loss function, whose gradient does not include the derivative of the logistic function."
  },
  {
    "objectID": "exercises/8-MLP-solution.html#data",
    "href": "exercises/8-MLP-solution.html#data",
    "title": "Multi-layer Perceptron",
    "section": "Data",
    "text": "Data\nThe MLP will be trained on a non-linear dataset with samples of each class forming a circle. Each sample has two input dimensions. In the cell below, blue points represent the positive class (t=1), orange ones the negative class (t=0).\n\nfrom sklearn.datasets import make_circles\n\nN = 100\nd = 2\nX, t = make_circles(n_samples=N, noise = 0.03, random_state=42)\n\nplt.figure(figsize=(10, 6))\nplt.scatter(X[t==1, 0], X[t==1, 1])\nplt.scatter(X[t==0, 0], X[t==0, 1])\nplt.show()\n\n\n\n\n\n\n\n\nQ: Split the data into a training and test set (80/20). Make sure to call them X_train, X_test, t_train, t_test.\n\nX_train, X_test, t_train, t_test = train_test_split(X, t, test_size=.2, random_state=42)"
  },
  {
    "objectID": "exercises/8-MLP-solution.html#class-definition",
    "href": "exercises/8-MLP-solution.html#class-definition",
    "title": "Multi-layer Perceptron",
    "section": "Class definition",
    "text": "Class definition\nThe neural network is entirely defined by its parameters, i.e. the weight matrices and bias vectors, as well the transfer function of the hidden neurons. In order to make your code more reusable, the MLP will be implemented as a Python class. The following cell defines the class, but we will explain it step by step afterwards.\n\nclass MLP:\n    \n    def __init__(self, d, K, activation_function, max_val, eta):\n        \n        self.d = d\n        self.K = K\n        self.activation_function = activation_function\n        self.eta = eta\n        \n        self.W1  = rng.uniform(-max_val, max_val, (K, d)) \n        self.b1  = rng.uniform(-max_val, max_val, (K, 1))\n    \n        self.W2 = rng.uniform(-max_val, max_val, (1, K)) \n        self.b2 = rng.uniform(-max_val, max_val, (1, 1))\n        \n    def feedforward(self, x):\n    \n        # Make sure x has 2 rows\n        x = np.array(x).reshape((self.d, -1))\n\n        # Hidden layer\n        self.h = self.activation_function(np.dot(self.W1, x) + self.b1)\n\n        # Output layer\n        self.y = logistic(np.dot(self.W2, self.h) + self.b2) \n        \n    \n    def train(self, X_train, t_train, nb_epochs, visualize=True):\n        errors = []\n        \n        for epoch in range(nb_epochs):\n            \n            nb_errors = 0\n\n            # Epoch\n            for i in range(X_train.shape[0]):\n\n                # Feedforward pass: sets self.h and self.y\n                self.feedforward(X_train[i, :])\n        \n                # Backpropagation\n                self.backprop(X_train[i, :], t_train[i])\n        \n                # Predict the class:         \n                if self.y[0, 0] > 0.5:\n                    c = 1\n                else:\n                    c = 0\n\n                # Count the number of misclassifications\n                if t_train[i] != c: \n                    nb_errors += 1\n            \n            # Compute the error rate\n            errors.append(nb_errors/X_train.shape[0])\n                \n            # Plot the decision function every 10 epochs\n            if epoch % 10 == 0 and visualize:\n                self.plot_classification() \n\n            # Stop when the error rate is 0\n            if nb_errors == 0:\n                if visualize:\n                    self.plot_classification() \n                break\n                \n        return errors, epoch+1\n\n    def backprop(self, x, t):\n    \n        # Make sure x has 2 rows\n        x = np.array(x).reshape((self.d, -1))\n\n        # TODO: implement backpropagation\n    \n    def test(self, X_test, t_test):\n    \n        nb_errors = 0\n        for i in range(X_test.shape[0]):\n\n            # Feedforward pass\n            self.feedforward(X_test[i, :]) \n\n            # Predict the class:         \n            if self.y[0, 0] > 0.5:\n                c = 1\n            else:\n                c = 0\n\n            # Count the number of misclassifications\n            if t_test[i] != c: \n                nb_errors += 1\n\n        return nb_errors/X_test.shape[0]\n        \n    def plot_classification(self):\n\n        # Allow redrawing \n        clear_output(wait=True)\n\n        x_min, x_max = X_train[:, 0].min(), X_train[:, 0].max()\n        y_min, y_max = X_train[:, 1].min(), X_train[:, 1].max()\n        xx, yy = np.meshgrid(np.arange(x_min, x_max, .02), np.arange(y_min, y_max, .02))\n\n        x1 = xx.ravel()\n        x2 = yy.ravel()    \n        x = np.array([[x1[i], x2[i]] for i in range(x1.shape[0])])\n\n        self.feedforward(x.T)\n        Z = self.y.copy()\n        Z[Z>0.5] = 1\n        Z[Z<=0.5] = 0\n\n        from matplotlib.colors import ListedColormap\n        cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n\n        fig = plt.figure(figsize=(10, 6))\n        plt.contourf(xx, yy, Z.reshape(xx.shape), cmap=cm_bright, alpha=.4)\n        plt.scatter(X_train[:, 0], X_train[:, 1], c=t_train, cmap=cm_bright, edgecolors='k')\n        plt.scatter(X_test[:, 0], X_test[:, 1], c=t_test, cmap=cm_bright, alpha=0.4, edgecolors='k')\n        plt.xlim(xx.min(), xx.max())\n        plt.ylim(yy.min(), yy.max())\n        plt.show()    \n\nThe constructor __init__ of the class accepts several arguments:\n\nd is the number inputs, here 2.\nK is the number of hidden neurons.\nactivation_function is the function to use for the hidden neurons, for example the logistic function defined at the beginning of the notebook. Note that the name of the method can be stored as a variable.\nmax_val is the maximum value used to initialize the weight matrices.\neta is the learning rate.\n\nThe constructor starts by saving these arguments as attributes, so that they can be used in other method as self.K:\ndef __init__(self, d, K, activation_function, max_val, eta):\n\n    self.d = d\n    self.K = K\n    self.activation_function = activation_function\n    self.eta = eta\nThe constructor then initializes randomly the weight matrices and bias vectors, uniformly between -max_val and max_val.\nself.W1  = rng.uniform(-max_val, max_val, (K, d)) \nself.b1  = rng.uniform(-max_val, max_val, (K, 1))\n\nself.W2 = rng.uniform(-max_val, max_val, (1, K)) \nself.b2 = rng.uniform(-max_val, max_val, (1, 1))\nYou can then already create the MLP object and observe how the parameters are initialized:\nmlp = MLP(d=2, K=15, activation_function=logistic, max_val=1.0, eta=0.05)\nQ: Create the object and print the weight matrices and bias vectors.\n\n# Parameters\nK = 15\nmax_val = 1.0\neta = 0.05\n\n# Create the MLP\nmlp = MLP(d, K, logistic, max_val, eta)\n\nprint(mlp.W1)\nprint(mlp.b1)\nprint(mlp.W2)\nprint(mlp.b2)\n\n[[-0.52897144  0.21044788]\n [-0.10686506  0.54739532]\n [-0.6209987   0.48604941]\n [ 0.03467845 -0.97584376]\n [-0.70737448 -0.50240962]\n [-0.12108698  0.75048116]\n [-0.06807349 -0.63271267]\n [ 0.760424   -0.9367907 ]\n [ 0.695763   -0.14137454]\n [-0.91937516 -0.14493619]\n [ 0.80299858  0.59695668]\n [-0.87467725 -0.60570258]\n [ 0.04708669  0.29757085]\n [ 0.08505661 -0.14859975]\n [-0.87574304  0.5493211 ]]\n[[-0.401296  ]\n [-0.23382752]\n [-0.04195576]\n [ 0.14020117]\n [-0.55679288]\n [-0.73736505]\n [-0.98903676]\n [ 0.76104882]\n [ 0.54034658]\n [-0.48841228]\n [ 0.79080937]\n [ 0.1840221 ]\n [-0.79145236]\n [ 0.87786545]\n [-0.70295808]]\n[[-0.56649207 -0.21574322  0.17677365 -0.98804858 -0.37548242  0.77594839\n  -0.54093023  0.03327597  0.25656232 -0.78975971 -0.25568574  0.68177235\n   0.71376027 -0.04533073 -0.4154252 ]]\n[[0.31429382]]\n\n\nThe feedforward method takes a vector x as input, reshapes it to make sure it has two rows, and computes the hidden activation \\mathbf{h} and the prediction \\mathbf{y}.\ndef feedforward(self, x):\n\n    # Make sure x has 2 rows\n    x = np.array(x).reshape((self.d, -1))\n\n    # Hidden layer\n    self.h = self.activation_function(np.dot(self.W1, x) + self.b1)\n\n    # Output layer\n    self.y = logistic(np.dot(self.W2, self.h) + self.b2) \nNotice the use of self. to access attributes, as well as the use of np.dot() to mulitply vectors and matrices.\nQ: Using the randomly initialized weights, apply the feedforward() method to an input vector (for example [0.5, 0.5]) and print h and y. What is the predicted class of the example?\n\nx = np.array([0.5, 0.5])\n\nmlp.feedforward(x)\n\nprint(mlp.h)\nprint(mlp.y)\n\nif mlp.y[0, 0] > 0.5:\n    print(\"positive class\")\nelse:\n    print(\"negative class\")\n\n[[0.36341841]\n [0.49660946]\n [0.47266967]\n [0.41814781]\n [0.23836126]\n [0.39587851]\n [0.20760384]\n [0.66214449]\n [0.69371407]\n [0.26491678]\n [0.81619637]\n [0.36443462]\n [0.3499808 ]\n [0.6997471 ]\n [0.29605207]]\n[[0.47570096]]\nnegative class\n\n\nThe class also provides a visualization method. It is not import to understand the code for the exercise, so you can safely skip it. It displays the training data as plain points, the test data as semi-transparent points and displays the decision function as a background color (all points in the blue region will be classified as negative examples).\nQ: Plot the initial classification on the dataset with random weights. Is there a need for learning? Reinitialize the weights and biases multiple times. What do you observe?\n\nmlp = MLP(d, K, logistic, max_val, eta)\nmlp.plot_classification() \n\n\n\n\n\n\n\n\nA: The output neurons answers the same class for the whole input space, or is linear at best depending on initialization. Let’s learn!"
  },
  {
    "objectID": "exercises/8-MLP-solution.html#backpropagation",
    "href": "exercises/8-MLP-solution.html#backpropagation",
    "title": "Multi-layer Perceptron",
    "section": "Backpropagation",
    "text": "Backpropagation\nThe train() method implements the training loop you have already implemented several times: several epochs over the training set, making a prediction for each input and modifying the parameters according to the prediction error:\ndef train(self, X_train, t_train, nb_epochs, visualize=True):\n    errors = []\n\n    for epoch in range(nb_epochs):\n\n        nb_errors = 0\n\n        # Epoch\n        for i in range(X_train.shape[0]):\n\n            # Feedforward pass: sets self.h and self.y\n            self.feedforward(X_train[i, :])\n\n            # Backpropagation\n            self.backprop(X_train[i, :], t_train[i])\n\n            # Predict the class:         \n            if self.y[0, 0] > 0.5:\n                c = 1\n            else:\n                c = 0\n\n            # Count the number of misclassifications\n            if t_train[i] != c: \n                nb_errors += 1\n\n        # Compute the error rate\n        errors.append(nb_errors/X_train.shape[0])\n\n        # Plot the decision function every 10 epochs\n        if epoch % 10 == 0 and visualize:\n            self.plot_classification() \n\n        # Stop when the error rate is 0\n        if nb_errors == 0:\n            if visualize:\n                self.plot_classification() \n            break\n\n    return errors, epoch+1\nThe training methods stops after nb_epochs epochs or when no error is made during the last epoch. The decision function is visualized every 10 epochs to better understand what is happening. The method returns a list containing the error rate after each epoch, as well as the number of epochs needed to reach an error rate of 0.\nThe only thing missing is the backprop(x, t) method, which currently does nothing:\ndef backprop(self, x, t):\n\n    # Make sure x has 2 rows\n    x = np.array(x).reshape((self.d, -1))\n\n    # TODO: implement backpropagation\nQ: Implement the online backpropagation algorithm.\nAll you have to do is to backpropagate the output error and adapt the parameters using the delta learning rule:\n\ncompute the output error delta.\ncompute the backpropagated error delta_hidden.\nincrement the parameters self.W1, self.b1, self.W2, self.b2 accordingly.\n\nThe only difficulty is to take care of the shape of each matrix (before multiplying two matrices or vectors, test what their shape is).\nNote: you can either edit directly the cell containing the definition of the class, or create a new class TrainableMLP inheriting from the class MLP and simply redefine the backprop() method. The solution will use the second option to be more readable, but it does not matter.\n\nclass TrainableMLP (MLP):\n\n    def backprop(self, x, t):\n    \n        # Make sure x has 2 rows\n        x = np.array(x).reshape((self.d, -1))\n\n        # Output error\n        delta = (t - self.y) \n\n        # Hidden error\n        delta_hidden =  np.dot(self.W2.T, delta) * self.h * (1. - self.h)\n        \n        # Learn the output weights\n        self.W2 += self.eta * delta * self.h.T\n\n        # Learn the output bias\n        self.b2 += self.eta * delta\n\n        # Learn the hidden weights\n        self.W1 += self.eta * np.outer(delta_hidden, x)\n\n        # Learn the hidden biases\n        self.b1 += self.eta * delta_hidden\n        \n\nQ: Train the MLP for 1000 epochs on the data using a learning rate of 0.05, 15 hidden neurons and weights initialized between -1 and 1. Plot the evolution of the training error.\n\nK = 15\nmax_val = 1.0\neta = 0.05\n\n# Create the MLP\nmlp = TrainableMLP(d, K, logistic, max_val, eta)\n\n# Train the MLP\ntraining_error, nb_epochs = mlp.train(X_train, t_train, 1000)\n\nprint('Number of epochs needed:', nb_epochs)\nprint('Training accuracy:', 1. - training_error[-1])\n\nplt.figure(figsize=(10, 6))\nplt.plot(training_error)\nplt.xlabel(\"Number of epochs\")\nplt.ylabel(\"Training error\")\nplt.show()\n\n\n\n\n\n\n\n\nNumber of epochs needed: 669\nTraining accuracy: 1.0\n\n\n\n\n\n\n\n\n\nQ: Use the test() method to compute the error on the test set. What is the test accuracy of your network after training? Compare it to the training accuracy.\n\ntest_error = mlp.test(X_test, t_test)\n\nprint('Test accuracy:', 1. - test_error)\n\nTest accuracy: 0.95"
  },
  {
    "objectID": "exercises/8-MLP-solution.html#experiments",
    "href": "exercises/8-MLP-solution.html#experiments",
    "title": "Multi-layer Perceptron",
    "section": "Experiments",
    "text": "Experiments\n\nInfluence of the number of hidden neurons\nQ: Try different values for the number of hidden neurons K (e.g. 2, 5, 10, 15, 20, 25, 50…) and observe how the accuracy and speed of convergence evolve.\n\nK = 3\nmax_val = 1.0\neta = 0.05\n\n# Create the MLP\nmlp = TrainableMLP(d, K, logistic, max_val, eta)\n\n# Train the MLP\ntraining_error, nb_epochs = mlp.train(X_train, t_train, 2000)\n\n# Test the MLP\ntest_error = mlp.test(X_test, t_test)\n\nprint('Number of epochs needed:', nb_epochs)\nprint('Training accuracy:', 1. - training_error[-1])\nprint('Test accuracy:', 1. - test_error)\n\nplt.figure(figsize=(10, 6))\nplt.plot(training_error)\nplt.xlabel(\"Number of epochs\")\nplt.ylabel(\"Training error\")\nplt.show()\n\n\n\n\n\n\n\n\nNumber of epochs needed: 2000\nTraining accuracy: 0.9875\nTest accuracy: 0.95\n\n\n\n\n\n\n\n\n\nA: Surprisingly, 3 hidden neurons are enough for the non-linear dataset, although you might need more epochs… This problem is really easy. The more hidden neurons, the faster it converges (in terms of epochs, not computation time…), as the model is more flexible.\n\n\nInfluence of the learning rate\nQ: Vary the learning rate between extreme values. How does the performance evolve?\n\nK = 15\nmax_val = 1.0\neta = 0.2\n\n# Create the MLP\nmlp = TrainableMLP(d, K, logistic, max_val, eta)\n\n# Train the MLP\ntraining_error, nb_epochs = mlp.train(X_train, t_train, 1000)\n\n# Test the MLP\ntest_error = mlp.test(X_test, t_test)\n\nprint('Number of epochs needed:', nb_epochs)\nprint('Training accuracy:', 1. - training_error[-1])\nprint('Test accuracy:', 1. - test_error)\n\nplt.figure(figsize=(10, 6))\nplt.plot(training_error)\nplt.xlabel(\"Number of epochs\")\nplt.ylabel(\"Training error\")\nplt.show()\n\n\n\n\n\n\n\n\nNumber of epochs needed: 300\nTraining accuracy: 1.0\nTest accuracy: 1.0\n\n\n\n\n\n\n\n\n\nA: For such small networks and easy problems, a high learning rate of 0.2 works well (ca 200 epochs to converge). This won’t be true for deeper networks. Learning rates smaller than 0.01 take forever.\n\n\nInfluence of weight initialization\nQ: The weights are initialized randomly between -1 and 1. Try to initialize them to 0. Does it work? Why?\n\nK = 15\nmax_val = 0.0\neta = 0.1\n\n# Create the MLP\nmlp = TrainableMLP(d, K, logistic, max_val, eta)\n\n# Train the MLP\ntraining_error, nb_epochs = mlp.train(X_train, t_train, 1000)\n\n# Test the MLP\ntest_error = mlp.test(X_test, t_test)\n\nprint('Number of epochs needed:', nb_epochs)\nprint('Training accuracy:', 1. - training_error[-1])\nprint('Test accuracy:', 1. - test_error)\n\nplt.figure(figsize=(10, 6))\nplt.plot(training_error)\nplt.xlabel(\"Number of epochs\")\nplt.ylabel(\"Training error\")\nplt.show()\n\n\n\n\n\n\n\n\nNumber of epochs needed: 1000\nTraining accuracy: 0.525\nTest accuracy: 0.30000000000000004\n\n\n\n\n\n\n\n\n\nA: This is a simple example of vanishing gradient. The backpropagated error gets multiplied by W2, which is initially zero. There is therefore no backpropagated error, the first layer cannot learn anything for a quite long time.\nQ: For a fixed number of hidden neurons (e.g. K=15) and a correct value of eta, train 10 times the network with different initial weights and superimpose on the same plot the evolution of the training error. Conclude.\n\nK = 15\nmax_val = 1.0\neta = 0.1\n\nplt.figure(figsize=(10, 6))\n\nfor trial in range(10):\n    # Create the MLP\n    mlp = TrainableMLP(d, K, logistic, max_val, eta)\n\n    # Train the MLP\n    training_error, nb_epochs = mlp.train(X_train, t_train, 1000, visualize=False)\n\n    plt.plot(training_error)\n\nplt.xlabel(\"Number of epochs\")\nplt.ylabel(\"Training error\")\nplt.show()\n\n\n\n\n\n\n\n\nA: Because of the random initialization of the weights, some networks converge faster than others, as their weights are initially closer to the solution and gradient descent is an iterative method.\nWith the current configuration, a maximum of 1000 epochs is not too much, but we can clearly do better.\n\n\nInfluence of the transfer function\nQ: Modify the backprop() method so that it applies backpropagation correctly for any of the four activation functions:\n\nlinear\nlogistic\ntanh\nrelu\n\n\n# Linear transfer function\ndef linear(x):\n    return x\n\n# tanh transfer function \ndef tanh(x):\n    return np.tanh(x)\n\n# ReLU transfer function\ndef relu(x):\n    x = x.copy()\n    x[x < 0.] = 0.\n    return x\n\nRemember that the derivatives of these activations functions are easy to compute:\n\nlinear: f'(x) = 1\nlogistic: f'(x) = f(x) \\, (1 - f(x))\ntanh: f'(x) = 1 - f^2(x)\nrelu: f'(x) = \\begin{cases}1 \\; \\text{if} \\; x>0\\\\ 0 \\; \\text{if} \\; x \\leq 0\\\\ \\end{cases}\n\nHint: activation_function is a variable like others, although it is the name of a method. You can apply comparisons on it:\nif self.activation_function == linear:\n    diff = something\nelif self.activation_function == logistic:\n    diff = something\nelif self.activation_function == tanh:\n    diff = something\nelif self.activation_function == relu:\n    diff = something\n\nclass TrainableMLP (MLP):\n\n    def backprop(self, x, t):\n    \n        # Make sure x has 2 rows\n        x = np.array(x).reshape((self.d, -1))\n\n        # Output error\n        delta = (t - self.y) \n        \n        # Derivative of the transfer function\n        if self.activation_function == linear:\n            diff = 1.0\n        elif self.activation_function == logistic:\n            diff = self.h * (1. - self.h)\n        elif self.activation_function == tanh:\n            diff = 1. - self.h * self.h\n        elif self.activation_function == relu:\n            diff = self.h.copy()\n            diff[diff <= 0] = 0\n            diff[diff > 0] = 1\n    \n        # Hidden error\n        delta_hidden =  np.dot(self.W2.T, delta) * diff\n        \n        # Learn the output weights\n        self.W2 += self.eta * delta * self.h.T\n\n        # Learn the output bias\n        self.b2 += self.eta * delta\n\n        # Learn the hidden weights\n        self.W1 += self.eta * np.outer(delta_hidden, x)\n\n        # Learn the hidden biases\n        self.b1 += self.eta * delta_hidden\n        \n\nQ: Use a linear transfer function for the hidden neurons. How does performance evolve? Is the non-linearity of the transfer function important for learning?\n\nK = 15\nmax_val = 1.0\neta = 0.1\nactivation_function=linear\n\n# Create the MLP\nmlp = TrainableMLP(d, K, activation_function, max_val, eta)\n\n# Train the MLP\ntraining_error, nb_epochs = mlp.train(X_train, t_train, 1000)\n\n# Test the MLP\ntest_error = mlp.test(X_test, t_test)\n\nprint('Number of epochs needed:', nb_epochs)\nprint('Training accuracy:', 1. - training_error[-1])\nprint('Test accuracy:', 1. - test_error)\n\nplt.figure(figsize=(10, 6))\nplt.plot(training_error)\nplt.xlabel(\"Number of epochs\")\nplt.ylabel(\"Training error\")\nplt.show()\n\n\n\n\n\n\n\n\nNumber of epochs needed: 1000\nTraining accuracy: 0.5125\nTest accuracy: 0.30000000000000004\n\n\n\n\n\n\n\n\n\nQ: Use this time the hyperbolic tangent function as a transfer function for the hidden neurons. Does it improve learning?\n\nK = 15\nmax_val = 1.0\neta = 0.1\nactivation_function=tanh\n\n# Create the MLP\nmlp = TrainableMLP(d, K, activation_function, max_val, eta)\n\n# Train the MLP\ntraining_error, nb_epochs = mlp.train(X_train, t_train, 1000)\n\n# Test the MLP\ntest_error = mlp.test(X_test, t_test)\n\nprint('Number of epochs needed:', nb_epochs)\nprint('Training accuracy:', 1. - training_error[-1])\nprint('Test accuracy:', 1. - test_error)\n\nplt.figure(figsize=(10, 6))\nplt.plot(training_error)\nplt.xlabel(\"Number of epochs\")\nplt.ylabel(\"Training error\")\nplt.show()\n\n\n\n\n\n\n\n\nNumber of epochs needed: 290\nTraining accuracy: 1.0\nTest accuracy: 0.95\n\n\n\n\n\n\n\n\n\nQ: Use the Rectified Linear Unit (ReLU) transfer function. What does it change? Conclude on the importance of the transfer function for the hidden neurons. Select the best one from now on.\n\nK = 15\nmax_val = 1.0\neta = 0.1\nactivation_function=relu\n\n# Create the MLP\nmlp = TrainableMLP(d, K, activation_function, max_val, eta)\n\n# Train the MLP\ntraining_error, nb_epochs = mlp.train(X_train, t_train, 1000)\n\n# Test the MLP\ntest_error = mlp.test(X_test, t_test)\n\nprint('Number of epochs needed:', nb_epochs)\nprint('Training accuracy:', 1. - training_error[-1])\nprint('Test accuracy:', 1. - test_error)\n\nplt.figure(figsize=(10, 6))\nplt.plot(training_error)\nplt.xlabel(\"Number of epochs\")\nplt.ylabel(\"Training error\")\nplt.show()\n\n\n\n\n\n\n\n\nNumber of epochs needed: 159\nTraining accuracy: 1.0\nTest accuracy: 0.95\n\n\n\n\n\n\n\n\n\nA: Using the linear function does not work at all, as the equivalent input/output model would be linear, and the data is non-linear. logistic and tanh work approximately in the same way. ReLU is the best.\n\n\nInfluence of data normalization\nThe input data returned by make_circles is nicely center around 0, with values between -1 and 1. What happens if this is not the case with your data?\nQ: Shift the input data X using the formula:\nX_\\text{shifted} = 8 \\, X + 2\nregenerate the training and test sets and train the MLP on them. What do you observe?\n\nX_shifted = 8* X + 2 \n\nplt.figure(figsize=(10, 6))\nplt.scatter(X_shifted[t==1, 0], X_shifted[t==1, 1])\nplt.scatter(X_shifted[t==0, 0], X_shifted[t==0, 1])\nplt.show()\n\nX_train, X_test, t_train, t_test = train_test_split(X_shifted, t, test_size=.3, random_state=42)\n\n\n\n\n\n\n\n\n\nK = 15\nmax_val = 1.0\neta = 0.1\nactivation_function=relu\n\n# Create the MLP\nmlp = TrainableMLP(d, K, activation_function, max_val, eta)\n\n# Train the MLP\ntraining_error, nb_epochs = mlp.train(X_train, t_train, 1000)\n\n# Test the MLP\ntest_error = mlp.test(X_test, t_test)\n\nprint('Number of epochs needed:', nb_epochs)\nprint('Training accuracy:', 1. - training_error[-1])\nprint('Test accuracy:', 1. - test_error)\n\nplt.figure(figsize=(10, 6))\nplt.plot(training_error)\nplt.xlabel(\"Number of epochs\")\nplt.ylabel(\"Training error\")\nplt.show()\n\n\n\n\n\n\n\n\nNumber of epochs needed: 1000\nTraining accuracy: 0.7571428571428571\nTest accuracy: 0.6333333333333333\n\n\n\n\n\n\n\n\n\nA: It does not work anymore…\nQ: Normalize the shifted data so that it has a mean of 0 and a variance of 1 in each dimension, using the formula:\nX_\\text{normalized} = \\dfrac{X_\\text{shifted} - \\text{mean}(X_\\text{shifted})}{\\text{std}(X_\\text{shifted})}\nand retrain the network. Conclude.\n\nX_scaled = (X_shifted - X_shifted.mean(axis=0))/(X_shifted.std(axis=0)) \n\nplt.figure(figsize=(10, 6))\nplt.scatter(X_scaled[t==1, 0], X_scaled[t==1, 1])\nplt.scatter(X_scaled[t==0, 0], X_scaled[t==0, 1])\nplt.show()\n\nX_train, X_test, t_train, t_test = train_test_split(X_scaled, t, test_size=.3, random_state=42)\n\n\n\n\n\n\n\n\n\nK = 15\nmax_val = 1.0\neta = 0.1\nactivation_function=relu\n\n# Create the MLP\nmlp = TrainableMLP(d, K, activation_function, max_val, eta)\n\n# Train the MLP\ntraining_error, nb_epochs = mlp.train(X_train, t_train, 1000)\n\n# Test the MLP\ntest_error = mlp.test(X_test, t_test)\n\nprint('Number of epochs needed:', nb_epochs)\nprint('Training accuracy:', 1. - training_error[-1])\nprint('Test accuracy:', 1. - test_error)\n\nplt.figure(figsize=(10, 6))\nplt.plot(training_error)\nplt.xlabel(\"Number of epochs\")\nplt.ylabel(\"Training error\")\nplt.show()\n\n\n\n\n\n\n\n\nNumber of epochs needed: 1000\nTraining accuracy: 0.9857142857142858\nTest accuracy: 0.9666666666666667\n\n\n\n\n\n\n\n\n\nA: Now it works again. Conclusion: always normalize the data before training so that each input dimension has a mean of 0 and variance of 1. Refer batch normalization later.\n\n\nInfluence of randomization\nThe training loop we used until now iterated over the training samples in the exact same order at every epoch. The samples are therefore not i.i.d (independent and identically distributed) as they follow the same sequence.\nQ: Modify the train() method so that the indices of the training samples are randomized between two epochs. Check the doc of rng.permutation() for help.\n\nclass StochasticMLP (TrainableMLP):\n        \n    \n    def train(self, X_train, t_train, nb_epochs, visualize=True):\n        errors = []\n        \n        for epoch in range(nb_epochs):\n            \n            nb_errors = 0\n\n            # Epoch\n            for i in rng.permutation(X_train.shape[0]):\n\n                # Feedforward pass: sets self.h and self.y\n                self.feedforward(X_train[i, :])\n        \n                # Backpropagation\n                self.backprop(X_train[i, :], t_train[i])\n        \n                # Predict the class:         \n                if self.y[0, 0] > 0.5:\n                    c = 1\n                else:\n                    c = 0\n\n                # Count the number of misclassifications\n                if t_train[i] != c: \n                    nb_errors += 1\n            \n            # Compute the error rate\n            errors.append(nb_errors/X_train.shape[0])\n                \n            # Plot the decision function every 10 epochs\n            if epoch % 10 == 0 and visualize:\n                self.plot_classification() \n\n            # Stop when the error rate is 0\n            if nb_errors == 0:\n                if visualize:\n                    self.plot_classification() \n                break\n                \n        return errors, epoch\n\n\nK = 15\nmax_val = 1.0\neta = 0.1\nactivation_function=relu\n\n# Create the MLP\nmlp = StochasticMLP(d, K, activation_function, max_val, eta)\n\n# Train the MLP\ntraining_error, nb_epochs = mlp.train(X_train, t_train, 1000)\n\n# Test the MLP\ntest_error = mlp.test(X_test, t_test)\n\nprint('Number of epochs needed:', nb_epochs)\nprint('Training accuracy:', 1. - training_error[-1])\nprint('Test accuracy:', 1. - test_error)\n\nplt.figure(figsize=(10, 6))\nplt.plot(training_error)\nplt.xlabel(\"Number of epochs\")\nplt.ylabel(\"Training error\")\nplt.show()\n\n\n\n\n\n\n\n\nNumber of epochs needed: 78\nTraining accuracy: 1.0\nTest accuracy: 0.9\n\n\n\n\n\n\n\n\n\n\n\nInfluence of weight initialization - part 2\nAccording to the empirical analysis by Glorot and Bengio in “Understanding the difficulty of training deep feedforward neural networks”, the optimal initial values for the weights between two layers of a MLP are uniformly taken in the range:\n\n   \\mathcal{U} ( - \\sqrt{\\frac{6}{N_{\\text{in}}+N_{\\text{out}}}} , \\sqrt{\\frac{6}{N_{\\text{in}}+N_{\\text{out}}}} )\n\nwhere N_{\\text{in}} is the number of neurons in the first layer and N_{\\text{out}} the number of neurons in the second layer.\nQ: Modify the constructor of your class to initialize both hidden and output weights with this new range. The biases should be initialized to 0. What is the effect?\n\nclass GlorotMLP (StochasticMLP):\n\n    def __init__(self, d, K, activation_function, eta):\n        \n        self.d = d\n        self.K = K\n        self.activation_function = activation_function\n        self.eta = eta\n        \n        max_val = np.sqrt(6./(d+K))\n        self.W1  = rng.uniform(-max_val, max_val, (K, d)) \n        self.b1  = np.zeros((K, 1))\n    \n        max_val = np.sqrt(6./(K+1))\n        self.W2 = rng.uniform(-max_val, max_val, (1, K)) \n        self.b2 = np.zeros((1, 1))\n\n\nK = 15\neta = 0.1\nactivation_function=relu\n\n# Create the MLP\nmlp = GlorotMLP(d, K, activation_function, eta)\n\n# Train the MLP\ntraining_error, nb_epochs = mlp.train(X_train, t_train, 1000)\n\n# Test the MLP\ntest_error = mlp.test(X_test, t_test)\n\nprint('Number of epochs needed:', nb_epochs)\nprint('Training accuracy:', 1. - training_error[-1])\nprint('Test accuracy:', 1. - test_error)\n\nplt.figure(figsize=(10, 6))\nplt.plot(training_error)\nplt.xlabel(\"Number of epochs\")\nplt.ylabel(\"Training error\")\nplt.show()\n\n\n\n\n\n\n\n\nNumber of epochs needed: 72\nTraining accuracy: 1.0\nTest accuracy: 0.9666666666666667\n\n\n\n\n\n\n\n\n\nA: The decision function seems already quite centered at the beginning (closer to the solution), so it converges faster.\n\n\nSummary\nQ: Now that we optimized the MLP, it is time to cross-validate again the number of hidden neurons and the learning rate. As the networks always get a training error rate of 0 and the test set is not very relevant, the maincriteria will be the number of epochs needed on average to converge. Find the best MLP for the dataset (there is not a single solution), for example by iterating over multiple values of K and eta. What do you think of the change in performance between the first naive implementation and the final one? What were the most critical changes?\n\nactivation_function = relu\n\ndef run(K, eta):\n    # Create the MLP\n    mlp = GlorotMLP(d, K, activation_function, eta)\n\n    # Train the MLP\n    training_error, nb_epochs = mlp.train(X_train, t_train, 1000, visualize=False)\n\n    # Test the MLP\n    test_error = mlp.test(X_test, t_test)\n\n    print('K:', K, 'eta:', eta)\n    print('Number of epochs needed:', nb_epochs)\n    print('Training accuracy:', 1. - training_error[-1])\n    print('Test accuracy:', 1. - test_error)\n    print('-'*20)\n\n    return 1. - test_error\n\nbest_test_accuracy = 0.0\nbest_K, best_eta = 0, 0\n\nfor K in [10, 15, 20, 25]:\n    for eta in [0.01, 0.05, 0.1, 0.2, 0.3]:\n        acc = run(K, eta)\n        if acc > best_test_accuracy:\n            best_K, best_eta = K, eta\n\nprint(\"Best hyperparameters: K =\", best_K, \"; eta = \", best_eta)\n\nK: 10 eta: 0.01\nNumber of epochs needed: 330\nTraining accuracy: 1.0\nTest accuracy: 0.9\n--------------------\nK: 10 eta: 0.05\nNumber of epochs needed: 86\nTraining accuracy: 1.0\nTest accuracy: 0.9333333333333333\n--------------------\nK: 10 eta: 0.1\nNumber of epochs needed: 86\nTraining accuracy: 1.0\nTest accuracy: 0.8333333333333334\n--------------------\nK: 10 eta: 0.2\nNumber of epochs needed: 56\nTraining accuracy: 1.0\nTest accuracy: 1.0\n--------------------\nK: 10 eta: 0.3\nNumber of epochs needed: 79\nTraining accuracy: 1.0\nTest accuracy: 0.9666666666666667\n--------------------\nK: 15 eta: 0.01\nNumber of epochs needed: 254\nTraining accuracy: 1.0\nTest accuracy: 0.8\n--------------------\nK: 15 eta: 0.05\nNumber of epochs needed: 87\nTraining accuracy: 1.0\nTest accuracy: 0.8666666666666667\n--------------------\nK: 15 eta: 0.1\nNumber of epochs needed: 57\nTraining accuracy: 1.0\nTest accuracy: 0.8333333333333334\n--------------------\nK: 15 eta: 0.2\nNumber of epochs needed: 46\nTraining accuracy: 1.0\nTest accuracy: 0.9333333333333333\n--------------------\nK: 15 eta: 0.3\nNumber of epochs needed: 69\nTraining accuracy: 1.0\nTest accuracy: 0.9666666666666667\n--------------------\nK: 20 eta: 0.01\nNumber of epochs needed: 269\nTraining accuracy: 1.0\nTest accuracy: 0.9333333333333333\n--------------------\nK: 20 eta: 0.05\nNumber of epochs needed: 117\nTraining accuracy: 1.0\nTest accuracy: 0.9333333333333333\n--------------------\nK: 20 eta: 0.1\nNumber of epochs needed: 62\nTraining accuracy: 1.0\nTest accuracy: 0.8666666666666667\n--------------------\nK: 20 eta: 0.2\nNumber of epochs needed: 72\nTraining accuracy: 1.0\nTest accuracy: 0.9333333333333333\n--------------------\nK: 20 eta: 0.3\nNumber of epochs needed: 69\nTraining accuracy: 1.0\nTest accuracy: 0.9666666666666667\n--------------------\nK: 25 eta: 0.01\nNumber of epochs needed: 178\nTraining accuracy: 1.0\nTest accuracy: 0.8333333333333334\n--------------------\nK: 25 eta: 0.05\nNumber of epochs needed: 79\nTraining accuracy: 1.0\nTest accuracy: 0.8666666666666667\n--------------------\nK: 25 eta: 0.1\nNumber of epochs needed: 78\nTraining accuracy: 1.0\nTest accuracy: 0.8666666666666667\n--------------------\nK: 25 eta: 0.2\nNumber of epochs needed: 56\nTraining accuracy: 1.0\nTest accuracy: 0.8666666666666667\n--------------------\nK: 25 eta: 0.3\nNumber of epochs needed: 69\nTraining accuracy: 1.0\nTest accuracy: 1.0\n--------------------\nBest hyperparameters: K = 25 ; eta =  0.3\n\n\nA: Quite small changes can drastically change the performance of the network, both in terms of accuracy and training time. Data normalization, Glorot initialization and the use of ReLU are the most determinant here."
  },
  {
    "objectID": "exercises/9-MNIST-solution.html#basic-model",
    "href": "exercises/9-MNIST-solution.html#basic-model",
    "title": "MNIST classification using keras",
    "section": "Basic model",
    "text": "Basic model\nYou are provided with a basic poor-performing keras model to get you started. The goal is to extend this model in order to obtain a satisfying accuracy on the test set.\n\nData preprocessing\nThe first step is to download the MNIST dataset. You could download the raw data from http://yann.lecun.com/exdb/mnist and process it, but that would take a while.\nFortunately, keras comes with a utility to automatically download MNIST, split it into training and test set and create nice numpy arrays:\n\n(X_train, t_train), (X_test, t_test) = tf.keras.datasets.mnist.load_data()\n\nHave a look at the doc of tf.keras.datasets to see what other datasets you can simply use.\nQ: Print the shape of the four numpy arrays (X_train, t_train), (X_test, t_test) and visualize some training examples to better understand what you are going to work on.\n\nprint(\"Training data:\", X_train.shape, t_train.shape)\nprint(\"Test data:\", X_test.shape, t_test.shape)\n\nidx = 682 # for example\nx = X_train[idx, :]\nt = t_train[idx]\nprint(\"x:\", x)\nprint(\"x (shape):\", x.shape)\nprint(\"t:\", t)\n\nplt.figure(figsize=(5, 5))\nplt.imshow(x, cmap=\"gray\")\nplt.colorbar()\nplt.show()\n\nTraining data: (60000, 28, 28) (60000,)\nTest data: (10000, 28, 28) (10000,)\nx: [[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0]\n [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0]\n [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0]\n [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0]\n [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  83 149   0   0\n    0   0   0   0   0   0   0   0   0   0]\n [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 104 245   9   0\n    0   0   0   0   0   0   0   0   0   0]\n [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  32 254  40   0\n    0   0   0   0   0   0   0   0   0   0]\n [  0   0   0   0   0   0   0  66  68   0   0   0   0   0  15 232 144   0\n    0   0   0   0   0   0   0   0   0   0]\n [  0   0   0   0   0   0   0 166 128   0   0   0   0   0   0 135 208   4\n    0   0   0   0   0   0   0   0   0   0]\n [  0   0   0   0   0   0   0 181 128   0   0   0   0   0   0  64 254  88\n    0   0   0   0   0   0   0   0   0   0]\n [  0   0   0   0   0   0   0 240 105   0   0   0   0   0   0  24 245 126\n    0   0   0   0   0   0   0   0   0   0]\n [  0   0   0   0   0   0   0 232  30   0   0   0   0   0   0   0 136 190\n    4   0   0   0   0   0   0   0   0   0]\n [  0   0   0   0   0   0  36 230   0   0   0   0   0   0   0   0  96 255\n   41   0   0   0   0   0   0   0   0   0]\n [  0   0   0   0   0   0  60 236   0   0   0   0   0   0   0   0  23 254\n   92   0   0   0   0   0   0   0   0   0]\n [  0   0   0   0   0   0 102 236   0   0   0   0   0  20   1  21  85 254\n   92   0   0   0   0   0   0   0   0   0]\n [  0   0   0   0   0   0 133 244 104 150 177 222 252 227 213 188 189 167\n  165   0   0   0   0   0   0   0   0   0]\n [  0   0   0   0   0   0  58 251 205 191 157 117  44   7   0   0   0 129\n  227   0   0   0   0   0   0   0   0   0]\n [  0   0   0   0   0   0   0  10   3   0   0   0   0   0   0   0   0 129\n  241   6   0   0   0   0   0   0   0   0]\n [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 129\n  254  59   0   0   0   0   0   0   0   0]\n [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 129\n  254  59   0   0   0   0   0   0   0   0]\n [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 129\n  247  31   0   0   0   0   0   0   0   0]\n [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 129\n  240   0   0   0   0   0   0   0   0   0]\n [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 129\n  246  27   0   0   0   0   0   0   0   0]\n [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  93\n  193   8   0   0   0   0   0   0   0   0]\n [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0]\n [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0]\n [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0]\n [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0]]\nx (shape): (28, 28)\nt: 4\n\n\n\n\n\n\n\n\n\nIn this exercise, we are going to use a regular MLP (with fully-connected layers). Convolutional layers will be seen next time.\nWe therefore need to transform the 28x28 input matrix into a 784 vector. Additionally, pixel values are integers between 0 and 255. We have to rescale them to floating values in [0, 1].\n\nX_train = X_train.reshape(X_train.shape[0], 784).astype('float32') / 255.\nX_test = X_test.reshape(X_test.shape[0], 784).astype('float32') / 255.\n\nWe saw in the last exercise that mean removal is crucial when training a neural network. The following cell removes the mean image of the training set from all examples.\nNote: you could also divide by the standard deviation to have a full normalization, but it does not bring much here.\n\nX_mean = np.mean(X_train, axis=0)\nX_train -= X_mean\nX_test -= X_mean\n\n\nplt.figure(figsize=(5, 5))\nplt.imshow(X_mean.reshape((28, 28))*255, cmap=\"gray\")\nplt.show()\n\n\n\n\n\n\n\n\nThe last preprocessing step is to perform one-hot encoding of the output labels. We want for example the digit 4 (index 5 in the outputs t) to be represented by the vector:\n[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\nkeras offers the utility utils.to_categorical to do that on the whole data:\n\nT_train = tf.keras.utils.to_categorical(t_train, 10)\nT_test = tf.keras.utils.to_categorical(t_test, 10)\n\nprint(T_train[idx])\n\n[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n\n\nAll set! The data is ready to be learned by a neural network. You should normally not have to re-run those cells again. If you do, do not forget to run all of them sequentially.\n\n\nModel definition\nLet’s now define a simple MLP with keras. When using a notebook, you can recreate models by simply re-running the cell, but this does not delete the previous networks which may end up filling your RAM. It is therefore good practice to start by telling tensorflow to delete all previous models (if this is what you want):\ntf.keras.backend.clear_session()\nOne way to define a neural network in keras is by stacking layers in a Sequential() model (you can later have a look at the doc of Model() for directed acyclic graphs).\nmodel = tf.keras.models.Sequential()\nThe input layer has 784 neurons, one per pixel in the input image. We only need to define a placeholder of the correct size to represent inputs and add() it to the model as its first layer:\nmodel.add(tf.keras.layers.Input(shape=(784,)))\nThe input layer goes into a hidden, fully-connected, layer of 100 neurons using the logistic (or sigmoid) transfer function This can be specified by adding to the model a Dense layer (in the sense “fully-connected”) with 100 units (another name for neuron), followed by an Activation layer using the ‘sigmoid’ function:\nmodel.add(tf.keras.layers.Dense(units=100))\nmodel.add(tf.keras.layers.Activation('sigmoid')) \nThe weight matrix and the biases are intialized automatically using the Glorot uniform scheme (seen in the last exercise) for the weights and zeros for the biases. Check the doc of the Dense layer to see how to change this: https://keras.io/layers/core/#dense.\nWe then add a softmax layer as output layer (classification problem), with 10 units (one per digit):\nmodel.add(tf.keras.layers.Dense(units=10))\nmodel.add(tf.keras.layers.Activation('softmax')) \nWeights and biases are initialized in the same manner. That’s all, keras now knows how to transform the input vector into class probabilities using randomly initialized weights!\nFor training, we need to choose an optimizer (learning rule). Several optimizers are available (https://keras.io/optimizers/). We pick simply Stochastic Gradient Descent with a learning rate of 0.01:\noptimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\nThe last step is to compile the network, so that keras computes how to implement the backpropagation algorithm. You need to specify:\n\nwhich loss function you want to minimize. The full list is at https://keras.io/losses/. Here, we will use the cross-entropy loss function as we have a clasification problem with softmax outputs.\nwhich optimizer you want to use.\nwhich metrics (accuracy, error, etc. - https://keras.io/metrics/) you want to track during learning.\n\nAfter the call to compile(), the neural network is instantiated and ready to learn.\n\n# Delete all previous models to free memory\ntf.keras.backend.clear_session()\n\n# Sequential model\nmodel = tf.keras.models.Sequential()\n\n# Input layer representing the 784 pixels\nmodel.add(tf.keras.layers.Input(shape=(784,)))\n\n# Hidden layer with 100 logistic neurons\nmodel.add(tf.keras.layers.Dense(units=100))\nmodel.add(tf.keras.layers.Activation('sigmoid')) \n\n# Softmax output layer over 10 classes\nmodel.add(tf.keras.layers.Dense(units=10))\nmodel.add(tf.keras.layers.Activation('softmax')) \n\n# Learning rule\noptimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n\n# Loss function\nmodel.compile(\n    loss='categorical_crossentropy', # loss function\n    optimizer=optimizer, # learning rule\n    metrics=['accuracy'] # show accuracy\n)\n\nMetal device set to: Apple M1 Pro\n\nsystemMemory: 16.00 GB\nmaxCacheSize: 5.33 GB\n\n\n\n2022-11-29 15:52:22.276272: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n2022-11-29 15:52:22.276416: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n\n\nA good practice after creating the model is to call model.summary() to see how many layers you have created and how many parameters each layer has.\nQ: Explain why you obtain this numbers of parameters in each layer.\n\nprint(model.summary())\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense (Dense)               (None, 100)               78500     \n                                                                 \n activation (Activation)     (None, 100)               0         \n                                                                 \n dense_1 (Dense)             (None, 10)                1010      \n                                                                 \n activation_1 (Activation)   (None, 10)                0         \n                                                                 \n=================================================================\nTotal params: 79,510\nTrainable params: 79,510\nNon-trainable params: 0\n_________________________________________________________________\nNone\n\n\nA: The hidden layer has a weight matrix of size 784x100 and 100 biases, what makes 78500 free parameters. The output layer has a weight matrix of size 100x10 and 10 biases, so 1010 parameters.\nNote that we have more free parameters than training examples, we are going to have to regularize quite hard…\n\n\nModel training\nNow is time to train the network on MNIST. The following cell creates a History() object that will record the progress of your network.\nIt then calls the model.fit() method, which tells the network to learn the MNIST dataset defined by the (X_train, Y_train) arrays. You have to specify:\n\nthe batch size, i.e. the number of training examples in each minibatch used by SGD.\nthe maximal number of epochs for training\nthe size of the validation, taken from the training set to track the progress (this is not the test set!). Here we reserve 10% of the training data to validate. If you do not have much data, you could set it to 0.\na callback, which will be called at the end of each epoch. Here it will save the metrics defined in model.compile() in the History() object.\n\nThe training process can take a while depending on how big your network is and how many data samples you have. You can interrupt the kernel using the menu if you want to stop the processing in the cell.\n\n# History tracks the evolution of the metrics during learning\nhistory = tf.keras.callbacks.History()\n\n# Training procedure\nmodel.fit(\n    X_train, T_train, # training data\n    batch_size=128,  # batch size\n    epochs=20, # Maximum number of epochs\n    validation_split=0.1, # Percentage of training data used for validation\n    callbacks=[history] # Track the metrics at the end of each epoch\n)\n\n2022-11-29 15:52:28.249092: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n2022-11-29 15:52:28.351132: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n\n\nEpoch 1/20\n420/422 [============================>.] - ETA: 0s - loss: 2.0709 - accuracy: 0.4651\n\n\n2022-11-29 15:52:32.137612: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n\n\n422/422 [==============================] - 4s 10ms/step - loss: 2.0698 - accuracy: 0.4659 - val_loss: 1.8160 - val_accuracy: 0.6918\nEpoch 2/20\n422/422 [==============================] - 4s 10ms/step - loss: 1.6281 - accuracy: 0.7030 - val_loss: 1.4038 - val_accuracy: 0.7813\nEpoch 3/20\n422/422 [==============================] - 4s 9ms/step - loss: 1.2881 - accuracy: 0.7664 - val_loss: 1.1032 - val_accuracy: 0.8277\nEpoch 4/20\n422/422 [==============================] - 4s 9ms/step - loss: 1.0516 - accuracy: 0.8007 - val_loss: 0.9015 - val_accuracy: 0.8522\nEpoch 5/20\n422/422 [==============================] - 4s 9ms/step - loss: 0.8933 - accuracy: 0.8212 - val_loss: 0.7664 - val_accuracy: 0.8685\nEpoch 6/20\n422/422 [==============================] - 4s 9ms/step - loss: 0.7848 - accuracy: 0.8353 - val_loss: 0.6726 - val_accuracy: 0.8765\nEpoch 7/20\n422/422 [==============================] - 4s 10ms/step - loss: 0.7074 - accuracy: 0.8447 - val_loss: 0.6047 - val_accuracy: 0.8832\nEpoch 8/20\n422/422 [==============================] - 4s 10ms/step - loss: 0.6499 - accuracy: 0.8525 - val_loss: 0.5532 - val_accuracy: 0.8888\nEpoch 9/20\n422/422 [==============================] - 4s 9ms/step - loss: 0.6057 - accuracy: 0.8578 - val_loss: 0.5139 - val_accuracy: 0.8927\nEpoch 10/20\n422/422 [==============================] - 4s 10ms/step - loss: 0.5707 - accuracy: 0.8632 - val_loss: 0.4823 - val_accuracy: 0.8960\nEpoch 11/20\n422/422 [==============================] - 4s 9ms/step - loss: 0.5424 - accuracy: 0.8672 - val_loss: 0.4569 - val_accuracy: 0.8980\nEpoch 12/20\n422/422 [==============================] - 4s 9ms/step - loss: 0.5191 - accuracy: 0.8703 - val_loss: 0.4359 - val_accuracy: 0.9005\nEpoch 13/20\n422/422 [==============================] - 4s 9ms/step - loss: 0.4995 - accuracy: 0.8728 - val_loss: 0.4183 - val_accuracy: 0.9025\nEpoch 14/20\n422/422 [==============================] - 4s 10ms/step - loss: 0.4828 - accuracy: 0.8748 - val_loss: 0.4033 - val_accuracy: 0.9043\nEpoch 15/20\n422/422 [==============================] - 4s 9ms/step - loss: 0.4685 - accuracy: 0.8773 - val_loss: 0.3903 - val_accuracy: 0.9058\nEpoch 16/20\n422/422 [==============================] - 4s 9ms/step - loss: 0.4560 - accuracy: 0.8791 - val_loss: 0.3793 - val_accuracy: 0.9065\nEpoch 17/20\n422/422 [==============================] - 4s 9ms/step - loss: 0.4450 - accuracy: 0.8816 - val_loss: 0.3695 - val_accuracy: 0.9083\nEpoch 18/20\n422/422 [==============================] - 4s 10ms/step - loss: 0.4352 - accuracy: 0.8833 - val_loss: 0.3607 - val_accuracy: 0.9088\nEpoch 19/20\n422/422 [==============================] - 4s 10ms/step - loss: 0.4265 - accuracy: 0.8848 - val_loss: 0.3530 - val_accuracy: 0.9093\nEpoch 20/20\n422/422 [==============================] - 5s 11ms/step - loss: 0.4186 - accuracy: 0.8862 - val_loss: 0.3462 - val_accuracy: 0.9107\n\n\n<keras.callbacks.History at 0x16c05dfd0>\n\n\nThe training has now run for 20 epochs on the training set. You see the evolution of loss function and accuracy for both the training and validation sets.\nTo test your trained model on the test set, you can call model.evaluate():\n\nscore = model.evaluate(X_test, T_test, verbose=0)\n\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n\nTest loss: 0.3901696503162384\nTest accuracy: 0.8963000178337097\n\n\nYou can also use the History() object to visualize the evolution of the the training and validation accuracy during learning.\n\nplt.figure(figsize=(15, 6))\n\nplt.subplot(121)\nplt.plot(history.history['loss'], '-r', label=\"Training\")\nplt.plot(history.history['val_loss'], '-b', label=\"Validation\")\nplt.xlabel('Epoch #')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.subplot(122)\nplt.plot(history.history['accuracy'], '-r', label=\"Training\")\nplt.plot(history.history['val_accuracy'], '-b', label=\"Validation\")\nplt.xlabel('Epoch #')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.show()\n\n\n\n\n\n\n\n\nQ: Did overfitting occur during learning? Why? Looking at the curves, does it make sense to continue learning for much more epochs?\nA: No, the training accuracy is always below the validation accuracy. The model is too small to overfit. The accuracy has started saturating, it will not get much better (perhaps one percent or two) or very slowly (you can let it learn for 500 epochs or more to see it).\nThe following cell makes predictions on the test set (model.predict(X_test)), computes the predicted classes by looking at the maximum probability for each example and displays some misclassified examples. The title of each subplot denotes the predicted class and the ground truth.\nQ: Are some mistakes understandable?\n\nY_test = model.predict(X_test)\nc_test = np.argmax(Y_test, axis=-1)\n\nmisclassification = (c_test != t_test).nonzero()[0]\n\nplt.figure(figsize=(10, 8))\nfor i in range(12):\n    plt.subplot(3, 4, i+1)\n    plt.imshow((X_test[misclassification[i], :] + X_mean).reshape((28, 28)), cmap=plt.cm.gray, interpolation='nearest')\n    plt.title('P = ' + str(c_test[misclassification[i]]) + ' ; T = ' + str(t_test[misclassification[i]]))\n    plt.xticks([]); plt.yticks([])\nplt.show()\n\n2022-11-29 15:53:52.060969: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled."
  },
  {
    "objectID": "exercises/9-MNIST-solution.html#questions",
    "href": "exercises/9-MNIST-solution.html#questions",
    "title": "MNIST classification using keras",
    "section": "Questions",
    "text": "Questions\nWith the provided model, you probably obtained a final accuracy on the test set around 90%. That is lame. The state-of-the-art performance is 99.7%.\nThe goal of this exercise is now to modify the network in order to obtain an accuracy of 98% (or more) in 20 epochs only.\nYou are free to use any improvement on the basic model, using the doc of Keras. Here are some suggestions:\n\nChange the learning rate of SGD.\nChange the number of neurons in the hidden layer.\nChange the number of hidden layers (just stack another Dense layer in the model).\n\nBeware: you do not have three weeks in front of you, so keep the complexity of your model in a reasonable range.\n\nChange the transfer function of the hidden neurons. See https://keras.io/activations/ for the different possibilities in keras. Check in particular the Rectifier Linear Unit (ReLU).\nChange the learning rule. Instead of the regular SGD, use for example the Nesterov Momentum method:\n\noptimizer = tf.keras.optimizers.SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\nor the Adam learning rule:\noptimizer = tf.keras.optimizers.Adam(lr=0.01)\n\nChange the batch size. What impact does it have on training time?\nApply L2- or L1-regularization to the weight updates to avoid overfitting https://keras.io/regularizers/:\n\nmodel.add(tf.keras.layers.Dense(50, kernel_regularizer=tf.keras.regularizers.l2(0.0001)))\n\nApply dropout regularization after each layer. Find a good level of dropout.\n\nmodel.add(tf.keras.layers.Dropout(0.5))\n\nAdd Batch normalization between the fully-connected layer and the transfer function.\n\nmodel.add(tf.keras.layers.Dense(100)) # Weights\nmodel.add(tf.keras.layers.BatchNormalization()) # Batch normalization\nmodel.add(tf.keras.layers.Activation('relu')) # Transfer function"
  },
  {
    "objectID": "exercises/9-MNIST-solution.html#solution",
    "href": "exercises/9-MNIST-solution.html#solution",
    "title": "MNIST classification using keras",
    "section": "Solution",
    "text": "Solution\nHere is a proposal which obtains 98% accuracy in a reasonable time. There are many other possible solutions, perhaps you found even better…\n\n# Delete all previous models to free memory\ntf.keras.backend.clear_session()\n\n# Sequential model\nmodel = tf.keras.models.Sequential()\n\n# Input layer representing the 784 pixels\nmodel.add(tf.keras.layers.Input(shape=(784,)))\n\n# Hidden layer with 150 relu neurons, BN and dropout\nmodel.add(tf.keras.layers.Dense(units=150))\nmodel.add(tf.keras.layers.BatchNormalization()) \nmodel.add(tf.keras.layers.Activation('relu'))\nmodel.add(tf.keras.layers.Dropout(0.3)) \n\n\n# Second hidden layer with 100 relu neurons, BN and dropout\nmodel.add(tf.keras.layers.Dense(100))\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(tf.keras.layers.Activation('relu'))\nmodel.add(tf.keras.layers.Dropout(0.3))\n\n# Softmax output layer over 10 classes\nmodel.add(tf.keras.layers.Dense(10))\nmodel.add(tf.keras.layers.Activation('softmax'))\n\n# Learning rule\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n\n# Loss function\nmodel.compile(\n    loss='categorical_crossentropy', # loss function\n    optimizer=optimizer, # learning rule\n    metrics=['accuracy'] # show accuracy\n)\n\nprint(model.summary())\n\n# Training\nhistory = tf.keras.callbacks.History()\nmodel.fit(\n    X_train, T_train,\n    batch_size=128, \n    epochs=20,\n    validation_split=0.1,\n    callbacks=[history]\n)\n\n# Testing\nscore = model.evaluate(X_test, T_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n\nplt.figure(figsize=(15, 6))\nplt.subplot(121)\nplt.plot(history.history['loss'], '-r', label=\"Training\")\nplt.plot(history.history['val_loss'], '-b', label=\"Validation\")\nplt.xlabel('Epoch #')\nplt.ylabel('Loss')\nplt.legend()\nplt.subplot(122)\nplt.plot(history.history['accuracy'], '-r', label=\"Training\")\nplt.plot(history.history['val_accuracy'], '-b', label=\"Validation\")\nplt.xlabel('Epoch #')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense (Dense)               (None, 150)               117750    \n                                                                 \n batch_normalization (BatchN  (None, 150)              600       \n ormalization)                                                   \n                                                                 \n activation (Activation)     (None, 150)               0         \n                                                                 \n dropout (Dropout)           (None, 150)               0         \n                                                                 \n dense_1 (Dense)             (None, 100)               15100     \n                                                                 \n batch_normalization_1 (Batc  (None, 100)              400       \n hNormalization)                                                 \n                                                                 \n activation_1 (Activation)   (None, 100)               0         \n                                                                 \n dropout_1 (Dropout)         (None, 100)               0         \n                                                                 \n dense_2 (Dense)             (None, 10)                1010      \n                                                                 \n activation_2 (Activation)   (None, 10)                0         \n                                                                 \n=================================================================\nTotal params: 134,860\nTrainable params: 134,360\nNon-trainable params: 500\n_________________________________________________________________\nNone\nEpoch 1/20\n\n\n2022-11-29 15:53:53.535515: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n\n\n422/422 [==============================] - ETA: 0s - loss: 0.4475 - accuracy: 0.8678\n\n\n2022-11-29 15:54:01.593654: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n\n\n422/422 [==============================] - 9s 18ms/step - loss: 0.4475 - accuracy: 0.8678 - val_loss: 0.1420 - val_accuracy: 0.9582\nEpoch 2/20\n422/422 [==============================] - 7s 16ms/step - loss: 0.2049 - accuracy: 0.9387 - val_loss: 0.0929 - val_accuracy: 0.9730\nEpoch 3/20\n422/422 [==============================] - 7s 16ms/step - loss: 0.1555 - accuracy: 0.9527 - val_loss: 0.0771 - val_accuracy: 0.9783\nEpoch 4/20\n422/422 [==============================] - 7s 16ms/step - loss: 0.1272 - accuracy: 0.9614 - val_loss: 0.0723 - val_accuracy: 0.9785\nEpoch 5/20\n422/422 [==============================] - 9s 20ms/step - loss: 0.1090 - accuracy: 0.9674 - val_loss: 0.0703 - val_accuracy: 0.9785\nEpoch 6/20\n422/422 [==============================] - 7s 16ms/step - loss: 0.0978 - accuracy: 0.9699 - val_loss: 0.0667 - val_accuracy: 0.9800\nEpoch 7/20\n422/422 [==============================] - 7s 17ms/step - loss: 0.0893 - accuracy: 0.9719 - val_loss: 0.0618 - val_accuracy: 0.9823\nEpoch 8/20\n422/422 [==============================] - 6s 15ms/step - loss: 0.0829 - accuracy: 0.9734 - val_loss: 0.0608 - val_accuracy: 0.9812\nEpoch 9/20\n422/422 [==============================] - 6s 15ms/step - loss: 0.0763 - accuracy: 0.9760 - val_loss: 0.0581 - val_accuracy: 0.9832\nEpoch 10/20\n422/422 [==============================] - 6s 15ms/step - loss: 0.0713 - accuracy: 0.9781 - val_loss: 0.0574 - val_accuracy: 0.9828\nEpoch 11/20\n422/422 [==============================] - 7s 16ms/step - loss: 0.0686 - accuracy: 0.9779 - val_loss: 0.0589 - val_accuracy: 0.9828\nEpoch 12/20\n422/422 [==============================] - 7s 17ms/step - loss: 0.0623 - accuracy: 0.9795 - val_loss: 0.0577 - val_accuracy: 0.9838\nEpoch 13/20\n422/422 [==============================] - 7s 16ms/step - loss: 0.0585 - accuracy: 0.9814 - val_loss: 0.0550 - val_accuracy: 0.9848\nEpoch 14/20\n422/422 [==============================] - 7s 17ms/step - loss: 0.0567 - accuracy: 0.9817 - val_loss: 0.0605 - val_accuracy: 0.9830\nEpoch 15/20\n422/422 [==============================] - 7s 16ms/step - loss: 0.0564 - accuracy: 0.9816 - val_loss: 0.0599 - val_accuracy: 0.9837\nEpoch 16/20\n422/422 [==============================] - 7s 16ms/step - loss: 0.0506 - accuracy: 0.9837 - val_loss: 0.0585 - val_accuracy: 0.9833\nEpoch 17/20\n422/422 [==============================] - 7s 16ms/step - loss: 0.0493 - accuracy: 0.9835 - val_loss: 0.0556 - val_accuracy: 0.9838\nEpoch 18/20\n422/422 [==============================] - 7s 17ms/step - loss: 0.0509 - accuracy: 0.9831 - val_loss: 0.0597 - val_accuracy: 0.9833\nEpoch 19/20\n422/422 [==============================] - 7s 16ms/step - loss: 0.0443 - accuracy: 0.9851 - val_loss: 0.0547 - val_accuracy: 0.9855\nEpoch 20/20\n422/422 [==============================] - 7s 17ms/step - loss: 0.0456 - accuracy: 0.9849 - val_loss: 0.0594 - val_accuracy: 0.9843\nTest loss: 0.05959333851933479\nTest accuracy: 0.9823000431060791"
  },
  {
    "objectID": "exercises/10-CNN-solution.html#training-a-cnn-on-mnist",
    "href": "exercises/10-CNN-solution.html#training-a-cnn-on-mnist",
    "title": "Convolutional neural networks",
    "section": "Training a CNN on MNIST",
    "text": "Training a CNN on MNIST\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\n\nprint(tf.__version__)\n\n2.7.0\n\n\nTip: CNNs are much slower to train on CPU than the DNN of the last exercise. It is feasible to do this exercise on a normal computer, but if you have a Google account, we suggest to use colab to run this notebook on a GPU for free (training time should be divided by a factor 5 or so).\nGo then in the menu, “Runtime” and “Change Runtime type”. You can then change the “Hardware accelerator” to GPU. Do not choose TPU, it will be as slow as CPU for the small networks we are using.\nWe import and normalize the MNIST data like last time, except we do not reshape the images: they stay with the shape (28, 28, 1):\n\n# Fetch the MNIST data\n(X_train, t_train), (X_test, t_test) = tf.keras.datasets.mnist.load_data()\nprint(\"Training data:\", X_train.shape, t_train.shape)\nprint(\"Test data:\", X_test.shape, t_test.shape)\n\n# Normalize the values\nX_train = X_train.reshape(-1, 28, 28, 1).astype('float32') / 255.\nX_test = X_test.reshape(-1, 28, 28, 1).astype('float32') / 255.\n\n# Mean removal\nX_mean = np.mean(X_train, axis=0)\nX_train -= X_mean\nX_test -= X_mean\n\n# One-hot encoding\nT_train = tf.keras.utils.to_categorical(t_train, 10)\nT_test = tf.keras.utils.to_categorical(t_test, 10)\n\nTraining data: (60000, 28, 28) (60000,)\nTest data: (10000, 28, 28) (10000,)\n\n\nWe can now define the CNN defined in the first image:\n\na convolutional layer with 16 3x3 filters, using valid padding and ReLU transfer functions,\na max-pooling layer over 2x2 regions,\na fully-connected layer with 100 ReLU neurons,\na softmax layer with 10 neurons.\n\nThe CNN will be trained on MNIST using SGD with momentum.\nThe following code defines this basic network in keras:\n\n# Delete all previous models to free memory\ntf.keras.backend.clear_session()\n\n# Sequential model\nmodel = tf.keras.models.Sequential()\n\n# Input layer representing the (28, 28) image\nmodel.add(tf.keras.layers.Input(shape=(28, 28, 1)))\n\n# Convolutional layer with 16 feature maps using 3x3 filters\nmodel.add(tf.keras.layers.Conv2D(16, (3, 3), padding='valid'))\nmodel.add(tf.keras.layers.Activation('relu')) \n\n# Max-pooling layerover 2x2 regions\nmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n\n# Flatten the feature maps into a vector\nmodel.add(tf.keras.layers.Flatten())\n\n# Fully-connected layer\nmodel.add(tf.keras.layers.Dense(units=100))\nmodel.add(tf.keras.layers.Activation('relu')) \n\n# Softmax output layer over 10 classes\nmodel.add(tf.keras.layers.Dense(units=10))\nmodel.add(tf.keras.layers.Activation('softmax')) \n\n# Learning rule\noptimizer = tf.keras.optimizers.SGD(learning_rate=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n\n# Loss function\nmodel.compile(\n    loss='categorical_crossentropy', # loss function\n    optimizer=optimizer, # learning rule\n    metrics=['accuracy'] # show accuracy\n)\n\nprint(model.summary())\n\nMetal device set to: Apple M1 Pro\n\nsystemMemory: 16.00 GB\nmaxCacheSize: 5.33 GB\n\n\n\n2022-11-15 10:18:43.571203: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n2022-11-15 10:18:43.571329: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d (Conv2D)             (None, 26, 26, 16)        160       \n                                                                 \n activation (Activation)     (None, 26, 26, 16)        0         \n                                                                 \n max_pooling2d (MaxPooling2D  (None, 13, 13, 16)       0         \n )                                                               \n                                                                 \n flatten (Flatten)           (None, 2704)              0         \n                                                                 \n dense (Dense)               (None, 100)               270500    \n                                                                 \n activation_1 (Activation)   (None, 100)               0         \n                                                                 \n dense_1 (Dense)             (None, 10)                1010      \n                                                                 \n activation_2 (Activation)   (None, 10)                0         \n                                                                 \n=================================================================\nTotal params: 271,670\nTrainable params: 271,670\nNon-trainable params: 0\n_________________________________________________________________\nNone\n\n\nNote the use of Flatten() to transform the 13x13x16 tensor representing the max-pooling layer into a vector of 2704 elements.\nNote also the use of padding='valid' and its effect on the size of the tensor corresponding to the convolutional layer. Change it to padding='same' and conclude on its effect.\nQ: Which layer has the most parameters? Why? Compare with the fully-connected MLPs you obtained during exercise 5.\nA: conv layers have relatively few parameters (10 per filter). The main bottleneck is when going from convolutions to fully-connected.\nLet’s now train this network on MNIST for 10 epochs, using minibatches of 64 images:\n\n# History tracks the evolution of the metrics during learning\nhistory = tf.keras.callbacks.History()\n\n# Training procedure\nmodel.fit(\n    X_train, T_train, # training data\n    batch_size=64,  # batch size\n    epochs=10, # Maximum number of epochs\n    validation_split=0.1, # Perceptage of training data used for validation\n    callbacks=[history] # Track the metrics at the end of each epoch\n)\n\n2022-11-15 10:19:02.344996: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n2022-11-15 10:19:02.507957: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n\n\nEpoch 1/10\n843/844 [============================>.] - ETA: 0s - loss: 0.1535 - accuracy: 0.9523\n\n\n2022-11-15 10:19:12.965693: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n\n\n844/844 [==============================] - 11s 12ms/step - loss: 0.1534 - accuracy: 0.9524 - val_loss: 0.0566 - val_accuracy: 0.9840\nEpoch 2/10\n844/844 [==============================] - 10s 12ms/step - loss: 0.0505 - accuracy: 0.9845 - val_loss: 0.0655 - val_accuracy: 0.9838\nEpoch 3/10\n844/844 [==============================] - 10s 12ms/step - loss: 0.0332 - accuracy: 0.9891 - val_loss: 0.0662 - val_accuracy: 0.9822\nEpoch 4/10\n844/844 [==============================] - 10s 12ms/step - loss: 0.0221 - accuracy: 0.9927 - val_loss: 0.0618 - val_accuracy: 0.9858\nEpoch 5/10\n844/844 [==============================] - 10s 12ms/step - loss: 0.0146 - accuracy: 0.9951 - val_loss: 0.0636 - val_accuracy: 0.9862\nEpoch 6/10\n844/844 [==============================] - 10s 12ms/step - loss: 0.0123 - accuracy: 0.9959 - val_loss: 0.0619 - val_accuracy: 0.9875\nEpoch 7/10\n844/844 [==============================] - 10s 12ms/step - loss: 0.0106 - accuracy: 0.9964 - val_loss: 0.0656 - val_accuracy: 0.9862\nEpoch 8/10\n844/844 [==============================] - 10s 12ms/step - loss: 0.0110 - accuracy: 0.9964 - val_loss: 0.0630 - val_accuracy: 0.9882\nEpoch 9/10\n844/844 [==============================] - 10s 12ms/step - loss: 0.0079 - accuracy: 0.9973 - val_loss: 0.0807 - val_accuracy: 0.9882\nEpoch 10/10\n844/844 [==============================] - 10s 12ms/step - loss: 0.0048 - accuracy: 0.9983 - val_loss: 0.0778 - val_accuracy: 0.9870\n\n\n<keras.callbacks.History at 0x1769d37c0>\n\n\nAs in the previous exercise, the next cells compute the test loss and accuracy and display the evolution of the training and validation accuracies:\n\nscore = model.evaluate(X_test, T_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n\nTest loss: 0.06820043921470642\nTest accuracy: 0.9861000180244446\n\n\n\nplt.figure(figsize=(15, 6))\n\nplt.subplot(121)\nplt.plot(history.history['loss'], '-r', label=\"Training\")\nplt.plot(history.history['val_loss'], '-b', label=\"Validation\")\nplt.xlabel('Epoch #')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.subplot(122)\nplt.plot(history.history['accuracy'], '-r', label=\"Training\")\nplt.plot(history.history['val_accuracy'], '-b', label=\"Validation\")\nplt.xlabel('Epoch #')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.show()\n\n\n\n\n\n\n\n\nQ: What do you think of 1) the final accuracy and 2) the training time, compared to the MLP of last time?\nQ: When does your network start to overfit? How to recognize it?\nQ: Try different values for the batch size (16, 32, 64, 128..). What is its influence?\nA: A CNN, even as shallow as this one, is more accurate but much slower (on CPU) than fully-connected networks. A network overfits when the training accuracy becomes better than the validation accuracy (learning by heart, not generalizing), which is the case here. When the batch size is too small, learning is unstable: the training loss increases again after a while. 128 is actually slightly better than 64.\nQ: Improve the CNN to avoid overfitting. The test accuracy should be around 99%.\nYou can:\n\nchange the learning rate\nadd another block on convolution + max-pooling before the fully-connected layer to reduce the number of parameters,\nadd dropout after some of the layers,\nuse L2 regularization,\nuse a different optimizer,\ndo whatever you want.\n\nBeware: training is now relatively slow, keep your number of tries limited. Once you find a good architecture that does not overfit, train it for 20 epochs and proceed to the next questions.\n\n# Delete all previous models to free memory\ntf.keras.backend.clear_session()\n\n# Sequential model\nmodel = tf.keras.Sequential()\n\nmodel.add(tf.keras.layers.Input((28, 28, 1)))\n\nmodel.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='valid'))\nmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\nmodel.add(tf.keras.layers.Dropout(0.5))\n\nmodel.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='valid'))\nmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\nmodel.add(tf.keras.layers.Dropout(0.5))\n\nmodel.add(tf.keras.layers.Flatten())\n\nmodel.add(tf.keras.layers.Dense(150, activation='relu'))\nmodel.add(tf.keras.layers.Dropout(0.5))\n\nmodel.add(tf.keras.layers.Dense(10, activation='softmax'))\n\n# Learning rule\noptimizer = tf.keras.optimizers.SGD(learning_rate=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n\n# Loss function\nmodel.compile(\n    loss='categorical_crossentropy', # loss\n    optimizer=optimizer, # learning rule\n    metrics=['accuracy'] # show accuracy\n)\n\nprint(model.summary())\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d (Conv2D)             (None, 26, 26, 32)        320       \n                                                                 \n max_pooling2d (MaxPooling2D  (None, 13, 13, 32)       0         \n )                                                               \n                                                                 \n dropout (Dropout)           (None, 13, 13, 32)        0         \n                                                                 \n conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     \n                                                                 \n max_pooling2d_1 (MaxPooling  (None, 5, 5, 64)         0         \n 2D)                                                             \n                                                                 \n dropout_1 (Dropout)         (None, 5, 5, 64)          0         \n                                                                 \n flatten (Flatten)           (None, 1600)              0         \n                                                                 \n dense (Dense)               (None, 150)               240150    \n                                                                 \n dropout_2 (Dropout)         (None, 150)               0         \n                                                                 \n dense_1 (Dense)             (None, 10)                1510      \n                                                                 \n=================================================================\nTotal params: 260,476\nTrainable params: 260,476\nNon-trainable params: 0\n_________________________________________________________________\nNone\n\n\n\nhistory = tf.keras.callbacks.History()\n\nmodel.fit(\n    X_train, T_train,\n    batch_size=64, \n    epochs=20,\n    validation_split=0.1,\n    callbacks=[history]\n)\n\nEpoch 1/20\n\n\n2022-11-15 10:21:29.499936: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n\n\n844/844 [==============================] - ETA: 0s - loss: 0.4567 - accuracy: 0.8509\n\n\n2022-11-15 10:21:42.950073: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n\n\n844/844 [==============================] - 14s 16ms/step - loss: 0.4567 - accuracy: 0.8509 - val_loss: 0.0836 - val_accuracy: 0.9762\nEpoch 2/20\n844/844 [==============================] - 13s 15ms/step - loss: 0.1364 - accuracy: 0.9580 - val_loss: 0.0549 - val_accuracy: 0.9842\nEpoch 3/20\n844/844 [==============================] - 13s 15ms/step - loss: 0.1057 - accuracy: 0.9675 - val_loss: 0.0502 - val_accuracy: 0.9867\nEpoch 4/20\n844/844 [==============================] - 13s 15ms/step - loss: 0.0888 - accuracy: 0.9726 - val_loss: 0.0431 - val_accuracy: 0.9867\nEpoch 5/20\n844/844 [==============================] - 13s 15ms/step - loss: 0.0799 - accuracy: 0.9757 - val_loss: 0.0413 - val_accuracy: 0.9875\nEpoch 6/20\n844/844 [==============================] - 13s 15ms/step - loss: 0.0731 - accuracy: 0.9774 - val_loss: 0.0372 - val_accuracy: 0.9898\nEpoch 7/20\n844/844 [==============================] - 13s 15ms/step - loss: 0.0636 - accuracy: 0.9801 - val_loss: 0.0378 - val_accuracy: 0.9900\nEpoch 8/20\n844/844 [==============================] - 13s 15ms/step - loss: 0.0602 - accuracy: 0.9811 - val_loss: 0.0346 - val_accuracy: 0.9890\nEpoch 9/20\n844/844 [==============================] - 13s 15ms/step - loss: 0.0583 - accuracy: 0.9812 - val_loss: 0.0337 - val_accuracy: 0.9903\nEpoch 10/20\n844/844 [==============================] - 13s 15ms/step - loss: 0.0546 - accuracy: 0.9829 - val_loss: 0.0322 - val_accuracy: 0.9907\nEpoch 11/20\n844/844 [==============================] - 13s 15ms/step - loss: 0.0509 - accuracy: 0.9840 - val_loss: 0.0301 - val_accuracy: 0.9918\nEpoch 12/20\n844/844 [==============================] - 13s 15ms/step - loss: 0.0498 - accuracy: 0.9845 - val_loss: 0.0315 - val_accuracy: 0.9905\nEpoch 13/20\n844/844 [==============================] - 13s 15ms/step - loss: 0.0483 - accuracy: 0.9841 - val_loss: 0.0324 - val_accuracy: 0.9903\nEpoch 14/20\n844/844 [==============================] - 13s 15ms/step - loss: 0.0472 - accuracy: 0.9855 - val_loss: 0.0292 - val_accuracy: 0.9915\nEpoch 15/20\n844/844 [==============================] - 13s 15ms/step - loss: 0.0441 - accuracy: 0.9860 - val_loss: 0.0294 - val_accuracy: 0.9915\nEpoch 16/20\n844/844 [==============================] - 113s 134ms/step - loss: 0.0429 - accuracy: 0.9864 - val_loss: 0.0309 - val_accuracy: 0.9913\nEpoch 17/20\n844/844 [==============================] - 13s 15ms/step - loss: 0.0399 - accuracy: 0.9874 - val_loss: 0.0269 - val_accuracy: 0.9915\nEpoch 18/20\n844/844 [==============================] - 13s 15ms/step - loss: 0.0419 - accuracy: 0.9861 - val_loss: 0.0290 - val_accuracy: 0.9917\nEpoch 19/20\n844/844 [==============================] - 13s 15ms/step - loss: 0.0381 - accuracy: 0.9877 - val_loss: 0.0270 - val_accuracy: 0.9922\nEpoch 20/20\n844/844 [==============================] - 13s 15ms/step - loss: 0.0372 - accuracy: 0.9885 - val_loss: 0.0275 - val_accuracy: 0.9923\n\n\n<keras.callbacks.History at 0x17cf7f790>\n\n\n\nscore = model.evaluate(X_test, T_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n\nTest loss: 0.019702225923538208\nTest accuracy: 0.9937000274658203\n\n\n\nplt.figure(figsize=(15, 6))\n\nplt.subplot(121)\nplt.plot(history.history['loss'], '-r', label=\"Training\")\nplt.plot(history.history['val_loss'], '-b', label=\"Validation\")\nplt.xlabel('Epoch #')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.subplot(122)\nplt.plot(history.history['accuracy'], '-r', label=\"Training\")\nplt.plot(history.history['val_accuracy'], '-b', label=\"Validation\")\nplt.xlabel('Epoch #')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.show()"
  },
  {
    "objectID": "exercises/10-CNN-solution.html#analysing-the-cnn",
    "href": "exercises/10-CNN-solution.html#analysing-the-cnn",
    "title": "Convolutional neural networks",
    "section": "Analysing the CNN",
    "text": "Analysing the CNN\nOnce a network has been trained, let’s see what has happened internally.\n\nAccessing trained weights\nEach layer of the network can be addressed individually. For example, model.layers[0] represents the first layer of your network (the first convolutional one, as the input layer does not count). The index of the other layers can be found by looking at the output of model.summary().\nYou can obtain the parameters of each layer (if any) with:\nW = model.layers[0].get_weights()[0]\nQ: Print the shape of these weights and relate them to the network.\n\nW = model.layers[0].get_weights()[0]\nprint(\"W shape : \", W.shape)\n\nW shape :  (3, 3, 1, 32)\n\n\nQ: Visualize with imshow() each of the 16 filters of the first convolutional layer. Interpret what kind of operation they perform on the image.\nHint: subplot() is going to be useful here. If you have 16 images img[i], you can visualize them in a 4x4 grid with:\nfor i in range(16):\n    plt.subplot(4, 4, i+1)\n    plt.imshow(img[i], cmap=plt.cm.gray)\n\nplt.figure(figsize=(12, 12))\nfor i in range(16):\n    plt.subplot(4, 4, i+1)\n    plt.imshow(W[:, :, 0, i], cmap=plt.cm.gray)\n    plt.xticks([]); plt.yticks([])\n    plt.colorbar()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nVisualizing the feature maps\nLet’s take a random image from the training set and visualize it:\n\nidx = 31727 # or any other digit\nx = X_train[idx, :, :, :].reshape(1, 28, 28, 1)\nt = t_train[idx]\n\nprint(t)\n\nplt.figure(figsize=(6, 6))\nplt.imshow(x[0, :, :, 0] + X_mean[:, :, 0], cmap=plt.cm.gray)\nplt.colorbar()\nplt.show()\n\n1\n\n\n\n\n\n\n\n\n\nThis example could be a 1 or 7. That is why you will never get 100% accuracy on MNIST: some examples are hard even for humans…\nQ: Print what the model predict for it, its true label, and visualize the probabilities in the softmax output layer (look at the doc of model.predict()):\n\n# Predict probabilities\noutput = model.predict([x])\n\n# The predicted class has the maximal probability\nprediction = output[0].argmax()\nprint('Predicted digit:', prediction, '; True digit:', t)\n\nplt.figure(figsize=(12, 5))\nplt.bar(range(10), output[0])\nplt.xlabel('Digit')\nplt.ylabel('Probability')\nplt.show()\n\n2022-11-15 10:28:07.144889: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n\n\nPredicted digit: 1 ; True digit: 1\n\n\n\n\n\n\n\n\n\nDepending on how your network converged, you may have the correct prediction or not.\nQ: Visualize the output of the network for different examples. Do these ambiguities happen often?\nNow let’s look inside the network. We will first visualize the 16 feature maps of the first convolutional layer.\nThis is actually very simple using tensorflow 2.x: One only needs to create a new model (class tf.keras.models.Model, not Sequential) taking the same inputs as the original model, but returning the output of the first layer (model.layers[0] is the first convolutional layer of the model, as the input layer does not count):\nmodel_conv = tf.keras.models.Model(inputs=model.inputs, outputs=model.layers[0].output)\nTo get the tensor corresponding to the first convolutional layer, one simply needs to call predict() on the new model:\nfeature_maps = model_conv.predict([x])\nQ: Visualize the 16 feature maps using subplot(). Relate these activation with the filters you have visualized previously.\n\nmodel_conv = tf.keras.models.Model(inputs=model.inputs, outputs=model.layers[0].output)\n\nfeature_maps = model_conv.predict([x])\nprint(feature_maps.shape)\n\nplt.figure(figsize=(12, 12))\nfor i in range(16):\n    plt.subplot(4, 4, i+1)\n    plt.imshow(feature_maps[0, :, :, i], cmap=plt.cm.gray)\n    plt.xticks([]); plt.yticks([])\n    plt.colorbar()\nplt.show()\n\n2022-11-15 10:28:38.895124: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n\n\n(1, 26, 26, 32)\n\n\n\n\n\n\n\n\n\nQ: Do the same with the output of the first max-pooling layer.\nHint: you need to find the index of that layer in model.summary().\n\nmodel_pool = tf.keras.models.Model(inputs=model.inputs, outputs=model.layers[1].output)\n\npooling_maps = model_pool.predict([x])\nprint(pooling_maps.shape)\n\nplt.figure(figsize=(12, 12))\nfor i in range(16):\n    plt.subplot(4, 4, i+1)\n    plt.imshow(pooling_maps[0, :, :, i], cmap=plt.cm.gray)\n    plt.xticks([]); plt.yticks([])\n    plt.colorbar()\nplt.show()\n\n2022-11-15 10:28:50.513222: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n\n\n(1, 13, 13, 32)\n\n\n\n\n\n\n\n\n\nBonus question: if you had several convolutional layers in your network, visualize them too. What do you think of the specificity of some features?\n\nmodel_conv = tf.keras.models.Model(inputs=model.inputs, outputs=model.layers[3].output)\n\nfeature_maps = model_conv.predict([x])\nprint(feature_maps.shape)\n\nplt.figure(figsize=(12, 12))\nfor i in range(16):\n    plt.subplot(4, 4, i+1)\n    plt.imshow(feature_maps[0, :, :, i], cmap=plt.cm.gray)\n    plt.xticks([]); plt.yticks([])\n    plt.colorbar()\nplt.show()\n\n2022-11-15 10:28:59.341295: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n\n\n(1, 11, 11, 64)"
  },
  {
    "objectID": "exercises/11-TransferLearning-solution.html#loading-the-cats-and-dogs-data",
    "href": "exercises/11-TransferLearning-solution.html#loading-the-cats-and-dogs-data",
    "title": "Transfer learning",
    "section": "Loading the cats and dogs data",
    "text": "Loading the cats and dogs data\nThe data we will use is a subset of the “Dogs vs. Cats” dataset dataset available on Kaggle, which contains 25,000 images. Here, we use only 1000 dogs and 1000 cats to decrease training time and make the problem harder.\nThe following cell downloads the data and decompresses it in /tmp/ (it will be erased at the next restart of your computer).\n\n!wget --no-check-certificate \\\n    https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\\n    -O /tmp/cats_and_dogs_filtered.zip\n\nimport os\nimport zipfile\n\nlocal_zip = '/tmp/cats_and_dogs_filtered.zip'\nzip_ref = zipfile.ZipFile(local_zip, 'r')\nzip_ref.extractall('/tmp')\nzip_ref.close()\n\n--2022-11-15 10:29:45--  https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\nResolving storage.googleapis.com (storage.googleapis.com)... 2a00:1450:4005:801::2010, 2a00:1450:4005:80b::2010, 2a00:1450:4005:802::2010, ...\nConnecting to storage.googleapis.com (storage.googleapis.com)|2a00:1450:4005:801::2010|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 68606236 (65M) [application/zip]\nSaving to: ‘/tmp/cats_and_dogs_filtered.zip’\n\n/tmp/cats_and_dogs_ 100%[===================>]  65,43M  5,39MB/s    in 14s     \n\n2022-11-15 10:29:59 (4,61 MB/s) - ‘/tmp/cats_and_dogs_filtered.zip’ saved [68606236/68606236]\n\n\n\nAll we have is a bunch of *.jpg images organized in a training and validation set, separated by their binary class dog vs. cat:\ncats_and_dogs_filtered/\n    train/\n        dogs/\n            dog001.jpg\n            dog002.jpg\n            ...\n        cats/\n            cat001.jpg\n            cat002.jpg\n            ...\n    validation/\n        dogs/\n            dog001.jpg\n            dog002.jpg\n            ...\n        cats/\n            cat001.jpg\n            cat002.jpg\n            ...\nFeel free to download the data on your computer and have a look at the images directly.\nThe next cell checks the structure of the image directory:\n\nbase_dir = '/tmp/cats_and_dogs_filtered'\ntrain_dir = base_dir + '/train'\nvalidation_dir = base_dir + '/validation'\n\nprint('total training cat images:', len(os.listdir(train_dir + '/cats')))\nprint('total training dog images:', len(os.listdir(train_dir + '/dogs')))\nprint('total validation cat images:', len(os.listdir(validation_dir + '/cats')))\nprint('total validation dog images:', len(os.listdir(validation_dir + '/dogs')))\n\ntotal training cat images: 1000\ntotal training dog images: 1000\ntotal validation cat images: 500\ntotal validation dog images: 500\n\n\nThe next cell visualizes some cats and dogs from the training set.\n\nimport matplotlib.image as mpimg\n\nfig = plt.figure(figsize=(16, 16))\n\nidx = rng.choice(1000-8, 1)[0]\n\nnext_cat_pix = [os.path.join(train_dir + '/cats', fname) for fname in os.listdir(train_dir + '/cats')[idx:idx+8]]\nnext_dog_pix = [os.path.join(train_dir + '/dogs', fname) for fname in os.listdir(train_dir + '/dogs')[idx:idx+8]]\n\nfor i, img_path in enumerate(next_cat_pix+next_dog_pix):\n  # Set up subplot; subplot indices start at 1\n  ax = plt.subplot(4, 4, i + 1)\n  ax.axis('Off') # Don't show axes (or gridlines)\n  img = mpimg.imread(img_path)\n  plt.imshow(img)\n\nplt.show()\n\n\n\n\n\n\n\n\nIn order to train a binary classifier on this data, we would need to load the images and transform them into Numpy arrays that can be passed to tensorflow. Fortunately, keras provides an utility to do it automatically: ImageDataGenerator. Doc:\nhttps://keras.io/api/preprocessing/image/\nThe procedure is to create an ImageDataGenerator instance and to create an iterator with flow_from_directory that will return minibatches on demand when training the neural network. The main advantage of this approach is that you do not need to load the whole dataset in the RAM (not possible for most realistic datasets), but adds an overhead between each minibatch.\ndatagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\ngenerator = datagen.flow_from_directory(\n        directory,  # This is the source directory for training images\n        target_size=(150, 150),  # All images will be resized to 150x150\n        batch_size=64,\n        # Since we use binary_crossentropy loss, we need binary labels\n        class_mode='binary')\nThe rescale argument makes sure that the pixels will be represented by float values between 0 and 1, not integers between 0 and 255. Unfortunately, it is not possible (or very hard) to perform mean-removal using this method. The image data generator accepts additional arguments that we will discuss in the section on data augmentation. directory must be set to the folder containing the images. We ask the generator to resize all images to 150x150 and will use a batch size of 64. As there are only two classes cat and dog, the labels will be binary (0 and 1).\nQ: Create two generators train_generator and validation_generator for the training and validation sets respectively, with a batch size of 64.\n\n# All images will be rescaled by 1./255\ntrain_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\nval_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n\n# Flow training images in batches \ntrain_generator = train_datagen.flow_from_directory(\n        train_dir,  \n        target_size=(150, 150),  \n        batch_size=64,\n        class_mode='binary')\n\n# Flow validation images in batches\nvalidation_generator = val_datagen.flow_from_directory(\n        validation_dir,\n        target_size=(150, 150),\n        batch_size=64,\n        class_mode='binary')\n\nFound 2000 images belonging to 2 classes.\nFound 1000 images belonging to 2 classes.\n\n\nQ: Sample a minibatch from the training generator by calling next() on it (X, t = train_generator.next()) and display the first image. Call the cell multiple times.\n\nX, t = train_generator.next()\nprint(X.shape)\n\nplt.figure()\nplt.imshow(X[0, :, :, :])\nplt.axis(\"Off\")\nplt.show()\n\n(64, 150, 150, 3)"
  },
  {
    "objectID": "exercises/11-TransferLearning-solution.html#functional-api-of-keras",
    "href": "exercises/11-TransferLearning-solution.html#functional-api-of-keras",
    "title": "Transfer learning",
    "section": "Functional API of Keras",
    "text": "Functional API of Keras\nIn the previous exercises, we used the Sequential API of keras, which stacks layers on top of each other:\nmodel = tf.keras.Sequential()\n\nmodel.add(tf.keras.layers.Input((28, 28, 1)))\n\nmodel.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu')\nmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(tf.keras.layers.Flatten())\n\nmodel.add(tf.keras.layers.Dense(10, activation='softmax'))\nIn this exercise, we will use the Functional API of keras, which gives much more freedom to the programmer. The main difference is that you can explicitly specify from which layer a layer should take its inputs:\ninputs = tf.keras.layers.Input((28, 28, 1))\n\nx = tf.keras.layers.Conv2D(32, (3, 3), activation='relu')(inputs)\nx = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n\nx = tf.keras.layers.Flatten()(x)\n\noutputs = tf.keras.layers.Dense(10, activation='softmax')(x)\n\nmodel = tf.keras.Model(inputs, outputs)\nThis allows to create complex architectures, for examples with several output layers.\nQ: Modify your CNN of last exercise so that it is defined with the Functional API and train it on MNIST.\n\n# Fetch the MNIST data\n(X_train, t_train), (X_test, t_test) = tf.keras.datasets.mnist.load_data()\nprint(\"Training data:\", X_train.shape, t_train.shape)\nprint(\"Test data:\", X_test.shape, t_test.shape)\n\n# Normalize the values\nX_train = X_train.reshape(-1, 28, 28, 1).astype('float32') / 255.\nX_test = X_test.reshape(-1, 28, 28, 1).astype('float32') / 255.\n\n# Mean removal\nX_mean = np.mean(X_train, axis=0)\nX_train -= X_mean\nX_test -= X_mean\n\n# One-hot encoding\nT_train = tf.keras.utils.to_categorical(t_train, 10)\nT_test = tf.keras.utils.to_categorical(t_test, 10)\n\n# Delete all previous models to free memory\ntf.keras.backend.clear_session()\n\n# Functional model\ninputs = tf.keras.layers.Input((28, 28, 1))\n\nx = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='valid')(inputs)\nx = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\nx = tf.keras.layers.Dropout(0.5)(x)\n\nx = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='valid')(x)\nx = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\nx = tf.keras.layers.Dropout(0.5)(x)\n\nx = tf.keras.layers.Flatten()(x)\n\nx = tf.keras.layers.Dense(150, activation='relu')(x)\nx = tf.keras.layers.Dropout(0.5)(x)\n\noutputs = tf.keras.layers.Dense(10, activation='softmax')(x)\n\nmodel = tf.keras.Model(inputs, outputs)\n\n# Learning rule\noptimizer = tf.keras.optimizers.SGD(learning_rate=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n\n# Loss function\nmodel.compile(\n    loss='categorical_crossentropy', # loss\n    optimizer=optimizer, # learning rule\n    metrics=['accuracy'] # show accuracy\n)\n\nprint(model.summary())\n\nhistory = tf.keras.callbacks.History()\n\nmodel.fit(\n    X_train, T_train,\n    batch_size=64, \n    epochs=20,\n    validation_split=0.1,\n    callbacks=[history]\n)\n\nscore = model.evaluate(X_test, T_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n\nplt.figure(figsize=(15, 6))\n\nplt.subplot(121)\nplt.plot(history.history['loss'], '-r', label=\"Training\")\nplt.plot(history.history['val_loss'], '-b', label=\"Validation\")\nplt.xlabel('Epoch #')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.subplot(122)\nplt.plot(history.history['accuracy'], '-r', label=\"Training\")\nplt.plot(history.history['val_accuracy'], '-b', label=\"Validation\")\nplt.xlabel('Epoch #')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.show()\n\nTraining data: (60000, 28, 28) (60000,)\nTest data: (10000, 28, 28) (10000,)\n\n\n2022-11-15 10:30:50.851406: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n2022-11-15 10:30:50.851519: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n\n\nMetal device set to: Apple M1 Pro\n\nsystemMemory: 16.00 GB\nmaxCacheSize: 5.33 GB\n\nModel: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 28, 28, 1)]       0         \n                                                                 \n conv2d (Conv2D)             (None, 26, 26, 32)        320       \n                                                                 \n max_pooling2d (MaxPooling2D  (None, 13, 13, 32)       0         \n )                                                               \n                                                                 \n dropout (Dropout)           (None, 13, 13, 32)        0         \n                                                                 \n conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     \n                                                                 \n max_pooling2d_1 (MaxPooling  (None, 5, 5, 64)         0         \n 2D)                                                             \n                                                                 \n dropout_1 (Dropout)         (None, 5, 5, 64)          0         \n                                                                 \n flatten (Flatten)           (None, 1600)              0         \n                                                                 \n dense (Dense)               (None, 150)               240150    \n                                                                 \n dropout_2 (Dropout)         (None, 150)               0         \n                                                                 \n dense_1 (Dense)             (None, 10)                1510      \n                                                                 \n=================================================================\nTotal params: 260,476\nTrainable params: 260,476\nNon-trainable params: 0\n_________________________________________________________________\nNone\nEpoch 1/20\n\n\n2022-11-15 10:30:51.651254: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n2022-11-15 10:30:51.888660: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n\n\n844/844 [==============================] - ETA: 0s - loss: 0.4516 - accuracy: 0.8523\n\n\n2022-11-15 10:31:08.543351: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n\n\n844/844 [==============================] - 18s 17ms/step - loss: 0.4516 - accuracy: 0.8523 - val_loss: 0.0851 - val_accuracy: 0.9770\nEpoch 2/20\n844/844 [==============================] - 14s 17ms/step - loss: 0.1421 - accuracy: 0.9561 - val_loss: 0.0598 - val_accuracy: 0.9837\nEpoch 3/20\n844/844 [==============================] - 13s 16ms/step - loss: 0.1043 - accuracy: 0.9677 - val_loss: 0.0517 - val_accuracy: 0.9858\nEpoch 4/20\n844/844 [==============================] - 13s 16ms/step - loss: 0.0914 - accuracy: 0.9725 - val_loss: 0.0498 - val_accuracy: 0.9853\nEpoch 5/20\n844/844 [==============================] - 14s 16ms/step - loss: 0.0796 - accuracy: 0.9744 - val_loss: 0.0413 - val_accuracy: 0.9873\nEpoch 6/20\n844/844 [==============================] - 13s 15ms/step - loss: 0.0718 - accuracy: 0.9773 - val_loss: 0.0385 - val_accuracy: 0.9892\nEpoch 7/20\n844/844 [==============================] - 13s 16ms/step - loss: 0.0659 - accuracy: 0.9793 - val_loss: 0.0346 - val_accuracy: 0.9900\nEpoch 8/20\n844/844 [==============================] - 13s 15ms/step - loss: 0.0586 - accuracy: 0.9816 - val_loss: 0.0328 - val_accuracy: 0.9910\nEpoch 9/20\n844/844 [==============================] - 13s 16ms/step - loss: 0.0579 - accuracy: 0.9820 - val_loss: 0.0325 - val_accuracy: 0.9907\nEpoch 10/20\n844/844 [==============================] - 13s 15ms/step - loss: 0.0547 - accuracy: 0.9829 - val_loss: 0.0329 - val_accuracy: 0.9905\nEpoch 11/20\n844/844 [==============================] - 13s 15ms/step - loss: 0.0526 - accuracy: 0.9835 - val_loss: 0.0323 - val_accuracy: 0.9908\nEpoch 12/20\n844/844 [==============================] - 13s 15ms/step - loss: 0.0512 - accuracy: 0.9841 - val_loss: 0.0303 - val_accuracy: 0.9913\nEpoch 13/20\n844/844 [==============================] - 13s 15ms/step - loss: 0.0473 - accuracy: 0.9850 - val_loss: 0.0305 - val_accuracy: 0.9902\nEpoch 14/20\n844/844 [==============================] - 13s 15ms/step - loss: 0.0475 - accuracy: 0.9859 - val_loss: 0.0308 - val_accuracy: 0.9908\nEpoch 15/20\n844/844 [==============================] - 13s 15ms/step - loss: 0.0441 - accuracy: 0.9860 - val_loss: 0.0329 - val_accuracy: 0.9908\nEpoch 16/20\n844/844 [==============================] - 13s 15ms/step - loss: 0.0421 - accuracy: 0.9870 - val_loss: 0.0292 - val_accuracy: 0.9912\nEpoch 17/20\n844/844 [==============================] - 13s 15ms/step - loss: 0.0399 - accuracy: 0.9871 - val_loss: 0.0303 - val_accuracy: 0.9915\nEpoch 18/20\n844/844 [==============================] - 13s 15ms/step - loss: 0.0407 - accuracy: 0.9875 - val_loss: 0.0288 - val_accuracy: 0.9923\nEpoch 19/20\n844/844 [==============================] - 13s 15ms/step - loss: 0.0371 - accuracy: 0.9882 - val_loss: 0.0274 - val_accuracy: 0.9925\nEpoch 20/20\n844/844 [==============================] - 13s 15ms/step - loss: 0.0367 - accuracy: 0.9880 - val_loss: 0.0269 - val_accuracy: 0.9915\nTest loss: 0.022337375208735466\nTest accuracy: 0.9923000335693359"
  },
  {
    "objectID": "exercises/11-TransferLearning-solution.html#training-a-cnn-from-scratch",
    "href": "exercises/11-TransferLearning-solution.html#training-a-cnn-from-scratch",
    "title": "Transfer learning",
    "section": "Training a CNN from scratch",
    "text": "Training a CNN from scratch\nLet’s now train a randomly-initialized CNN on the dog vs. cat data. You are free to choose any architecture you like, the only requirements are:\n\nThe input image must be 150x150x3:\n\ntf.keras.layers.Input(shape=(150, 150, 3))\n\nThe output neuron must use the logistic/sigmoid activation function (binary classification:\n\ntf.keras.layers.Dense(1, activation='sigmoid')\n\nThe loss function must be 'binary_crossentropy' and the metric binary_accuracy:\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer=optimizer,\n              metrics=['binary_accuracy'])\nThere is not a lot of data, so you can safely go deep with your architecture (i.e. with convolutional layers and max-pooling until the image dimensions are around 7x7), especially if you use the GPU on Colab.\nTo train and validate the network on the generators, just pass them to model.fit():\nmodel.fit(\n      train_generator,\n      epochs=20,\n      validation_data=validation_generator,\n      callbacks=[history])\nQ: Design a CNN and train it on the data for 30 epochs. A final validation accuracy around 72% - 75% is already good, you can then go to the next question.\n\ndef random_model():\n    # Delete all previous models to free memory\n    tf.keras.backend.clear_session()\n\n    # Our input feature map is 150x150x3: 150x150 for the image pixels, and 3 for\n    # the three color channels: R, G, and B\n    inputs = tf.keras.layers.Input(shape=(150, 150, 3))\n\n    # First convolution extracts 16 filters that are 3x3\n    # Convolution is followed by max-pooling layer with a 2x2 window\n    x = tf.keras.layers.Conv2D(16, 3)(inputs)\n    x = tf.keras.layers.Activation(\"relu\")(x)\n    x = tf.keras.layers.MaxPooling2D(2)(x)\n    x = tf.keras.layers.Dropout(0.2)(x)\n\n    # Second convolution extracts 32 filters that are 3x3\n    # Convolution is followed by max-pooling layer with a 2x2 window\n    x = tf.keras.layers.Conv2D(32, 3)(x)\n    x = tf.keras.layers.Activation(\"relu\")(x)\n    x = tf.keras.layers.MaxPooling2D(2)(x)\n    x = tf.keras.layers.Dropout(0.2)(x)\n\n    # Third convolution extracts 64 filters that are 3x3\n    # Convolution is followed by max-pooling layer with a 2x2 window\n    x = tf.keras.layers.Conv2D(64, 3)(x)\n    x = tf.keras.layers.Activation(\"relu\")(x)\n    x = tf.keras.layers.MaxPooling2D(2)(x)\n    x = tf.keras.layers.Dropout(0.2)(x)\n\n    # Fourth convolution extracts 64 filters that are 3x3\n    # Convolution is followed by max-pooling layer with a 2x2 window\n    x = tf.keras.layers.Conv2D(64, 3)(x)\n    x = tf.keras.layers.Activation(\"relu\")(x)\n    x = tf.keras.layers.MaxPooling2D(2)(x)\n    x = tf.keras.layers.Dropout(0.2)(x)\n\n    # Flatten feature map to a 1-dim tensor so we can add fully connected layers\n    x = tf.keras.layers.Flatten()(x)\n\n    # Create a fully connected layer with ReLU activation and 512 hidden units\n    x = tf.keras.layers.Dense(512, activation='relu')(x)\n    x = tf.keras.layers.Dropout(0.2)(x)\n\n    # Create output layer with a single node and sigmoid activation\n    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n\n    # Create model:\n    # input = input feature map\n    # output = input feature map + stacked convolution/maxpooling layers + fully \n    # connected layer + sigmoid output layer\n    model = tf.keras.Model(inputs, outputs)\n\n    # Compile model for binary classification\n    model.compile(loss='binary_crossentropy',\n                optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n                metrics=['binary_accuracy'])\n\n    print(model.summary())\n\n    return model\n\n\nmodel = random_model()\n\nhistory = tf.keras.callbacks.History()\nmodel.fit(\n      train_generator,\n      epochs=30,\n      validation_data=validation_generator,\n      callbacks=[history])\n\nplt.figure(figsize=(15, 6))\n\nplt.subplot(121)\nplt.plot(history.history['loss'], '-r', label=\"Training\")\nplt.plot(history.history['val_loss'], '-b', label=\"Validation\")\nplt.xlabel('Epoch #')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.subplot(122)\nplt.plot(history.history['binary_accuracy'], '-r', label=\"Training\")\nplt.plot(history.history['val_binary_accuracy'], '-b', label=\"Validation\")\nplt.xlabel('Epoch #')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.show()\n\nModel: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 150, 150, 3)]     0         \n                                                                 \n conv2d (Conv2D)             (None, 148, 148, 16)      448       \n                                                                 \n activation (Activation)     (None, 148, 148, 16)      0         \n                                                                 \n max_pooling2d (MaxPooling2D  (None, 74, 74, 16)       0         \n )                                                               \n                                                                 \n dropout (Dropout)           (None, 74, 74, 16)        0         \n                                                                 \n conv2d_1 (Conv2D)           (None, 72, 72, 32)        4640      \n                                                                 \n activation_1 (Activation)   (None, 72, 72, 32)        0         \n                                                                 \n max_pooling2d_1 (MaxPooling  (None, 36, 36, 32)       0         \n 2D)                                                             \n                                                                 \n dropout_1 (Dropout)         (None, 36, 36, 32)        0         \n                                                                 \n conv2d_2 (Conv2D)           (None, 34, 34, 64)        18496     \n                                                                 \n activation_2 (Activation)   (None, 34, 34, 64)        0         \n                                                                 \n max_pooling2d_2 (MaxPooling  (None, 17, 17, 64)       0         \n 2D)                                                             \n                                                                 \n dropout_2 (Dropout)         (None, 17, 17, 64)        0         \n                                                                 \n conv2d_3 (Conv2D)           (None, 15, 15, 64)        36928     \n                                                                 \n activation_3 (Activation)   (None, 15, 15, 64)        0         \n                                                                 \n max_pooling2d_3 (MaxPooling  (None, 7, 7, 64)         0         \n 2D)                                                             \n                                                                 \n dropout_3 (Dropout)         (None, 7, 7, 64)          0         \n                                                                 \n flatten (Flatten)           (None, 3136)              0         \n                                                                 \n dense (Dense)               (None, 512)               1606144   \n                                                                 \n dropout_4 (Dropout)         (None, 512)               0         \n                                                                 \n dense_1 (Dense)             (None, 1)                 513       \n                                                                 \n=================================================================\nTotal params: 1,667,169\nTrainable params: 1,667,169\nNon-trainable params: 0\n_________________________________________________________________\nNone\nEpoch 1/30\n\n\n2022-11-15 10:35:37.036309: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n\n\n32/32 [==============================] - ETA: 0s - loss: 0.7086 - binary_accuracy: 0.5200\n\n\n2022-11-15 10:35:42.569436: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n\n\n32/32 [==============================] - 8s 170ms/step - loss: 0.7086 - binary_accuracy: 0.5200 - val_loss: 0.6926 - val_binary_accuracy: 0.5100\nEpoch 2/30\n32/32 [==============================] - 4s 134ms/step - loss: 0.6920 - binary_accuracy: 0.5240 - val_loss: 0.6893 - val_binary_accuracy: 0.5560\nEpoch 3/30\n32/32 [==============================] - 4s 126ms/step - loss: 0.6926 - binary_accuracy: 0.5100 - val_loss: 0.6926 - val_binary_accuracy: 0.5000\nEpoch 4/30\n32/32 [==============================] - 4s 127ms/step - loss: 0.6837 - binary_accuracy: 0.5505 - val_loss: 0.6865 - val_binary_accuracy: 0.5550\nEpoch 5/30\n32/32 [==============================] - 4s 128ms/step - loss: 0.6594 - binary_accuracy: 0.5990 - val_loss: 0.6629 - val_binary_accuracy: 0.6010\nEpoch 6/30\n32/32 [==============================] - 4s 129ms/step - loss: 0.6442 - binary_accuracy: 0.6170 - val_loss: 0.6575 - val_binary_accuracy: 0.6000\nEpoch 7/30\n32/32 [==============================] - 4s 138ms/step - loss: 0.6057 - binary_accuracy: 0.6555 - val_loss: 0.6330 - val_binary_accuracy: 0.6100\nEpoch 8/30\n32/32 [==============================] - 4s 131ms/step - loss: 0.5850 - binary_accuracy: 0.6940 - val_loss: 0.6408 - val_binary_accuracy: 0.6100\nEpoch 9/30\n32/32 [==============================] - 4s 127ms/step - loss: 0.5491 - binary_accuracy: 0.7150 - val_loss: 0.6049 - val_binary_accuracy: 0.6550\nEpoch 10/30\n32/32 [==============================] - 4s 123ms/step - loss: 0.5261 - binary_accuracy: 0.7330 - val_loss: 0.5941 - val_binary_accuracy: 0.6720\nEpoch 11/30\n32/32 [==============================] - 4s 134ms/step - loss: 0.5159 - binary_accuracy: 0.7370 - val_loss: 0.5622 - val_binary_accuracy: 0.6960\nEpoch 12/30\n32/32 [==============================] - 4s 130ms/step - loss: 0.4882 - binary_accuracy: 0.7560 - val_loss: 0.5616 - val_binary_accuracy: 0.6980\nEpoch 13/30\n32/32 [==============================] - 4s 133ms/step - loss: 0.4752 - binary_accuracy: 0.7755 - val_loss: 0.5695 - val_binary_accuracy: 0.6820\nEpoch 14/30\n32/32 [==============================] - 4s 123ms/step - loss: 0.4569 - binary_accuracy: 0.7865 - val_loss: 0.6656 - val_binary_accuracy: 0.6430\nEpoch 15/30\n32/32 [==============================] - 4s 123ms/step - loss: 0.4456 - binary_accuracy: 0.7885 - val_loss: 0.6187 - val_binary_accuracy: 0.6720\nEpoch 16/30\n32/32 [==============================] - 4s 122ms/step - loss: 0.4379 - binary_accuracy: 0.7870 - val_loss: 0.5857 - val_binary_accuracy: 0.6860\nEpoch 17/30\n32/32 [==============================] - 4s 123ms/step - loss: 0.3996 - binary_accuracy: 0.8130 - val_loss: 0.5482 - val_binary_accuracy: 0.7070\nEpoch 18/30\n32/32 [==============================] - 4s 123ms/step - loss: 0.3852 - binary_accuracy: 0.8290 - val_loss: 0.5514 - val_binary_accuracy: 0.7250\nEpoch 19/30\n32/32 [==============================] - 4s 124ms/step - loss: 0.3729 - binary_accuracy: 0.8320 - val_loss: 0.5720 - val_binary_accuracy: 0.7100\nEpoch 20/30\n32/32 [==============================] - 4s 123ms/step - loss: 0.3229 - binary_accuracy: 0.8515 - val_loss: 0.5621 - val_binary_accuracy: 0.7310\nEpoch 21/30\n32/32 [==============================] - 4s 125ms/step - loss: 0.3205 - binary_accuracy: 0.8560 - val_loss: 0.6413 - val_binary_accuracy: 0.6870\nEpoch 22/30\n32/32 [==============================] - 4s 123ms/step - loss: 0.2989 - binary_accuracy: 0.8635 - val_loss: 0.5969 - val_binary_accuracy: 0.7190\nEpoch 23/30\n32/32 [==============================] - 4s 125ms/step - loss: 0.2685 - binary_accuracy: 0.8870 - val_loss: 0.5636 - val_binary_accuracy: 0.7370\nEpoch 24/30\n32/32 [==============================] - 4s 123ms/step - loss: 0.2532 - binary_accuracy: 0.8925 - val_loss: 0.5702 - val_binary_accuracy: 0.7310\nEpoch 25/30\n32/32 [==============================] - 4s 123ms/step - loss: 0.2554 - binary_accuracy: 0.8980 - val_loss: 0.5543 - val_binary_accuracy: 0.7420\nEpoch 26/30\n32/32 [==============================] - 4s 123ms/step - loss: 0.2184 - binary_accuracy: 0.9155 - val_loss: 0.5953 - val_binary_accuracy: 0.7370\nEpoch 27/30\n32/32 [==============================] - 4s 126ms/step - loss: 0.1950 - binary_accuracy: 0.9195 - val_loss: 0.6326 - val_binary_accuracy: 0.7410\nEpoch 28/30\n32/32 [==============================] - 4s 127ms/step - loss: 0.1646 - binary_accuracy: 0.9335 - val_loss: 0.6373 - val_binary_accuracy: 0.7480\nEpoch 29/30\n32/32 [==============================] - 4s 123ms/step - loss: 0.1419 - binary_accuracy: 0.9495 - val_loss: 0.6665 - val_binary_accuracy: 0.7430\nEpoch 30/30\n32/32 [==============================] - 4s 124ms/step - loss: 0.1276 - binary_accuracy: 0.9535 - val_loss: 0.6939 - val_binary_accuracy: 0.7400\n\n\n\n\n\n\n\n\n\nA: There is no unique solution, but it is very difficult to avoid overfitting with such a low amount of data. The validation accuracy saturates between 70% and 75% while the training accuracy reaches 100% if you train for more epochs."
  },
  {
    "objectID": "exercises/11-TransferLearning-solution.html#data-augmentation",
    "href": "exercises/11-TransferLearning-solution.html#data-augmentation",
    "title": "Transfer learning",
    "section": "Data augmentation",
    "text": "Data augmentation\nThe 2000 training images will never be enough to train a CNN from scratch without overfitting, no matter how much regularization you use. A first trick that may help is data augmentation, i.e. to artificially create variations of each training image (translation, rotation, scaling, flipping, etc) while preserving the class of the images (a cat stays a cat after rotating the image).\nImageDataGenerator allows to automatically apply various transformations when retrieving a minibatch (beware, it can be slow).\ndatagen = tf.keras.preprocessing.image.ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=40,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest'\n)\nRefer the doc for the meaning of the parameters.\nTo investigate the transformations, let’s apply them on a single image, for example the first cat of the training set:\nimg = tf.keras.preprocessing.image.load_img('/tmp/cats_and_dogs_filtered/train/cats/cat.0.jpg')\nimg = tf.keras.preprocessing.image.img_to_array(img)  \nimg = img.reshape((1,) + img.shape)\nWe can pass this image to the data generator and retrieve minibatches of augmented images:\ngenerator = datagen.flow(img, batch_size=1)\naugmented = generator.next()\nQ: Display various augmented images. Vary the parameters individually by setting all but one to their default value in order to understand their effect.\n\ntest_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=40,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest'\n)\n\nimg = tf.keras.preprocessing.image.load_img('/tmp/cats_and_dogs_filtered/train/cats/cat.0.jpg')\nimg = tf.keras.preprocessing.image.img_to_array(img)  \nimg = img.reshape((1,) + img.shape)\n\ntest_generator = test_datagen.flow(img, batch_size=1)\naugmented = test_generator.next()\n\nplt.figure(figsize=(8, 8))\nplt.imshow(img[0, :, :, :]/255.)\nplt.axis(\"Off\")\nplt.title(\"Original\")\nplt.figure(figsize=(8, 8))\nplt.imshow(augmented[0, :, :, :])\nplt.axis(\"Off\")\nplt.title(\"Augmented\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQ: Create an augmented training set using the parameters defined in the previous question (feel free to experiment, but that can cost time). Leave the validation generator without data augmentation (only rescale=1./255). Train the exact same network as before on this augmented data. What happens? You may need to train much longer in order to see the effect.\n\n# Data augmentation\naugmented_train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=40,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest'\n)\n\n# Flow training images in batches of 20 using train_datagen generator\naugmented_train_generator = augmented_train_datagen.flow_from_directory(\n        train_dir, \n        target_size=(150, 150),  \n        batch_size=64,\n        class_mode='binary')\n\nFound 2000 images belonging to 2 classes.\n\n\n\nmodel = random_model()\n\nhistory = tf.keras.callbacks.History()\nmodel.fit(\n      augmented_train_generator,\n      epochs=100,\n      validation_data=validation_generator,\n      callbacks=[history])\n\nplt.figure(figsize=(15, 6))\n\nplt.subplot(121)\nplt.plot(history.history['loss'], '-r', label=\"Training\")\nplt.plot(history.history['val_loss'], '-b', label=\"Validation\")\nplt.xlabel('Epoch #')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.subplot(122)\nplt.plot(history.history['binary_accuracy'], '-r', label=\"Training\")\nplt.plot(history.history['val_binary_accuracy'], '-b', label=\"Validation\")\nplt.xlabel('Epoch #')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.show()\n\nModel: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 150, 150, 3)]     0         \n                                                                 \n conv2d (Conv2D)             (None, 148, 148, 16)      448       \n                                                                 \n activation (Activation)     (None, 148, 148, 16)      0         \n                                                                 \n max_pooling2d (MaxPooling2D  (None, 74, 74, 16)       0         \n )                                                               \n                                                                 \n dropout (Dropout)           (None, 74, 74, 16)        0         \n                                                                 \n conv2d_1 (Conv2D)           (None, 72, 72, 32)        4640      \n                                                                 \n activation_1 (Activation)   (None, 72, 72, 32)        0         \n                                                                 \n max_pooling2d_1 (MaxPooling  (None, 36, 36, 32)       0         \n 2D)                                                             \n                                                                 \n dropout_1 (Dropout)         (None, 36, 36, 32)        0         \n                                                                 \n conv2d_2 (Conv2D)           (None, 34, 34, 64)        18496     \n                                                                 \n activation_2 (Activation)   (None, 34, 34, 64)        0         \n                                                                 \n max_pooling2d_2 (MaxPooling  (None, 17, 17, 64)       0         \n 2D)                                                             \n                                                                 \n dropout_2 (Dropout)         (None, 17, 17, 64)        0         \n                                                                 \n conv2d_3 (Conv2D)           (None, 15, 15, 64)        36928     \n                                                                 \n activation_3 (Activation)   (None, 15, 15, 64)        0         \n                                                                 \n max_pooling2d_3 (MaxPooling  (None, 7, 7, 64)         0         \n 2D)                                                             \n                                                                 \n dropout_3 (Dropout)         (None, 7, 7, 64)          0         \n                                                                 \n flatten (Flatten)           (None, 3136)              0         \n                                                                 \n dense (Dense)               (None, 512)               1606144   \n                                                                 \n dropout_4 (Dropout)         (None, 512)               0         \n                                                                 \n dense_1 (Dense)             (None, 1)                 513       \n                                                                 \n=================================================================\nTotal params: 1,667,169\nTrainable params: 1,667,169\nNon-trainable params: 0\n_________________________________________________________________\nNone\nEpoch 1/100\n\n\n2022-11-15 10:37:55.950584: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n\n\n32/32 [==============================] - ETA: 0s - loss: 0.6987 - binary_accuracy: 0.5190\n\n\n2022-11-15 10:38:03.512980: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n\n\n32/32 [==============================] - 9s 276ms/step - loss: 0.6987 - binary_accuracy: 0.5190 - val_loss: 0.6914 - val_binary_accuracy: 0.5920\nEpoch 2/100\n32/32 [==============================] - 9s 275ms/step - loss: 0.6870 - binary_accuracy: 0.5295 - val_loss: 0.6747 - val_binary_accuracy: 0.5940\nEpoch 3/100\n32/32 [==============================] - 9s 273ms/step - loss: 0.6677 - binary_accuracy: 0.5645 - val_loss: 0.6702 - val_binary_accuracy: 0.5680\nEpoch 4/100\n32/32 [==============================] - 9s 270ms/step - loss: 0.6601 - binary_accuracy: 0.6015 - val_loss: 0.6424 - val_binary_accuracy: 0.6300\nEpoch 5/100\n32/32 [==============================] - 9s 276ms/step - loss: 0.6520 - binary_accuracy: 0.5960 - val_loss: 0.6468 - val_binary_accuracy: 0.6030\nEpoch 6/100\n32/32 [==============================] - 9s 268ms/step - loss: 0.6399 - binary_accuracy: 0.6325 - val_loss: 0.6120 - val_binary_accuracy: 0.6630\nEpoch 7/100\n32/32 [==============================] - 8s 265ms/step - loss: 0.6410 - binary_accuracy: 0.6170 - val_loss: 0.6188 - val_binary_accuracy: 0.6490\nEpoch 8/100\n32/32 [==============================] - 9s 271ms/step - loss: 0.6234 - binary_accuracy: 0.6585 - val_loss: 0.6012 - val_binary_accuracy: 0.6530\nEpoch 9/100\n32/32 [==============================] - 9s 268ms/step - loss: 0.6183 - binary_accuracy: 0.6540 - val_loss: 0.6085 - val_binary_accuracy: 0.6480\nEpoch 10/100\n32/32 [==============================] - 9s 270ms/step - loss: 0.6095 - binary_accuracy: 0.6510 - val_loss: 0.6147 - val_binary_accuracy: 0.6450\nEpoch 11/100\n32/32 [==============================] - 9s 267ms/step - loss: 0.6000 - binary_accuracy: 0.6600 - val_loss: 0.6377 - val_binary_accuracy: 0.6330\nEpoch 12/100\n32/32 [==============================] - 8s 262ms/step - loss: 0.6090 - binary_accuracy: 0.6575 - val_loss: 0.6048 - val_binary_accuracy: 0.6510\nEpoch 13/100\n32/32 [==============================] - 9s 268ms/step - loss: 0.5895 - binary_accuracy: 0.6760 - val_loss: 0.5961 - val_binary_accuracy: 0.6740\nEpoch 14/100\n32/32 [==============================] - 8s 264ms/step - loss: 0.5822 - binary_accuracy: 0.6835 - val_loss: 0.6069 - val_binary_accuracy: 0.6560\nEpoch 15/100\n32/32 [==============================] - 9s 266ms/step - loss: 0.5749 - binary_accuracy: 0.7015 - val_loss: 0.5993 - val_binary_accuracy: 0.6740\nEpoch 16/100\n32/32 [==============================] - 9s 267ms/step - loss: 0.5894 - binary_accuracy: 0.6765 - val_loss: 0.5944 - val_binary_accuracy: 0.6620\nEpoch 17/100\n32/32 [==============================] - 8s 265ms/step - loss: 0.5900 - binary_accuracy: 0.6705 - val_loss: 0.5792 - val_binary_accuracy: 0.6860\nEpoch 18/100\n32/32 [==============================] - 8s 265ms/step - loss: 0.5741 - binary_accuracy: 0.6885 - val_loss: 0.6015 - val_binary_accuracy: 0.6640\nEpoch 19/100\n32/32 [==============================] - 8s 260ms/step - loss: 0.5642 - binary_accuracy: 0.7045 - val_loss: 0.5731 - val_binary_accuracy: 0.7000\nEpoch 20/100\n32/32 [==============================] - 8s 264ms/step - loss: 0.5609 - binary_accuracy: 0.7025 - val_loss: 0.6254 - val_binary_accuracy: 0.6730\nEpoch 21/100\n32/32 [==============================] - 8s 260ms/step - loss: 0.5781 - binary_accuracy: 0.7020 - val_loss: 0.5878 - val_binary_accuracy: 0.6860\nEpoch 22/100\n32/32 [==============================] - 8s 257ms/step - loss: 0.5531 - binary_accuracy: 0.7020 - val_loss: 0.5665 - val_binary_accuracy: 0.7120\nEpoch 23/100\n32/32 [==============================] - 8s 259ms/step - loss: 0.5498 - binary_accuracy: 0.7170 - val_loss: 0.5854 - val_binary_accuracy: 0.6870\nEpoch 24/100\n32/32 [==============================] - 8s 262ms/step - loss: 0.5519 - binary_accuracy: 0.7105 - val_loss: 0.5744 - val_binary_accuracy: 0.6960\nEpoch 25/100\n32/32 [==============================] - 8s 262ms/step - loss: 0.5455 - binary_accuracy: 0.7210 - val_loss: 0.6396 - val_binary_accuracy: 0.6670\nEpoch 26/100\n32/32 [==============================] - 8s 258ms/step - loss: 0.5536 - binary_accuracy: 0.7110 - val_loss: 0.5603 - val_binary_accuracy: 0.6920\nEpoch 27/100\n32/32 [==============================] - 8s 258ms/step - loss: 0.5476 - binary_accuracy: 0.7175 - val_loss: 0.6090 - val_binary_accuracy: 0.6820\nEpoch 28/100\n32/32 [==============================] - 8s 259ms/step - loss: 0.5404 - binary_accuracy: 0.7225 - val_loss: 0.5681 - val_binary_accuracy: 0.7130\nEpoch 29/100\n32/32 [==============================] - 8s 258ms/step - loss: 0.5247 - binary_accuracy: 0.7335 - val_loss: 0.6184 - val_binary_accuracy: 0.6640\nEpoch 30/100\n32/32 [==============================] - 8s 257ms/step - loss: 0.5264 - binary_accuracy: 0.7450 - val_loss: 0.5810 - val_binary_accuracy: 0.7010\nEpoch 31/100\n32/32 [==============================] - 8s 260ms/step - loss: 0.5124 - binary_accuracy: 0.7495 - val_loss: 0.6055 - val_binary_accuracy: 0.6810\nEpoch 32/100\n32/32 [==============================] - 8s 265ms/step - loss: 0.5537 - binary_accuracy: 0.7085 - val_loss: 0.5449 - val_binary_accuracy: 0.7130\nEpoch 33/100\n32/32 [==============================] - 9s 266ms/step - loss: 0.5221 - binary_accuracy: 0.7445 - val_loss: 0.6216 - val_binary_accuracy: 0.6520\nEpoch 34/100\n32/32 [==============================] - 8s 262ms/step - loss: 0.5175 - binary_accuracy: 0.7400 - val_loss: 0.5457 - val_binary_accuracy: 0.7080\nEpoch 35/100\n32/32 [==============================] - 9s 264ms/step - loss: 0.5224 - binary_accuracy: 0.7430 - val_loss: 0.5182 - val_binary_accuracy: 0.7410\nEpoch 36/100\n32/32 [==============================] - 9s 266ms/step - loss: 0.5141 - binary_accuracy: 0.7435 - val_loss: 0.5616 - val_binary_accuracy: 0.7000\nEpoch 37/100\n32/32 [==============================] - 9s 269ms/step - loss: 0.5052 - binary_accuracy: 0.7520 - val_loss: 0.5460 - val_binary_accuracy: 0.7200\nEpoch 38/100\n32/32 [==============================] - 110s 4s/step - loss: 0.5164 - binary_accuracy: 0.7485 - val_loss: 0.6408 - val_binary_accuracy: 0.6360\nEpoch 39/100\n32/32 [==============================] - 8s 252ms/step - loss: 0.5176 - binary_accuracy: 0.7415 - val_loss: 0.6007 - val_binary_accuracy: 0.6900\nEpoch 40/100\n32/32 [==============================] - 8s 250ms/step - loss: 0.5006 - binary_accuracy: 0.7545 - val_loss: 0.5449 - val_binary_accuracy: 0.7070\nEpoch 41/100\n32/32 [==============================] - 8s 253ms/step - loss: 0.4983 - binary_accuracy: 0.7655 - val_loss: 0.5754 - val_binary_accuracy: 0.6920\nEpoch 42/100\n32/32 [==============================] - 8s 257ms/step - loss: 0.4989 - binary_accuracy: 0.7565 - val_loss: 0.5492 - val_binary_accuracy: 0.7220\nEpoch 43/100\n32/32 [==============================] - 8s 250ms/step - loss: 0.4948 - binary_accuracy: 0.7620 - val_loss: 0.5719 - val_binary_accuracy: 0.7140\nEpoch 44/100\n32/32 [==============================] - 8s 249ms/step - loss: 0.4802 - binary_accuracy: 0.7655 - val_loss: 0.6214 - val_binary_accuracy: 0.7000\nEpoch 45/100\n32/32 [==============================] - 8s 246ms/step - loss: 0.4960 - binary_accuracy: 0.7595 - val_loss: 0.5552 - val_binary_accuracy: 0.7130\nEpoch 46/100\n32/32 [==============================] - 8s 248ms/step - loss: 0.4943 - binary_accuracy: 0.7600 - val_loss: 0.5689 - val_binary_accuracy: 0.7110\nEpoch 47/100\n32/32 [==============================] - 9s 275ms/step - loss: 0.4898 - binary_accuracy: 0.7560 - val_loss: 0.6013 - val_binary_accuracy: 0.7090\nEpoch 48/100\n32/32 [==============================] - 11s 342ms/step - loss: 0.4824 - binary_accuracy: 0.7690 - val_loss: 0.4911 - val_binary_accuracy: 0.7610\nEpoch 49/100\n32/32 [==============================] - 10s 309ms/step - loss: 0.4827 - binary_accuracy: 0.7635 - val_loss: 0.5036 - val_binary_accuracy: 0.7410\nEpoch 50/100\n32/32 [==============================] - 10s 312ms/step - loss: 0.4741 - binary_accuracy: 0.7620 - val_loss: 0.5096 - val_binary_accuracy: 0.7430\nEpoch 51/100\n32/32 [==============================] - 10s 296ms/step - loss: 0.4653 - binary_accuracy: 0.7810 - val_loss: 0.5334 - val_binary_accuracy: 0.7310\nEpoch 52/100\n32/32 [==============================] - 9s 294ms/step - loss: 0.4807 - binary_accuracy: 0.7645 - val_loss: 0.5092 - val_binary_accuracy: 0.7490\nEpoch 53/100\n32/32 [==============================] - 10s 304ms/step - loss: 0.4642 - binary_accuracy: 0.7790 - val_loss: 0.5362 - val_binary_accuracy: 0.7410\nEpoch 54/100\n32/32 [==============================] - 9s 275ms/step - loss: 0.4573 - binary_accuracy: 0.7815 - val_loss: 0.5207 - val_binary_accuracy: 0.7300\nEpoch 55/100\n32/32 [==============================] - 8s 261ms/step - loss: 0.4654 - binary_accuracy: 0.7795 - val_loss: 0.5284 - val_binary_accuracy: 0.7420\nEpoch 56/100\n32/32 [==============================] - 8s 255ms/step - loss: 0.4572 - binary_accuracy: 0.7785 - val_loss: 0.5014 - val_binary_accuracy: 0.7430\nEpoch 57/100\n32/32 [==============================] - 8s 253ms/step - loss: 0.4654 - binary_accuracy: 0.7750 - val_loss: 0.4811 - val_binary_accuracy: 0.7670\nEpoch 58/100\n32/32 [==============================] - 8s 249ms/step - loss: 0.4480 - binary_accuracy: 0.7925 - val_loss: 0.5795 - val_binary_accuracy: 0.7120\nEpoch 59/100\n32/32 [==============================] - 8s 250ms/step - loss: 0.4503 - binary_accuracy: 0.7805 - val_loss: 0.5151 - val_binary_accuracy: 0.7440\nEpoch 60/100\n32/32 [==============================] - 8s 252ms/step - loss: 0.4572 - binary_accuracy: 0.7890 - val_loss: 0.5013 - val_binary_accuracy: 0.7500\nEpoch 61/100\n32/32 [==============================] - 9s 268ms/step - loss: 0.4552 - binary_accuracy: 0.7775 - val_loss: 0.5036 - val_binary_accuracy: 0.7500\nEpoch 62/100\n32/32 [==============================] - 9s 266ms/step - loss: 0.4418 - binary_accuracy: 0.7920 - val_loss: 0.5908 - val_binary_accuracy: 0.7110\nEpoch 63/100\n32/32 [==============================] - 9s 268ms/step - loss: 0.4342 - binary_accuracy: 0.7960 - val_loss: 0.5061 - val_binary_accuracy: 0.7590\nEpoch 64/100\n32/32 [==============================] - 8s 264ms/step - loss: 0.4471 - binary_accuracy: 0.7865 - val_loss: 0.4961 - val_binary_accuracy: 0.7550\nEpoch 65/100\n32/32 [==============================] - 9s 266ms/step - loss: 0.4494 - binary_accuracy: 0.7900 - val_loss: 0.5424 - val_binary_accuracy: 0.7320\nEpoch 66/100\n32/32 [==============================] - 9s 265ms/step - loss: 0.4257 - binary_accuracy: 0.7995 - val_loss: 0.5377 - val_binary_accuracy: 0.7520\nEpoch 67/100\n32/32 [==============================] - 8s 261ms/step - loss: 0.4504 - binary_accuracy: 0.7815 - val_loss: 0.6068 - val_binary_accuracy: 0.7150\nEpoch 68/100\n32/32 [==============================] - 8s 253ms/step - loss: 0.4275 - binary_accuracy: 0.8045 - val_loss: 0.4592 - val_binary_accuracy: 0.7720\nEpoch 69/100\n32/32 [==============================] - 8s 248ms/step - loss: 0.4429 - binary_accuracy: 0.7850 - val_loss: 0.4845 - val_binary_accuracy: 0.7550\nEpoch 70/100\n32/32 [==============================] - 8s 250ms/step - loss: 0.4171 - binary_accuracy: 0.8040 - val_loss: 0.6250 - val_binary_accuracy: 0.7110\nEpoch 71/100\n32/32 [==============================] - 8s 252ms/step - loss: 0.4440 - binary_accuracy: 0.7875 - val_loss: 0.5684 - val_binary_accuracy: 0.7190\nEpoch 72/100\n32/32 [==============================] - 8s 253ms/step - loss: 0.4044 - binary_accuracy: 0.8105 - val_loss: 0.4783 - val_binary_accuracy: 0.7740\nEpoch 73/100\n32/32 [==============================] - 8s 254ms/step - loss: 0.4366 - binary_accuracy: 0.7935 - val_loss: 0.4487 - val_binary_accuracy: 0.7840\nEpoch 74/100\n32/32 [==============================] - 8s 253ms/step - loss: 0.4385 - binary_accuracy: 0.7900 - val_loss: 0.4717 - val_binary_accuracy: 0.7630\nEpoch 75/100\n32/32 [==============================] - 8s 252ms/step - loss: 0.4435 - binary_accuracy: 0.7900 - val_loss: 0.4625 - val_binary_accuracy: 0.7830\nEpoch 76/100\n32/32 [==============================] - 8s 253ms/step - loss: 0.4228 - binary_accuracy: 0.8010 - val_loss: 0.5609 - val_binary_accuracy: 0.7120\nEpoch 77/100\n32/32 [==============================] - 8s 254ms/step - loss: 0.4428 - binary_accuracy: 0.7860 - val_loss: 0.4750 - val_binary_accuracy: 0.7720\nEpoch 78/100\n32/32 [==============================] - 8s 253ms/step - loss: 0.4294 - binary_accuracy: 0.7920 - val_loss: 0.4582 - val_binary_accuracy: 0.7770\nEpoch 79/100\n32/32 [==============================] - 8s 251ms/step - loss: 0.4172 - binary_accuracy: 0.7985 - val_loss: 0.5194 - val_binary_accuracy: 0.7460\nEpoch 80/100\n32/32 [==============================] - 8s 252ms/step - loss: 0.4132 - binary_accuracy: 0.8075 - val_loss: 0.5521 - val_binary_accuracy: 0.7350\nEpoch 81/100\n32/32 [==============================] - 8s 251ms/step - loss: 0.4329 - binary_accuracy: 0.7945 - val_loss: 0.4473 - val_binary_accuracy: 0.7900\nEpoch 82/100\n32/32 [==============================] - 8s 250ms/step - loss: 0.4109 - binary_accuracy: 0.8100 - val_loss: 0.5535 - val_binary_accuracy: 0.7400\nEpoch 83/100\n32/32 [==============================] - 8s 250ms/step - loss: 0.4134 - binary_accuracy: 0.8150 - val_loss: 0.5771 - val_binary_accuracy: 0.7240\nEpoch 84/100\n32/32 [==============================] - 8s 251ms/step - loss: 0.4013 - binary_accuracy: 0.8210 - val_loss: 0.4499 - val_binary_accuracy: 0.7800\nEpoch 85/100\n32/32 [==============================] - 8s 251ms/step - loss: 0.4045 - binary_accuracy: 0.8205 - val_loss: 0.4520 - val_binary_accuracy: 0.7880\nEpoch 86/100\n32/32 [==============================] - 8s 253ms/step - loss: 0.4060 - binary_accuracy: 0.8090 - val_loss: 0.4711 - val_binary_accuracy: 0.7760\nEpoch 87/100\n32/32 [==============================] - 8s 257ms/step - loss: 0.3990 - binary_accuracy: 0.8125 - val_loss: 0.5322 - val_binary_accuracy: 0.7430\nEpoch 88/100\n32/32 [==============================] - 8s 251ms/step - loss: 0.3723 - binary_accuracy: 0.8315 - val_loss: 0.5687 - val_binary_accuracy: 0.7500\nEpoch 89/100\n32/32 [==============================] - 8s 251ms/step - loss: 0.3987 - binary_accuracy: 0.8180 - val_loss: 0.6043 - val_binary_accuracy: 0.7290\nEpoch 90/100\n32/32 [==============================] - 8s 250ms/step - loss: 0.4002 - binary_accuracy: 0.8170 - val_loss: 0.4668 - val_binary_accuracy: 0.7720\nEpoch 91/100\n32/32 [==============================] - 8s 250ms/step - loss: 0.3821 - binary_accuracy: 0.8275 - val_loss: 0.5863 - val_binary_accuracy: 0.7410\nEpoch 92/100\n32/32 [==============================] - 8s 251ms/step - loss: 0.4089 - binary_accuracy: 0.8115 - val_loss: 0.4899 - val_binary_accuracy: 0.7590\nEpoch 93/100\n32/32 [==============================] - 8s 251ms/step - loss: 0.3953 - binary_accuracy: 0.8200 - val_loss: 0.4942 - val_binary_accuracy: 0.7660\nEpoch 94/100\n32/32 [==============================] - 8s 251ms/step - loss: 0.3827 - binary_accuracy: 0.8275 - val_loss: 0.5453 - val_binary_accuracy: 0.7430\nEpoch 95/100\n32/32 [==============================] - 8s 252ms/step - loss: 0.3939 - binary_accuracy: 0.8205 - val_loss: 0.4597 - val_binary_accuracy: 0.7770\nEpoch 96/100\n32/32 [==============================] - 8s 252ms/step - loss: 0.3811 - binary_accuracy: 0.8370 - val_loss: 0.5437 - val_binary_accuracy: 0.7490\nEpoch 97/100\n32/32 [==============================] - 8s 252ms/step - loss: 0.3731 - binary_accuracy: 0.8370 - val_loss: 0.4411 - val_binary_accuracy: 0.7780\nEpoch 98/100\n32/32 [==============================] - 8s 252ms/step - loss: 0.3776 - binary_accuracy: 0.8330 - val_loss: 0.4540 - val_binary_accuracy: 0.7820\nEpoch 99/100\n32/32 [==============================] - 8s 252ms/step - loss: 0.3838 - binary_accuracy: 0.8245 - val_loss: 0.5430 - val_binary_accuracy: 0.7450\nEpoch 100/100\n32/32 [==============================] - 8s 252ms/step - loss: 0.3801 - binary_accuracy: 0.8245 - val_loss: 0.5083 - val_binary_accuracy: 0.7580\n\n\n\n\n\n\n\n\n\nA: Data augmentation prevents overfitting, as the network never sees twice the same image. Learning is much slower, but it can bring the validation accuracy significantly higher (80% after 100 epochs)."
  },
  {
    "objectID": "exercises/11-TransferLearning-solution.html#transfer-learning",
    "href": "exercises/11-TransferLearning-solution.html#transfer-learning",
    "title": "Transfer learning",
    "section": "Transfer learning",
    "text": "Transfer learning\nData augmentation helps randomly initialized to learn from small datasets, but the best solution is to start training with already good weights.\nTransfer learning allows to reuse the weights of a CNN trained on a bigger dataset (e.g. ImageNet) to either extract features for a shallow classifier or to allow fine-tuning of all weights.\nKeras provides a considerable number of pre-trained CNNs:\nhttps://keras.io/api/applications/\nIn this exercise, we will use the Xception network for feature extraction, but feel free to experiment with other architectures. To download the weights and create the keras model, simply call:\nxception = tf.keras.applications.Xception(\n        weights=\"imagenet\",  # Load weights pre-trained on ImageNet.\n        input_shape=(150, 150, 3), # Input shape\n        include_top=False, # Only the convolutional layers, not the last fully-connected ones\n    )\ninclude_top=False removes the last fully-connected layers used to predict the ImageNet classes, as we only care about the binary cat/dog classification.\nQ: Download Xception and print its summary. Make sense of the various layers (the paper might help: http://arxiv.org/abs/1610.02357). What is the size of the final tensor?\n\ntf.keras.backend.clear_session()\n\nxception = tf.keras.applications.Xception(\n        weights=\"imagenet\",  # Load weights pre-trained on ImageNet.\n        input_shape=(150, 150, 3), # Input shape\n        include_top=False, # Only the convolutional layers, not the last fully-connected ones\n    )\n\nprint(xception.summary())\n\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n83689472/83683744 [==============================] - 11s 0us/step\n83697664/83683744 [==============================] - 11s 0us/step\nModel: \"xception\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_1 (InputLayer)           [(None, 150, 150, 3  0           []                               \n                                )]                                                                \n                                                                                                  \n block1_conv1 (Conv2D)          (None, 74, 74, 32)   864         ['input_1[0][0]']                \n                                                                                                  \n block1_conv1_bn (BatchNormaliz  (None, 74, 74, 32)  128         ['block1_conv1[0][0]']           \n ation)                                                                                           \n                                                                                                  \n block1_conv1_act (Activation)  (None, 74, 74, 32)   0           ['block1_conv1_bn[0][0]']        \n                                                                                                  \n block1_conv2 (Conv2D)          (None, 72, 72, 64)   18432       ['block1_conv1_act[0][0]']       \n                                                                                                  \n block1_conv2_bn (BatchNormaliz  (None, 72, 72, 64)  256         ['block1_conv2[0][0]']           \n ation)                                                                                           \n                                                                                                  \n block1_conv2_act (Activation)  (None, 72, 72, 64)   0           ['block1_conv2_bn[0][0]']        \n                                                                                                  \n block2_sepconv1 (SeparableConv  (None, 72, 72, 128)  8768       ['block1_conv2_act[0][0]']       \n 2D)                                                                                              \n                                                                                                  \n block2_sepconv1_bn (BatchNorma  (None, 72, 72, 128)  512        ['block2_sepconv1[0][0]']        \n lization)                                                                                        \n                                                                                                  \n block2_sepconv2_act (Activatio  (None, 72, 72, 128)  0          ['block2_sepconv1_bn[0][0]']     \n n)                                                                                               \n                                                                                                  \n block2_sepconv2 (SeparableConv  (None, 72, 72, 128)  17536      ['block2_sepconv2_act[0][0]']    \n 2D)                                                                                              \n                                                                                                  \n block2_sepconv2_bn (BatchNorma  (None, 72, 72, 128)  512        ['block2_sepconv2[0][0]']        \n lization)                                                                                        \n                                                                                                  \n conv2d (Conv2D)                (None, 36, 36, 128)  8192        ['block1_conv2_act[0][0]']       \n                                                                                                  \n block2_pool (MaxPooling2D)     (None, 36, 36, 128)  0           ['block2_sepconv2_bn[0][0]']     \n                                                                                                  \n batch_normalization (BatchNorm  (None, 36, 36, 128)  512        ['conv2d[0][0]']                 \n alization)                                                                                       \n                                                                                                  \n add (Add)                      (None, 36, 36, 128)  0           ['block2_pool[0][0]',            \n                                                                  'batch_normalization[0][0]']    \n                                                                                                  \n block3_sepconv1_act (Activatio  (None, 36, 36, 128)  0          ['add[0][0]']                    \n n)                                                                                               \n                                                                                                  \n block3_sepconv1 (SeparableConv  (None, 36, 36, 256)  33920      ['block3_sepconv1_act[0][0]']    \n 2D)                                                                                              \n                                                                                                  \n block3_sepconv1_bn (BatchNorma  (None, 36, 36, 256)  1024       ['block3_sepconv1[0][0]']        \n lization)                                                                                        \n                                                                                                  \n block3_sepconv2_act (Activatio  (None, 36, 36, 256)  0          ['block3_sepconv1_bn[0][0]']     \n n)                                                                                               \n                                                                                                  \n block3_sepconv2 (SeparableConv  (None, 36, 36, 256)  67840      ['block3_sepconv2_act[0][0]']    \n 2D)                                                                                              \n                                                                                                  \n block3_sepconv2_bn (BatchNorma  (None, 36, 36, 256)  1024       ['block3_sepconv2[0][0]']        \n lization)                                                                                        \n                                                                                                  \n conv2d_1 (Conv2D)              (None, 18, 18, 256)  32768       ['add[0][0]']                    \n                                                                                                  \n block3_pool (MaxPooling2D)     (None, 18, 18, 256)  0           ['block3_sepconv2_bn[0][0]']     \n                                                                                                  \n batch_normalization_1 (BatchNo  (None, 18, 18, 256)  1024       ['conv2d_1[0][0]']               \n rmalization)                                                                                     \n                                                                                                  \n add_1 (Add)                    (None, 18, 18, 256)  0           ['block3_pool[0][0]',            \n                                                                  'batch_normalization_1[0][0]']  \n                                                                                                  \n block4_sepconv1_act (Activatio  (None, 18, 18, 256)  0          ['add_1[0][0]']                  \n n)                                                                                               \n                                                                                                  \n block4_sepconv1 (SeparableConv  (None, 18, 18, 728)  188672     ['block4_sepconv1_act[0][0]']    \n 2D)                                                                                              \n                                                                                                  \n block4_sepconv1_bn (BatchNorma  (None, 18, 18, 728)  2912       ['block4_sepconv1[0][0]']        \n lization)                                                                                        \n                                                                                                  \n block4_sepconv2_act (Activatio  (None, 18, 18, 728)  0          ['block4_sepconv1_bn[0][0]']     \n n)                                                                                               \n                                                                                                  \n block4_sepconv2 (SeparableConv  (None, 18, 18, 728)  536536     ['block4_sepconv2_act[0][0]']    \n 2D)                                                                                              \n                                                                                                  \n block4_sepconv2_bn (BatchNorma  (None, 18, 18, 728)  2912       ['block4_sepconv2[0][0]']        \n lization)                                                                                        \n                                                                                                  \n conv2d_2 (Conv2D)              (None, 9, 9, 728)    186368      ['add_1[0][0]']                  \n                                                                                                  \n block4_pool (MaxPooling2D)     (None, 9, 9, 728)    0           ['block4_sepconv2_bn[0][0]']     \n                                                                                                  \n batch_normalization_2 (BatchNo  (None, 9, 9, 728)   2912        ['conv2d_2[0][0]']               \n rmalization)                                                                                     \n                                                                                                  \n add_2 (Add)                    (None, 9, 9, 728)    0           ['block4_pool[0][0]',            \n                                                                  'batch_normalization_2[0][0]']  \n                                                                                                  \n block5_sepconv1_act (Activatio  (None, 9, 9, 728)   0           ['add_2[0][0]']                  \n n)                                                                                               \n                                                                                                  \n block5_sepconv1 (SeparableConv  (None, 9, 9, 728)   536536      ['block5_sepconv1_act[0][0]']    \n 2D)                                                                                              \n                                                                                                  \n block5_sepconv1_bn (BatchNorma  (None, 9, 9, 728)   2912        ['block5_sepconv1[0][0]']        \n lization)                                                                                        \n                                                                                                  \n block5_sepconv2_act (Activatio  (None, 9, 9, 728)   0           ['block5_sepconv1_bn[0][0]']     \n n)                                                                                               \n                                                                                                  \n block5_sepconv2 (SeparableConv  (None, 9, 9, 728)   536536      ['block5_sepconv2_act[0][0]']    \n 2D)                                                                                              \n                                                                                                  \n block5_sepconv2_bn (BatchNorma  (None, 9, 9, 728)   2912        ['block5_sepconv2[0][0]']        \n lization)                                                                                        \n                                                                                                  \n block5_sepconv3_act (Activatio  (None, 9, 9, 728)   0           ['block5_sepconv2_bn[0][0]']     \n n)                                                                                               \n                                                                                                  \n block5_sepconv3 (SeparableConv  (None, 9, 9, 728)   536536      ['block5_sepconv3_act[0][0]']    \n 2D)                                                                                              \n                                                                                                  \n block5_sepconv3_bn (BatchNorma  (None, 9, 9, 728)   2912        ['block5_sepconv3[0][0]']        \n lization)                                                                                        \n                                                                                                  \n add_3 (Add)                    (None, 9, 9, 728)    0           ['block5_sepconv3_bn[0][0]',     \n                                                                  'add_2[0][0]']                  \n                                                                                                  \n block6_sepconv1_act (Activatio  (None, 9, 9, 728)   0           ['add_3[0][0]']                  \n n)                                                                                               \n                                                                                                  \n block6_sepconv1 (SeparableConv  (None, 9, 9, 728)   536536      ['block6_sepconv1_act[0][0]']    \n 2D)                                                                                              \n                                                                                                  \n block6_sepconv1_bn (BatchNorma  (None, 9, 9, 728)   2912        ['block6_sepconv1[0][0]']        \n lization)                                                                                        \n                                                                                                  \n block6_sepconv2_act (Activatio  (None, 9, 9, 728)   0           ['block6_sepconv1_bn[0][0]']     \n n)                                                                                               \n                                                                                                  \n block6_sepconv2 (SeparableConv  (None, 9, 9, 728)   536536      ['block6_sepconv2_act[0][0]']    \n 2D)                                                                                              \n                                                                                                  \n block6_sepconv2_bn (BatchNorma  (None, 9, 9, 728)   2912        ['block6_sepconv2[0][0]']        \n lization)                                                                                        \n                                                                                                  \n block6_sepconv3_act (Activatio  (None, 9, 9, 728)   0           ['block6_sepconv2_bn[0][0]']     \n n)                                                                                               \n                                                                                                  \n block6_sepconv3 (SeparableConv  (None, 9, 9, 728)   536536      ['block6_sepconv3_act[0][0]']    \n 2D)                                                                                              \n                                                                                                  \n block6_sepconv3_bn (BatchNorma  (None, 9, 9, 728)   2912        ['block6_sepconv3[0][0]']        \n lization)                                                                                        \n                                                                                                  \n add_4 (Add)                    (None, 9, 9, 728)    0           ['block6_sepconv3_bn[0][0]',     \n                                                                  'add_3[0][0]']                  \n                                                                                                  \n block7_sepconv1_act (Activatio  (None, 9, 9, 728)   0           ['add_4[0][0]']                  \n n)                                                                                               \n                                                                                                  \n block7_sepconv1 (SeparableConv  (None, 9, 9, 728)   536536      ['block7_sepconv1_act[0][0]']    \n 2D)                                                                                              \n                                                                                                  \n block7_sepconv1_bn (BatchNorma  (None, 9, 9, 728)   2912        ['block7_sepconv1[0][0]']        \n lization)                                                                                        \n                                                                                                  \n block7_sepconv2_act (Activatio  (None, 9, 9, 728)   0           ['block7_sepconv1_bn[0][0]']     \n n)                                                                                               \n                                                                                                  \n block7_sepconv2 (SeparableConv  (None, 9, 9, 728)   536536      ['block7_sepconv2_act[0][0]']    \n 2D)                                                                                              \n                                                                                                  \n block7_sepconv2_bn (BatchNorma  (None, 9, 9, 728)   2912        ['block7_sepconv2[0][0]']        \n lization)                                                                                        \n                                                                                                  \n block7_sepconv3_act (Activatio  (None, 9, 9, 728)   0           ['block7_sepconv2_bn[0][0]']     \n n)                                                                                               \n                                                                                                  \n block7_sepconv3 (SeparableConv  (None, 9, 9, 728)   536536      ['block7_sepconv3_act[0][0]']    \n 2D)                                                                                              \n                                                                                                  \n block7_sepconv3_bn (BatchNorma  (None, 9, 9, 728)   2912        ['block7_sepconv3[0][0]']        \n lization)                                                                                        \n                                                                                                  \n add_5 (Add)                    (None, 9, 9, 728)    0           ['block7_sepconv3_bn[0][0]',     \n                                                                  'add_4[0][0]']                  \n                                                                                                  \n block8_sepconv1_act (Activatio  (None, 9, 9, 728)   0           ['add_5[0][0]']                  \n n)                                                                                               \n                                                                                                  \n block8_sepconv1 (SeparableConv  (None, 9, 9, 728)   536536      ['block8_sepconv1_act[0][0]']    \n 2D)                                                                                              \n                                                                                                  \n block8_sepconv1_bn (BatchNorma  (None, 9, 9, 728)   2912        ['block8_sepconv1[0][0]']        \n lization)                                                                                        \n                                                                                                  \n block8_sepconv2_act (Activatio  (None, 9, 9, 728)   0           ['block8_sepconv1_bn[0][0]']     \n n)                                                                                               \n                                                                                                  \n block8_sepconv2 (SeparableConv  (None, 9, 9, 728)   536536      ['block8_sepconv2_act[0][0]']    \n 2D)                                                                                              \n                                                                                                  \n block8_sepconv2_bn (BatchNorma  (None, 9, 9, 728)   2912        ['block8_sepconv2[0][0]']        \n lization)                                                                                        \n                                                                                                  \n block8_sepconv3_act (Activatio  (None, 9, 9, 728)   0           ['block8_sepconv2_bn[0][0]']     \n n)                                                                                               \n                                                                                                  \n block8_sepconv3 (SeparableConv  (None, 9, 9, 728)   536536      ['block8_sepconv3_act[0][0]']    \n 2D)                                                                                              \n                                                                                                  \n block8_sepconv3_bn (BatchNorma  (None, 9, 9, 728)   2912        ['block8_sepconv3[0][0]']        \n lization)                                                                                        \n                                                                                                  \n add_6 (Add)                    (None, 9, 9, 728)    0           ['block8_sepconv3_bn[0][0]',     \n                                                                  'add_5[0][0]']                  \n                                                                                                  \n block9_sepconv1_act (Activatio  (None, 9, 9, 728)   0           ['add_6[0][0]']                  \n n)                                                                                               \n                                                                                                  \n block9_sepconv1 (SeparableConv  (None, 9, 9, 728)   536536      ['block9_sepconv1_act[0][0]']    \n 2D)                                                                                              \n                                                                                                  \n block9_sepconv1_bn (BatchNorma  (None, 9, 9, 728)   2912        ['block9_sepconv1[0][0]']        \n lization)                                                                                        \n                                                                                                  \n block9_sepconv2_act (Activatio  (None, 9, 9, 728)   0           ['block9_sepconv1_bn[0][0]']     \n n)                                                                                               \n                                                                                                  \n block9_sepconv2 (SeparableConv  (None, 9, 9, 728)   536536      ['block9_sepconv2_act[0][0]']    \n 2D)                                                                                              \n                                                                                                  \n block9_sepconv2_bn (BatchNorma  (None, 9, 9, 728)   2912        ['block9_sepconv2[0][0]']        \n lization)                                                                                        \n                                                                                                  \n block9_sepconv3_act (Activatio  (None, 9, 9, 728)   0           ['block9_sepconv2_bn[0][0]']     \n n)                                                                                               \n                                                                                                  \n block9_sepconv3 (SeparableConv  (None, 9, 9, 728)   536536      ['block9_sepconv3_act[0][0]']    \n 2D)                                                                                              \n                                                                                                  \n block9_sepconv3_bn (BatchNorma  (None, 9, 9, 728)   2912        ['block9_sepconv3[0][0]']        \n lization)                                                                                        \n                                                                                                  \n add_7 (Add)                    (None, 9, 9, 728)    0           ['block9_sepconv3_bn[0][0]',     \n                                                                  'add_6[0][0]']                  \n                                                                                                  \n block10_sepconv1_act (Activati  (None, 9, 9, 728)   0           ['add_7[0][0]']                  \n on)                                                                                              \n                                                                                                  \n block10_sepconv1 (SeparableCon  (None, 9, 9, 728)   536536      ['block10_sepconv1_act[0][0]']   \n v2D)                                                                                             \n                                                                                                  \n block10_sepconv1_bn (BatchNorm  (None, 9, 9, 728)   2912        ['block10_sepconv1[0][0]']       \n alization)                                                                                       \n                                                                                                  \n block10_sepconv2_act (Activati  (None, 9, 9, 728)   0           ['block10_sepconv1_bn[0][0]']    \n on)                                                                                              \n                                                                                                  \n block10_sepconv2 (SeparableCon  (None, 9, 9, 728)   536536      ['block10_sepconv2_act[0][0]']   \n v2D)                                                                                             \n                                                                                                  \n block10_sepconv2_bn (BatchNorm  (None, 9, 9, 728)   2912        ['block10_sepconv2[0][0]']       \n alization)                                                                                       \n                                                                                                  \n block10_sepconv3_act (Activati  (None, 9, 9, 728)   0           ['block10_sepconv2_bn[0][0]']    \n on)                                                                                              \n                                                                                                  \n block10_sepconv3 (SeparableCon  (None, 9, 9, 728)   536536      ['block10_sepconv3_act[0][0]']   \n v2D)                                                                                             \n                                                                                                  \n block10_sepconv3_bn (BatchNorm  (None, 9, 9, 728)   2912        ['block10_sepconv3[0][0]']       \n alization)                                                                                       \n                                                                                                  \n add_8 (Add)                    (None, 9, 9, 728)    0           ['block10_sepconv3_bn[0][0]',    \n                                                                  'add_7[0][0]']                  \n                                                                                                  \n block11_sepconv1_act (Activati  (None, 9, 9, 728)   0           ['add_8[0][0]']                  \n on)                                                                                              \n                                                                                                  \n block11_sepconv1 (SeparableCon  (None, 9, 9, 728)   536536      ['block11_sepconv1_act[0][0]']   \n v2D)                                                                                             \n                                                                                                  \n block11_sepconv1_bn (BatchNorm  (None, 9, 9, 728)   2912        ['block11_sepconv1[0][0]']       \n alization)                                                                                       \n                                                                                                  \n block11_sepconv2_act (Activati  (None, 9, 9, 728)   0           ['block11_sepconv1_bn[0][0]']    \n on)                                                                                              \n                                                                                                  \n block11_sepconv2 (SeparableCon  (None, 9, 9, 728)   536536      ['block11_sepconv2_act[0][0]']   \n v2D)                                                                                             \n                                                                                                  \n block11_sepconv2_bn (BatchNorm  (None, 9, 9, 728)   2912        ['block11_sepconv2[0][0]']       \n alization)                                                                                       \n                                                                                                  \n block11_sepconv3_act (Activati  (None, 9, 9, 728)   0           ['block11_sepconv2_bn[0][0]']    \n on)                                                                                              \n                                                                                                  \n block11_sepconv3 (SeparableCon  (None, 9, 9, 728)   536536      ['block11_sepconv3_act[0][0]']   \n v2D)                                                                                             \n                                                                                                  \n block11_sepconv3_bn (BatchNorm  (None, 9, 9, 728)   2912        ['block11_sepconv3[0][0]']       \n alization)                                                                                       \n                                                                                                  \n add_9 (Add)                    (None, 9, 9, 728)    0           ['block11_sepconv3_bn[0][0]',    \n                                                                  'add_8[0][0]']                  \n                                                                                                  \n block12_sepconv1_act (Activati  (None, 9, 9, 728)   0           ['add_9[0][0]']                  \n on)                                                                                              \n                                                                                                  \n block12_sepconv1 (SeparableCon  (None, 9, 9, 728)   536536      ['block12_sepconv1_act[0][0]']   \n v2D)                                                                                             \n                                                                                                  \n block12_sepconv1_bn (BatchNorm  (None, 9, 9, 728)   2912        ['block12_sepconv1[0][0]']       \n alization)                                                                                       \n                                                                                                  \n block12_sepconv2_act (Activati  (None, 9, 9, 728)   0           ['block12_sepconv1_bn[0][0]']    \n on)                                                                                              \n                                                                                                  \n block12_sepconv2 (SeparableCon  (None, 9, 9, 728)   536536      ['block12_sepconv2_act[0][0]']   \n v2D)                                                                                             \n                                                                                                  \n block12_sepconv2_bn (BatchNorm  (None, 9, 9, 728)   2912        ['block12_sepconv2[0][0]']       \n alization)                                                                                       \n                                                                                                  \n block12_sepconv3_act (Activati  (None, 9, 9, 728)   0           ['block12_sepconv2_bn[0][0]']    \n on)                                                                                              \n                                                                                                  \n block12_sepconv3 (SeparableCon  (None, 9, 9, 728)   536536      ['block12_sepconv3_act[0][0]']   \n v2D)                                                                                             \n                                                                                                  \n block12_sepconv3_bn (BatchNorm  (None, 9, 9, 728)   2912        ['block12_sepconv3[0][0]']       \n alization)                                                                                       \n                                                                                                  \n add_10 (Add)                   (None, 9, 9, 728)    0           ['block12_sepconv3_bn[0][0]',    \n                                                                  'add_9[0][0]']                  \n                                                                                                  \n block13_sepconv1_act (Activati  (None, 9, 9, 728)   0           ['add_10[0][0]']                 \n on)                                                                                              \n                                                                                                  \n block13_sepconv1 (SeparableCon  (None, 9, 9, 728)   536536      ['block13_sepconv1_act[0][0]']   \n v2D)                                                                                             \n                                                                                                  \n block13_sepconv1_bn (BatchNorm  (None, 9, 9, 728)   2912        ['block13_sepconv1[0][0]']       \n alization)                                                                                       \n                                                                                                  \n block13_sepconv2_act (Activati  (None, 9, 9, 728)   0           ['block13_sepconv1_bn[0][0]']    \n on)                                                                                              \n                                                                                                  \n block13_sepconv2 (SeparableCon  (None, 9, 9, 1024)  752024      ['block13_sepconv2_act[0][0]']   \n v2D)                                                                                             \n                                                                                                  \n block13_sepconv2_bn (BatchNorm  (None, 9, 9, 1024)  4096        ['block13_sepconv2[0][0]']       \n alization)                                                                                       \n                                                                                                  \n conv2d_3 (Conv2D)              (None, 5, 5, 1024)   745472      ['add_10[0][0]']                 \n                                                                                                  \n block13_pool (MaxPooling2D)    (None, 5, 5, 1024)   0           ['block13_sepconv2_bn[0][0]']    \n                                                                                                  \n batch_normalization_3 (BatchNo  (None, 5, 5, 1024)  4096        ['conv2d_3[0][0]']               \n rmalization)                                                                                     \n                                                                                                  \n add_11 (Add)                   (None, 5, 5, 1024)   0           ['block13_pool[0][0]',           \n                                                                  'batch_normalization_3[0][0]']  \n                                                                                                  \n block14_sepconv1 (SeparableCon  (None, 5, 5, 1536)  1582080     ['add_11[0][0]']                 \n v2D)                                                                                             \n                                                                                                  \n block14_sepconv1_bn (BatchNorm  (None, 5, 5, 1536)  6144        ['block14_sepconv1[0][0]']       \n alization)                                                                                       \n                                                                                                  \n block14_sepconv1_act (Activati  (None, 5, 5, 1536)  0           ['block14_sepconv1_bn[0][0]']    \n on)                                                                                              \n                                                                                                  \n block14_sepconv2 (SeparableCon  (None, 5, 5, 2048)  3159552     ['block14_sepconv1_act[0][0]']   \n v2D)                                                                                             \n                                                                                                  \n block14_sepconv2_bn (BatchNorm  (None, 5, 5, 2048)  8192        ['block14_sepconv2[0][0]']       \n alization)                                                                                       \n                                                                                                  \n block14_sepconv2_act (Activati  (None, 5, 5, 2048)  0           ['block14_sepconv2_bn[0][0]']    \n on)                                                                                              \n                                                                                                  \n==================================================================================================\nTotal params: 20,861,480\nTrainable params: 20,806,952\nNon-trainable params: 54,528\n__________________________________________________________________________________________________\nNone\n\n\nLet’s now use transfer learning using this network. The first thing to do is to freeze Xception to make sure that it does learn from the cats and dogs data:\nxception.trainable = False\nWe can then connect Xception to the inputs, making sure again that the network won’t learn (in particular, the parameters of batch normalization are kept):\ninputs = tf.keras.Input(shape=(150, 150, 3))\nx = xception(inputs, training=False)\nWe can now use the layer x and stack what we want on top of it. Instead of flattening the 5x5x2048 tensor, it is usually better to apply average-pooling (or mean-pooling) on each 5x5 feature map to obtain a vector with 2048 elements:\nx = tf.keras.layers.GlobalAveragePooling2D()(x)\nQ: Perform a soft linear classification on this vector with 2048 elements to recognize cats from dogs (using non-augmented data). Do not hesitate to use some dropout and to boost your learning rate, there are only 2049 trainable parameters. Conclude.\n\ndef transfer_model():\n\n    # Delete all previous models to free memory\n    tf.keras.backend.clear_session()\n\n    # Use Xception as a feature extractor\n    xception = tf.keras.applications.Xception(\n        weights=\"imagenet\",  # Load weights pre-trained on ImageNet.\n        input_shape=(150, 150, 3), # Input shape\n        include_top=False, # Only the convolutional layers, not the last fully-connected ones\n    )  # Do not include the ImageNet classifier at the top.\n\n    # Freeze the base model\n    xception.trainable = False\n\n    # Create new model on top\n    inputs = tf.keras.Input(shape=(150, 150, 3))\n\n    # The base model contains batchnorm layers. We want to keep them in inference mode\n    # when we unfreeze the base model for fine-tuning, so we make sure that the\n    # base_model is running in inference mode here.\n    x = xception(inputs, training=False)\n\n    # GlobalAveragePooling2D applies average-pooling on each 5x5 feature map\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n\n    # Regularize with dropout\n    x = tf.keras.layers.Dropout(0.5)(x)  \n\n    # Output layer for binary classification\n    outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n\n    # Model\n    model = tf.keras.Model(inputs, outputs)\n\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n        loss='binary_crossentropy',\n        metrics=['binary_accuracy'],\n    )\n\n\n    print(model.summary())\n\n    return model\n\n\nmodel = transfer_model()\n\nhistory = tf.keras.callbacks.History()\nmodel.fit(\n      train_generator,\n      epochs=10,\n      validation_data=validation_generator,\n      callbacks=[history])\n\nplt.figure(figsize=(15, 6))\n\nplt.subplot(121)\nplt.plot(history.history['loss'], '-r', label=\"Training\")\nplt.plot(history.history['val_loss'], '-b', label=\"Validation\")\nplt.xlabel('Epoch #')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.subplot(122)\nplt.plot(history.history['binary_accuracy'], '-r', label=\"Training\")\nplt.plot(history.history['val_binary_accuracy'], '-b', label=\"Validation\")\nplt.xlabel('Epoch #')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.show()\n\nModel: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_2 (InputLayer)        [(None, 150, 150, 3)]     0         \n                                                                 \n xception (Functional)       (None, 5, 5, 2048)        20861480  \n                                                                 \n global_average_pooling2d (G  (None, 2048)             0         \n lobalAveragePooling2D)                                          \n                                                                 \n dropout (Dropout)           (None, 2048)              0         \n                                                                 \n dense (Dense)               (None, 1)                 2049      \n                                                                 \n=================================================================\nTotal params: 20,863,529\nTrainable params: 2,049\nNon-trainable params: 20,861,480\n_________________________________________________________________\nNone\nEpoch 1/10\n\n\n2022-11-15 11:09:19.851972: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n\n\n32/32 [==============================] - ETA: 0s - loss: 0.3178 - binary_accuracy: 0.8790\n\n\n2022-11-15 11:09:28.778983: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n\n\n32/32 [==============================] - 13s 343ms/step - loss: 0.3178 - binary_accuracy: 0.8790 - val_loss: 0.1438 - val_binary_accuracy: 0.9470\nEpoch 2/10\n32/32 [==============================] - 10s 317ms/step - loss: 0.1343 - binary_accuracy: 0.9530 - val_loss: 0.1119 - val_binary_accuracy: 0.9550\nEpoch 3/10\n32/32 [==============================] - 10s 317ms/step - loss: 0.1121 - binary_accuracy: 0.9620 - val_loss: 0.0979 - val_binary_accuracy: 0.9620\nEpoch 4/10\n32/32 [==============================] - 10s 317ms/step - loss: 0.0916 - binary_accuracy: 0.9660 - val_loss: 0.0965 - val_binary_accuracy: 0.9590\nEpoch 5/10\n32/32 [==============================] - 10s 323ms/step - loss: 0.0858 - binary_accuracy: 0.9675 - val_loss: 0.0919 - val_binary_accuracy: 0.9620\nEpoch 6/10\n32/32 [==============================] - 10s 318ms/step - loss: 0.0798 - binary_accuracy: 0.9730 - val_loss: 0.0917 - val_binary_accuracy: 0.9630\nEpoch 7/10\n32/32 [==============================] - 10s 317ms/step - loss: 0.0727 - binary_accuracy: 0.9750 - val_loss: 0.0879 - val_binary_accuracy: 0.9640\nEpoch 8/10\n32/32 [==============================] - 10s 319ms/step - loss: 0.0672 - binary_accuracy: 0.9795 - val_loss: 0.0846 - val_binary_accuracy: 0.9600\nEpoch 9/10\n32/32 [==============================] - 10s 318ms/step - loss: 0.0689 - binary_accuracy: 0.9705 - val_loss: 0.0839 - val_binary_accuracy: 0.9590\nEpoch 10/10\n32/32 [==============================] - 10s 320ms/step - loss: 0.0634 - binary_accuracy: 0.9785 - val_loss: 0.0854 - val_binary_accuracy: 0.9630\n\n\n\n\n\n\n\n\n\nA: Using feature extraction, we obtain very quickly a validation accuracy around 95% on the small unaugmented dataset, a performance out of reach of randomly initialized networks even with data augmentation. Conclusion: if you can use transfer learning, use it."
  },
  {
    "objectID": "exercises/12-VAE-solution.html#gradient-tapes-redefining-the-learning-procedure",
    "href": "exercises/12-VAE-solution.html#gradient-tapes-redefining-the-learning-procedure",
    "title": "Variational autoencoder",
    "section": "Gradient tapes: redefining the learning procedure",
    "text": "Gradient tapes: redefining the learning procedure\nLet’s first have a look at how to define custom losses. There is an easier way to define custom losses with keras (https://keras.io/api/losses/#creating-custom-losses), but we will need this sightly more complicated variant for the VAE.\nLet’s reuse the CNN you implemented last time using the functional API on MNIST, but not compile it yet:\n\ndef create_model():\n\n    inputs = tf.keras.layers.Input((28, 28, 1))\n\n    x = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='valid')(inputs)\n    x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n    x = tf.keras.layers.Dropout(0.5)(x)\n\n    x = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='valid')(x)\n    x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n    x = tf.keras.layers.Dropout(0.5)(x)\n\n    x = tf.keras.layers.Flatten()(x)\n\n    x = tf.keras.layers.Dense(150, activation='relu')(x)\n    x = tf.keras.layers.Dropout(0.5)(x)\n\n    outputs = tf.keras.layers.Dense(10, activation='softmax')(x)\n\n    model = tf.keras.Model(inputs, outputs)\n    print(model.summary())\n\n    return model\n\nIn order to have access to the internals of the training procedure, one of the possible methods is to inherit the tf.keras.Model class and redefine the train_step and (optionally) test_step methods.\nThe following cell redefines a model for the previous CNN and minimizes the categorical cross-entropy while tracking the loss and accuracy, so it is completely equivalent to:\nmodel.compile(\n    loss=\"categorical_crossentropy\", \n    optimizer=optimizer, \n    metrics=['accuracy'])\nHave a look at the code, but we will go through it step by step afterwards.\n\nclass CNN(tf.keras.Model):\n\n    def __init__(self):\n        super(CNN, self).__init__()\n\n        # Model\n        self.model = create_model()\n\n        # Metrics\n        self.loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n        self.accuracy_tracker = tf.keras.metrics.Accuracy(name=\"accuracy\")\n\n    @property\n    def metrics(self):\n        \"Track the loss and accuracy\"\n        return [self.loss_tracker, self.accuracy_tracker]\n\n    def train_step(self, data):\n        \n        # Get the data of the minibatch\n        X, t = data\n        \n        # Use GradientTape to record everything we need to compute the gradient\n        with tf.GradientTape() as tape:\n\n            # Prediction using the model\n            y = self.model(X, training=True)\n            \n            # Cross-entropy loss\n            loss = tf.reduce_mean(\n                tf.reduce_sum(\n                    - t * tf.math.log(y), # Cross-entropy\n                    axis=1 # First index is the batch size, the second is the classes\n                )\n            )\n        \n        # Compute gradients\n        grads = tape.gradient(loss, self.trainable_weights)\n        \n        # Apply gradients using the optimizer\n        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n        \n        # Update metrics \n        self.loss_tracker.update_state(loss)\n        true_class = tf.reshape(tf.argmax(t, axis=1), shape=(-1, 1))\n        predicted_class = tf.reshape(tf.argmax(y, axis=1), shape=(-1, 1))\n        self.accuracy_tracker.update_state(true_class, predicted_class)\n        \n        # Return a dict mapping metric names to current value\n        return {\"loss\": self.loss_tracker.result(), 'accuracy': self.accuracy_tracker.result()} \n\n    def test_step(self, data):\n        \n        # Get data\n        X, t = data\n        \n        # Prediction\n        y = self.model(X, training=False)\n            \n        # Loss\n        loss = tf.reduce_mean(\n            tf.reduce_sum(\n                    - t * tf.math.log(y), # Cross-entropy\n                    axis=1\n            )\n        )\n        \n        # Update metrics \n        self.loss_tracker.update_state(loss)\n        true_class = tf.reshape(tf.argmax(t, axis=1), shape=(-1, 1))\n        predicted_class = tf.reshape(tf.argmax(y, axis=1), shape=(-1, 1))\n        self.accuracy_tracker.update_state(true_class, predicted_class)\n        \n        # Return a dict mapping metric names to current value\n        return {\"loss\": self.loss_tracker.result(), 'accuracy': self.accuracy_tracker.result()} \n                \n\nThe constructor of the new CNN class creates the model defined by create_model() and stores it as an attribute.\nNote: it would be actually more logical to create layers directly here, as we now have a model containing a model, but this is simpler for the VAE architecture.\nThe constructor also defines the metrics that should be tracked when training. Here we track the loss and accuracy of the model, using objects of tf.keras.metrics (check https://keras.io/api/metrics/ for a list of metrics you can track).\nThe metrics are furthermore declared in the metrics property, so that you can now avoid passing metrics=['accuracy'] to compile(). The default Model only has 'loss' as a default metric.\nclass CNN(tf.keras.Model):\n\n    def __init__(self):\n        super(CNN, self).__init__()\n\n        # Model\n        self.model = create_model()\n\n        # Metrics\n        self.loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n        self.accuracy_tracker = tf.keras.metrics.Accuracy(name=\"accuracy\")\n\n    @property\n    def metrics(self):\n        \"Track the loss and accuracy\"\n        return [self.loss_tracker, self.accuracy_tracker]\nThe training procedure is defined in the train_step(data) method of the class.\n    def train_step(self, data):\n        \n        # Get the data of the minibatch\n        X, t = data\ndata is a minibatch of data iteratively passed by model.fit(). X and t are tensors (multi-dimensional arrays) representing the inputs and targets. On MNIST, X has the shape (batch_size, 28, 28, 1) and t is (batch_size, 10). The rest of the method defines the loss function, computes its gradient w.r.t the learnable parameters and pass it the optimizer to change their value.\nTo get the output of the network on the minibatch, one simply has to call:\ny = self.model(X)\nwhich returns a (batch_size, 10) tensor. However, this forward pass does not keep in memory the activity of the hidden layers: all it cares about is the prediction. But when applying backpropagation, you need this internal information to compute the gradient.\nIn tensorflow 2.x, you can force the model to record internal activity using the eager execution mode and gradient tapes (as in the tape of an audio recorder):\nwith tf.GradientTape() as tape:\n    y = self.model(X, training=True)\nIt is not a big problem if you are not familiar with Python contexts: all you need to know is that the tape object will “see” everything that happens when calling y = self.model(X, training=True), i.e. it will record the hidden activations in the model.\nThe next thing to do inside the tape is to compute the loss of the model on the minibatch. Here we minimize the categorical cross-entropy:\n\\mathcal{L}(\\theta) = \\frac{1}{N} \\, \\sum_{i=1}^N \\sum_{j=1}^C - t^i_j \\, \\log y^i_j\nwhere N is the batch size, C the number of classes, t^i_j the j-th element of the i-th target vector and y^i_j the predicted probability for class j and the i-th sample.\nWe therefore need to take our two tensors t and y and compute that loss function, but recording everything (so inside the tape context).\nThere are several ways to do that, for example by calling directly the built-in categorical cross-entropy object of keras on the data:\nloss = tf.keras.losses.CategoricalCrossentropy()(t, y)\nAnother way to do it is to realize that tensorflow tensors are completely equivalent to numpy arrays: you can apply mathematical operations (sum, element-wise multiplication, log, etc.) on them as if they were regular arrays (internally, that is another story…).\nYou can for example add t and two times y as they have the same shape:\nloss = t + 2.0 * y\nloss would then be a tensor of the same shape. You can get the shape of a tensor with tf.shape(loss) just like in numpy.\nMathematical operation are in the tf.math module (https://www.tensorflow.org/api_docs/python/tf/math), for example with the log:\nloss = t + tf.math.log(y)\n* is by default the element-wise multiplication:\nloss = - t * tf.math.log(y)\nHere, loss is still a (batch_size, 10) tensor. We still need to sum over the 10 classes and take the mean over the minibatch to get a single number.\nSumming over the second dimension of this tensor can be done with tf.reduce_sum:\nloss = tf.reduce_sum(\n    - t * tf.math.log(y), \n    axis=1 # First index is the batch size, the second is the classes\n)\nThis gives us a vector with batch_size elements containing the individual losses for the minibatch. In order to compute its mean over the minibatch, we only need to call tf.reduce_mean():\nloss = tf.reduce_mean(\n            tf.reduce_sum(\n                - t * tf.math.log(y),\n                axis=1 \n            )\n        )\nThat’s it, we have redefined the categorical cross-entropy loss function on a minibatch using elementary numerical operations! Doing this inside the tape allows tensorflow to keep track of each sample of the minibatch individually: otherwise, it would not know how the loss (a single number) depends on each prediction y^i and therefore on the parameters of the NN.\nNow that we have the loss function as a function of the trainable parameters of the NN on the minibatch, we can ask tensorflow for its gradient:\ngrads = tape.gradient(loss, self.trainable_weights)\nBackpropagation is still a one-liner. self.trainable_weights contains all weights and biases in the model, while tape.gradient() apply backpropagation to compute the gradient of the loss function w.r.t them.\nWe can then pass this gradient to the optimizer (SGD or Adam, which will be passed to compile()) so that it updates the parameters:\nself.optimizer.apply_gradients(zip(grads, self.trainable_weights))\nFinally, we can update our metrics so that our custom loss and the accuracy are tracked during training:\nself.loss_tracker.update_state(loss)\n\ntrue_class = tf.reshape(tf.argmax(t, axis=1), shape=(-1, 1))\npredicted_class = tf.reshape(tf.argmax(y, axis=1), shape=(-1, 1))\nself.accuracy_tracker.update_state(true_class, predicted_class)\nFor the accuracy, we need to pass the class (predicted or ground truth), not the probabilities.\nThe test_step() method does roughly the same as train_step(), except that it does not modify the parameters: it is called on the validation data in order to compute the metrics. As we do not learn, we do not actually need the tape.\nQ: Create the custom CNN model and train it on MNIST. When compiling the model, you only need to pass it the right optimizer, as the loss function and the metrics are already defined in the model. Check that you get the same results as last time.\n\n# Delete all previous models to free memory\ntf.keras.backend.clear_session()\n    \n# Create the custom model\nmodel = CNN()\n\n# Optimizer\noptimizer = tf.keras.optimizers.SGD(learning_rate=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n\n# Compile\nmodel.compile(\n    optimizer=optimizer, # learning rule\n)\n\n# Training\nhistory = tf.keras.callbacks.History()\nmodel.fit(\n    X_train, T_train,\n    batch_size=64, \n    epochs=20,\n    validation_split=0.1,\n    callbacks=[history]\n)\n\n# Testing\nscore = model.evaluate(X_test, T_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n\nplt.figure(figsize=(15, 6))\n\nplt.subplot(121)\nplt.plot(history.history['loss'], '-r', label=\"Training\")\nplt.plot(history.history['val_loss'], '-b', label=\"Validation\")\nplt.xlabel('Epoch #')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.subplot(122)\nplt.plot(history.history['accuracy'], '-r', label=\"Training\")\nplt.plot(history.history['val_accuracy'], '-b', label=\"Validation\")\nplt.xlabel('Epoch #')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.show()\n\nModel: \"model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         [(None, 28, 28, 1)]       0         \n_________________________________________________________________\nconv2d (Conv2D)              (None, 26, 26, 32)        320       \n_________________________________________________________________\nmax_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n_________________________________________________________________\ndropout (Dropout)            (None, 13, 13, 32)        0         \n_________________________________________________________________\nconv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 5, 5, 64)          0         \n_________________________________________________________________\nflatten (Flatten)            (None, 1600)              0         \n_________________________________________________________________\ndense (Dense)                (None, 150)               240150    \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 150)               0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 10)                1510      \n=================================================================\nTotal params: 260,476\nTrainable params: 260,476\nNon-trainable params: 0\n_________________________________________________________________\nNone\nEpoch 1/20\n844/844 [==============================] - 6s 4ms/step - loss: 0.5395 - accuracy: 0.8229 - val_loss: 0.0923 - val_accuracy: 0.9733\nEpoch 2/20\n844/844 [==============================] - 3s 3ms/step - loss: 0.1972 - accuracy: 0.9394 - val_loss: 0.0651 - val_accuracy: 0.9823\nEpoch 3/20\n844/844 [==============================] - 3s 3ms/step - loss: 0.1541 - accuracy: 0.9521 - val_loss: 0.0520 - val_accuracy: 0.9852\nEpoch 4/20\n844/844 [==============================] - 3s 3ms/step - loss: 0.1298 - accuracy: 0.9602 - val_loss: 0.0453 - val_accuracy: 0.9865\nEpoch 5/20\n844/844 [==============================] - 3s 3ms/step - loss: 0.1193 - accuracy: 0.9632 - val_loss: 0.0424 - val_accuracy: 0.9883\nEpoch 6/20\n844/844 [==============================] - 3s 3ms/step - loss: 0.1094 - accuracy: 0.9661 - val_loss: 0.0391 - val_accuracy: 0.9883\nEpoch 7/20\n844/844 [==============================] - 3s 3ms/step - loss: 0.1001 - accuracy: 0.9688 - val_loss: 0.0360 - val_accuracy: 0.9900\nEpoch 8/20\n844/844 [==============================] - 3s 3ms/step - loss: 0.0938 - accuracy: 0.9713 - val_loss: 0.0366 - val_accuracy: 0.9890\nEpoch 9/20\n844/844 [==============================] - 3s 3ms/step - loss: 0.0882 - accuracy: 0.9725 - val_loss: 0.0357 - val_accuracy: 0.9910\nEpoch 10/20\n844/844 [==============================] - 3s 3ms/step - loss: 0.0847 - accuracy: 0.9741 - val_loss: 0.0343 - val_accuracy: 0.9893\nEpoch 11/20\n844/844 [==============================] - 3s 3ms/step - loss: 0.0815 - accuracy: 0.9750 - val_loss: 0.0339 - val_accuracy: 0.9900\nEpoch 12/20\n844/844 [==============================] - 3s 3ms/step - loss: 0.0798 - accuracy: 0.9756 - val_loss: 0.0327 - val_accuracy: 0.9913\nEpoch 13/20\n844/844 [==============================] - 3s 3ms/step - loss: 0.0780 - accuracy: 0.9761 - val_loss: 0.0301 - val_accuracy: 0.9915\nEpoch 14/20\n844/844 [==============================] - 3s 3ms/step - loss: 0.0730 - accuracy: 0.9775 - val_loss: 0.0287 - val_accuracy: 0.9917\nEpoch 15/20\n844/844 [==============================] - 3s 3ms/step - loss: 0.0722 - accuracy: 0.9774 - val_loss: 0.0302 - val_accuracy: 0.9917\nEpoch 16/20\n844/844 [==============================] - 3s 3ms/step - loss: 0.0712 - accuracy: 0.9776 - val_loss: 0.0299 - val_accuracy: 0.9907\nEpoch 17/20\n844/844 [==============================] - 3s 3ms/step - loss: 0.0696 - accuracy: 0.9790 - val_loss: 0.0288 - val_accuracy: 0.9922\nEpoch 18/20\n844/844 [==============================] - 3s 3ms/step - loss: 0.0676 - accuracy: 0.9792 - val_loss: 0.0281 - val_accuracy: 0.9920\nEpoch 19/20\n844/844 [==============================] - 3s 3ms/step - loss: 0.0658 - accuracy: 0.9791 - val_loss: 0.0285 - val_accuracy: 0.9915\nEpoch 20/20\n844/844 [==============================] - 3s 3ms/step - loss: 0.0632 - accuracy: 0.9799 - val_loss: 0.0273 - val_accuracy: 0.9922\nTest loss: 0.02126365341246128\nTest accuracy: 0.9923999905586243\n\n\n\n\n\n\n\n\n\nQ: Redefine the model so that it minimizes the mean-square error (t-y)^2 instead of the cross-entropy. What happens?\nHint: squaring a tensor element-wise is done by applying **2 on it just like in numpy.\n\nclass CNN(tf.keras.Model):\n\n    def __init__(self):\n        super(CNN, self).__init__()\n\n        # Model\n        self.model = create_model()\n\n        # Metrics\n        self.loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n        self.accuracy_tracker = tf.keras.metrics.Accuracy(name=\"accuracy\")\n\n    @property\n    def metrics(self):\n        \"Track the loss and accuracy\"\n        return [self.loss_tracker, self.accuracy_tracker]\n        \n    def train_step(self, data):\n        # Get the data of the minibatch\n        X, t = data\n        \n        # Use GradientTape to record everything we need to compute the gradient\n        with tf.GradientTape() as tape:\n\n            # Prediction using the model\n            y = self.model(X, training=True)\n            \n            # Cross-entropy loss\n            loss = tf.reduce_mean(\n                tf.reduce_sum(\n                    (t - y)**2, # Mean square error\n                    axis=1 # First index is the batch size, the second is the classes\n                )\n            )\n        \n        # Compute gradients\n        grads = tape.gradient(loss, self.trainable_weights)\n        \n        # Apply gradients using the optimizer\n        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n        \n        # Update metrics \n        self.loss_tracker.update_state(loss)\n        true_class = tf.reshape(tf.argmax(t, axis=1), shape=(-1, 1))\n        predicted_class = tf.reshape(tf.argmax(y, axis=1), shape=(-1, 1))\n        self.accuracy_tracker.update_state(true_class, predicted_class)\n        \n        # Return a dict mapping metric names to current value\n        return {\"loss\": self.loss_tracker.result(), 'accuracy': self.accuracy_tracker.result()} \n\n    def test_step(self, data):\n        \n        # Get data\n        X, t = data\n        \n        # Prediction\n        y = self.model(X, training=False)\n            \n        # Loss\n        loss = tf.reduce_mean(\n            tf.reduce_sum(\n                    (t - y)**2, # Mean square error\n                    axis=1\n            )\n        )\n        \n        # Update metrics \n        self.loss_tracker.update_state(loss)\n        true_class = tf.reshape(tf.argmax(t, axis=1), shape=(-1, 1))\n        predicted_class = tf.reshape(tf.argmax(y, axis=1), shape=(-1, 1))\n        self.accuracy_tracker.update_state(true_class, predicted_class)\n        \n        # Return a dict mapping metric names to current value\n        return {\"loss\": self.loss_tracker.result(), 'accuracy': self.accuracy_tracker.result()} \n                \n\n\n# Delete all previous models to free memory\ntf.keras.backend.clear_session()\n    \n# Create the custom model\nmodel = CNN()\n\n# Optimizer\noptimizer = tf.keras.optimizers.SGD(learning_rate=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n\n# Compile\nmodel.compile(\n    optimizer=optimizer, # learning rule\n)\n\n# Training\nhistory = tf.keras.callbacks.History()\nmodel.fit(\n    X_train, T_train,\n    batch_size=64, \n    epochs=20,\n    validation_split=0.1,\n    callbacks=[history]\n)\n\n# Testing\nscore = model.evaluate(X_test, T_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n\nplt.figure(figsize=(15, 6))\n\nplt.subplot(121)\nplt.plot(history.history['loss'], '-r', label=\"Training\")\nplt.plot(history.history['val_loss'], '-b', label=\"Validation\")\nplt.xlabel('Epoch #')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.subplot(122)\nplt.plot(history.history['accuracy'], '-r', label=\"Training\")\nplt.plot(history.history['val_accuracy'], '-b', label=\"Validation\")\nplt.xlabel('Epoch #')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.show()\n\nModel: \"model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         [(None, 28, 28, 1)]       0         \n_________________________________________________________________\nconv2d (Conv2D)              (None, 26, 26, 32)        320       \n_________________________________________________________________\nmax_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n_________________________________________________________________\ndropout (Dropout)            (None, 13, 13, 32)        0         \n_________________________________________________________________\nconv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 5, 5, 64)          0         \n_________________________________________________________________\nflatten (Flatten)            (None, 1600)              0         \n_________________________________________________________________\ndense (Dense)                (None, 150)               240150    \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 150)               0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 10)                1510      \n=================================================================\nTotal params: 260,476\nTrainable params: 260,476\nNon-trainable params: 0\n_________________________________________________________________\nNone\nEpoch 1/20\n844/844 [==============================] - 3s 3ms/step - loss: 0.4282 - accuracy: 0.6646 - val_loss: 0.0705 - val_accuracy: 0.9577\nEpoch 2/20\n844/844 [==============================] - 3s 3ms/step - loss: 0.1390 - accuracy: 0.9084 - val_loss: 0.0445 - val_accuracy: 0.9717\nEpoch 3/20\n844/844 [==============================] - 3s 3ms/step - loss: 0.1036 - accuracy: 0.9320 - val_loss: 0.0347 - val_accuracy: 0.9775\nEpoch 4/20\n844/844 [==============================] - 3s 3ms/step - loss: 0.0877 - accuracy: 0.9425 - val_loss: 0.0302 - val_accuracy: 0.9803\nEpoch 5/20\n844/844 [==============================] - 3s 3ms/step - loss: 0.0784 - accuracy: 0.9493 - val_loss: 0.0269 - val_accuracy: 0.9827\nEpoch 6/20\n844/844 [==============================] - 3s 3ms/step - loss: 0.0699 - accuracy: 0.9542 - val_loss: 0.0256 - val_accuracy: 0.9827\nEpoch 7/20\n844/844 [==============================] - 3s 3ms/step - loss: 0.0646 - accuracy: 0.9584 - val_loss: 0.0230 - val_accuracy: 0.9838\nEpoch 8/20\n844/844 [==============================] - 3s 3ms/step - loss: 0.0593 - accuracy: 0.9622 - val_loss: 0.0222 - val_accuracy: 0.9853\nEpoch 9/20\n844/844 [==============================] - 3s 3ms/step - loss: 0.0558 - accuracy: 0.9636 - val_loss: 0.0203 - val_accuracy: 0.9862\nEpoch 10/20\n844/844 [==============================] - 3s 3ms/step - loss: 0.0535 - accuracy: 0.9651 - val_loss: 0.0193 - val_accuracy: 0.9878\nEpoch 11/20\n844/844 [==============================] - 3s 3ms/step - loss: 0.0523 - accuracy: 0.9660 - val_loss: 0.0193 - val_accuracy: 0.9868\nEpoch 12/20\n844/844 [==============================] - 3s 3ms/step - loss: 0.0498 - accuracy: 0.9676 - val_loss: 0.0182 - val_accuracy: 0.9887\nEpoch 13/20\n844/844 [==============================] - 3s 3ms/step - loss: 0.0467 - accuracy: 0.9701 - val_loss: 0.0175 - val_accuracy: 0.9880\nEpoch 14/20\n844/844 [==============================] - 3s 3ms/step - loss: 0.0463 - accuracy: 0.9701 - val_loss: 0.0166 - val_accuracy: 0.9888\nEpoch 15/20\n844/844 [==============================] - 3s 3ms/step - loss: 0.0438 - accuracy: 0.9717 - val_loss: 0.0165 - val_accuracy: 0.9890\nEpoch 16/20\n844/844 [==============================] - 3s 3ms/step - loss: 0.0421 - accuracy: 0.9731 - val_loss: 0.0161 - val_accuracy: 0.9892\nEpoch 17/20\n844/844 [==============================] - 3s 3ms/step - loss: 0.0418 - accuracy: 0.9728 - val_loss: 0.0155 - val_accuracy: 0.9893\nEpoch 18/20\n844/844 [==============================] - 3s 3ms/step - loss: 0.0398 - accuracy: 0.9737 - val_loss: 0.0150 - val_accuracy: 0.9898\nEpoch 19/20\n844/844 [==============================] - 3s 3ms/step - loss: 0.0392 - accuracy: 0.9748 - val_loss: 0.0147 - val_accuracy: 0.9902\nEpoch 20/20\n844/844 [==============================] - 3s 3ms/step - loss: 0.0387 - accuracy: 0.9747 - val_loss: 0.0144 - val_accuracy: 0.9905\nTest loss: 0.01496267318725586\nTest accuracy: 0.9898999929428101\n\n\n\n\n\n\n\n\n\nA: Nothing, it also works… Only the loss has different values."
  },
  {
    "objectID": "exercises/12-VAE-solution.html#custom-layers",
    "href": "exercises/12-VAE-solution.html#custom-layers",
    "title": "Variational autoencoder",
    "section": "Custom layers",
    "text": "Custom layers\nKeras layers take a tensor as input (the output of the previous layer on a minibatch) and transform it into another tensor, possibly using trainable parameters. As we have seen, tensorflow allows to manipulate tensors and apply differentiable operations on them, so we could redefine the function made by a keras layer using tensorflow operations.\nThe following cell shows how to implement a dummy layer that takes a tensor T as input (the first dimension is the batch size) and returns the tensor \\exp - \\lambda \\, T, \\lambda being a fixed parameter.\n\nclass ExponentialLayer(tf.keras.layers.Layer):\n    \"\"\"Layer performing element-wise exponentiation.\"\"\"\n\n    def __init__(self, factor=1.0):\n        super(ExponentialLayer, self).__init__()\n        self.factor = factor\n\n    def call(self, inputs):\n        return tf.exp(- self.factor*inputs)\n\nExponentialLayer inherits from tf.keras.layers.Layer and redefines the call() method that defines the forward pass. Here we simply return the corresponding tensor.\nThe layer can then be used in a functional model directly:\nx = ExponentialLayer(factor=1.0)(x)\nAs we use tensorflow operators, it knows how to differentiate it when applying backpropagation.\nMore information on how to create new layers can be found at https://keras.io/guides/making_new_layers_and_models_via_subclassing. FYI, this is how you would redefine a fully-connected layer without an activation function, using a trainable weight matrix and bias vector:\nclass Linear(tf.keras.layers.Layer):\n    def __init__(self, units=32):\n        \"Number of neurons in the layer.\"\n        super(Linear, self).__init__()\n        self.units = units\n\n    def build(self, input_shape):\n        \"Create the weight matrix and bias vector once we know the shape of the previous layer.\"\n        self.w = self.add_weight(\n            shape=(input_shape[-1], self.units),\n            initializer=\"random_normal\",\n            trainable=True,\n        )\n        self.b = self.add_weight(\n            shape=(self.units,), initializer=\"random_normal\", trainable=True\n        )\n\n    def call(self, inputs):\n        \"Return W*X + b\"\n        return tf.matmul(inputs, self.w) + self.b\nQ: Add the exponential layer to the CNN between the last FC layer and the output layer. Change the value of the parameter. Does it still work?\n\n# Model\ninputs = tf.keras.layers.Input((28, 28, 1))\n\nx = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='valid')(inputs)\nx = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\nx = tf.keras.layers.Dropout(0.5)(x)\n\nx = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='valid')(x)\nx = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\nx = tf.keras.layers.Dropout(0.5)(x)\n\nx = tf.keras.layers.Flatten()(x)\n\nx = tf.keras.layers.Dense(150, activation='relu')(x)\nx = tf.keras.layers.Dropout(0.5)(x)\n\nx = ExponentialLayer(factor=1.0)(x)\n\noutputs = tf.keras.layers.Dense(10, activation='softmax')(x)\n\nmodel = tf.keras.Model(inputs, outputs)\nprint(model.summary())\n\n# Optimizer\noptimizer = tf.keras.optimizers.SGD(learning_rate=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n\n# Compile\nmodel.compile(\n    loss=\"categorical_crossentropy\",\n    optimizer=optimizer, # learning rule\n    metrics=[\"accuracy\"]\n)\n\n# Training\nhistory = tf.keras.callbacks.History()\nmodel.fit(\n    X_train, T_train,\n    batch_size=64, \n    epochs=20,\n    validation_split=0.1,\n    callbacks=[history]\n)\n\n# Testing\nscore = model.evaluate(X_test, T_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n\n\nplt.figure(figsize=(15, 6))\n\nplt.subplot(121)\nplt.plot(history.history['loss'], '-r', label=\"Training\")\nplt.plot(history.history['val_loss'], '-b', label=\"Validation\")\nplt.xlabel('Epoch #')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.subplot(122)\nplt.plot(history.history['accuracy'], '-r', label=\"Training\")\nplt.plot(history.history['val_accuracy'], '-b', label=\"Validation\")\nplt.xlabel('Epoch #')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.show()\n\nModel: \"model_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_2 (InputLayer)         [(None, 28, 28, 1)]       0         \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 26, 26, 32)        320       \n_________________________________________________________________\nmax_pooling2d_2 (MaxPooling2 (None, 13, 13, 32)        0         \n_________________________________________________________________\ndropout_3 (Dropout)          (None, 13, 13, 32)        0         \n_________________________________________________________________\nconv2d_3 (Conv2D)            (None, 11, 11, 64)        18496     \n_________________________________________________________________\nmax_pooling2d_3 (MaxPooling2 (None, 5, 5, 64)          0         \n_________________________________________________________________\ndropout_4 (Dropout)          (None, 5, 5, 64)          0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 1600)              0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 150)               240150    \n_________________________________________________________________\ndropout_5 (Dropout)          (None, 150)               0         \n_________________________________________________________________\nexponential_layer (Exponenti (None, 150)               0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 10)                1510      \n=================================================================\nTotal params: 260,476\nTrainable params: 260,476\nNon-trainable params: 0\n_________________________________________________________________\nNone\nEpoch 1/20\n844/844 [==============================] - 4s 4ms/step - loss: 1.3410 - accuracy: 0.5668 - val_loss: 0.1330 - val_accuracy: 0.9615\nEpoch 2/20\n844/844 [==============================] - 3s 4ms/step - loss: 0.2935 - accuracy: 0.9180 - val_loss: 0.0806 - val_accuracy: 0.9763\nEpoch 3/20\n844/844 [==============================] - 3s 4ms/step - loss: 0.2070 - accuracy: 0.9410 - val_loss: 0.0668 - val_accuracy: 0.9815\nEpoch 4/20\n844/844 [==============================] - 3s 4ms/step - loss: 0.1692 - accuracy: 0.9503 - val_loss: 0.0576 - val_accuracy: 0.9835\nEpoch 5/20\n844/844 [==============================] - 3s 4ms/step - loss: 0.1427 - accuracy: 0.9598 - val_loss: 0.0521 - val_accuracy: 0.9857\nEpoch 6/20\n844/844 [==============================] - 3s 4ms/step - loss: 0.1257 - accuracy: 0.9629 - val_loss: 0.0499 - val_accuracy: 0.9858\nEpoch 7/20\n844/844 [==============================] - 3s 3ms/step - loss: 0.1143 - accuracy: 0.9657 - val_loss: 0.0479 - val_accuracy: 0.9860\nEpoch 8/20\n844/844 [==============================] - 3s 3ms/step - loss: 0.1108 - accuracy: 0.9666 - val_loss: 0.0452 - val_accuracy: 0.9872\nEpoch 9/20\n844/844 [==============================] - 3s 4ms/step - loss: 0.1032 - accuracy: 0.9691 - val_loss: 0.0447 - val_accuracy: 0.9873\nEpoch 10/20\n844/844 [==============================] - 3s 4ms/step - loss: 0.1023 - accuracy: 0.9690 - val_loss: 0.0434 - val_accuracy: 0.9872\nEpoch 11/20\n844/844 [==============================] - 3s 4ms/step - loss: 0.0949 - accuracy: 0.9721 - val_loss: 0.0418 - val_accuracy: 0.9880\nEpoch 12/20\n844/844 [==============================] - 3s 4ms/step - loss: 0.0940 - accuracy: 0.9706 - val_loss: 0.0402 - val_accuracy: 0.9883\nEpoch 13/20\n844/844 [==============================] - 3s 4ms/step - loss: 0.0902 - accuracy: 0.9735 - val_loss: 0.0412 - val_accuracy: 0.9875\nEpoch 14/20\n844/844 [==============================] - 3s 4ms/step - loss: 0.0882 - accuracy: 0.9735 - val_loss: 0.0389 - val_accuracy: 0.9888\nEpoch 15/20\n844/844 [==============================] - 3s 4ms/step - loss: 0.0847 - accuracy: 0.9753 - val_loss: 0.0362 - val_accuracy: 0.9898\nEpoch 16/20\n844/844 [==============================] - 3s 4ms/step - loss: 0.0849 - accuracy: 0.9744 - val_loss: 0.0392 - val_accuracy: 0.9890\nEpoch 17/20\n844/844 [==============================] - 3s 3ms/step - loss: 0.0803 - accuracy: 0.9765 - val_loss: 0.0363 - val_accuracy: 0.9895\nEpoch 18/20\n844/844 [==============================] - 3s 3ms/step - loss: 0.0820 - accuracy: 0.9751 - val_loss: 0.0357 - val_accuracy: 0.9895\nEpoch 19/20\n844/844 [==============================] - 3s 4ms/step - loss: 0.0778 - accuracy: 0.9767 - val_loss: 0.0374 - val_accuracy: 0.9903\nEpoch 20/20\n844/844 [==============================] - 3s 4ms/step - loss: 0.0742 - accuracy: 0.9771 - val_loss: 0.0362 - val_accuracy: 0.9900\nTest loss: 0.03063642792403698\nTest accuracy: 0.9900000095367432\n\n\n\n\n\n\n\n\n\nA: Surprisingly, it still works, unless you pick a high value for the parameter. The exponential layer only outputs positive values, but that is enough information for the output layer to do its job."
  },
  {
    "objectID": "exercises/12-VAE-solution.html#variational-autoencoder",
    "href": "exercises/12-VAE-solution.html#variational-autoencoder",
    "title": "Variational autoencoder",
    "section": "Variational autoencoder",
    "text": "Variational autoencoder\nWe are now ready to implement the VAE. We are going to redefine the training set, as we want pixel values to be between 0 and 1 (so that we can compute a cross-entropy). Therefore, we do not perform removal:\n\n# Fetch the MNIST data\n(X_train, t_train), (X_test, t_test) = tf.keras.datasets.mnist.load_data()\nprint(\"Training data:\", X_train.shape, t_train.shape)\nprint(\"Test data:\", X_test.shape, t_test.shape)\n\n# Normalize the values\nX_train = X_train.reshape(-1, 28, 28, 1).astype('float32') / 255.\nX_test = X_test.reshape(-1, 28, 28, 1).astype('float32') / 255.\n\n# One-hot encoding\nT_train = tf.keras.utils.to_categorical(t_train, 10)\nT_test = tf.keras.utils.to_categorical(t_test, 10)\n\nTraining data: (60000, 28, 28) (60000,)\nTest data: (10000, 28, 28) (10000,)\n\n\n\nEncoder\nThe encoder can have any form, the only constraint is that is takes an input (28, 28, 1) and outputs two vectors \\mu and \\log(\\sigma) of size latent_dim, the parameters of the normal distribution representing the input. We are going to use only latent_dim=2 latent dimensions, but let’s make the code generic.\nFor a network to have two outputs, one just needs to use the functional API to create the graph:\n# Previous layer\nx = tf.keras.layers.Dense(N, activation=\"relu\")(x)\n\n# First output takes input from x\nz_mean = tf.keras.layers.Dense(latent_dim)(x)\n\n# Second output also takes input from x  \nz_log_var = tf.keras.layers.Dense(latent_dim)(x)\nThis would not be possible using the Sequential API, but is straightforward using the functional one, as you decide from where a layer takes its inputs.\nWhat we still need and is not standard in keras is a sampling layer that implements the reparameterization trick:\n\\mathbf{z} = \\mu + \\sigma \\, \\xi\nwhere \\xi comes from the standard normal distribution \\mathcal{N}(0, 1).\nFor technical reasons, it is actually better when z_log_var represents \\log \\sigma^2 instead of \\sigma, as it can take both positive and negative values, while \\sigma could only be strictly positive.\nWe therefore want a layer that computes:\nz = z_mean + tf.math.exp(0.5 * z_log_var) * xi\non the tensors of shape (batch_size, latent_dim). To sample the standard normal distribution, you can use tensorflow:\nxi = tf.random.normal(shape=(batch_size, latent_dim) mean=0.0, stddev=1.0)\nQ: Create a custom SamplingLayer layer that takes inputs from z_mean and z_log_var, being called like this:\nz = SamplingLayer()([z_mean, z_log_var])\nIn order to get each input separately, the inputs argument can be split:\ndef call(self, inputs):\n    z_mean, z_log_var = inputs\nThe only difficulty is to pass the correct dimensions to xi, as you do not know the batch size yet. You can retrieve it using the shape of z_mean:\nbatch_size = tf.shape(z_mean)[0]\nlatent_dim = tf.shape(z_mean)[1]\n\nclass SamplingLayer(tf.keras.layers.Layer):\n    \"\"\"Uses (z_mean, z_log_var) to sample z.\"\"\"\n\n    def __init__(self):\n        super(SamplingLayer, self).__init__()\n\n    def call(self, inputs):\n        # Retrieve inputs mu and 2*log(sigma)\n        z_mean, z_log_var = inputs\n\n        # Batch size and latent dimension\n        batch_size = tf.shape(z_mean)[0]\n        latent_dim = tf.shape(z_mean)[1]\n        \n        # Random variable from the standard normal distribution\n        xi = tf.random.normal(shape=(batch_size, latent_dim), mean=0.0, stddev=1.0)\n        \n        # Reparameterization trick\n        return z_mean + tf.math.exp(0.5 * z_log_var) * xi\n\nWe can now create the encoder in a create_encoder(latent_dim) method that return an uncompiled model.\nYou can put what you want in the encoder as long as it takes a (28, 28, 1) input and returns the three layers [z_mean, z_log_var, z] (we need z_mean and z_log_var to define the loss, normally you only need z):\ndef create_encoder(latent_dim):\n\n    inputs = tf.keras.layers.Input(shape=(28, 28, 1))\n    \n    # Stuff, with x being the last FC layer\n\n    z_mean = tf.keras.layers.Dense(latent_dim)(x)\n    \n    z_log_var = tf.keras.layers.Dense(latent_dim)(x)\n    \n    z = SamplingLayer()([z_mean, z_log_var])\n\n    model = tf.keras.Model(inputs, [z_mean, z_log_var, z])\n    \n    print(model.summary())\n\n    return model\nOne suggestion would be to use two convolutional layers with a stride of 2 (replacing max-pooling) and one fully-connected layer with enough neurons, but you do what you want.\nQ: Create the encoder.\n\ndef create_encoder(latent_dim):\n\n    inputs = tf.keras.layers.Input(shape=(28, 28, 1))\n    \n    x = tf.keras.layers.Conv2D(32, (3, 3), strides=2, activation='relu', padding='valid')(inputs)\n\n    x = tf.keras.layers.Conv2D(64, (3, 3), strides=2, activation='relu', padding='valid')(x)\n\n    x = tf.keras.layers.Flatten()(x)\n\n    x = tf.keras.layers.Dense(16, activation=\"relu\")(x)\n\n    z_mean = tf.keras.layers.Dense(latent_dim)(x)\n    \n    z_log_var = tf.keras.layers.Dense(latent_dim)(x)\n    \n    z = SamplingLayer()([z_mean, z_log_var])\n\n    model = tf.keras.Model(inputs, [z_mean, z_log_var, z])\n    \n    print(model.summary())\n\n    return model\n\nThe decoder is a bit more tricky. It takes the vector z as an input (latent_dim=2 dimensions) and should output an image (28, 28, 1) with pixels normailzed between 0 and 1. The output layer should therefore use the 'sigmoid' transfer function:\ndef create_decoder(latent_dim):\n    \n    inputs = tf.keras.layers.Input(shape=(latent_dim,))\n\n    # Stuff, with x being a transposed convolution layer of shape (28, 28, N)\n    \n    outputs = tf.keras.layers.Conv2DTranspose(1, (3, 3), activation=\"sigmoid\", padding=\"same\")(x)\n    \n    model = tf.keras.Model(inputs, outputs)\n    print(model.summary())\n    \n    return model\nThe decoder has to use transposed convolutions to upsample the tensors instead of downsampling them. Check the doc of Conv2DTranspose at https://keras.io/api/layers/convolution_layers/convolution2d_transpose/.\nIn order to build the decoder, you have to be careful when it comes to tensor shapes: the output must be exactly (28, 28, 1), not (26, 26, 1), otherwise you will not be able to compute the reconstruction loss. You need to be careful with the stride (upsampling ratio) and padding method (‘same’ or ‘valid’) of the layers you add. Do not hesitate to create dummy models and print their summary to see the shapes.\nAnother trick is that you need to transform the vector z with latent_dim=2 elements into a 3D tensor before applying transposed convolutions (i.e. the inverse of Flatten()). If you for example want a tensor of shape (7, 7, 64) as the input to the first transposed convolution, you could project the vector to a fully connected layer with 7*7*64 neurons:\nx = tf.keras.layers.Dense(7 * 7 * 64, activation=\"relu\")(inputs)\nand reshape it to a (7, 7, 64) tensor:\nx = tf.keras.layers.Reshape((7, 7, 64))(x)\nQ: Create the decoder.\n\ndef create_decoder(latent_dim):\n    \n    inputs = tf.keras.layers.Input(shape=(latent_dim,))\n\n    x = tf.keras.layers.Dense(7 * 7 * 64, activation=\"relu\")(inputs)\n    x = tf.keras.layers.Reshape((7, 7, 64))(x)\n\n    x = tf.keras.layers.Conv2DTranspose(64, (3, 3), activation=\"relu\", strides=2, padding=\"same\")(x)\n    x = tf.keras.layers.Conv2DTranspose(32, (3, 3), activation=\"relu\", strides=2, padding=\"same\")(x)\n    \n    outputs = tf.keras.layers.Conv2DTranspose(1, (3, 3), activation=\"sigmoid\", padding=\"same\")(x)\n    \n    model = tf.keras.Model(inputs, outputs)\n    print(model.summary())\n    \n    return model\n\nQ: Create a custom VAE model (inheriting from tf.keras.Model) that:\n\ntakes the latent dimension as argument:\n\nvae = VAE(latent_dim=2)\n\ncreates the encoder and decoder in the constructor.\ntracks the reconstruction and KL losses as metrics.\ndoes not use validation data (i.e., do not implement test_step() and do not provide any validation data to fit()).\ncomputes the reconstruction loss using binary cross-entropy over all pixels of the reconstructed image:\n\n\\mathcal{L}_\\text{reconstruction}(\\theta) = \\frac{1}{N} \\sum_{i=1}^N \\sum_{w, h \\in \\text{pixels}} - t^i(w, h) \\, \\log y^i(w, h) - (1 - t^i(w, h)) \\, \\log(1 - y^i(w, h))\nwhere t^i(w, h) is the pixel of coordinates (w, h) (between 0 and 27) of the i-th image of the minibatch.\n\ncomputes the KL divergence loss for the encoder:\n\n\\mathcal{L}_\\text{KL}(\\theta) = \\frac{1}{N} \\sum_{i=1}^N -0.5 \\, (1 + \\text{z\\_log\\_var}^i - (\\text{z\\_mean}^i)^2 - \\exp(\\text{z\\_log\\_var}^i))\n\nminimizes the total loss:\n\n\\mathcal{L}(\\theta) = \\mathcal{L}_\\text{reconstruction}(\\theta) + \\mathcal{L}_\\text{KL}(\\theta)\nTrain it on the MNIST images for 30 epochs with the right batch size and a good optimizer (only the images: vae.fit(X_train, X_train, epochs=30, batch_size=b)). How do the losses evolve?\nHint: for the reconstruction loss, you can implement the formula using tensorflow operations, or call tf.keras.losses.binary_crossentropy(t, y) directly.\nDo not worry if your reconstruction loss does not go to zero, but stays in the hundreds, it is normal. Use the next cell to visualize the reconstructions.\n\nclass VAE(tf.keras.Model):\n    def __init__(self, latent_dim):\n        super(VAE, self).__init__()\n\n        # Encoder\n        self.encoder = create_encoder(latent_dim)\n\n        # Decoder\n        self.decoder = create_decoder(latent_dim)\n        \n        # Track losses\n        self.total_loss_tracker = tf.keras.metrics.Mean(name=\"total_loss\")\n        self.reconstruction_loss_tracker = tf.keras.metrics.Mean(name=\"reconstruction_loss\")\n        self.kl_loss_tracker = tf.keras.metrics.Mean(name=\"kl_loss\")\n\n    def train_step(self, data):\n\n        with tf.GradientTape() as tape:\n\n            # Data: input = output\n            X, t = data\n\n            # Encoder\n            z_mean, z_log_var, z = self.encoder(X)\n            \n            # Decoder\n            y = self.decoder(z)\n            \n            reconstruction_loss = tf.reduce_mean(\n                tf.reduce_sum(\n                   #- t * tf.math.log(y) - (1. - t) * tf.math.log(1. - y), \n                   tf.keras.losses.binary_crossentropy(t, y),\n                    axis=(1, 2)\n                )\n            )\n\n            kl_loss = tf.reduce_mean(\n                tf.reduce_sum(\n                    -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)), \n                    axis=1\n                )\n            )\n            \n            total_loss = reconstruction_loss + kl_loss\n        \n        grads = tape.gradient(total_loss, self.trainable_weights)\n        \n        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n        \n        self.total_loss_tracker.update_state(total_loss)\n        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n        self.kl_loss_tracker.update_state(kl_loss)\n        \n        return {\n            \"loss\": self.total_loss_tracker.result(),\n            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n            \"kl_loss\": self.kl_loss_tracker.result(),\n        }\n\n\n    @property\n    def metrics(self):\n        return [\n            self.total_loss_tracker,\n            self.reconstruction_loss_tracker,\n            self.kl_loss_tracker,\n        ]\n\n\n# Delete all previous models to free memory\ntf.keras.backend.clear_session()\n\n# Create the VAE with 2 latent variables\nvae = VAE(latent_dim=2)\n\n# Optimizer\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n\n# Compile\nvae.compile(optimizer=optimizer)\n\n# Train the VAE\nvae.fit(X_train, X_train, epochs=30, batch_size=128)\n\nModel: \"model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            [(None, 28, 28, 1)]  0                                            \n__________________________________________________________________________________________________\nconv2d (Conv2D)                 (None, 13, 13, 32)   320         input_1[0][0]                    \n__________________________________________________________________________________________________\nconv2d_1 (Conv2D)               (None, 6, 6, 64)     18496       conv2d[0][0]                     \n__________________________________________________________________________________________________\nflatten (Flatten)               (None, 2304)         0           conv2d_1[0][0]                   \n__________________________________________________________________________________________________\ndense (Dense)                   (None, 16)           36880       flatten[0][0]                    \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, 2)            34          dense[0][0]                      \n__________________________________________________________________________________________________\ndense_2 (Dense)                 (None, 2)            34          dense[0][0]                      \n__________________________________________________________________________________________________\nsampling_layer (SamplingLayer)  (None, 2)            0           dense_1[0][0]                    \n                                                                 dense_2[0][0]                    \n==================================================================================================\nTotal params: 55,764\nTrainable params: 55,764\nNon-trainable params: 0\n__________________________________________________________________________________________________\nNone\nModel: \"model_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_2 (InputLayer)         [(None, 2)]               0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 3136)              9408      \n_________________________________________________________________\nreshape (Reshape)            (None, 7, 7, 64)          0         \n_________________________________________________________________\nconv2d_transpose (Conv2DTran (None, 14, 14, 64)        36928     \n_________________________________________________________________\nconv2d_transpose_1 (Conv2DTr (None, 28, 28, 32)        18464     \n_________________________________________________________________\nconv2d_transpose_2 (Conv2DTr (None, 28, 28, 1)         289       \n=================================================================\nTotal params: 65,089\nTrainable params: 65,089\nNon-trainable params: 0\n_________________________________________________________________\nNone\nEpoch 1/30\n469/469 [==============================] - 4s 8ms/step - loss: 393.0657 - reconstruction_loss: 284.5855 - kl_loss: 6.9494\nEpoch 2/30\n469/469 [==============================] - 4s 8ms/step - loss: 207.5198 - reconstruction_loss: 200.5430 - kl_loss: 4.4740\nEpoch 3/30\n469/469 [==============================] - 4s 8ms/step - loss: 199.5044 - reconstruction_loss: 194.5823 - kl_loss: 4.0284\nEpoch 4/30\n469/469 [==============================] - 4s 8ms/step - loss: 195.3614 - reconstruction_loss: 190.8111 - kl_loss: 3.7996\nEpoch 5/30\n469/469 [==============================] - 4s 8ms/step - loss: 191.7219 - reconstruction_loss: 187.5892 - kl_loss: 3.4334\nEpoch 6/30\n469/469 [==============================] - 4s 8ms/step - loss: 188.8362 - reconstruction_loss: 184.9203 - kl_loss: 3.0996\nEpoch 7/30\n469/469 [==============================] - 4s 8ms/step - loss: 185.9255 - reconstruction_loss: 180.6233 - kl_loss: 3.4042\nEpoch 8/30\n469/469 [==============================] - 4s 8ms/step - loss: 176.6383 - reconstruction_loss: 170.8149 - kl_loss: 4.6435\nEpoch 9/30\n469/469 [==============================] - 4s 8ms/step - loss: 171.3536 - reconstruction_loss: 165.9545 - kl_loss: 4.7176\nEpoch 10/30\n469/469 [==============================] - 4s 8ms/step - loss: 168.2131 - reconstruction_loss: 162.9994 - kl_loss: 4.7839\nEpoch 11/30\n469/469 [==============================] - 4s 8ms/step - loss: 166.1654 - reconstruction_loss: 160.9594 - kl_loss: 4.8625\nEpoch 12/30\n469/469 [==============================] - 4s 8ms/step - loss: 165.0664 - reconstruction_loss: 159.4020 - kl_loss: 4.9347\nEpoch 13/30\n469/469 [==============================] - 4s 8ms/step - loss: 163.4539 - reconstruction_loss: 158.1412 - kl_loss: 4.9901\nEpoch 14/30\n469/469 [==============================] - 4s 8ms/step - loss: 162.0623 - reconstruction_loss: 157.0392 - kl_loss: 5.0722\nEpoch 15/30\n469/469 [==============================] - 4s 8ms/step - loss: 161.2617 - reconstruction_loss: 156.0749 - kl_loss: 5.1209\nEpoch 16/30\n469/469 [==============================] - 4s 8ms/step - loss: 160.7616 - reconstruction_loss: 155.2767 - kl_loss: 5.1756\nEpoch 17/30\n469/469 [==============================] - 4s 8ms/step - loss: 160.0449 - reconstruction_loss: 154.5632 - kl_loss: 5.2174\nEpoch 18/30\n469/469 [==============================] - 4s 8ms/step - loss: 159.1217 - reconstruction_loss: 153.9499 - kl_loss: 5.2415\nEpoch 19/30\n469/469 [==============================] - 4s 8ms/step - loss: 158.5849 - reconstruction_loss: 153.3928 - kl_loss: 5.2878\nEpoch 20/30\n469/469 [==============================] - 4s 8ms/step - loss: 158.3445 - reconstruction_loss: 152.9665 - kl_loss: 5.3126\nEpoch 21/30\n469/469 [==============================] - 4s 8ms/step - loss: 157.8068 - reconstruction_loss: 152.4942 - kl_loss: 5.3551\nEpoch 22/30\n469/469 [==============================] - 4s 8ms/step - loss: 157.6673 - reconstruction_loss: 152.0566 - kl_loss: 5.3859\nEpoch 23/30\n469/469 [==============================] - 4s 8ms/step - loss: 157.2331 - reconstruction_loss: 151.6619 - kl_loss: 5.4183\nEpoch 24/30\n469/469 [==============================] - 4s 8ms/step - loss: 156.8239 - reconstruction_loss: 151.2990 - kl_loss: 5.4547\nEpoch 25/30\n469/469 [==============================] - 4s 8ms/step - loss: 156.7459 - reconstruction_loss: 151.0214 - kl_loss: 5.4676\nEpoch 26/30\n469/469 [==============================] - 4s 8ms/step - loss: 156.3166 - reconstruction_loss: 150.7014 - kl_loss: 5.5080\nEpoch 27/30\n469/469 [==============================] - 4s 8ms/step - loss: 155.6717 - reconstruction_loss: 150.3687 - kl_loss: 5.5188\nEpoch 28/30\n469/469 [==============================] - 4s 8ms/step - loss: 155.2762 - reconstruction_loss: 150.0548 - kl_loss: 5.5485\nEpoch 29/30\n469/469 [==============================] - 4s 8ms/step - loss: 155.5086 - reconstruction_loss: 149.7990 - kl_loss: 5.5619\nEpoch 30/30\n469/469 [==============================] - 4s 8ms/step - loss: 154.9969 - reconstruction_loss: 149.5608 - kl_loss: 5.5754\n\n\n<tensorflow.python.keras.callbacks.History at 0x7fe71f074710>\n\n\nQ: The following cell allows to regularly sample the latent space and reconstruct the images. It makes the assumption that the decoder is stored at vae.decoder, adapt it otherwise. Comment on the generated samples. Observe in particluar the smooth transitions between similar digits.\n\ndef plot_latent_space(vae, n=30, figsize=15):\n    # display a n*n 2D manifold of digits\n    digit_size = 28\n    scale = 2.0\n    figure = np.zeros((digit_size * n, digit_size * n))\n    # linearly spaced coordinates corresponding to the 2D plot\n    # of digit classes in the latent space\n    grid_x = np.linspace(-scale, scale, n)\n    grid_y = np.linspace(-scale, scale, n)[::-1]\n\n    for i, yi in enumerate(grid_y):\n        for j, xi in enumerate(grid_x):\n            z_sample = np.array([[xi, yi]])\n            x_decoded = vae.decoder.predict(z_sample)\n            digit = x_decoded[0].reshape(digit_size, digit_size)\n            figure[\n                i * digit_size : (i + 1) * digit_size,\n                j * digit_size : (j + 1) * digit_size,\n            ] = digit\n\n    plt.figure(figsize=(figsize, figsize))\n    start_range = digit_size // 2\n    end_range = n * digit_size + start_range\n    pixel_range = np.arange(start_range, end_range, digit_size)\n    sample_range_x = np.round(grid_x, 1)\n    sample_range_y = np.round(grid_y, 1)\n    plt.xticks(pixel_range, sample_range_x)\n    plt.yticks(pixel_range, sample_range_y)\n    plt.xlabel(\"z[0]\")\n    plt.ylabel(\"z[1]\")\n    plt.imshow(figure, cmap=\"Greys_r\")\n    plt.show()\n\n\nplot_latent_space(vae)\n\n\n\n\n\n\n\n\nQ: The following cell visualizes the latent representation for the training data, using different colors for the digits. What do you think?\n\ndef plot_label_clusters(vae, data, labels):\n    # display a 2D plot of the digit classes in the latent space\n    z_mean, _, _ = vae.encoder.predict(data)\n    plt.figure(figsize=(12, 10))\n    plt.scatter(z_mean[:, 0], z_mean[:, 1], c=labels)\n    plt.colorbar()\n    plt.xlabel(\"z[0]\")\n    plt.ylabel(\"z[1]\")\n    plt.show()\n\n\nplot_label_clusters(vae, X_train, t_train)\n\n\n\n\n\n\n\n\nA: Without having been instructed to, the encoder already separates quite well the different classes of digits in the latent space. A shallow classifier on the latent space with 2 dimensions might be able to classify the MNIST data!\nHere, we used labelled data to train the autoencoder, but the use-case would be semi-supervised learning: train the autoencoder on unsupervised unlabelled data, and then train a classifier on its latent space using a small amount of labelled data."
  },
  {
    "objectID": "exercises/13-RNN-solution.html#sentiment-analysis",
    "href": "exercises/13-RNN-solution.html#sentiment-analysis",
    "title": "Recurrent neural networks",
    "section": "Sentiment analysis",
    "text": "Sentiment analysis\nThe goal to use recurrent neural networks (LSTM) to perform sentiment analysis on short sentences, i.e. to predict whether the sentence has a positive or negative meaning.\nThe following cells represent your training and test data. They are lists of lists, where the first element is the sentence as a string, and the second a boolean, with True for positive sentences, False for negative ones.\nNotice how some sentences are ambiguous (if you do not notice the “not”, the sentiment might be very different).\n\ntrain_data = [\n  ['good', True],\n  ['bad', False],\n  ['happy', True],\n  ['sad', False],\n  ['not good', False],\n  ['not bad', True],\n  ['not happy', False],\n  ['not sad', True],\n  ['very good', True],\n  ['very bad', False],\n  ['very happy', True],\n  ['very sad', False],\n  ['i am happy', True],\n  ['this is good', True],\n  ['i am bad', False],\n  ['this is bad', False],\n  ['i am sad', False],\n  ['this is sad', False],\n  ['i am not happy', False],\n  ['this is not good', False],\n  ['i am not bad', True],\n  ['this is not sad', True],\n  ['i am very happy', True],\n  ['this is very good', True],\n  ['i am very bad', False],\n  ['this is very sad', False],\n  ['this is very happy', True],\n  ['i am good not bad', True],\n  ['this is good not bad', True],\n  ['i am bad not good', False],\n  ['i am good and happy', True],\n  ['this is not good and not happy', False],\n  ['i am not at all good', False],\n  ['i am not at all bad', True],\n  ['i am not at all happy', False],\n  ['this is not at all sad', True],\n  ['this is not at all happy', False],\n  ['i am good right now', True],\n  ['i am bad right now', False],\n  ['this is bad right now', False],\n  ['i am sad right now', False],\n  ['i was good earlier', True],\n  ['i was happy earlier', True],\n  ['i was bad earlier', False],\n  ['i was sad earlier', False],\n  ['i am very bad right now', False],\n  ['this is very good right now', True],\n  ['this is very sad right now', False],\n  ['this was bad earlier', False],\n  ['this was very good earlier', True],\n  ['this was very bad earlier', False],\n  ['this was very happy earlier', True],\n  ['this was very sad earlier', False],\n  ['i was good and not bad earlier', True],\n  ['i was not good and not happy earlier', False],\n  ['i am not at all bad or sad right now', True],\n  ['i am not at all good or happy right now', False],\n  ['this was not happy and not good earlier', False],\n]\n\n\ntest_data = [\n  ['this is happy', True],\n  ['i am good', True],\n  ['this is not happy', False],\n  ['i am not good', False],\n  ['this is not bad', True],\n  ['i am not sad', True],\n  ['i am very good', True],\n  ['this is very bad', False],\n  ['i am very sad', False],\n  ['this is bad not good', False],\n  ['this is good and happy', True],\n  ['i am not good and not happy', False],\n  ['i am not at all sad', True],\n  ['this is not at all good', False],\n  ['this is not at all bad', True],\n  ['this is good right now', True],\n  ['this is sad right now', False],\n  ['this is very bad right now', False],\n  ['this was good earlier', True],\n  ['i was not happy and not good earlier', False],\n  ['earlier i was good and not bad', True],\n]\n\n\nN_train = len(train_data)\nN_test = len(test_data)\nprint(N_train, \"training sentences.\")\nprint(N_test, \"test sentences.\")\n\n58 training sentences.\n21 test sentences.\n\n\n\nData preparation\nThe most boring part when training LSTMs on text is to prepare the data correctly. Sentences are sequences of words (possibly a huge number of words), with a variable length (some sentences are shorter than others).\nWhat neural networks expect as input is a fixed-length sequence of numerical vectors \\{\\mathbf{x}_t\\}_{t=0}^T, i.e. they must have a fixed size. So we need to transform each sentence into this format.\nThe first thing to do is to identify the vocabulary, i.e. the unique words in the training set (fortunately, the test set uses the same exact words) as well as the maximal number of words in each sentence (again, the test set does not have longer sentences).\nQ: Create a list vocabulary of unique words in the training set and compute the maximal length nb_words of a sentence.\nTo extract the words in each sentence, the split() method of Python strings might come handy:\nsentence = \"I fear this exercise will be difficult\"\nprint(sentence.split(\" \"))\nYou will also find the set Python object useful to identify unique works. Check the doc. But there are many ways to do that (for loops), just do it the way you prefer.\n\nvocabulary = list(set([w for pair in train_data for w in pair[0].split(' ')]))\nprint('Unique words found', len(vocabulary)) # 18 unique words found\n\nnb_words = np.max([len(pair[0].split(' ')) for pair in train_data])\nprint(\"Maximum sequence length:\", nb_words)\n\nUnique words found 18\nMaximum sequence length: 10\n\n\nNow that we have found our list of 18 unique words, we need to able to perform one-hot encoding of each word, i.e. write a method def one_hot_encoding(word, vocabulary) that takes a word (e.g. “good”) and the vocabulary, and returns a vector of size 18, with mostly zeros, except for a 1.0 at the location of the word in the vocabulary.\nFor example, if your vocabulary is [\"I\", \"love\", \"you\"], the one-hot encoding of “I” should be np.array([1., 0., 0.]), the one of “love” is np.array([0., 1., 0.]), etc.\nQ: Implement the one_hot_encoding() method for single words.\nHint: you might find the method index() of list objects interesting.\n\ndef one_hot_encoding(word, vocabulary):\n    r = np.zeros(len(vocabulary))\n    r[vocabulary.index(word)] = 1.0\n    return r\n\nQ: You can now create the training set X_train, T_train and the test set X_test, T_test.\nThe training input data X_train should be a numpy array with 3 dimensions:\n X_train = np.zeros((N_train, nb_words, len(vocabulary)))\nThe first index corresponds to each sentence. The second index represents the index of each word in the sentence (maximally nb_words=10). The third index is for the one-hot encoding (18 elements).\nBeware: most sentences are shorter than nb_words=10. In that case, the words should be set at the end of the sequence, i.e. you prepend zero vectors.\nFor example, “I love you” should be encoded as:\n\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"I\", \"love\", \"you\"\nnot as:\n\"I\", \"love\", \"you\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"\nThe reason for that is that the LSTM will get the words one by one and only respond “positive” or “negative” after the last word has been seen. If the words are provided at the beginning of the sequence, vanishing gradients might delete them.\nThe same holds for the test set, it only has less sentences.\n\ndef prepare_data(data, vocabulary, nb_words):\n    \n    N = len(data)\n    X = np.zeros((N, nb_words, len(vocabulary)))\n    T = np.zeros((N, ))\n    \n    # Iterate over the data\n    for i in range(N):\n        x, t = data[i]\n        # Transform the sentence\n        words = x.split(\" \")\n        for j in range(len(words)):\n            word = words[j]\n            encoding = one_hot_encoding(word, vocabulary)\n            X[i, -len(words) + j, :] = encoding\n        # Transform the output\n        T[i] = int(t)\n    return X, T\n\n\nX_train, T_train = prepare_data(train_data, vocabulary, nb_words)\nX_test, T_test = prepare_data(test_data, vocabulary, nb_words)\nprint(X_train.shape)\nprint(T_train.shape)\nprint(X_test.shape)\nprint(T_test.shape)\n\n(58, 10, 18)\n(58,)\n(21, 10, 18)\n(21,)\n\n\n\n\nTraining the LSTM\nNow we just have to provide the data to a recurrent network. The problem is not very complicated, so we will need a single LSTM layer, followed by a single output neuron (with the logistic transfer function) whose role is to output 1 for the positive class, 0 for the negative one.\nQ: Check the documentation for the LSTM layer of keras: https://keras.io/api/layers/recurrent_layers/lstm/. It has many parameters:\ntf.keras.layers.LSTM(\n    units, \n    activation='tanh', \n    recurrent_activation='sigmoid', \n    use_bias=True, \n    kernel_initializer='glorot_uniform', \n    recurrent_initializer='orthogonal', \n    bias_initializer='zeros', \n    unit_forget_bias=True, \n    kernel_regularizer=None, \n    recurrent_regularizer=None, bias_regularizer=None, \n    activity_regularizer=None, kernel_constraint=None, \n    recurrent_constraint=None, bias_constraint=None, \n    dropout=0.0, recurrent_dropout=0.0, \n    implementation=2, \n    return_sequences=False, return_state=False, \n    go_backwards=False, stateful=False, unroll=False)\nThe default value for the parameters is the vanilla LSTM seen in the lectures, but you have the possibility to change the activation functions for the inputs and outputs (not the gates, it must be a sigmoid!), initialize the weights differently, add regularization or dropout, use biases or not, etc. That’s a lot to play with. For this exercise, stick to the default parameters at the beginning. The only thing you need to define is the number of neurons units of the layer.\ntf.keras.layers.LSTM(units=N)\nNote that an important parameter is return_sequences. When set to False (the default), the LSTM layer will process the complete sequence of 10 word vectors, and output a single vector of N values (the number of units). When set to True, the layer would return a sequence of 10 vectors of size N.\nHere, we only want the LSTM layer to encode the sentence and feed a single vector to the output layer, so we can leave it to False. If we wanted to stack two LSTM layers on top of each other, we would need to set return_sequences to True for the first layer and False for the second one (you can try that later):\ntf.keras.layers.LSTM(N, return_sequences=True)\ntf.keras.layers.LSTM(M, return_sequences=False)\nQ: Create a model with one LSTM layer (with enough units) and one output layer with one neuron ('sigmoid' activation function). Choose an optimizer (SGD, RMSprop, Adam, etc) and a good learning rate. When compiling the model, use the 'binary_crossentropy' loss function as it is a binary classification.\nThe input layer of the network must take a (nb_words, len(vocabulary)) matrix as input, i.e. (window, nb_features).\ntf.keras.layers.Input((nb_words, len(vocabulary)))\nWhen training the model with model.fit(), you can pass the test set as validation data, as we do not have too many examples:\nmodel.fit(X_train, T_train, validation_data=(X_test, T_test), ...)\nTrain the model for enough epochs, using a batch size big enough but not too big. In other terms: do the hyperparameter search yourself ;).\n\ndef small_model():\n    tf.keras.backend.clear_session()\n\n    model = tf.keras.models.Sequential()\n    \n    model.add(tf.keras.layers.Input((nb_words, len(vocabulary))))\n    \n    model.add(tf.keras.layers.LSTM(10))\n    \n    model.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n    \n    optimizer = tf.keras.optimizers.Adam(lr=0.01)\n    \n    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['binary_accuracy'])\n    print(model.summary())\n    return model\n\n\nmodel = small_model()\n\nhistory = tf.keras.callbacks.History()\n\nmodel.fit(X_train, T_train, validation_data=(X_test, T_test), epochs=30, batch_size=10, callbacks=[history])\n\nscore = model.evaluate(X_test, T_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n\nplt.figure(figsize=(15, 6))\n\nplt.subplot(121)\nplt.plot(history.history['loss'], '-r', label=\"Training\")\nplt.plot(history.history['val_loss'], '-b', label=\"Validation\")\nplt.xlabel('Epoch #')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.subplot(122)\nplt.plot(history.history['binary_accuracy'], '-r', label=\"Training\")\nplt.plot(history.history['val_binary_accuracy'], '-b', label=\"Validation\")\nplt.xlabel('Epoch #')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.show()\n\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nlstm (LSTM)                  (None, 10)                1160      \n_________________________________________________________________\ndense (Dense)                (None, 1)                 11        \n=================================================================\nTotal params: 1,171\nTrainable params: 1,171\nNon-trainable params: 0\n_________________________________________________________________\nNone\nEpoch 1/30\n6/6 [==============================] - 4s 114ms/step - loss: 0.6982 - binary_accuracy: 0.4164 - val_loss: 0.6966 - val_binary_accuracy: 0.5238\nEpoch 2/30\n6/6 [==============================] - 0s 10ms/step - loss: 0.6710 - binary_accuracy: 0.6405 - val_loss: 0.7026 - val_binary_accuracy: 0.4762\nEpoch 3/30\n6/6 [==============================] - 0s 9ms/step - loss: 0.6980 - binary_accuracy: 0.5057 - val_loss: 0.6954 - val_binary_accuracy: 0.4762\nEpoch 4/30\n6/6 [==============================] - 0s 9ms/step - loss: 0.6846 - binary_accuracy: 0.5564 - val_loss: 0.6934 - val_binary_accuracy: 0.4762\nEpoch 5/30\n6/6 [==============================] - 0s 9ms/step - loss: 0.6814 - binary_accuracy: 0.5319 - val_loss: 0.6898 - val_binary_accuracy: 0.4762\nEpoch 6/30\n6/6 [==============================] - 0s 9ms/step - loss: 0.6605 - binary_accuracy: 0.6392 - val_loss: 0.6916 - val_binary_accuracy: 0.5238\nEpoch 7/30\n6/6 [==============================] - 0s 9ms/step - loss: 0.6487 - binary_accuracy: 0.5997 - val_loss: 0.6900 - val_binary_accuracy: 0.5714\nEpoch 8/30\n6/6 [==============================] - 0s 10ms/step - loss: 0.6536 - binary_accuracy: 0.6133 - val_loss: 0.6799 - val_binary_accuracy: 0.4762\nEpoch 9/30\n6/6 [==============================] - 0s 9ms/step - loss: 0.6062 - binary_accuracy: 0.7737 - val_loss: 0.6733 - val_binary_accuracy: 0.5238\nEpoch 10/30\n6/6 [==============================] - 0s 9ms/step - loss: 0.6316 - binary_accuracy: 0.6892 - val_loss: 0.6538 - val_binary_accuracy: 0.5238\nEpoch 11/30\n6/6 [==============================] - 0s 9ms/step - loss: 0.5812 - binary_accuracy: 0.6942 - val_loss: 0.6212 - val_binary_accuracy: 0.5714\nEpoch 12/30\n6/6 [==============================] - 0s 8ms/step - loss: 0.5466 - binary_accuracy: 0.7803 - val_loss: 0.5744 - val_binary_accuracy: 0.7619\nEpoch 13/30\n6/6 [==============================] - 0s 28ms/step - loss: 0.5181 - binary_accuracy: 0.7368 - val_loss: 0.4873 - val_binary_accuracy: 0.8095\nEpoch 14/30\n6/6 [==============================] - 0s 10ms/step - loss: 0.4618 - binary_accuracy: 0.8465 - val_loss: 0.4202 - val_binary_accuracy: 0.7619\nEpoch 15/30\n6/6 [==============================] - 0s 9ms/step - loss: 0.3706 - binary_accuracy: 0.8382 - val_loss: 0.3515 - val_binary_accuracy: 0.8095\nEpoch 16/30\n6/6 [==============================] - 0s 11ms/step - loss: 0.2915 - binary_accuracy: 0.8628 - val_loss: 0.2920 - val_binary_accuracy: 0.9524\nEpoch 17/30\n6/6 [==============================] - 0s 9ms/step - loss: 0.2362 - binary_accuracy: 0.9731 - val_loss: 0.2651 - val_binary_accuracy: 0.9524\nEpoch 18/30\n6/6 [==============================] - 0s 9ms/step - loss: 0.1909 - binary_accuracy: 0.9306 - val_loss: 0.2213 - val_binary_accuracy: 0.9524\nEpoch 19/30\n6/6 [==============================] - 0s 9ms/step - loss: 0.1888 - binary_accuracy: 0.9767 - val_loss: 0.1791 - val_binary_accuracy: 1.0000\nEpoch 20/30\n6/6 [==============================] - 0s 9ms/step - loss: 0.1485 - binary_accuracy: 1.0000 - val_loss: 0.1478 - val_binary_accuracy: 1.0000\nEpoch 21/30\n6/6 [==============================] - 0s 9ms/step - loss: 0.1104 - binary_accuracy: 1.0000 - val_loss: 0.1258 - val_binary_accuracy: 1.0000\nEpoch 22/30\n6/6 [==============================] - 0s 9ms/step - loss: 0.1032 - binary_accuracy: 1.0000 - val_loss: 0.1074 - val_binary_accuracy: 1.0000\nEpoch 23/30\n6/6 [==============================] - 0s 9ms/step - loss: 0.0974 - binary_accuracy: 1.0000 - val_loss: 0.0922 - val_binary_accuracy: 1.0000\nEpoch 24/30\n6/6 [==============================] - 0s 9ms/step - loss: 0.0590 - binary_accuracy: 1.0000 - val_loss: 0.0757 - val_binary_accuracy: 1.0000\nEpoch 25/30\n6/6 [==============================] - 0s 9ms/step - loss: 0.0511 - binary_accuracy: 1.0000 - val_loss: 0.0619 - val_binary_accuracy: 1.0000\nEpoch 26/30\n6/6 [==============================] - 0s 9ms/step - loss: 0.0432 - binary_accuracy: 1.0000 - val_loss: 0.0517 - val_binary_accuracy: 1.0000\nEpoch 27/30\n6/6 [==============================] - 0s 10ms/step - loss: 0.0445 - binary_accuracy: 1.0000 - val_loss: 0.0449 - val_binary_accuracy: 1.0000\nEpoch 28/30\n6/6 [==============================] - 0s 9ms/step - loss: 0.0360 - binary_accuracy: 1.0000 - val_loss: 0.0386 - val_binary_accuracy: 1.0000\nEpoch 29/30\n6/6 [==============================] - 0s 9ms/step - loss: 0.0370 - binary_accuracy: 1.0000 - val_loss: 0.0353 - val_binary_accuracy: 1.0000\nEpoch 30/30\n6/6 [==============================] - 0s 9ms/step - loss: 0.0296 - binary_accuracy: 1.0000 - val_loss: 0.0307 - val_binary_accuracy: 1.0000\nTest loss: 0.030699074268341064\nTest accuracy: 1.0\n\n\n\n\n\n\n\n\n\nA: It works easily.\nQ. Once you have been able to successfully train the network, vary the different parts of the model to understand their influence: learning rate, number of units, optimizer, etc. Add another LSTM layer to see what happens. Exchange the LSTM layer with the GRU layer.\n\ndef big_model():\n    tf.keras.backend.clear_session()\n    model = tf.keras.models.Sequential()\n    model.add(tf.keras.layers.Input((nb_words, len(vocabulary))))\n    model.add(tf.keras.layers.LSTM(20, return_sequences=True))\n    model.add(tf.keras.layers.LSTM(10, return_sequences=False))\n    model.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n    optimizer = tf.keras.optimizers.Adam(lr=0.01)\n    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['binary_accuracy'])\n    print(model.summary())\n    return model\n\n\nmodel = big_model()\n\nhistory = tf.keras.callbacks.History()\n\nmodel.fit(X_train, T_train, validation_data=(X_test, T_test), epochs=30, batch_size=10, callbacks=[history])\n\nscore = model.evaluate(X_test, T_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n\nplt.figure(figsize=(15, 6))\n\nplt.subplot(121)\nplt.plot(history.history['loss'], '-r', label=\"Training\")\nplt.plot(history.history['val_loss'], '-b', label=\"Validation\")\nplt.xlabel('Epoch #')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.subplot(122)\nplt.plot(history.history['binary_accuracy'], '-r', label=\"Training\")\nplt.plot(history.history['val_binary_accuracy'], '-b', label=\"Validation\")\nplt.xlabel('Epoch #')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.show()\n\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nlstm (LSTM)                  (None, 10, 20)            3120      \n_________________________________________________________________\nlstm_1 (LSTM)                (None, 10)                1240      \n_________________________________________________________________\ndense (Dense)                (None, 1)                 11        \n=================================================================\nTotal params: 4,371\nTrainable params: 4,371\nNon-trainable params: 0\n_________________________________________________________________\nNone\nEpoch 1/30\n6/6 [==============================] - 3s 126ms/step - loss: 0.6949 - binary_accuracy: 0.6490 - val_loss: 0.7004 - val_binary_accuracy: 0.4762\nEpoch 2/30\n6/6 [==============================] - 0s 11ms/step - loss: 0.6915 - binary_accuracy: 0.5153 - val_loss: 0.6925 - val_binary_accuracy: 0.4762\nEpoch 3/30\n6/6 [==============================] - 0s 11ms/step - loss: 0.6813 - binary_accuracy: 0.5579 - val_loss: 0.6919 - val_binary_accuracy: 0.4762\nEpoch 4/30\n6/6 [==============================] - 0s 11ms/step - loss: 0.6781 - binary_accuracy: 0.5419 - val_loss: 0.6874 - val_binary_accuracy: 0.4762\nEpoch 5/30\n6/6 [==============================] - 0s 11ms/step - loss: 0.6644 - binary_accuracy: 0.5853 - val_loss: 0.6798 - val_binary_accuracy: 0.4762\nEpoch 6/30\n6/6 [==============================] - 0s 11ms/step - loss: 0.6725 - binary_accuracy: 0.6397 - val_loss: 0.6461 - val_binary_accuracy: 0.7619\nEpoch 7/30\n6/6 [==============================] - 0s 11ms/step - loss: 0.6507 - binary_accuracy: 0.7549 - val_loss: 0.6170 - val_binary_accuracy: 0.7619\nEpoch 8/30\n6/6 [==============================] - 0s 12ms/step - loss: 0.6102 - binary_accuracy: 0.7229 - val_loss: 0.5384 - val_binary_accuracy: 0.7619\nEpoch 9/30\n6/6 [==============================] - 0s 12ms/step - loss: 0.5777 - binary_accuracy: 0.7188 - val_loss: 0.5100 - val_binary_accuracy: 0.7619\nEpoch 10/30\n6/6 [==============================] - 0s 13ms/step - loss: 0.4356 - binary_accuracy: 0.8078 - val_loss: 0.4245 - val_binary_accuracy: 0.7619\nEpoch 11/30\n6/6 [==============================] - 0s 11ms/step - loss: 0.3224 - binary_accuracy: 0.8566 - val_loss: 0.3798 - val_binary_accuracy: 0.7619\nEpoch 12/30\n6/6 [==============================] - 0s 11ms/step - loss: 0.2309 - binary_accuracy: 0.9162 - val_loss: 0.3239 - val_binary_accuracy: 0.8095\nEpoch 13/30\n6/6 [==============================] - 0s 11ms/step - loss: 0.1888 - binary_accuracy: 0.8865 - val_loss: 0.2813 - val_binary_accuracy: 0.8095\nEpoch 14/30\n6/6 [==============================] - 0s 12ms/step - loss: 0.2265 - binary_accuracy: 0.8508 - val_loss: 0.2611 - val_binary_accuracy: 0.8095\nEpoch 15/30\n6/6 [==============================] - 0s 11ms/step - loss: 0.1821 - binary_accuracy: 0.8974 - val_loss: 0.2211 - val_binary_accuracy: 0.8095\nEpoch 16/30\n6/6 [==============================] - 0s 11ms/step - loss: 0.1445 - binary_accuracy: 0.9070 - val_loss: 0.1734 - val_binary_accuracy: 1.0000\nEpoch 17/30\n6/6 [==============================] - 0s 11ms/step - loss: 0.1258 - binary_accuracy: 1.0000 - val_loss: 0.1343 - val_binary_accuracy: 1.0000\nEpoch 18/30\n6/6 [==============================] - 0s 11ms/step - loss: 0.0961 - binary_accuracy: 1.0000 - val_loss: 0.1001 - val_binary_accuracy: 1.0000\nEpoch 19/30\n6/6 [==============================] - 0s 11ms/step - loss: 0.0575 - binary_accuracy: 1.0000 - val_loss: 0.0631 - val_binary_accuracy: 1.0000\nEpoch 20/30\n6/6 [==============================] - 0s 11ms/step - loss: 0.0367 - binary_accuracy: 1.0000 - val_loss: 0.0423 - val_binary_accuracy: 1.0000\nEpoch 21/30\n6/6 [==============================] - 0s 11ms/step - loss: 0.0290 - binary_accuracy: 1.0000 - val_loss: 0.0247 - val_binary_accuracy: 1.0000\nEpoch 22/30\n6/6 [==============================] - 0s 11ms/step - loss: 0.0100 - binary_accuracy: 1.0000 - val_loss: 0.0123 - val_binary_accuracy: 1.0000\nEpoch 23/30\n6/6 [==============================] - 0s 12ms/step - loss: 0.0091 - binary_accuracy: 1.0000 - val_loss: 0.0070 - val_binary_accuracy: 1.0000\nEpoch 24/30\n6/6 [==============================] - 0s 11ms/step - loss: 0.0045 - binary_accuracy: 1.0000 - val_loss: 0.0053 - val_binary_accuracy: 1.0000\nEpoch 25/30\n6/6 [==============================] - 0s 10ms/step - loss: 0.0042 - binary_accuracy: 1.0000 - val_loss: 0.0043 - val_binary_accuracy: 1.0000\nEpoch 26/30\n6/6 [==============================] - 0s 11ms/step - loss: 0.0023 - binary_accuracy: 1.0000 - val_loss: 0.0031 - val_binary_accuracy: 1.0000\nEpoch 27/30\n6/6 [==============================] - 0s 10ms/step - loss: 0.0026 - binary_accuracy: 1.0000 - val_loss: 0.0026 - val_binary_accuracy: 1.0000\nEpoch 28/30\n6/6 [==============================] - 0s 11ms/step - loss: 0.0021 - binary_accuracy: 1.0000 - val_loss: 0.0022 - val_binary_accuracy: 1.0000\nEpoch 29/30\n6/6 [==============================] - 0s 11ms/step - loss: 0.0018 - binary_accuracy: 1.0000 - val_loss: 0.0021 - val_binary_accuracy: 1.0000\nEpoch 30/30\n6/6 [==============================] - 0s 11ms/step - loss: 0.0013 - binary_accuracy: 1.0000 - val_loss: 0.0019 - val_binary_accuracy: 1.0000\nTest loss: 0.0019315623212605715\nTest accuracy: 1.0"
  },
  {
    "objectID": "exercises/13-RNN-solution.html#time-series-prediction",
    "href": "exercises/13-RNN-solution.html#time-series-prediction",
    "title": "Recurrent neural networks",
    "section": "Time series prediction",
    "text": "Time series prediction\nAnother useful function of RNNs is forecasting, i.e. predicting the rest of a sequence (financial markets, weather, etc.) based on its history.\nLet’s generate a dummy one-dimensional signal with 10000 points:\n\nN = 10000\ntime_axis = np.arange(N)\n\nsignal = 0.8*np.sin(time_axis/700) + 0.15*np.sin(time_axis/40)\n\nplt.figure(figsize=(10, 6))\nplt.plot(signal)\nplt.show()\n\n\n\n\n\n\n\n\nWe are going to use a small window (50 points) to feed the LSTM. The goal will be to perform one-step ahead prediction: given the last 50 points, what will be the next one?\nThe following cell prepares the data for the problem. Check that the data is what you expect.\n\nwindow = 50\n\nX = np.array(\n    [signal[t: t+ window] for t in time_axis[:-window]]\n)\n\nt = signal[time_axis[1:-window+1]]\n\nprint(X.shape)\nprint(t.shape)\n\n(9950, 50)\n(9950,)\n\n\nWe now split the signal into training and test sets. The training set consists of the 9000 first points, while the test set consists of the remaining 1000 points (the future). Note that the test set is not exactly contained in the training set as the function is not periodic, but quite.\n\nnb_train = 9000\n\nX_train = X[:nb_train]\nT_train = t[:nb_train]\nX_test = X[nb_train:]\nT_test = t[nb_train:]\n\nQ: Create a neural network taking a (window, 1) input, with one LSTM layer and one output neuron using the tanh activation function (as the targets are between -1 and 1). Train it on the data as a regression problem (use many epochs). Track the mean average error (metrics=['mae']) in addition to the mse, as it indicates better the prediction error. After training, plot the prediction for the test set and compare it to the ground truth.\n\ndef create_model(window):\n\n    tf.keras.backend.clear_session()\n    \n    inputs = tf.keras.layers.Input((window, 1))\n\n    x = tf.keras.layers.LSTM(20)(inputs)\n\n    output = tf.keras.layers.Dense(1, activation=\"tanh\")(x)\n\n    model = tf.keras.models.Model(inputs, output)\n\n    optimizer = tf.keras.optimizers.Adam(lr=0.01)\n\n    model.compile(loss='mse', optimizer=optimizer, metrics=['mae'])\n\n    print(model.summary())\n\n    return model\n\n\nmodel = create_model(window)\n\nhistory = tf.keras.callbacks.History()\n\nmodel.fit(X_train, T_train, validation_data=(X_test, T_test), epochs=50, batch_size=64, callbacks=[history])\n\nscore = model.evaluate(X_test, T_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n\nplt.figure(figsize=(15, 6))\n\nplt.subplot(121)\nplt.plot(history.history['loss'], '-r', label=\"Training\")\nplt.plot(history.history['val_loss'], '-b', label=\"Validation\")\nplt.xlabel('Epoch #')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.subplot(122)\nplt.plot(history.history['mae'], '-r', label=\"Training\")\nplt.plot(history.history['val_mae'], '-b', label=\"Validation\")\nplt.xlabel('Epoch #')\nplt.ylabel('Mean Average Error')\nplt.legend()\n\nplt.show()\n\nModel: \"model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         [(None, 50, 1)]           0         \n_________________________________________________________________\nlstm (LSTM)                  (None, 20)                1760      \n_________________________________________________________________\ndense (Dense)                (None, 1)                 21        \n=================================================================\nTotal params: 1,781\nTrainable params: 1,781\nNon-trainable params: 0\n_________________________________________________________________\nNone\nEpoch 1/50\n141/141 [==============================] - 2s 7ms/step - loss: 0.0321 - mae: 0.1348 - val_loss: 0.0260 - val_mae: 0.1357\nEpoch 2/50\n141/141 [==============================] - 1s 5ms/step - loss: 0.0095 - mae: 0.0818 - val_loss: 0.0052 - val_mae: 0.0596\nEpoch 3/50\n141/141 [==============================] - 1s 5ms/step - loss: 0.0039 - mae: 0.0510 - val_loss: 0.0020 - val_mae: 0.0384\nEpoch 4/50\n141/141 [==============================] - 1s 5ms/step - loss: 8.8932e-04 - mae: 0.0237 - val_loss: 2.3679e-04 - val_mae: 0.0132\nEpoch 5/50\n141/141 [==============================] - 1s 5ms/step - loss: 2.3213e-04 - mae: 0.0122 - val_loss: 4.4895e-04 - val_mae: 0.0187\nEpoch 6/50\n141/141 [==============================] - 1s 5ms/step - loss: 4.5989e-04 - mae: 0.0159 - val_loss: 3.8623e-04 - val_mae: 0.0158\nEpoch 7/50\n141/141 [==============================] - 1s 5ms/step - loss: 2.5193e-04 - mae: 0.0126 - val_loss: 1.4829e-04 - val_mae: 0.0103\nEpoch 8/50\n141/141 [==============================] - 1s 5ms/step - loss: 1.9236e-04 - mae: 0.0107 - val_loss: 1.1680e-04 - val_mae: 0.0089\nEpoch 9/50\n141/141 [==============================] - 1s 5ms/step - loss: 1.5906e-04 - mae: 0.0099 - val_loss: 1.0358e-04 - val_mae: 0.0085\nEpoch 10/50\n141/141 [==============================] - 1s 5ms/step - loss: 8.3204e-05 - mae: 0.0072 - val_loss: 6.3628e-04 - val_mae: 0.0238\nEpoch 11/50\n141/141 [==============================] - 1s 5ms/step - loss: 2.3202e-04 - mae: 0.0123 - val_loss: 8.6499e-05 - val_mae: 0.0071\nEpoch 12/50\n141/141 [==============================] - 1s 5ms/step - loss: 1.5292e-04 - mae: 0.0098 - val_loss: 1.8333e-04 - val_mae: 0.0125\nEpoch 13/50\n141/141 [==============================] - 1s 5ms/step - loss: 1.3011e-04 - mae: 0.0090 - val_loss: 1.2432e-04 - val_mae: 0.0092\nEpoch 14/50\n141/141 [==============================] - 1s 5ms/step - loss: 1.2748e-04 - mae: 0.0090 - val_loss: 7.3832e-05 - val_mae: 0.0074\nEpoch 15/50\n141/141 [==============================] - 1s 5ms/step - loss: 8.2734e-05 - mae: 0.0072 - val_loss: 5.1241e-05 - val_mae: 0.0058\nEpoch 16/50\n141/141 [==============================] - 1s 5ms/step - loss: 1.0381e-04 - mae: 0.0081 - val_loss: 2.6565e-05 - val_mae: 0.0044\nEpoch 17/50\n141/141 [==============================] - 1s 5ms/step - loss: 6.0350e-05 - mae: 0.0061 - val_loss: 4.3032e-05 - val_mae: 0.0056\nEpoch 18/50\n141/141 [==============================] - 1s 5ms/step - loss: 7.3179e-05 - mae: 0.0070 - val_loss: 2.6526e-04 - val_mae: 0.0147\nEpoch 19/50\n141/141 [==============================] - 1s 5ms/step - loss: 1.5532e-04 - mae: 0.0098 - val_loss: 4.6380e-05 - val_mae: 0.0058\nEpoch 20/50\n141/141 [==============================] - 1s 5ms/step - loss: 1.8684e-04 - mae: 0.0104 - val_loss: 7.1898e-05 - val_mae: 0.0077\nEpoch 21/50\n141/141 [==============================] - 1s 5ms/step - loss: 2.7324e-05 - mae: 0.0042 - val_loss: 2.1653e-05 - val_mae: 0.0038\nEpoch 22/50\n141/141 [==============================] - 1s 5ms/step - loss: 3.3472e-05 - mae: 0.0046 - val_loss: 1.3280e-05 - val_mae: 0.0029\nEpoch 23/50\n141/141 [==============================] - 1s 5ms/step - loss: 5.0352e-05 - mae: 0.0055 - val_loss: 1.3503e-05 - val_mae: 0.0030\nEpoch 24/50\n141/141 [==============================] - 1s 5ms/step - loss: 5.2304e-05 - mae: 0.0054 - val_loss: 3.2720e-04 - val_mae: 0.0172\nEpoch 25/50\n141/141 [==============================] - 1s 5ms/step - loss: 1.6084e-04 - mae: 0.0098 - val_loss: 4.7370e-05 - val_mae: 0.0061\nEpoch 26/50\n141/141 [==============================] - 1s 5ms/step - loss: 3.0319e-05 - mae: 0.0044 - val_loss: 1.1324e-04 - val_mae: 0.0081\nEpoch 27/50\n141/141 [==============================] - 1s 6ms/step - loss: 4.3116e-05 - mae: 0.0051 - val_loss: 1.7483e-04 - val_mae: 0.0121\nEpoch 28/50\n141/141 [==============================] - 1s 5ms/step - loss: 6.8618e-05 - mae: 0.0061 - val_loss: 1.2294e-04 - val_mae: 0.0096\nEpoch 29/50\n141/141 [==============================] - 1s 5ms/step - loss: 4.5403e-05 - mae: 0.0052 - val_loss: 6.6824e-06 - val_mae: 0.0021\nEpoch 30/50\n141/141 [==============================] - 1s 5ms/step - loss: 5.2677e-04 - mae: 0.0155 - val_loss: 1.3084e-05 - val_mae: 0.0028\nEpoch 31/50\n141/141 [==============================] - 1s 5ms/step - loss: 4.1677e-05 - mae: 0.0050 - val_loss: 1.2648e-05 - val_mae: 0.0030\nEpoch 32/50\n141/141 [==============================] - 1s 5ms/step - loss: 2.1466e-05 - mae: 0.0037 - val_loss: 7.2209e-06 - val_mae: 0.0022\nEpoch 33/50\n141/141 [==============================] - 1s 5ms/step - loss: 8.8189e-06 - mae: 0.0024 - val_loss: 9.4702e-06 - val_mae: 0.0026\nEpoch 34/50\n141/141 [==============================] - 1s 5ms/step - loss: 1.6518e-05 - mae: 0.0031 - val_loss: 4.7648e-06 - val_mae: 0.0018\nEpoch 35/50\n141/141 [==============================] - 1s 5ms/step - loss: 6.3752e-05 - mae: 0.0055 - val_loss: 8.0761e-06 - val_mae: 0.0023\nEpoch 36/50\n141/141 [==============================] - 1s 5ms/step - loss: 1.2616e-05 - mae: 0.0025 - val_loss: 4.8663e-06 - val_mae: 0.0019\nEpoch 37/50\n141/141 [==============================] - 1s 5ms/step - loss: 1.1288e-04 - mae: 0.0075 - val_loss: 1.8049e-05 - val_mae: 0.0038\nEpoch 38/50\n141/141 [==============================] - 1s 5ms/step - loss: 4.6925e-05 - mae: 0.0053 - val_loss: 7.7753e-06 - val_mae: 0.0024\nEpoch 39/50\n141/141 [==============================] - 1s 5ms/step - loss: 1.5035e-05 - mae: 0.0031 - val_loss: 4.5659e-06 - val_mae: 0.0018\nEpoch 40/50\n141/141 [==============================] - 1s 5ms/step - loss: 5.1643e-05 - mae: 0.0039 - val_loss: 6.4413e-04 - val_mae: 0.0217\nEpoch 41/50\n141/141 [==============================] - 1s 5ms/step - loss: 3.4189e-04 - mae: 0.0120 - val_loss: 5.2492e-06 - val_mae: 0.0019\nEpoch 42/50\n141/141 [==============================] - 1s 5ms/step - loss: 9.2156e-06 - mae: 0.0024 - val_loss: 2.2826e-05 - val_mae: 0.0040\nEpoch 43/50\n141/141 [==============================] - 1s 5ms/step - loss: 9.4703e-06 - mae: 0.0024 - val_loss: 5.3999e-06 - val_mae: 0.0020\nEpoch 44/50\n141/141 [==============================] - 1s 5ms/step - loss: 1.5472e-05 - mae: 0.0031 - val_loss: 1.2471e-05 - val_mae: 0.0030\nEpoch 45/50\n141/141 [==============================] - 1s 5ms/step - loss: 9.6372e-06 - mae: 0.0024 - val_loss: 1.6067e-05 - val_mae: 0.0037\nEpoch 46/50\n141/141 [==============================] - 1s 5ms/step - loss: 1.1083e-05 - mae: 0.0022 - val_loss: 3.5924e-04 - val_mae: 0.0182\nEpoch 47/50\n141/141 [==============================] - 1s 5ms/step - loss: 8.1012e-05 - mae: 0.0068 - val_loss: 4.1036e-05 - val_mae: 0.0057\nEpoch 48/50\n141/141 [==============================] - 1s 5ms/step - loss: 4.1891e-05 - mae: 0.0048 - val_loss: 2.2498e-06 - val_mae: 0.0012\nEpoch 49/50\n141/141 [==============================] - 1s 5ms/step - loss: 4.7253e-06 - mae: 0.0017 - val_loss: 1.5864e-06 - val_mae: 9.7631e-04\nEpoch 50/50\n141/141 [==============================] - 1s 5ms/step - loss: 2.3313e-06 - mae: 0.0012 - val_loss: 2.7683e-06 - val_mae: 0.0012\nTest loss: 2.7682647214533063e-06\nTest accuracy: 0.0012316412758082151\n\n\n\n\n\n\n\n\n\n\ny = model.predict(X_test)\n\nplt.figure(figsize=(10, 6))\nplt.plot(T_test, label=\"Ground truth\")\nplt.plot(y[:, 0], label=\"Prediction\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nIt seems possible to get a high precision on the test set, but there is a trick. The sequence fed as an input when testing consists of real measurements. The network has only learned to predict the next data point. Can we use that model to predict the next 1000 points without seeing the true data?\nFor that, we need to build an auto-regressive model, i.e. to feed inputs to the network consisting of predictions, not of real data points. We need a structure that can represent a fixed-size window of data points, where we can append predictions one by one. The following cell provides you with a simple implementation:\n\nfrom collections import deque\n\nclass Buffer:\n    \"Fixed size buffer allowing to append predictions.\"\n    def __init__(self, window, data):\n        self.window = window\n        self.data  = data.reshape((1, window))\n\n    def append(self, value):\n        d = deque([x for x in list(self.data[0, :])])\n        d.popleft()\n        d.append(value)\n        self.data = np.array(d).reshape((1, self.window))\n\nYou can create the buffer by intializing it with the first test sample consisting of 50 real data points:\nbuffer = Buffer(window, X_test[0, :])\nbuffer.data can be passed directly to the model in order to make a prediction:\ny = model.predict(buffer.data)[0, 0]\nThis prediction can be appended to the buffer, which can be used as the next input to the model:\nbuffer.append(y)\nQ: Make recursive prediction using your trained model. Does it work?\n\npredictions = []\n\nbuffer = Buffer(window, X_test[0, :])\n\nfor t in range(100):\n    \n    # Make a prediction using the current buffer\n    y = model.predict(buffer.data)[0, 0]\n\n    # Store the prediction\n    predictions.append(y)\n\n    # Append the prediction to buffer\n    buffer.append(y)\n   \n    \nplt.figure(figsize=(10, 6))\nplt.plot(T_test[:100], label=\"Ground truth\")\nplt.plot(predictions, label=\"Prediction\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nA: No. The slightest imprecision in the prediction accumulates in the input. After a while, the input to the model does not correspond to something that has been learned, and the output stops making sense. RNN do not have their own dynamics and are quite bad at predicting time series unless you have a lot of training data. With little data, reservoir computing is a much better solution."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Abramson, J., Ahuja, A., Brussee, A., Carnevale, F., Cassin, M.,\nFischer, F., et al. (2022). Creating Multimodal Interactive\nAgents with Imitation and Self-Supervised\nLearning. doi:10.48550/arXiv.2112.03763.\n\n\nArjovsky, M., Chintala, S., and Bottou, L. (2017). Wasserstein\nGAN. http://arxiv.org/abs/1701.07875.\n\n\nAtito, S., Awais, M., and Kittler, J. (2021). SiT: Self-supervised vIsion Transformer. http://arxiv.org/abs/2104.03602.\n\n\nBa, J. L., Kiros, J. R., and Hinton, G. E. (2016). Layer\nNormalization. http://arxiv.org/abs/1607.06450.\n\n\nBadrinarayanan, V., Kendall, A., and Cipolla, R. (2016).\nSegNet: A Deep Convolutional Encoder-Decoder\nArchitecture for Image Segmentation. http://arxiv.org/abs/1511.00561.\n\n\nBahdanau, D., Cho, K., and Bengio, Y. (2016). Neural Machine\nTranslation by Jointly Learning to\nAlign and Translate. http://arxiv.org/abs/1409.0473.\n\n\nBinder, A., Montavon, G., Bach, S., Müller, K.-R., and Samek, W. (2016).\nLayer-wise Relevance Propagation for Neural\nNetworks with Local Renormalization Layers. http://arxiv.org/abs/1604.00825.\n\n\nBojarski, M., Del Testa, D., Dworakowski, D., Firner, B., Flepp, B.,\nGoyal, P., et al. (2016). End to End Learning for\nSelf-Driving Cars. http://arxiv.org/abs/1604.07316.\n\n\nBrette, R., and Gerstner, W. (2005). Adaptive Exponential Integrate-and-Fire Model as an\nEffective Description of Neuronal Activity.\nJournal of Neurophysiology 94, 3637–3642. doi:10.1152/jn.00686.2005.\n\n\nCaron, M., Touvron, H., Misra, I., Jégou, H., Mairal, J., Bojanowski,\nP., et al. (2021). Emerging Properties in\nSelf-Supervised Vision Transformers. http://arxiv.org/abs/2104.14294.\n\n\nChollet, F. (2017a). Deep Learning with\nPython. Manning publications https://www.manning.com/books/deep-learning-with-python.\n\n\nChollet, F. (2017b). Xception: Deep Learning with\nDepthwise Separable Convolutions. http://arxiv.org/abs/1610.02357.\n\n\nChung, J., Gulcehre, C., Cho, K., and Bengio, Y. (2014). Empirical\nEvaluation of Gated Recurrent Neural Networks\non Sequence Modeling. http://arxiv.org/abs/1412.3555.\n\n\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019).\nBERT: Pre-training of\nDeep Bidirectional Transformers for Language\nUnderstanding. http://arxiv.org/abs/1810.04805.\n\n\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X.,\nUnterthiner, T., et al. (2021). An Image is\nWorth 16x16 Words: Transformers\nfor Image Recognition at Scale. http://arxiv.org/abs/2010.11929.\n\n\nFukushima, K. (1980). Neocognitron: A self-organizing\nneural network model for a mechanism of pattern recognition unaffected\nby shift in position. Biol. Cybernetics 36, 193–202. doi:10.1007/BF00344251.\n\n\nGers, F. A., and Schmidhuber, J. (2000). Recurrent nets that time and\ncount. in Proceedings of the IEEE-INNS-ENNS International\nJoint Conference on Neural Networks.\nIJCNN 2000. Neural Computing: New\nChallenges and Perspectives for the New\nMillennium, 189–194 vol.3. doi:10.1109/IJCNN.2000.861302.\n\n\nGirshick, R. (2015). Fast R-CNN. http://arxiv.org/abs/1504.08083.\n\n\nGirshick, R., Donahue, J., Darrell, T., and Malik, J. (2014). Rich\nfeature hierarchies for accurate object detection and semantic\nsegmentation. http://arxiv.org/abs/1311.2524.\n\n\nGlorot, X., and Bengio, Y. (2010). Understanding the difficulty of\ntraining deep feedforward neural networks. in\nAISTATS, 8.\n\n\nGoodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley,\nD., Ozair, S., et al. (2014). Generative Adversarial\nNetworks. http://arxiv.org/abs/1406.2661.\n\n\nGoodfellow, I. J., Shlens, J., and Szegedy, C. (2015). Explaining and\nHarnessing Adversarial Examples. http://arxiv.org/abs/1412.6572.\n\n\nGoodfellow, I., Bengio, Y., and Courville, A. (2016). Deep\nLearning. MIT Press http://www.deeplearningbook.org.\n\n\nGou, J., Yu, B., Maybank, S. J., and Tao, D. (2020). Knowledge\nDistillation: A Survey. http://arxiv.org/abs/2006.05525.\n\n\nGuo, X., Liu, X., Zhu, E., and Yin, J. (2017). Deep\nClustering with Convolutional Autoencoders. in\nNeural Information Processing Lecture\nNotes in Computer Science., eds. D. Liu, S.\nXie, Y. Li, D. Zhao, and E.-S. M. El-Alfy (Cham:\nSpringer International Publishing), 373–382. doi:10.1007/978-3-319-70096-0_39.\n\n\nGupta, S., Girshick, R., Arbeláez, P., and Malik, J. (2014). Learning\nRich Features from RGB-D Images for\nObject Detection and Segmentation. http://arxiv.org/abs/1407.5736.\n\n\nHannun, A., Case, C., Casper, J., Catanzaro, B., Diamos, G., Elsen, E.,\net al. (2014). Deep Speech: Scaling up\nend-to-end speech recognition. http://arxiv.org/abs/1412.5567.\n\n\nHaykin, S. S. (2009). Neural Networks and\nLearning Machines, 3rd Edition.\nPearson http://dai.fmph.uniba.sk/courses/NN/haykin.neural-networks.3ed.2009.pdf.\n\n\nHe, K., Gkioxari, G., Dollár, P., and Girshick, R. (2018). Mask\nR-CNN. http://arxiv.org/abs/1703.06870.\n\n\nHe, K., Zhang, X., Ren, S., and Sun, J. (2015a). Deep Residual\nLearning for Image Recognition. http://arxiv.org/abs/1512.03385.\n\n\nHe, K., Zhang, X., Ren, S., and Sun, J. (2015b). Delving\nDeep into Rectifiers: Surpassing\nHuman-Level Performance on ImageNet Classification.\nhttp://arxiv.org/abs/1502.01852.\n\n\nHiggins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X., Botvinick,\nM., et al. (2016). Beta-VAE: Learning Basic Visual\nConcepts with a Constrained Variational Framework.\nin ICLR 2017 https://openreview.net/forum?id=Sy2fzU9gl.\n\n\nHinton, G. E., Osindero, S., and Teh, Y.-W. (2006). A fast learning\nalgorithm for deep belief nets. Neural Comput. 18, 1527–1554.\ndoi:10.1162/neco.2006.18.7.1527.\n\n\nHinton, G. E., and Salakhutdinov, R. R. (2006). Reducing the\nDimensionality of Data with Neural\nNetworks. Science 313, 504–507. doi:10.1126/science.1127647.\n\n\nHinton, G., Vinyals, O., and Dean, J. (2015). Distilling the\nKnowledge in a Neural Network. http://arxiv.org/abs/1503.02531.\n\n\nHochreiter, S. (1991). Untersuchungen zu dynamischen neuronalen\nNetzen. http://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf.\n\n\nHochreiter, S., and Schmidhuber, J. (1997). Long short-term memory.\nNeural computation 9, 1735–80. https://www.ncbi.nlm.nih.gov/pubmed/9377276.\n\n\nHuang, G., Liu, Z., van der Maaten, L., and Weinberger, K. Q. (2018).\nDensely Connected Convolutional Networks. http://arxiv.org/abs/1608.06993.\n\n\nIoffe, S., and Szegedy, C. (2015). Batch Normalization:\nAccelerating Deep Network Training by Reducing\nInternal Covariate Shift. http://arxiv.org/abs/1502.03167.\n\n\nIsola, P., Zhu, J.-Y., Zhou, T., and Efros, A. A. (2018).\nImage-to-Image Translation with Conditional\nAdversarial Networks. http://arxiv.org/abs/1611.07004.\n\n\nIzhikevich, E. M. (2003). Simple model of spiking neurons. IEEE\ntransactions on neural networks 14, 1569–72. doi:10.1109/TNN.2003.820440.\n\n\nKarras, T., Laine, S., Aittala, M., Hellsten, J., Lehtinen, J., and\nAila, T. (2020). Analyzing and Improving the Image\nQuality of StyleGAN. http://arxiv.org/abs/1912.04958.\n\n\nKendall, A., Grimes, M., and Cipolla, R. (2016). PoseNet:\nA Convolutional Network for Real-Time\n6-DOF Camera Relocalization. http://arxiv.org/abs/1505.07427.\n\n\nKim, Y. (2014). Convolutional Neural Networks for\nSentence Classification. http://arxiv.org/abs/1408.5882.\n\n\nKingma, D. P., and Welling, M. (2013). Auto-Encoding Variational\nBayes. http://arxiv.org/abs/1312.6114.\n\n\nKingma, D., and Ba, J. (2014). Adam: A Method for\nStochastic Optimization. in Proc.\nICLR, 1–13. doi:10.1145/1830483.1830503.\n\n\nKrizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). ImageNet\nClassification with Deep Convolutional Neural\nNetworks. in Advances in Neural Information Processing\nSystems (NIPS) https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf.\n\n\nLapuschkin, S., Wäldchen, S., Binder, A., Montavon, G., Samek, W., and\nMüller, K.-R. (2019). Unmasking Clever Hans predictors and\nassessing what machines really learn. Nature Communications 10,\n1096. doi:10.1038/s41467-019-08987-4.\n\n\nLe, Q. V. (2013). Building high-level features using large scale\nunsupervised learning. in 2013 IEEE International\nConference on Acoustics, Speech and\nSignal Processing (Vancouver, BC, Canada:\nIEEE), 8595–8598. doi:10.1109/ICASSP.2013.6639343.\n\n\nLeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. (1998). Gradient\nBased Learning Applied to Document\nRecognition. Proceedings of the IEEE 86, 2278–2324.\ndoi:10.1109/5.726791.\n\n\nLi, H., Xu, Z., Taylor, G., Studer, C., and Goldstein, T. (2018).\nVisualizing the Loss Landscape of Neural Nets.\nhttp://arxiv.org/abs/1712.09913.\n\n\nLillicrap, T. P., Cownden, D., Tweed, D. B., and Akerman, C. J. (2016).\nRandom synaptic feedback weights support error backpropagation for deep\nlearning. Nat Commun 7, 1–10. doi:10.1038/ncomms13276.\n\n\nLinnainmaa, S. (1970). The representation of the cumulative rounding\nerror of an algorithm as a Taylor expansion of the local\nrounding errors.\n\n\nLiu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C.-Y., et\nal. (2016). SSD: Single Shot MultiBox\nDetector. 9905, 21–37. doi:10.1007/978-3-319-46448-0_2.\n\n\nMaas, A. L., Hannun, A. Y., and Ng, A. Y. (2013). Rectifier\nNonlinearities Improve Neural Network Acoustic Models. in\nICML, 6.\n\n\nMalinowski, M., Rohrbach, M., and Fritz, M. (2015). Ask Your\nNeurons: A Neural-based Approach to\nAnswering Questions about Images. http://arxiv.org/abs/1505.01121.\n\n\nMcInnes, L., Healy, J., and Melville, J. (2020). UMAP:\nUniform Manifold Approximation and Projection\nfor Dimension Reduction. http://arxiv.org/abs/1802.03426.\n\n\nMikolov, T., Chen, K., Corrado, G., and Dean, J. (2013). Efficient\nEstimation of Word Representations in\nVector Space. http://arxiv.org/abs/1301.3781.\n\n\nMirza, M., and Osindero, S. (2014). Conditional Generative\nAdversarial Nets. http://arxiv.org/abs/1411.1784.\n\n\nNowozin, S., Cseke, B., and Tomioka, R. (2016). F-GAN:\nTraining Generative Neural Samplers using Variational\nDivergence Minimization. http://arxiv.org/abs/1606.00709.\n\n\nOlshausen, B. A., and Field, D. J. (1997). Sparse coding with an\novercomplete basis set: A strategy employed by\nV1? Vision Research 37, 3311–3325. doi:10.1016/S0042-6989(97)00169-7.\n\n\nOord, A. van den, Dieleman, S., Zen, H., Simonyan, K., Vinyals, O.,\nGraves, A., et al. (2016). WaveNet: A Generative\nModel for Raw Audio. http://arxiv.org/abs/1609.03499.\n\n\nRadford, A., Metz, L., and Chintala, S. (2015). Unsupervised\nRepresentation Learning with Deep Convolutional\nGenerative Adversarial Networks. http://arxiv.org/abs/1511.06434.\n\n\nRazavi, A., Oord, A. van den, and Vinyals, O. (2019). Generating\nDiverse High-Fidelity Images with VQ-VAE-2. http://arxiv.org/abs/1906.00446.\n\n\nRedmon, J., Divvala, S., Girshick, R., and Farhadi, A. (2016). You\nOnly Look Once: Unified, Real-Time\nObject Detection. http://arxiv.org/abs/1506.02640.\n\n\nRedmon, J., and Farhadi, A. (2016). YOLO9000:\nBetter, Faster, Stronger. http://arxiv.org/abs/1612.08242.\n\n\nRedmon, J., and Farhadi, A. (2018). YOLOv3: An\nIncremental Improvement. http://arxiv.org/abs/1804.02767.\n\n\nReed, S., Akata, Z., Yan, X., Logeswaran, L., Schiele, B., and Lee, H.\n(2016). Generative Adversarial Text to Image\nSynthesis. http://arxiv.org/abs/1605.05396.\n\n\nReed, S., Zolna, K., Parisotto, E., Colmenarejo, S. G., Novikov, A.,\nBarth-Maron, G., et al. (2022). A Generalist Agent. http://arxiv.org/abs/2205.06175.\n\n\nRen, S., He, K., Girshick, R., and Sun, J. (2016). Faster\nR-CNN: Towards Real-Time Object Detection with\nRegion Proposal Networks. http://arxiv.org/abs/1506.01497.\n\n\nRonneberger, O., Fischer, P., and Brox, T. (2015). U-Net:\nConvolutional Networks for Biomedical Image\nSegmentation. http://arxiv.org/abs/1505.04597.\n\n\nRumelhart, D. E., Hinton, G. E., and Williams, R. J. (1986). Learning\nrepresentations by back-propagating errors. Nature 323,\n533–536. doi:10.1038/323533a0.\n\n\nSalimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A., and\nChen, X. (2016). Improved Techniques for Training\nGANs. http://arxiv.org/abs/1606.03498.\n\n\nSathe, S., Shinde, S., Chorge, S., Thakare, S., and Kulkarni, L. (2022).\nOverview of Image Caption Generators and Its\nApplications. in Proceeding of International\nConference on Computational Science and\nApplications Algorithms for Intelligent\nSystems., eds. S. Bhalla, M. Bedekar, R. Phalnikar, and S.\nSirsikar (Singapore: Springer Nature),\n105–110. doi:10.1007/978-981-19-0863-7_8.\n\n\nSimonyan, K., and Zisserman, A. (2015). Very Deep Convolutional\nNetworks for Large-Scale Image Recognition.\nInternational Conference on Learning Representations (ICRL),\n1–14. doi:10.1016/j.infsof.2008.09.005.\n\n\nSohn, K., Lee, H., and Yan, X. (2015). “Learning Structured\nOutput Representation using Deep Conditional Generative\nModels,” in Advances in Neural Information\nProcessing Systems 28, eds. C. Cortes, N. D. Lawrence, D. D.\nLee, M. Sugiyama, and R. Garnett (Curran Associates, Inc.),\n3483–3491. http://papers.nips.cc/paper/5775-learning-structured-output-representation-using-deep-conditional-generative-models.pdf.\n\n\nSpringenberg, J. T., Dosovitskiy, A., Brox, T., and Riedmiller, M.\n(2015). Striving for Simplicity: The All\nConvolutional Net. http://arxiv.org/abs/1412.6806.\n\n\nSrivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and\nSalakhutdinov, R. (2014). Dropout: A Simple Way to\nPrevent Neural Networks from Overfitting.\nJournal of Machine Learning Research 15, 1929–1958. http://jmlr.org/papers/v15/srivastava14a.html.\n\n\nSrivastava, R. K., Greff, K., and Schmidhuber, J. (2015). Highway\nNetworks. http://arxiv.org/abs/1505.00387.\n\n\nSutskever, I., Vinyals, O., and Le, Q. V. (2014). Sequence to\nSequence Learning with Neural Networks. http://arxiv.org/abs/1409.3215.\n\n\nSzegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna, Z. (2015).\nRethinking the Inception Architecture for Computer\nVision. http://arxiv.org/abs/1512.00567.\n\n\nTaigman, Y., Yang, M., Ranzato, M., and Wolf, L. (2014).\nDeepFace: Closing the Gap to\nHuman-Level Performance in Face Verification.\nin 2014 IEEE Conference on Computer Vision\nand Pattern Recognition (Columbus, OH,\nUSA: IEEE), 1701–1708. doi:10.1109/CVPR.2014.220.\n\n\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez,\nA. N., et al. (2017). Attention Is All You Need. http://arxiv.org/abs/1706.03762.\n\n\nVincent, P., Larochelle, H., Lajoie, I., Bengio, Y., and Manzagol, P.-A.\n(2010). Stacked Denoising Autoencoders: Learning\nUseful Representations in a Deep Network with a\nLocal Denoising Criterion. Journal of Machine Learning\nResearch, 38.\n\n\nVinyals, O., Toshev, A., Bengio, S., and Erhan, D. (2015). Show and\nTell: A Neural Image Caption Generator. http://arxiv.org/abs/1411.4555.\n\n\nWang, B., Zheng, H., Liang, X., Chen, Y., Lin, L., and Yang, M. (2018).\nToward Characteristic-Preserving Image-based\nVirtual Try-On Network. http://arxiv.org/abs/1807.07688.\n\n\nWerbos, P. J. (1982). Applications of advances in nonlinear sensitivity\nanalysis. in System Modeling and\nOptimization: Proc. IFIP\n(Springer).\n\n\nWu, N., Green, B., Ben, X., and O’Banion, S. (2020). Deep\nTransformer Models for Time Series\nForecasting: The Influenza Prevalence Case. http://arxiv.org/abs/2001.08317.\n\n\nWu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., Macherey, W., et\nal. (2016). Google’s Neural Machine Translation System:\nBridging the Gap between Human\nand Machine Translation. https://arxiv.org/abs/1609.08144v2.\n\n\nXu, K., Ba, J. L., Kiros, R., Cho, K., Courville, A., Salakhutdinov, R.,\net al. (2015). Show, Attend and Tell:\nNeural Image Caption Generation with Visual\nAttention. in Proceedings of the 32nd International\nConference on Machine Learning - Volume\n37 ICML’15. (JMLR.org), 2048–2057. http://dl.acm.org/citation.cfm?id=3045118.3045336.\n\n\nZhou, Y., and Tuzel, O. (2017). VoxelNet: End-to-End Learning for Point Cloud Based 3D\nObject Detection. http://arxiv.org/abs/1711.06396.\n\n\nZhu, Y., Gao, T., Fan, L., Huang, S., Edmonds, M., Liu, H., et al.\n(2020). Dark, Beyond Deep: A Paradigm Shift to\nCognitive AI with Humanlike Common Sense.\nEngineering. doi:10.1016/j.eng.2020.01.011."
  }
]