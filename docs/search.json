[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Neurocomputing",
    "section": "",
    "text": "These are the lecture notes for the module Neurocomputing taught by Dr. Julien Vitay at the Technische Universität Chemnitz, Faculty of Computer Science, Professorship for Artificial Intelligence.\nEach section/lecture is accompanied by a set of videos, the slides and some lecture notes which summarize the most important points to understand. Some sections are optional in the sense that no questions will be asked at the exam, but those interested in becoming neural network experts should feel free to study them. The videos are integrated in the lecture notes, but you can also access the complete playlist on Youtube.\nExercises are provided in the form of Jupyter notebooks, allowing to implement in Python at your own pace the algorithms seen in the lectures and learn to use machine learning libraries such as scikit-learn, keras and tensorflow. A notebook to work on (locally or on Colab) and the solution are downloadable at the top-right of each page. A video explaining the exercise and one commenting the solution are available, with the playlist being on Youtube.\nhtml\nRecommended readings:\n\n(Goodfellow, Bengio, and Courville 2016) Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016. http://www.deeplearningbook.org.\nHaykin (2009) Simon S. Haykin. Neural Networks and Learning Machines, 3rd Edition. Pearson, 2009. http://dai.fmph.uniba.sk/courses/NN/haykin.neural-networks.3ed.2009.pdf.\nChollet (2017) François Chollet. Deep Learning with Python. Manning publications, 2017. https://www.manning.com/books/deep-learning-with-python.\nGerstner et al. (2014) Wulfram Gerstner, Werner Kistler, Richard Naud, and Liam Paninski. Neuronal Dynamics - a Neuroscience Textbook. Cambridge University Press., 2014. https://neuronaldynamics.epfl.ch/index.html.\n\n\n\n\n\nChollet, François. 2017. Deep Learning with Python. Manning publications.\n\n\nGerstner, Wulfram, Werner Kistler, Richard Naud, and Liam Paninski. 2014. Neuronal Dynamics - a Neuroscience Textbook. Cambridge University Press.\n\n\nGoodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. Deep Learning. MIT Press.\n\n\nHaykin, Simon S. 2009. Neural Networks and Learning Machines, 3rd Edition. Pearson."
  },
  {
    "objectID": "notes/1.1-Introduction.html",
    "href": "notes/1.1-Introduction.html",
    "title": "1  Introduction",
    "section": "",
    "text": "Slides: html pdf"
  },
  {
    "objectID": "notes/1.1-Introduction.html#what-is-neurocomputing",
    "href": "notes/1.1-Introduction.html#what-is-neurocomputing",
    "title": "1  Introduction",
    "section": "1.1 What is neurocomputing?",
    "text": "1.1 What is neurocomputing?\n\nLet’s first discuss the difference between Artificial Intelligence (AI), Machine Learning (ML), Deep Learning (DL) and Neurocomputing. Nowadays, these terms are used almost interchangeably, but there are historical and methodological differences.\n\n\n\nSource: https://data-science-blog.com/blog/2018/05/14/machine-learning-vs-deep-learning-wo-liegt-der-unterschied\n\n\nThe term Artificial Intelligence was coined by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence in 1956:\n\nThe study is to proceed on the basis of the conjecture that every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it.\n\nGood old-fashion AI (GOFAI) approaches were purely symbolic (logical systems, knowledge-based systems) or using linear neural networks. They were able to play checkers, prove mathematical theorems, make simple conversations (ELIZA), translate languages…\nMachine learning (ML) is a branch of AI that focuses on learning from examples (data-driven AI). It is sometimes also referred to as big data, data science, operational research, pattern recognition… ML algorithms include:\n\nArtificial Neural Networks (multi-layer perceptrons)\nStatistical analysis (Bayesian modeling, PCA)\nClustering algorithms (k-means, GMM, spectral clustering)\nSupport vector machines\nDecision trees, random forests\n\nDeep Learning is a recent re-branding of artificial neural networks. It focuses on learning high-level representations of the data, using highly non-linear neural networks. Many architectures have been developped, including:\n\nDeep neural networks (DNN)\nConvolutional neural networks (CNN)\nRecurrent neural networks (RNN)\nGenerative models (GAN, VAE)\nDeep reinforcement learning (DQN, PPO, AlphaGo)\nTransformers\nGraph neural networks\n\n\nNeurocomputing is at the intersection between computational neuroscience and artificial neural networks (deep learning). Computational neuroscience studies the functioning of the brain (human or animal) through biologically detailed models, either at the functional level (e.g. visual attention, decision-making) or cellular level (individual neurons, synapses, neurotransmitters, etc). The goal of computational neuroscience is 1) to provide theoretical explanations to the experimental observations made by neuroscientists and 2) make predictions that can be verified experimentally. Moreover, understanding how the brain solves real-life problems might allow to design better AI algorithms. If you are interested in computational neuroscience, make sure to visit the courses Neurokognition I and II taught by Prof. Dr. Hamker:\nhttps://www.tu-chemnitz.de/informatik/KI/edu/neurokognition/\nNeurocomputing aims at bringing the mechanisms underlying human cognition into artificial intelligence. The first part of this course focuses on deep learning, while the second will discuss how more biologically realistic neural networks could help designing better AI systems."
  },
  {
    "objectID": "notes/1.1-Introduction.html#applications-of-deep-learning",
    "href": "notes/1.1-Introduction.html#applications-of-deep-learning",
    "title": "1  Introduction",
    "section": "1.2 Applications of deep learning",
    "text": "1.2 Applications of deep learning\nMachine Learning applications are generally divided into three main branches:\n\nSupervised learning: The program is trained on a pre-defined set of training examples and used to make correct predictions when given new data.\nUnsupervised learning: The program is given a bunch of data and must find patterns and relationships therein.\nReinforcement learning: The program explores its environment by producing actions and receiving rewards.\n\n\n\n\nSource: http://www.isaziconsulting.co.za/machinelearning.html\n\n\nDeep learning has recently revolutionized these types of machine learning, so let’s have a look at some concrete examples for motivation. At the end of the course, if you also perform all exercises, you should be able to reproduce these applications.\n\n1.2.1 Supervised learning\n\n\n\n\nPrinciple of supervised learning. Source: Andrew Ng, Stanford CS229, https://see.stanford.edu/materials/aimlcs229/cs229-notes1.pdf\n\n\nIn a supervised learning, we have a training set (or training data) consisting of \\(N\\) samples (or examples) from which we want to learn the underlying function or distribution. Each sample consists of an input \\(\\mathbf{x}_i\\) and an output (also called ground truth, desired output or target) \\(t_i\\).\nWhat we want to learn is parameterized model \\(y_i = f_\\theta (\\mathbf{x}_i)\\) which can predict the correct output for the inputs of the training set. The goal of learning (or training) is to find which value of the parameters \\(\\theta\\) allows to reduce (minimize) the prediction error. i.e. the discrepancy between the prediction \\(y_i = f_\\theta (\\mathbf{x}_i)\\) and the desired output \\(t_i\\).\nDepending on the nature of the outputs \\(t\\), we have two different supervised problems:\n\nIn regression tasks, the outputs can take an infinity of values (e.g. real numbers). The following figure shows how examples of flat surfaces (input \\(x_i\\)) and prices (output \\(t_i\\)) collected in the neighborhood can be used to predict the price of a new flat. After collecting enough samples, a model is trained to minimize its prediction error. Here, a linear model is used (black line) as we perform linear regression, but any other type of function could be used. The parameters of the line (slope and intercept) are adapted so that the line lies close to the data: the predicted price \\(y_i\\) is never far from the ground truth \\(t_i\\). Using that line after learning, we can predict that a 60 square meters flat should be rented around 550 euros/month.\n\n\n\n\n````{figure} ../img/regression-animation3.png\n\n\n\n\nwidth: 80%\n\n\n\n\n* In **classification** tasks, the outputs are discrete, i.e. take only a finite number of different values (called classes or labels). When there are only two classes, they are called the positive and negative classes and the problem is a **binary classification**. The two classes can represent yes/no binary values, such as when when a test is positive or negative. When there are more than two classes, they can for example represent different objects (car / bike / dog / cat...) that can be recognized on an image. The following figure depicts a binary classifiation problem, where two input features $x_1$ and $x_2$ (temperature and blood pressure) are used to predict the occurence of an illness (yes = ill, no = sane). The linear model is a line that separates the input space into two separate regions: all points above the line are categorized (classified) as ill, all points below as sane, even if they were not in the training data.\n\n````{figure} ../img/classification-animation3.png\n---\nwidth: 80%\n---\nIn practice, when using neural networks, the distinction between classification and regression is not very important, but it can be relevant for other ML techniques (decision trees only work for classification problems, for example).\n\n1.2.1.1 Feedforward neural networks\nAs we will see later, an artificial neuron is a mathematical model able to perform linear classification or regression using weighted sums of inputs:\n\\[y = f(\\sum_{i=1}^d w_i \\, x_i + b)\\]\n\n\n\n```{figure} ../img/artificialneuron.svg\n\n\n\n\nwidth: 60%\n\n\n\nArtificial neuron.\n\nBy stacking layers of artificial neurons, we obtain a **feedforward neural network** able to solve non-linear classification and regression problems.\n\n```{figure} ../img/deep.svg\n---\nwidth: 60%\n---\nFeedforward neural network.\nFully-connected layers of neurons can be replaced by convolutional layers when dealing with images as inputs, leading to the very successful convolutional neural networks (CNN).\n\n\n\n```{figure} ../img/dcn.png\n\n\n\n\nwidth: 90%\n\n\n\nTypical CNN architecture. Source: Albelwi S, Mahmood A. 2017. A Framework for Designing the Architectures of Deep Convolutional Neural Networks. Entropy 19:242. doi:10.3390/e19060242\n\nThe \"only\" thing to do is to feed these networks with a lot of training data (inputs and desired outputs) and let them adjust their weights to minimize their prediction error using the backpropagation algorithm {cite}`Rumelhart1986a` (more on that later). Neural networks (including CNNs) are a very old technology, dating back from the 60's, with a resurgence in the 80's thanks to the backpropation algorithm. They had been able to learn small datasets, but their performance was limited by the availability of data and the computing power available at the time. One classical example is the use of a CNN {cite}`LeCun1998` by Yann LeCun in 1998 to automatically classify single digits on ZIP postal codes (what led to the development of the MNIST dataset, the \"Hello World!\" of machine learning which we will use in the exercises). \n\n```{figure} ../img/lenet5.gif\n---\nwidth: 50%\n---\nLeNet5 CNN trained on MNIST. Source: <http://yann.lecun.com/exdb/lenet/>\nThe revival of artificial neural networks marketed as deep learning at the end of the 2000’s was principally due the availability of massive amounts of training data (thanks to search engines and social networks) and the availability of consumer graphics GPUs able to perform scientific computations, especially using Nvidia’s CUDA programming framework.\nThe first badge of honour obtained by deep learning methods happened during the ImageNet challenge in 2012. The challenge was made for computer vision (CV) scientists to compare their algorithms on a huge dataset of 14 billion annotated images for object recognition (what is on the image?), object detection (which objects are in the image and where?) and object segmentation (which pixels belong to which object?). The object recognition challenge was indeed quite hard, with 1000 different classes (sometimes exotic, such as “ladle” or “porcupine”) with a great variety of backgrounds or lightning conditions. Classical CV methods based on feature extraction and simple classifiers performed reasonably well, with an error rate around 30%.\nHowever, Krizhevsky, Sutskever and Hinton {cite}Krizhevsky2012 trained a CNN entirely on the images, without any form of preprocessing, and obtained an error rate of 15%, half of the other methods. This achievement marked the beginning of the deep learning era, attracted the attention of the major industrial players (Google, Facebook and soon the rest of the world) who have already invested hundreds of billions on AI research.\nThe whole field of computer vision was taken by storm, and CNNs were able to outperform the state-of-the-art of many vision-related tasks, such as object detection with the YOLO (You Only Look Once) network {cite}Redmon2016:\n\n\n\n\nor semantic segmention with SegNet {cite}Badrinarayanan2016 or its variants such as Mask RCNN {cite}He2018:\n\n\n\n\nCNNs can even be used to control autonomous cars, by learning to reproduce human commands for a given input image {cite}Bojarski2016:\n\n\n\n\nCNNs are also gaining an increasing importance in medical applications, for example to help histologists detect cancerous cells:\n\n\n\n\n\n\n1.2.1.2 Recurrent neural networks\n\n\n\n\nAnother field that was heavily transformed by deep learning is natural language processing (NLP), i.e. the automatic processing of language, be it text understanding, translation, summarization, question answering or even speech recognition and synthesis. In short, everything needed under the hood when you talk to Siri or Alexa.\nThe key neural network involved in this paradigmatic change is the recurrent neural network (RNN), with the most prominent model being the long short-term memory (LSTM) network {cite}Hochreiter1997.\n\n\n\n```{figure} ../img/LSTM3-chain.png\n\n\n\n\nwidth: 90%\n\n\n\nLSTM cell. Source: http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n\nThe main difference with feedforward neural networks is that RNNs can be applied on sequences (of words, but it could also be video frames or any time-dependent signal). At each step, a RNN produces an output not only depending on its current input, but also on its previous output, implementing a form of memory of past events. \n\nMore recent advances furthermore introduce the concept of **attention** for processing sequences. This is now at the heart of all translation systems or in BERT, the language understanding module behind Google search. The neural architectures may seem complex, but we will break them down in this course.\n\n```{figure} ../img/google-nmt-lstm.png\n---\nwidth: 100%\n---\nGoogle Neural Machine Translation. Source: <https://ai.google/research/pubs/pub45610>\n\n\n\n1.2.2 Unsupervised learning\n\n\n\n\nIn supervised learning, we use annotated data, i.e. pairs \\((x_i, t_i)\\) of input/output examples. This requires to know the ground truth for each sample, what can be be very tedious and expensive if humans have to do it.\nIn unsupervised learning, we only have inputs. The goal of the algorithms is to make sense out of the data: extract regularities, model the underlying distribution, group examples into clusters, etc… It may seem much harder than supervised learning, as there is no ground truth to measure performance, but data is very cheap to obtain.\n\n1.2.2.1 Clustering and feature extraction\nClustering is a classical machine technique allowing to group examples in clusters based on their respective distances: close examples should belong to the same cluster. The most well-know algorithms are k-means and Gaussian mixture models (GMM). But the quality of the clustering depends on the space in which the inputs are represented: two images may be similar not because their pixels are similar (e.g. two dark images), but because they contain similar objects (fishes, birds). Neural networks can be used to learn a feature space where distances between inputs are meaningful.\n\n\n\n```{figure} ../img/unsupervised-learning.png\n\n\n\n\nwidth: 100%\n\n\n\nClustering. Source: https://learn.g2.com/supervised-vs-unsupervised-learning\n\n#### Dimensionality reduction and autoencoders\n\n\nData such as images have a lot of dimensions (one per pixel), most of which are redundant. **Dimensionality reduction** techniques allow to reduce this number of dimensions by projecting the data into a **latent space** while keeping the information.\n\n**Autoencoders** (AE) are neural networks that learn to reproduce their inputs (unsupervised learning, as there are no labels) by compressing information through a bottleneck. The **encoder** projects the input data onto the latent space, while the **decoder** recreates the input. The latent space has much less dimensions than the input images, but must contain enough information in order to reconstruct the image. \n\n\n```{figure} ../img/latent-space.png\n---\nwidth: 100%\n---\nAutoencoders. Source: <https://hackernoon.com/autoencoders-deep-learning-bits-1-11731e200694g>\nApart from compression, one important application of dimensionality reduction is visualization when the latent space has 2 or 3 dimensions: you can visualize the distribution of your data and estimate how hard the classification/regression will be. Classical ML techniques include PCA (principal component analysis) and t-SNE, but autoencoders can also be used, for example the UMAP (Uniform Manifold Approximation and Projection for Dimension Reduction) architecture {cite}McInnes2020.\nAnother application of autoencoders is the pretraining (feature extraction) of neural networks on unsupervised data before fine-tuning the resulting classifier on supervised data. This allows self-taught learning or semi-supervised learning, when the annotated data available for supervised learning is scarce, but a lot of unsupervised data from the same domain is available.\n\n\n1.2.2.2 Generative models\nThe other major advantage of autoencoders is their decoder: from a low-dimensional latent representation, it is able after training to generate high-dimensional data such as images. By sampling the latent space, one could in principle generate an infinity of new images.\nOne particular form of autoencoder which is very useful for data generation is the variational autoencoder (VAE) {cite}Kingma2013. The main difference with a regular AE is that the latent encodes a probability distribution instead of a single latent vector, what allows to sample new but realistic outputs. For example, a VAE trained to reproduce faces can generate new hybrid faces depending on how the sampling is done:\n\n\n\n```{figure} ../img/vae-faces.jpg\n\n\n\n\nwidth: 100%\n\n\n\nSampling the latent space of a VAE trained on faces allows to generate new but realistic faces. Source: https://hackernoon.com/latent-space-visualization-deep-learning-bits-2-bd09a46920df\n\nVAE are in particular central to **DeepFakes** which have widely reached the media because of their impressive possibilities but also ethical issues:\n\n<div class='embed-container'><iframe src='https://www.youtube.com/embed/JbzVhzNaTdI' frameborder='0' allowfullscreen></iframe></div>\n\nAnother class of generative models are **generative adversarial networks** (GAN) {cite}`Goodfellow2014` which consist of a **generator** (decoder) and a **discriminator** that compete to produce realistic images while trying to discriminate generated from real images. \n\n```{figure} ../img/gan.jpg\n---\nwidth: 100%\n---\nGenerative adversarial network.\nSeveral evolutions of GANs have allowed to produce increasingly realistic images, such as conditional GANs who permit to generate images of a desired class, or CycleGAN which allows to replace an object with another:\n\n\n\n```{figure} ../img/cycleGAN4.jpg\n\n\n\n\nwidth: 100%\n\n\n\nCycleGAN. https://github.com/junyanz/CycleGAN\n\n### Reinforcement learning\n\n<div class='embed-container'><iframe src='https://www.youtube.com/embed/fczritSOcSM' frameborder='0' allowfullscreen></iframe></div>\n\nReinforcement learning (RL) is not part of this module, as we offer a complete course on it:\n\n<https://www.tu-chemnitz.de/informatik/KI/edu/deeprl/>\n\nbut it has recently gained a lot of importance when coupled with deep learning principles. Here we just present a couple of application of **deep reinforcement learning** to motivate you to also assist to this course. \n\nRL models the sequential interaction between an **agent** (algorithm, robot) and its **environment**. At each time step $t$, the agent is a state $s_t$ and selects an action $a_t$ according to its policy (or strategy) $\\pi$. This brings the agent in a new state $s_{t+1}$ and provides a reward $r_{t+1}$. The reward is the only feedback that the agent receives about its action: when it is positive, it is good; when it is negative, it is bad. The goal of the of the agent is to maximize the sum of rewards that it receives **on the long-term**. For example in a video game, the states would correspond to each video frame, the actions are joystick movements and the rewards are scores increases and decreases. The goal is to move the joystick correctly so that the final cumulated score is maximal.\n\n\n```{figure} ../img/rl-loop.png\n---\nwidth: 100%\n---\nAgent-environment interaction in RL. Source: <https://ieeexplore.ieee.org/document/7839568>\nIn deep RL, the policy \\(\\pi\\) is implemented using a deep neural network whose job is to predict which action in a given state is the most likely to provide reward in the long-term. Contrary to supervised learning, we do not know which action should have been performed (ground truth), we only get rewards indicating if this was a good choice or not. This makes the learning problem much harder. But the deep RL methods are quite generic: any problem that can be described in terms of states, actions and rewards (formally, a Markov decision process) can be solved by deep RL techniques, at the cost of quite long training times. Let’s have a look at some applications:\n\nThe first achievement of deep RL was the deep Q-network (DQN) of Deepmind able to solve a multitude of old Atari games from scratch using raw video inputs:\n\n\n\n\n\n\nDeep RL methods have since then been applied to more complex games, such as Starcraft II:\n\n\n\n\n\nor DotA 2:\n\n\n\n\n\nAnother famous achievement of deep RL is when Google Deepmind’s AlphaGo beat Lee Sedol, 19 times world champion, in 2016:\n\n\n\n\n\n\nDeep RL is also a very promising to robotics, be it in simulation:\n\n\n\n\n\nor in reality:\n\n\n\n\n\nIt is also promising for autonomous driving:"
  },
  {
    "objectID": "notes/1.1-Introduction.html#outlook",
    "href": "notes/1.1-Introduction.html#outlook",
    "title": "1  Introduction",
    "section": "1.3 Outlook",
    "text": "1.3 Outlook"
  },
  {
    "objectID": "notes/1.2-Math.html",
    "href": "notes/1.2-Math.html",
    "title": "2  Math basics (optional)",
    "section": "",
    "text": "Slides: html pdf\nThis chapter is not part of the course itself (there will not be questions at the exam on basic mathematics) but serves as a reminder of the important mathematical notions that are needed to understand this course. Students who have studied mathematics as a major can safely skip this part, as there is nothing fancy (although the section on information theory could be worth a read).\nIt is not supposed to replace any course in mathematics (we won’t show any proof and will skip what we do not need) but rather to provide a high-level understanding of the most important concepts and set the notations. Nothing should be really new to you, but it may be useful to have everything summarized at the same place.\nReferences: Part I of Goodfellow et al. (2016) {cite}Goodfellow2016. Any mathematics textbook can be used in addition."
  },
  {
    "objectID": "notes/1.2-Math.html#linear-algebra",
    "href": "notes/1.2-Math.html#linear-algebra",
    "title": "2  Math basics (optional)",
    "section": "2.1 Linear algebra",
    "text": "2.1 Linear algebra\n\n\n\n\nSeveral mathematical objects are manipulated in linear algebra:\n\nScalars \\(x\\) are 0-dimensional values (single numbers, so to speak). They can either take real values (\\(x \\in \\Re\\), e.g. \\(x = 1.4573\\), floats in CS) or natural values (\\(x \\in \\mathbb{N}\\), e.g. \\(x = 3\\), integers in CS).\nVectors \\(\\mathbf{x}\\) are 1-dimensional arrays of length \\(d\\). The bold notation \\(\\mathbf{x}\\) will be used in this course, but you may also be accustomed to the arrow notation \\(\\overrightarrow{x}\\) used on the blackboard. When using real numbers, the vector space with \\(d\\) dimensions is noted \\(\\Re^d\\), so we can note \\(\\mathbf{x} \\in \\Re^d\\). Vectors are typically represented vertically to outline their \\(d\\) elements \\(x_1, x_2, \\ldots, x_d\\):\n\n\\[\\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_d \\end{bmatrix}\\]\n\nMatrices \\(A\\) are 2-dimensional arrays of size (or shape) \\(m \\times n\\) (\\(m\\) rows, \\(n\\) columns, \\(A \\in \\Re^{m \\times n}\\)). They are represented by a capital letter to distinguish them from scalars (classically also in bold \\(\\mathbf{A}\\) but not here). The element \\(a_{ij}\\) of a matrix \\(A\\) is the element on the \\(i\\)-th row and \\(j\\)-th column.\n\n\\[A = \\begin{bmatrix}\na_{11} & a_{12} & \\cdots & a_{1n} \\\\\na_{21} & a_{22} & \\cdots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m1} & a_{m2} & \\cdots & a_{mn} \\\\\n\\end{bmatrix}\\]\n\nTensors \\(\\mathcal{A}\\) are arrays with more than two dimensions. We will not really do math on these objects, but they are useful internally (hence the name of the tensorflow library).\n\n\n2.1.1 Vectors\nA vector can be thought of as the coordinates of a point in an Euclidean space (such the 2D space), relative to the origin. A vector space relies on two fundamental operations, which are that:\n\nVectors can be added:\n\n\\[\\mathbf{x} + \\mathbf{y} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_d \\end{bmatrix} + \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_d \\end{bmatrix} = \\begin{bmatrix} x_1 + y_1 \\\\ x_2 + y_2 \\\\ \\vdots \\\\ x_d + y_d \\end{bmatrix}\\]\n\nVectors can be multiplied by a scalar:\n\n\\[a \\, \\mathbf{x} = a \\, \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_d \\end{bmatrix} = \\begin{bmatrix} a \\, x_1 \\\\ a \\, x_2 \\\\ \\vdots \\\\ a \\, x_d \\end{bmatrix}\\]\n\n\n\n```{figure} ../img/vectorspace.png\n\n\n\n\nwidth: 50%\n\n\n\nVector spaces allow additions of vectors. Source: https://mathinsight.org/image/vector_2d_add\n\nThese two operations generate a lot of nice properties (see <https://en.wikipedia.org/wiki/Vector_space> for a full list), including:\n\n* associativity: $\\mathbf{x} + (\\mathbf{y} + \\mathbf{z}) = (\\mathbf{x} + \\mathbf{y}) + \\mathbf{z}$\n* commutativity: $\\mathbf{x} + \\mathbf{y} = \\mathbf{y} + \\mathbf{x}$\n* the existence of a zero vector $\\mathbf{x} + \\mathbf{0} = \\mathbf{x}$\n* inversion: $\\mathbf{x} + (-\\mathbf{x}) = \\mathbf{0}$\n* distributivity: $a \\, (\\mathbf{x} + \\mathbf{y}) = a \\, \\mathbf{x} + a \\, \\mathbf{y}$\n\nVectors have a **norm** (or length) $||\\mathbf{x}||$. The most intuitive one (if you know the Pythagoras theorem) is the **Euclidean norm** or $L^2$-norm, which sums the square of each element:\n\n$$||\\mathbf{x}||_2 = \\sqrt{x_1^2 + x_2^2 + \\ldots + x_d^2}$$\n\nOther norms exist, distinguished by the subscript. The **$L^1$-norm** (also called Taxicab or Manhattan norm) sums the absolute value of each element:\n\n$$||\\mathbf{x}||_1 = |x_1| + |x_2| + \\ldots + |x_d|$$\n\nThe **p-norm** generalizes the Euclidean norm to other powers $p$: \n\n$$||\\mathbf{x}||_p = (|x_1|^p + |x_2|^p + \\ldots + |x_d|^p)^{\\frac{1}{p}}$$\n\nThe **infinity norm** (or maximum norm) $L^\\infty$ returns the maximum element of the vector:\n\n$$||\\mathbf{x}||_\\infty = \\max(|x_1|, |x_2|, \\ldots, |x_d|)$$\n\n\nOne important operation for vectors is the **dot product** (also called scalar product or inner product) between two vectors:\n\n$$\\langle \\mathbf{x} \\cdot \\mathbf{y} \\rangle = \\langle \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_d \\end{bmatrix} \\cdot \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_d \\end{bmatrix} \\rangle = x_1 \\, y_1 + x_2 \\, y_2 + \\ldots + x_d \\, y_d$$\n\nThe dot product basically sums one by one the product of the elements of each vector. The angular brackets are sometimes omitted ($\\mathbf{x} \\cdot \\mathbf{y}$) but we will use them in this course for clarity.\n\n\nOne can notice immediately that the dot product is **symmetric**: \n\n$$\\langle \\mathbf{x} \\cdot \\mathbf{y} \\rangle = \\langle \\mathbf{y} \\cdot \\mathbf{x} \\rangle$$\n\nand **linear**:\n\n$$\\langle (a \\, \\mathbf{x} + b\\, \\mathbf{y}) \\cdot \\mathbf{z} \\rangle = a\\, \\langle \\mathbf{x} \\cdot \\mathbf{z} \\rangle + b \\, \\langle \\mathbf{y} \\cdot \\mathbf{z} \\rangle$$\n\nThe dot product is an indirect measurement of the **angle** $\\theta$ between two vectors:\n\n\n$$\\langle \\mathbf{x} \\cdot \\mathbf{y} \\rangle = ||\\mathbf{x}||_2 \\, ||\\mathbf{y}||_2 \\, \\cos(\\theta)$$\n\n```{figure} ../img/dotproduct.png\n---\nwidth: 50%\n---\nThe dot product between two vectors is proportional to the cosine of the angle between the two vectors. Source: <https://en.wikipedia.org/wiki/Dot_product>\nIf you normalize the two vectors by dividing them by their norm (which is a scalar), we indeed have the cosine of the angle between them: The higher the normalized dot product, the more the two vectors point towards the same direction (cosine distance between two vectors).\n\\[\\langle \\displaystyle\\frac{\\mathbf{x}}{||\\mathbf{x}||_2} \\cdot \\frac{\\mathbf{y}}{||\\mathbf{y}||_2} \\rangle =  \\cos(\\theta)\\]\n\n\n2.1.2 Matrices\nMatrices are derived from vectors, so most of the previous properties will be true. Let’s consider this 4x3 matrix:\n\\[A = \\begin{bmatrix}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23} \\\\\na_{31} & a_{32} & a_{33} \\\\\na_{41} & a_{42} & a_{43} \\\\\n\\end{bmatrix}\\]\nEach column of the matrix is a vector with 4 elements:\n\\[\\mathbf{a}_1 = \\begin{bmatrix}\na_{11} \\\\\na_{21} \\\\\na_{31} \\\\\na_{41} \\\\\n\\end{bmatrix} \\qquad\n\\mathbf{a}_2 = \\begin{bmatrix}\na_{12} \\\\\na_{22} \\\\\na_{32} \\\\\na_{42} \\\\\n\\end{bmatrix} \\qquad\n\\mathbf{a}_3 = \\begin{bmatrix}\na_{13} \\\\\na_{23} \\\\\na_{33} \\\\\na_{43} \\\\\n\\end{bmatrix} \\qquad\n\\]\nA \\(m \\times n\\) matrix is therefore a collection of \\(n\\) vectors of size \\(m\\) put side by side column-wise:\n\\[A = \\begin{bmatrix}\n\\mathbf{a}_1 & \\mathbf{a}_2 & \\mathbf{a}_3\\\\\n\\end{bmatrix}\\]\nSo all properties of the vector spaces (associativity, commutativity, distributivity) also apply to matrices, as additions and multiplications with a scalar are defined.\n\\[\\alpha \\, A + \\beta \\, B = \\begin{bmatrix}\n\\alpha\\, a_{11} + \\beta \\, b_{11} & \\alpha\\, a_{12} + \\beta \\, b_{12} & \\alpha\\, a_{13} + \\beta \\, b_{13} \\\\\n\\alpha\\, a_{21} + \\beta \\, b_{21} & \\alpha\\, a_{22} + \\beta \\, b_{22} & \\alpha\\, a_{23} + \\beta \\, b_{23} \\\\\n\\alpha\\, a_{31} + \\beta \\, b_{31} & \\alpha\\, a_{32} + \\beta \\, b_{32} & \\alpha\\, a_{33} + \\beta \\, b_{33} \\\\\n\\alpha\\, a_{41} + \\beta \\, b_{41} & \\alpha\\, a_{42} + \\beta \\, b_{42} & \\alpha\\, a_{43} + \\beta \\, b_{43} \\\\\n\\end{bmatrix}\\]\nBeware, you can only add matrices of the same dimensions $m\\times n$. You cannot add a $2\\times 3$ matrix to a $5 \\times 4$ one.\nThe transpose \\(A^T\\) of a \\(m \\times n\\) matrix \\(A\\) is a \\(n \\times m\\) matrix, where the row and column indices are swapped:\n\\[A = \\begin{bmatrix}\na_{11} & a_{12} & \\cdots & a_{1n} \\\\\na_{21} & a_{22} & \\cdots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m1} & a_{m2} & \\cdots & a_{mn} \\\\\n\\end{bmatrix}, \\qquad\nA^T = \\begin{bmatrix}\na_{11} & a_{21} & \\cdots & a_{m1} \\\\\na_{12} & a_{22} & \\cdots & a_{m2} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{1n} & a_{2n} & \\cdots & a_{mn} \\\\\n\\end{bmatrix}\n\\]\nThis is also true for vectors, which become horizontal after transposition:\n\\[\\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_d \\end{bmatrix}, \\qquad\n\\mathbf{x}^T = \\begin{bmatrix} x_1 & x_2 & \\ldots & x_d \\end{bmatrix}\n\\]\nA very important operation is the matrix multiplication. If \\(A\\) is a \\(m\\times n\\) matrix and \\(B\\) a \\(n \\times p\\) matrix:\n\\[\nA=\\begin{bmatrix}\na_{11} & a_{12} & \\cdots & a_{1n} \\\\\na_{21} & a_{22} & \\cdots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m1} & a_{m2} & \\cdots & a_{mn} \\\\\n\\end{bmatrix},\\quad\nB=\\begin{bmatrix}\nb_{11} & b_{12} & \\cdots & b_{1p} \\\\\nb_{21} & b_{22} & \\cdots & b_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nb_{n1} & b_{n2} & \\cdots & b_{np} \\\\\n\\end{bmatrix}\n\\]\nwe can multiply them to obtain a \\(m \\times p\\) matrix:\n\\[\nC = A \\times B =\\begin{bmatrix}\nc_{11} & c_{12} & \\cdots & c_{1p} \\\\\nc_{21} & c_{22} & \\cdots & c_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nc_{m1} & c_{m2} & \\cdots & c_{mp} \\\\\n\\end{bmatrix}\n\\]\nwhere each element \\(c_{ij}\\) is the dot product of the \\(i\\)th row of \\(A\\) and \\(j\\)th column of \\(B\\):\n\\[c_{ij} = \\langle A_{i, :} \\cdot B_{:, j} \\rangle = a_{i1}b_{1j} + a_{i2}b_{2j} +\\cdots + a_{in}b_{nj}= \\sum_{k=1}^n a_{ik}b_{kj}\\]\n$n$, the number of columns of $A$ and rows of $B$, must be the same!\n\n\n\n```{figure} ../img/matrixmultiplication.jpg\n\n\n\n\nwidth: 60%\n\n\n\nThe element \\(c_{ij}\\) of \\(C = A \\times B\\) is the dot product between the \\(i\\)th row of \\(A\\) and the \\(j\\)th column of \\(B\\). Source: https://chem.libretexts.org/Bookshelves/Physical_and_Theoretical_Chemistry_Textbook_Maps/Book%3A_Mathematical_Methods_in_Chemistry_(Levitus)/15%3A_Matrices/15.03%3A_Matrix_Multiplication CC BY-NC-SA; Marcia Levitus\n\nThinking of vectors as $n \\times 1$ matrices, we can multiply a matrix $m \\times n$ with a vector:\n\n$$\n\\mathbf{y} = A \\times \\mathbf{x} = \\begin{bmatrix}\n a_{11} & a_{12} & \\cdots & a_{1n} \\\\\n a_{21} & a_{22} & \\cdots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n a_{m1} & a_{m2} & \\cdots & a_{mn} \\\\\n\\end{bmatrix} \\times \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_m \\end{bmatrix}\n$$\n\nThe result $\\mathbf{y}$ is a vector of size $m$. In that sense, a matrix $A$ can transform a vector of size $n$ into a vector of size $m$: $A$ represents a **projection** from $\\Re^n$ to $\\Re^m$.\n\n```{figure} ../img/projection.png\n---\nwidth: 60%\n---\nA $2 \\times 3$ projection matrix allows to project any 3D vector onto a 2D plane. This is for example what happens inside a camera. Source: <https://en.wikipedia.org/wiki/Homogeneous_coordinate>\nNote that the dot product between two vectors of size \\(n\\) is the matrix multiplication between the transpose of the first vector and the second one:\n\\[\\mathbf{x}^T \\times \\mathbf{y} = \\begin{bmatrix} x_1 & x_2 & \\ldots & x_n \\end{bmatrix} \\times \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix} = x_1 \\, y_1 + x_2 \\, y_2 + \\ldots + x_n \\, y_n = \\langle \\mathbf{x} \\cdot \\mathbf{y} \\rangle\\]\nSquare matrices of size \\(n \\times n\\) can be inverted. The inverse \\(A^{-1}\\) of a matrix \\(A\\) is defined by:\n\\[A \\times A^{-1} = A^{-1} \\times A = I\\]\nwhere \\(I\\) is the identity matrix (a matrix with ones on the diagonal and 0 otherwise). Not all matrices have an inverse (those who don’t are called singular or degenerate). There are plenty of conditions for a matrix to be invertible (for example its determinant is non-zero, see https://en.wikipedia.org/wiki/Invertible_matrix), but they will not matter in this course. Non-square matrices are generally not invertible, but see the pseudoinverse (https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse).\nMatrix inversion allows to solve linear systems of equations. Given the problem:\n\\[\n\\begin{cases}\n    a_{11} \\, x_1 + a_{12} \\, x_2 + \\ldots + a_{1n} \\, x_n = b_1 \\\\\n    a_{21} \\, x_1 + a_{22} \\, x_2 + \\ldots + a_{2n} \\, x_n = b_2 \\\\\n    \\ldots \\\\\n    a_{n1} \\, x_1 + a_{n2} \\, x_2 + \\ldots + a_{nn} \\, x_n = b_n \\\\\n\\end{cases}\n\\]\nwhich is equivalent to:\n\\[A \\times \\mathbf{x} = \\mathbf{b}\\]\nwe can multiply both sides to the left with \\(A^{-1}\\) (if it exists) and obtain:\n\\[\\mathbf{x} = A^{-1} \\times \\mathbf{b}\\]"
  },
  {
    "objectID": "notes/1.2-Math.html#calculus",
    "href": "notes/1.2-Math.html#calculus",
    "title": "2  Math basics (optional)",
    "section": "2.2 Calculus",
    "text": "2.2 Calculus\n\n\n\n\n\n2.2.1 Functions\nA univariate function \\(f\\) associates to any real number \\(x \\in \\Re\\) (or a subset of \\(\\Re\\) called the support of the function) another (unique) real number \\(f(x)\\):\n\\[\n\\begin{align}\nf\\colon \\quad \\Re &\\to \\Re\\\\\nx &\\mapsto f(x),\\end{align}\n\\]\n\n\n\n```{figure} ../img/function.png\n\n\n\n\nwidth: 60%\n\n\n\nExample of univariate function, here the quadratic function \\(f(x) = x^2 - 2 \\, x + 1\\).\n\n\nA **multivariate function** $f$ associates to any vector $\\mathbf{x} \\in \\Re^n$ (or a subset) a real number $f(\\mathbf{x})$:\n\n$$\n\\begin{align}\nf\\colon \\quad \\Re^n &\\to \\Re\\\\\n\\mathbf{x} &\\mapsto f(\\mathbf{x}),\\end{align}\n$$\n\nThe variables of the function are the elements of the vector. For low-dimensional vector spaces, it is possible to represent each element explicitly, for example:\n\n$$\n\\begin{align}\nf\\colon \\quad\\Re^3 &\\to \\Re\\\\\nx, y, z &\\mapsto f(x, y, z),\\end{align}\n$$\n\n```{figure} ../img/multivariatefunction.png\n---\nwidth: 60%\n---\nExample of a multivariate function $f(x_1, x_2)$ mapping $\\Re^2$ to $\\Re$. Source: <https://en.wikipedia.org/wiki/Function_of_several_real_variables>\nVector fields associate to any vector \\(\\mathbf{x} \\in \\Re^n\\) (or a subset) another vector (possibly of different size):\n\\[\n\\begin{align}\n\\overrightarrow{f}\\colon \\quad \\Re^n &\\to \\Re^m\\\\\n\\mathbf{x} &\\mapsto \\overrightarrow{f}(\\mathbf{x}),\\end{align}\n\\]\n\n\n\n```{figure} ../img/vectorfield.png\n\n\n\n\nwidth: 40%\n\n\n\nVector field associating to each point of \\(\\Re^2\\) another vector. Source: https://en.wikipedia.org/wiki/Vector_field\n\n\n\n```{note}\nThe matrix-vector multiplication $\\mathbf{y} = A \\times \\mathbf{x}$ is a linear vector field, mapping any vector $\\mathbf{x}$ into another vector $\\mathbf{y}$.\n\n\n2.2.2 Differentiation\n\n2.2.2.1 Derivatives\nDifferential calculus deals with the derivative of a function, a process called differentiation.\nThe derivative \\(f'(x)\\) or \\(\\displaystyle\\frac{d f(x)}{dx}\\) of a univariate function \\(f(x)\\) is defined as the local slope of the tangent to the function for a given value of \\(x\\):\n\\[f'(x) = \\lim_{h \\to 0} \\frac{f(x + h) - f(x)}{h}\\]\nThe line passing through the points \\((x, f(x))\\) and \\((x + h, f(x + h))\\) becomes tangent to the function when \\(h\\) becomes very small:\n\n\n\n```{figure} ../img/derivative-approx.png\n\n\n\n\nwidth: 60%\n\n\n\nThe derivative of the function \\(f(x)\\) can be approximated by the slope of the line passing through \\((x, f(x))\\) and \\((x + h, f(x + h))\\) when \\(h\\) becomes very small.\n\nThe sign of the derivative tells you how the function behaves locally:\n\n* If the derivative is positive, increasing a little bit $x$ increases the function $f(x)$, so the function is **locally increasing**.\n* If the derivative is negative, increasing a little bit $x$ decreases the function $f(x)$, so the function is **locally decreasing**.\n\nIt basically allows you to measure the local influence of $x$ on $f(x)$: if I change a little bit the value $x$, what happens to $f(x)$? This will be very useful in machine learning.\n\nA special case is when the derivative is equal to 0 in $x$. $x$ is then called an **extremum** (or optimum) of the function, i.e. it can be a maximum or minimum. \n\n\n\n```{note}\nIf you differentiate $f'(x)$ itself, you obtain the **second-order derivative** $f''(x)$. You can repeat that process and obtain higher order derivatives. \n\nFor example, if $x(t)$ represents the position $x$ of an object depending on time $t$, the first-order derivative $x'(t)$ denotes the **speed** of the object and the second-order derivative $x''(t)$ its **acceleration**.\nYou can tell whether an extremum is a maximum or a minimum by looking at its second-order derivative:\n\nIf \\(f''(x) > 0\\), the extremum is a minimum.\nIf \\(f''(x) < 0\\), the extremum is a maximum.\nIf \\(f''(x) = 0\\), the extremum is a saddle point.\n\n\n\n\n```{figure} ../img/optimization-example.png\n\n\n\n\nwidth: 80%\n\n\n\nQuadratic functions have only one extremum (here a minimum in -1), as their derivative is linear and is equal to zero for only one value.\n\nThe derivative of a **multivariate function** $f(\\mathbf{x})$ is a vector of partial derivatives called the **gradient of the function** $\\nabla_\\mathbf{x} \\, f(\\mathbf{x})$:\n\n$$\n    \\nabla_\\mathbf{x} \\, f(\\mathbf{x}) = \\begin{bmatrix}\n        \\displaystyle\\frac{\\partial f(\\mathbf{x})}{\\partial x_1} \\\\\n        \\displaystyle\\frac{\\partial f(\\mathbf{x})}{\\partial x_2} \\\\\n        \\ldots \\\\\n        \\displaystyle\\frac{\\partial f(\\mathbf{x})}{\\partial x_n} \\\\\n    \\end{bmatrix}\n$$\n\n\nThe subscript to the $\\nabla$ operator denotes *with respect to* (w.r.t) which variable the differentiation is done.\n\nA **partial derivative** w.r.t. to particular variable (or element of the vector) is simply achieved by differentiating the function while considering all other variables to be **constant**. For example the function:\n\n$$f(x, y) = x^2 + 3 \\, x \\, y + 4 \\, x \\, y^2 - 1$$\n\ncan be partially differentiated w.r.t. $x$ and $y$ as:\n\n$$\\begin{cases}\n\\displaystyle\\frac{\\partial f(x, y)}{\\partial x} = 2 \\, x + 3\\, y + 4 \\, y^2 \\\\\n\\\\\n\\displaystyle\\frac{\\partial f(x, y)}{\\partial y} = 3 \\, x + 8\\, x \\, y\n\\end{cases}$$\n\nThe gradient can be generalized to **vector fields**, where the **Jacobian** or **Jacobi matrix** is a matrix containing all partial derivatives.\n\n$$\nJ = \\begin{bmatrix}\n    \\dfrac{\\partial \\mathbf{f}}{\\partial x_1} & \\cdots & \\dfrac{\\partial \\mathbf{f}}{\\partial x_n} \\end{bmatrix}\n= \\begin{bmatrix}\n    \\dfrac{\\partial f_1}{\\partial x_1} & \\cdots & \\dfrac{\\partial f_1}{\\partial x_n}\\\\\n    \\vdots & \\ddots & \\vdots\\\\\n    \\dfrac{\\partial f_m}{\\partial x_1} & \\cdots & \\dfrac{\\partial f_m}{\\partial x_n} \\end{bmatrix}\n$$\n\n#### Analytical properties\n\nThe analytical form of the derivative of most standard mathematical functions is known. The following table lists the most useful ones in this course:\n\n```{list-table}\n:header-rows: 1\n:name: example-table\n\n* - $f(x)$\n  - $f'(x)$\n* - $x$\n  - $1$\n* - $x^p$\n  - $p \\, x^{p-1}$\n* - $\\displaystyle\\frac{1}{x}$\n  - $- \\displaystyle\\frac{1}{x^2}$\n* - $e^x$\n  - $e^x$\n* - $\\ln x$\n  - $\\displaystyle\\frac{1}{x}$\nDifferentiation is linear, which means that if we define the function:\n\\[h(x) = a \\, f(x) + b \\, g(x)\\]\nits derivative is:\n\\[h'(x) = a \\, f'(x) + b \\, g'(x)\\]\nA product of functions can also be differentiated analytically:\n\\[(f(x) \\times g(x))' = f'(x) \\times g(x) + f(x) \\times g'(x)\\]\n```{admonition} Example \\[f(x) = x^2 \\, e^x\\]\n\\[f'(x) = 2 \\, x \\, e^x + x^2 \\cdot e^x\\]\n\n#### Chain rule\n\nA very important concept for neural networks is the **chain rule**, which tells how to differentiate **function compositions** (functions of a function) of the form:\n\n$$(f \\circ g) (x) = f(g(x))$$\n\nThe derivative of $f \\circ g$ is:\n\n$$(f \\circ g)' (x) = (f' \\circ g) (x) \\times g'(x)$$\n\nThe chain rule may be more understandable using Leibniz's notation:\n\n$$\\frac{d f \\circ g (x)}{dx} = \\frac{d f (g (x))}{d g(x)} \\times \\frac{d g (x)}{dx}$$\n\nBy posing $y = g(x)$ as an intermediary variable, it becomes:\n\n\n$$\\frac{d f(y)}{dx} = \\frac{d f(y)}{dy} \\times \\frac{dy}{dx}$$\n\n\n```{admonition} Example\nThe function :\n\n$$h(x) = \\frac{1}{2 \\, x + 1}$$\n\nis the function composition of $g(x) = 2 \\, x + 1$ and $f(x) = \\displaystyle\\frac{1}{x}$, whose derivatives are known:\n\n$$g'(x) = 2$$\n$$f'(x) = -\\displaystyle\\frac{1}{x^2}$$\n\nIts derivative according to the **chain rule** is:\n\n$$h'(x) = f'(g(x)) \\times g'(x) = -\\displaystyle\\frac{1}{(2 \\, x + 1)^2} \\times 2$$\nThe chain rule also applies to partial derivatives:\n\\[\n    \\displaystyle\\frac{\\partial f \\circ g (x, y)}{\\partial x} = \\frac{\\partial f \\circ g (x, y)}{\\partial g (x, y)} \\times \\frac{\\partial g (x, y)}{\\partial x}\n\\]\nand gradients:\n\\[\n    \\nabla_\\mathbf{x} \\, f \\circ g (\\mathbf{x}) = \\nabla_{g(\\mathbf{x})} \\, f \\circ g (\\mathbf{x}) \\times \\nabla_\\mathbf{x} \\, g (\\mathbf{x})\n\\]\n\n\n\n2.2.3 Integration\nThe opposite operation of differentation is integration. Given a function \\(f(x)\\), we search a function \\(F(x)\\) whose derivative is \\(f(x)\\):\n\\[F'(x) = f(x)\\]\nThe integral of \\(f\\) is noted:\n\\[F(x) = \\int f(x) \\, dx\\]\n\\(dx\\) being an infinitesimal interval (similar \\(h\\) in the definition of the derivative). There are tons of formal definitions of integrals (Riemann, Lebesgue, Darboux…) and we will not get into details here as we will not use integrals a lot.\nThe most important to understand for now is maybe that the integral of a function is the area under the curve. The area under the curve of a function \\(f\\) on the interval \\([a, b]\\) is:\n\\[\\mathcal{S} = \\int_a^b f(x) \\, dx\\]\n\n\n\n```{figure} ../img/riemann-sum1.svg\n\n\n\n\nwidth: 60%\n\n\n\nThe integral of \\(f\\) on \\([a, b]\\) is the area of the surface between the function and the x-axis. Note that it can become negative when the function is mostly negative on \\([a, b]\\). Source: https://www.math24.net/riemann-sums-definite-integral/\n\nOne way to approximate this surface is to split the interval $[a, b]$ into $n$ intervals of width $dx$ with the points $x_1, x_2, \\ldots, x_n$. This defines $n$ rectangles of width $dx$ and height $f(x_i)$, so their surface is $f(x_i) \\, dx$. The area under the curve can then be approximated by the sum of the surfaces of all these rectangles.\n\n```{figure} ../img/riemann-sum.svg\n---\nwidth: 60%\n---\nThe interval$[a, b]$ can be split in $n$ small intervals of width $dx$, defining $n$ rectangles whose sum is close to the area under the curve. Source: <https://www.math24.net/riemann-sums-definite-integral/>\nWhen \\(n \\to \\infty\\), or equivalently \\(dx \\to 0\\), the sum of these rectangular areas (called the Riemann sum) becomes exactly the area under the curve. This is the definition of the definite integral:\n\\[\\int_a^b f(x) \\, dx = \\lim_{dx \\to 0} \\sum_{i=1}^n f(x_i) \\, dx\\]\nVery roughly speaking, the integral can be considered as the equivalent of a sum for continuous functions."
  },
  {
    "objectID": "notes/1.2-Math.html#probability-theory",
    "href": "notes/1.2-Math.html#probability-theory",
    "title": "2  Math basics (optional)",
    "section": "2.3 Probability theory",
    "text": "2.3 Probability theory\n\n\n\n\n\n2.3.1 Discrete probability distributions\nLet’s note \\(X\\) a discrete random variable with \\(n\\) realizations (or outcomes) \\(x_1, \\ldots, x_n\\).\n\nA coin has two outcomes: head and tails.\nA dice has six outcomes: 1, 2, 3, 4, 5, 6.\n\nThe probability that \\(X\\) takes the value \\(x_i\\) is defined in the frequentist sense by the relative frequency of occurrence, i.e. the proportion of samples having the value \\(x_i\\), when the total number \\(N\\) of samples tends to infinity:\n\\[\n    P(X = x_i) = \\frac{\\text{Number of favorable cases}}{\\text{Total number of samples}}\n\\]\nThe set of probabilities \\(\\{P(X = x_i)\\}_{i=1}^n\\) define the probability distribution for the random variable (or probability mass function, pmf). By definition, we have \\(0 \\leq P(X = x_i) \\leq 1\\) and the probabilities have to respect:\n\\[\n    \\sum_{i=1}^n P(X = x_i) = 1\n\\]\nAn important metric for a random variable is its mathematical expectation or expected value, i.e. its “mean” realization weighted by the probabilities:\n\\[\n    \\mathbb{E}[X] = \\sum_{i=1}^n P(X = x_i) \\, x_i\n\\]\nThe expectation does not even need to be a valid realization:\n\\[\n    \\mathbb{E}[\\text{Coin}] = \\frac{1}{2} \\, 0 + \\frac{1}{2} \\, 1 = 0.5\n\\]\n\\[\n    \\mathbb{E}[\\text{Dice}] = \\frac{1}{6} \\, (1 + 2 + 3 + 4 + 5 + 6) = 3.5\n\\]\nWe can also compute the mathematical expectation of functions of a random variable:\n\\[\n    \\mathbb{E}[f(X)] = \\sum_{i=1}^n P(X = x_i) \\, f(x_i)\n\\]\nThe variance of a random variable is the squared deviation around the mean:\n\\[\n    \\text{Var}(X) = \\mathbb{E}[(X - \\mathbb{E}[X])^2] = \\sum_{i=1}^n P(X = x_i) \\, (x_i - \\mathbb{E}[X])^2\n\\]\nVariance of a coin:\n\\[\n    \\text{Var}(\\text{Coin}) = \\frac{1}{2} \\, (0 - 0.5)^2 + \\frac{1}{2} \\, (1 - 0.5)^2 = 0.25\n\\]\nVariance of a dice:\n\\[\n    \\text{Var}(\\text{Dice}) = \\frac{1}{6} \\, ((1-3.5)^2 + (2-3.5)^2 + (3-3.5)^2 + (4-3.5)^2 + (5-3.5)^2 + (6-3.5)^2) = \\frac{105}{36}\n\\]\n\n\n2.3.2 Continuous probability distributions\nContinuous random variables can take infinitely many values in a continuous interval, e.g. \\(\\Re\\) or some subset. The closed set of values they can take is called the support \\(\\mathcal{D}_X\\) of the probability distribution. The probability distribution is described by a probability density function (pdf) \\(f(x)\\).\n\n\n\n```{figure} ../img/normaldistribution.png\n\n\n\n\nwidth: 60%\n\n\n\nNormal distributions are continuous distributions. The area under the curve is always 1.\n\nThe pdf of a distribution must be positive ($f(x) \\geq 0 \\, \\forall x \\in \\mathcal{D}_X$) and its integral (area under the curve) must be equal to 1:\n\n$$\n    \\int_{x \\in \\mathcal{D}_X} f(x) \\, dx = 1\n$$\n\nThe pdf does not give the probability of taking a particular value $x$ (it is 0), but allows to get the probability that a value lies in a specific interval:\n\n$$\n    P(a \\leq X \\leq b) = \\int_{a}^b f(x) \\, dx \n$$\n\n\nOne can however think of the pdf as the **likelihood** that a value $x$ comes from that distribution.\n\n\nFor continuous distributions, the mathematical expectation is now defined by an integral instead of a sum:\n\n$$\n    \\mathbb{E}[X] = \\int_{x \\in \\mathcal{D}_X} f(x) \\, x \\, dx\n$$\n\nthe variance also:\n\n$$\n    \\text{Var}(X) = \\int_{x \\in \\mathcal{D}_X} f(x) \\, (x - \\mathbb{E}[X])^2 \\, dx\n$$\n\nor a function of the random variable:\n\n$$\n    \\mathbb{E}[g(X)] = \\int_{x \\in \\mathcal{D}_X} f(x) \\, g(x) \\, dx\n$$\n\nNote that the expectation operator is **linear**:\n\n$$\n    \\mathbb{E}[a \\, X + b \\, Y] = a \\, \\mathbb{E}[X] + b \\, \\mathbb{E}[Y]\n$$\n\nbut not the variance, even when the distributions are independent:\n\n$$\n    \\text{Var}[a \\, X + b \\, Y] = a^2 \\, \\text{Var}[X] + b^2 \\, \\text{Var}[Y]\n$$\n\n### Standard distributions\n\nProbability distributions can in principle have any form: $f(x)$ is unknown. However, specific parameterized distributions can be very useful: their pmf/pdf is fully determined by a couple of parameters.\n\n* The **Bernouilli** distribution is a binary (discrete, 0 or 1) distribution with a parameter $p$ specifying the probability to obtain the outcome 1 (e.g. a coin):\n\n$$\n    P(X = 1) = p \\; \\text{and} \\; P(X=0) = 1 - p \n$$\n$$P(X=x) = p^x \\, (1-p)^{1-x}$$\n$$\\mathbb{E}[X] = p$$\n\n* The **Multinouilli** or **categorical** distribution is a discrete distribution with $k$ realizations (e.g. a dice). Each realization $x_i$ is associated with a parameter $p_i >0$ representing its probability. We have $\\sum_i p_i = 1$.\n\n$$P(X = x_i) = p_i$$\n\n\n* The **uniform distribution** has an equal and constant probability of returning values between $a$ and $b$, never outside this range. It is parameterized by the start of the range $a$ and the end of the range $b$. Its support is $[a, b]$. The pdf of the uniform distribution $\\mathcal{U}(a, b)$ is defined on $[a, b]$ as:\n\n$$\n    f(x; a, b) = \\frac{1}{b - a}\n$$\n\n* The **normal distribution** is the most frequently encountered continuous distribution. It is parameterized by two parameters: the mean $\\mu$ and the variance $\\sigma^2$ (or standard deviation $\\sigma$). Its support is $\\Re$. The pdf of the normal distribution $\\mathcal{N}(\\mu, \\sigma)$ is defined on $\\Re$ as:\n\n$$\n    f(x; \\mu, \\sigma) = \\frac{1}{\\sqrt{2\\,\\pi\\,\\sigma^2}} \\, e^{-\\displaystyle\\frac{(x - \\mu)^2}{2\\,\\sigma^2}}\n$$\n\n\n* The **exponential distribution** is the probability distribution of the time between events in a Poisson point process, i.e., a process in which events occur continuously and independently at a constant average rate. It is parameterized by one parameter: the rate $\\lambda$. Its support is $\\Re^+$ ($x > 0$).\nThe pdf of the exponential distribution is defined on $\\Re^+$ as:\n\n$$\n    f(x; \\lambda) = \\lambda \\, e^{-\\lambda \\, x}\n$$\n\n\n### Joint and conditional probabilities\n\nLet's now suppose that we have two random variables $X$ and $Y$ with different probability distributions $P(X)$ and $P(Y)$.  The **joint probability** $P(X, Y)$ denotes the probability of observing the realizations $x$ **and** $y$ at the same time:\n\n$$P(X=x, Y=y)$$\n\nIf the random variables are **independent**, we have:\n\n$$P(X=x, Y=y) = P(X=x) \\, P(Y=y)$$\n\nIf you know the joint probability, you can compute the **marginal probability distribution** of each variable:\n\n$$P(X=x) = \\sum_y P(X=x, Y=y)$$\n\nThe same is true for continuous probability distributions:\n\n$$\n    f(x) = \\int f(x, y) \\, dy\n$$\n\nSome useful information between two random variables is the **conditional probability**. $P(X=x | Y=y)$ is the conditional probability that $X=x$, **given** that $Y=y$ is observed.\n\n* $Y=y$ is not random anymore: it is a **fact** (at least theoretically).\n\n* You wonder what happens to the probability distribution of $X$ now that you know the value of $Y$.\n\nConditional probabilities are linked to the joint probability by:\n\n$$\n    P(X=x | Y=y) = \\frac{P(X=x, Y=y)}{P(Y=y)}\n$$\n\nIf $X$ and $Y$ are **independent**, we have $P(X=x | Y=y) = P(X=x)$ (knowing $Y$ does not change anything to the probability distribution of $X$). We can use the same notation for the complete probability distributions:\n\n$$\n    P(X | Y) = \\frac{P(X, Y)}{P(Y)}\n$$\n\n**Example**\n\n\n```{figure} ../img/conditionalprobability.png\n---\nwidth: 60%\n---\nSource: <https://www.elevise.co.uk/g-e-m-h-5-u.html>.\nYou ask 50 people whether they like cats or dogs:\n\n18 like both cats and dogs.\n21 like only dogs.\n5 like only cats.\n6 like none of them.\n\nWe consider loving cats and dogs as random variables (and that our sample size is big enough to use probabilities…). Among the 23 who love cats, which proportion also loves dogs?\n```{dropdown} Answer We have \\(P(\\text{dog}) = \\displaystyle\\frac{18+21}{50}= \\displaystyle\\frac{39}{50}\\) and \\(P(\\text{cat}) = \\displaystyle\\frac{18+5}{50} = \\frac{23}{50}\\).\nThe joint probability of loving both cats and dogs is \\(P(\\text{cat}, \\text{dog}) = \\displaystyle\\frac{18}{50}\\).\nThe conditional probability of loving dogs given one loves cats is:\n\\[P(\\text{dog} | \\text{cat}) = \\displaystyle\\frac{P(\\text{cat}, \\text{dog})}{P(\\text{cat})} = \\frac{\\frac{18}{50}}{\\frac{23}{50}} = \\frac{18}{23}\\]\n\n\n### Bayes' rule\n\nNoticing that the definition of conditional probabilities is symmetric:\n\n$$\n    P(X, Y) = P(X | Y) \\, P(Y) = P(Y | X) \\, P(X)\n$$\n\nwe can obtain the **Bayes' rule**:\n\n$$\n    P(Y | X) = \\frac{P(X|Y) \\, P(Y)}{P(X)}\n$$\n\nIt is very useful when you already know $P(X|Y)$ and want to obtain $P(Y|X)$ (**Bayesian inference**).\n\n* $P(Y | X)$ is called the **posterior probability**.\n\n* $P(X | Y)$ is called the **likelihood**.\n\n* $P(Y)$ is called the **prior probability** (belief).\n\n* $P(X)$ is called the **model evidence** or **marginal likelihood**.\n\n\n**Example**\n\nLet's consider a disease $D$ (binary random variable) and a medical test $T$ (also binary). The disease affects 10% of the general population: \n\n$$P(D=1)= 0.1 \\qquad \\qquad P(D=0)=0.9$$\n\nWhen a patient has the disease, the test is positive 80% of the time (true positives):\n\n$$P(T=1 | D=1) = 0.8 \\qquad \\qquad P(T=0 | D=1) = 0.2$$\n\nWhen a patient does not have the disease, the test is still positive 10% of the time (false positives):\n\n$$P(T=1 | D=0) = 0.1 \\qquad \\qquad P(T=0 | D=0) = 0.9$$\n\nGiven that the test is positive, what is the probability that the patient is ill?\n\n\n```{dropdown} Answer\n$$\n\\begin{aligned}\n    P(D=1|T=1) &= \\frac{P(T=1 | D=1) \\, P(D=1)}{P(T=1)} \\\\\n               &\\\\\n               &= \\frac{P(T=1 | D=1) \\, P(D=1)}{P(T=1 | D=1) \\, P(D=1) + P(T=1 | D=0) \\, P(D=0)} \\\\\n               &\\\\\n               &= \\frac{0.8 \\times 0.1}{0.8 \\times 0.1 + 0.1 \\times 0.9} \\\\\n               &\\\\\n               & = 0.47 \\\\\n\\end{aligned}\n$$"
  },
  {
    "objectID": "notes/1.2-Math.html#statistics",
    "href": "notes/1.2-Math.html#statistics",
    "title": "2  Math basics (optional)",
    "section": "2.4 Statistics",
    "text": "2.4 Statistics\n\n\n\n\n\n2.4.1 Monte Carlo sampling\nRandom sampling or Monte Carlo sampling (MC) consists of taking \\(N\\) samples \\(x_i\\) out of the distribution \\(X\\) (discrete or continuous) and computing the sample average:\n\\[\n    \\mathbb{E}[X] = \\mathbb{E}_{x \\sim X} [x] \\approx \\frac{1}{N} \\, \\sum_{i=1}^N x_i\n\\]\n\n\n\n```{figure} ../img/normaldistribution.svg\n\n\n\n\nwidth: 60%\n\n\n\nSamples taken from a normal distribution will mostly be around the mean.\n\nMore samples will be obtained where $f(x)$ is high ($x$ is probable), so the average of the sampled data will be close to the expected value of the distribution.\n\n**Law of big numbers**\n\n> As the number of identically distributed, randomly generated variables increases, their sample mean (average) approaches their theoretical mean.\n\n\nMC estimates are only correct when: \n\n* the samples are **i.i.d** (independent and identically distributed):\n\n    * independent: the samples must be unrelated with each other.\n\n    * identically distributed: the samples must come from the same distribution $X$.\n\n* the number of samples is large enough. Usually $N > 30$ for simple distributions.\n\nOne can estimate any function of the random variable with random sampling:\n\n$$\n    \\mathbb{E}[f(X)] = \\mathbb{E}_{x \\sim X} [f(x)] \\approx \\frac{1}{N} \\, \\sum_{i=1}^N f(x_i)\n$$\n\n```{figure} ../img/montecarlo.svg\n---\nwidth: 100%\n---\nSampling can be used to estimate $\\pi$: when sampling $x$ and $y$ uniformly in $[0, 1]$, the proportion of points with a norm smaller than tends to $\\pi/4$. Source: <https://towardsdatascience.com/an-overview-of-monte-carlo-methods-675384eb1694>\n\n\n2.4.2 Central limit theorem\nSuppose we have an unknown distribution \\(X\\) with expected value \\(\\mu = \\mathbb{E}[X]\\) and variance \\(\\sigma^2\\). We can take randomly \\(N\\) samples from \\(X\\) to compute the sample average:\n\\[\n    S_N = \\frac{1}{N} \\, \\sum_{i=1}^N x_i\n\\]\nThe Central Limit Theorem (CLT) states that:\n\nThe distribution of sample averages is normally distributed with mean \\(\\mu\\) and variance \\(\\frac{\\sigma^2}{N}\\).\n\n\\[S_N \\sim \\mathcal{N}(\\mu, \\frac{\\sigma}{\\sqrt{N}})\\]\nIf we perform the sampling multiple times, even with few samples, the average of the sampling averages will be very close to the expected value. The more samples we get, the smaller the variance of the estimates. Although the distribution \\(X\\) can be anything, the sampling averages are normally distributed.\n\n\n\n```{figure} ../img/IllustrationCentralTheorem.png\n\n\n\n\nwidth: 100%\n\n\n\nSource: https://en.wikipedia.org/wiki/Central_limit_theorem\n\n### Estimators\n\nCLT shows that the sampling average is an **unbiased estimator** of the expected value of a distribution:\n\n$$\\mathbb{E}[S_N] = \\mathbb{E}[X]$$\n\nAn estimator is a random variable used to measure parameters of a distribution (e.g. its expectation). The problem is that estimators can generally be **biased**.\n\nTake the example of a thermometer $M$ measuring the temperature $T$. $T$ is a random variable (normally distributed with $\\mu=20$ and $\\sigma=10$) and the measurements $M$ relate to the temperature with the relation:\n\n$$\n    M = 0.95 \\, T + 0.65\n$$\n\n```{figure} ../img/estimators-temperature.png\n---\nwidth: 100%\n---\nLeft: measurement as a function of the temperature. Right: distribution of temperature.\nThe thermometer is not perfect, but do random measurements allow us to estimate the expected value of the temperature?\nWe could repeatedly take 100 random samples of the thermometer and see how the distribution of sample averages look like:\n\n\n\n```{figure} ../img/estimators-temperature2.png\n\n\n\n\nwidth: 60%\n\n\n\nSampled measurements.\n\nBut, as the expectation is linear, we actually have:\n\n$$\n    \\mathbb{E}[M] = \\mathbb{E}[0.95 \\, T + 0.65] = 0.95 \\, \\mathbb{E}[T] + 0.65 = 19.65 \\neq \\mathbb{E}[T]\n$$\n\nThe thermometer is a **biased estimator** of the temperature.\n\nLet's note $\\theta$ a parameter of a probability distribution $X$ that we want to estimate (it does not have to be its mean). An **estimator** $\\hat{\\theta}$ is a random variable mapping the sample space of $X$ to a set of sample estimates.\n\n* The **bias** of an estimator is the mean error made by the estimator:\n\n$$\n    \\mathcal{B}(\\hat{\\theta}) = \\mathbb{E}[\\hat{\\theta} - \\theta] = \\mathbb{E}[\\hat{\\theta}] - \\theta\n$$\n\n* The **variance** of an estimator is the deviation of the samples around the expected value:\n\n$$\n    \\text{Var}(\\hat{\\theta}) = \\mathbb{E}[(\\hat{\\theta} - \\mathbb{E}[\\hat{\\theta}] )^2]\n$$\n\nIdeally, we would like estimators with:\n\n* **low bias**: the estimations are correct on average (= equal to the true parameter).\n\n* **low variance**: we do not need many estimates to get a correct estimate (CLT: $\\frac{\\sigma}{\\sqrt{N}}$)\n\n\n```{figure} ../img/biasvariance3.png\n---\nwidth: 60%\n---\nBias-variance trade-off.\nUnfortunately, the perfect estimator does not exist in practice. One usually talks of a bias/variance trade-off: if you have a small bias, you will have a high variance, or vice versa. In machine learning, bias corresponds to underfitting, variance to overfitting."
  },
  {
    "objectID": "notes/1.2-Math.html#information-theory",
    "href": "notes/1.2-Math.html#information-theory",
    "title": "2  Math basics (optional)",
    "section": "2.5 Information theory",
    "text": "2.5 Information theory\n\n\n\n\n\n2.5.1 Entropy\nInformation theory (a field founded by Claude Shannon) asks how much information is contained in a probability distribution. Information is related to surprise or uncertainty: are the outcomes of a random variable surprising?\n\nAlmost certain outcomes (\\(P \\sim 1\\)) are not surprising because they happen all the time.\nAlmost impossible outcomes (\\(P \\sim 0\\)) are very surprising because they are very rare.\n\n\n\n\n```{figure} ../img/selfinformation.png\n\n\n\n\nwidth: 80%\n\n\n\nSelf-information.\n\nA useful measurement of how surprising is an outcome $x$ is the **self-information**:\n\n$$\n    I (x) = - \\log P(X = x)\n$$\n\nDepending on which log is used, self-information has different units, but it is just a rescaling, the base never matters:\n\n* $\\log_2$: bits or shannons.\n* $\\log_e = \\ln$: nats.\n\n\nThe **entropy** (or Shannon entropy) of a probability distribution is the expected value of the self-information of its outcomes:\n\n$$\n    H(X) = \\mathbb{E}_{x \\sim X} [I(x)] = \\mathbb{E}_{x \\sim X} [- \\log P(X = x)] \n$$\n\nIt measures the **uncertainty**, **randomness** or **information content** of the random variable.\n\nIn the discrete case:\n\n$$\n    H(X) = - \\sum_x P(x) \\, \\log P(x)\n$$\n\nIn the continuous case:\n\n$$\n    H(X) = - \\int_x f(x) \\, \\log f(x) \\, dx\n$$\n\nThe entropy of a Bernouilli variable is maximal when both outcomes are **equiprobable**. If a variable is **deterministic**, its entropy is minimal and equal to zero.\n\n\n```{figure} ../img/entropy-binomial.png\n---\nwidth: 80%\n---\nThe entropy of a Bernouilli distribution is maximal when the two outcomes are equiprobable.\nThe joint entropy of two random variables \\(X\\) and \\(Y\\) is defined by:\n\\[\n    H(X, Y) = \\mathbb{E}_{x \\sim X, y \\sim Y} [- \\log P(X=x, Y=y)]\n\\]\nThe conditional entropy of two random variables \\(X\\) and \\(Y\\) is defined by:\n\\[\n    H(X | Y) = \\mathbb{E}_{x \\sim X, y \\sim Y} [- \\log P(X=x | Y=y)]  = \\mathbb{E}_{x \\sim X, y \\sim Y} [- \\log \\frac{P(X=x , Y=y)}{P(Y=y)}]\n\\]\nIf the variables are independent, we have:\n\\[\n    H(X, Y) = H(X) + H(Y)\n\\] \\[\n    H(X | Y) = H(X)\n\\]\nBoth are related by:\n\\[\n    H(X | Y) = H(X, Y) - H(Y)\n\\]\nThe equivalent of Bayes’ rule is:\n\\[\n    H(Y |X) = H(X |Y) + H(Y) - H(X)\n\\]\n\n\n2.5.2 Mutual Information, cross-entropy and Kullback-Leibler divergence\nThe most important information measurement between two variables is the mutual information MI (or information gain):\n\\[\n    I(X, Y) = H(X) - H(X | Y) = H(Y) - H(Y | X)\n\\]\nIt measures how much information the variable \\(X\\) holds on \\(Y\\):\n\nIf the two variables are independent, the MI is 0 : \\(X\\) is as random, whether you know \\(Y\\) or not.\n\n\\[\n        I (X, Y) = 0\n\\]\n\nIf the two variables are dependent, knowing \\(Y\\) gives you information on \\(X\\), which becomes less random, i.e. less uncertain / surprising.\n\n\\[\n        I (X, Y) > 0\n\\]\nIf you can fully predict \\(X\\) when you know \\(Y\\), it becomes deterministic (\\(H(X|Y)=0\\)) so the mutual information is maximal (\\(I(X, Y) = H(X)\\)).\nThe cross-entropy between two distributions \\(X\\) and \\(Y\\) is defined as:\n\\[\n    H(X, Y) = \\mathbb{E}_{x \\sim X}[- \\log P(Y=x)]\n\\]\nBeware that the notation $H(X, Y)$ is the same as the joint entropy, but it is a different concept!\n\n\n\n```{figure} ../img/crossentropy.svg\n\n\n\n\nwidth: 100%\n\n\n\nThe cross-entropy measures the overlap between two probability distributions.\n\nThe cross-entropy measures the **negative log-likelihood** that a sample $x$ taken from the distribution $X$ could also come from the distribution $Y$. More exactly, it measures how many bits of information one would need to distinguish the two distributions $X$ and $Y$.\n\n$$\n    H(X, Y) = \\mathbb{E}_{x \\sim X}[- \\log P(Y=x)]\n$$\n\nIf the two distributions are the same *almost anywhere*, one cannot distinguish samples from the two distributions, the cross-entropy is the same as the entropy of $X$. If the two distributions are completely different, one can tell whether a sample $Z$ comes from $X$ or $Y$, the cross-entropy is higher than the entropy of $X$.\n\n\nIn practice, the **Kullback-Leibler divergence** $\\text{KL}(X ||Y)$ is a better measurement of the similarity (statistical distance) between two probability distributions:\n\n$$\n    \\text{KL}(X ||Y) = \\mathbb{E}_{x \\sim X}[- \\log \\frac{P(Y=x)}{P(X=x)}]\n$$\n\nIt is linked to the cross-entropy by:\n\n$$\n    \\text{KL}(X ||Y) = H(X, Y) - H(X)\n$$\n\nIf the two distributions are the same *almost anywhere*, the KL divergence is zero. If the two distributions are different, the KL divergence is positive. Minimizing the KL between two distributions is the same as making the two distributions \"equal\". But remember: the KL is not a metric, as it is not symmetric.\n\n\n\n```{note}\nRefer \n<https://towardsdatascience.com/entropy-cross-entropy-and-kl-divergence-explained-b09cdae917a> for nice visual explanantions of the cross-entropy."
  },
  {
    "objectID": "notes/1.3-Neurons.html",
    "href": "notes/1.3-Neurons.html",
    "title": "3  Neurons",
    "section": "",
    "text": "Slides: pdf"
  },
  {
    "objectID": "notes/1.3-Neurons.html#biological-neurons",
    "href": "notes/1.3-Neurons.html#biological-neurons",
    "title": "3  Neurons",
    "section": "3.1 Biological neurons",
    "text": "3.1 Biological neurons\n\n\n\n\nThe human brain is composed of 100 billion neurons. A biological neuron is a cell, composed of a cell body (soma), multiple dendrites and an axon. The axon of a neuron can contact the dendrites of another through synapses to transmit information. There are hundreds of different types of neurons, each with different properties.\n\n\n\n```{figure} ../img/biologicalneuron.png\n\n\n\n\nwidth: 100%\n\n\n\nBiological neuron. Source: https://en.wikipedia.org/wiki/Neuron\n\nNeurons are negatively charged: they have a resting potential at around -70 mV. When a neuron receives enough input currents, its **membrane potential** can exceed a threshold and the neuron emits an **action potential** (or **spike**) along its axon. \n\n\n```{figure} ../img/actionpotential.gif\n---\nwidth: 100%\n---\nPropagation of an action potential along the axon. Source: <https://en.wikipedia.org/wiki/Action_potential>\nA spike has a very small duration (1 or 2 ms) and its amplitude is rather constant. It is followed by a refractory period where the neuron is hyperpolarized, limiting the number of spikes per second to 200.\n\n\n\n```{figure} ../img/actionpotential.png\n\n\n\n\nwidth: 50%\n\n\n\nAction potential or spike. Source: https://en.wikipedia.org/wiki/Action_potential\n\nThe action potential arrives at the synapses and releases **neurotransmitters** in the synaptic cleft: glutamate (AMPA, NMDA), GABA, dopamine, serotonin, nicotin, etc...\nNeurotransmitters can enter the receiving neuron through **receptors** and change its potential: the neuron may emit a spike too. Synaptic currents change the membrane potential of the post.synaptic neuron. The change depends on the strength of the synapse called the **synaptic efficiency** or **weight**. Some synapses are stronger than others, and have a larger influence on the post-synaptic cell.\n\n```{figure} ../img/chemicalsynapse.jpg\n---\nwidth: 60%\n---\nNeurotransmitter release at the synapse. Source: <https://en.wikipedia.org/wiki/Neuron>\n\n\n\n\n\nThe two important dimensions of the information exchanged by neurons are:\n\nThe instantaneous frequency or firing rate: number of spikes per second (Hz).\nThe precise timing of the spike trains.\n\n\n\n\n```{figure} ../img/oscillations.png\n\n\n\n\nwidth: 80%\n\n\n\nNeurons emit spikes at varying frequencies (firing rate) and variable timings. Source: https://en.wikipedia.org/wiki/Neural_oscillation\n\nThe shape of the spike (amplitude, duration) does not matter much for synaptic transission: spikes can be considered as binary signals (0 or 1) occuring at precise moments of time.\n\nSome neuron models called **rate-coded models** only represent the firing rate of a neuron and ignore spike timing at all. Other models called **spiking models** represent explicitly the spiking behavior.\n\n\n## Hodgkin-Huxley neurons\n\n<div class='embed-container'><iframe src='https://www.youtube.com/embed/WAfOUZW4rq8' frameborder='0' allowfullscreen></iframe></div>\n\n\nAlan Hodgkin and Andrew Huxley (Nobel prize 1963) were the first to propose a detailed mathematical model of the giant squid neuron. The membrane potential $V$ of the neuron is governed by an electrical circuit, including sodium and potassium channels. The membrane has a **capacitance** $C$ that models the dynamics of the membrane (time constant). The **conductance** $g_L$ allows the membrane potential to relax back to its resting potential $E_L$ in the absence of external currents.  External currents (synaptic inputs) perturb the membrane potential and can bring the neuron to fire an action potential. \n\nTheir neuron model include:\n\n* An ordinary differential equation (ODE) for the membrane potential $v$.\n\n* Three ODEs for $n$, $m$ and $h$ representing potassium channel activation, sodium channel activation, and sodium channel inactivation.\n\n* Several parameters determined experimentally.\n\n$$\n\\begin{aligned}\n    a_n &= 0.01 \\, (v + 60) / (1.0 - \\exp(-0.1\\, (v + 60) ) ) \\\\\n    a_m &= 0.1 \\, (v + 45) / (1.0 - \\exp (- 0.1 \\, ( v + 45 ))) \\\\\n    a_h &= 0.07 \\, \\exp(- 0.05 \\, ( v + 70 )) \\\\\n    b_n &= 0.125 \\, \\exp (- 0.0125 \\, (v + 70)) \\\\\n    b_m &= 4 \\,  \\exp (- (v + 70) / 80) \\\\\n    b_h &= 1/(1 + \\exp (- 0.1 \\, ( v + 40 )) ) \\\\\n    & \\\\\n    \\frac{dn}{dt} &= a_n \\, (1 - n) - b_n \\, n  \\\\\n    \\frac{dm}{dt} &= a_m \\, (1 - m) - b_m \\, m  \\\\\n    \\frac{dh}{dt} &= a_h \\, (1 - h) - b_h \\, h  \\\\\n\\end{aligned}\n$$\n\n$$\n\\begin{aligned}\n    C \\, \\frac{dv}{dt} = g_L \\, (V_L - v) &+ g_K \\, n^4 \\, (V_K - v) \\\\\n        & + g_\\text{Na} \\, m^3 \\, h \\, (V_\\text{Na} - v) + I \\\\\n\\end{aligned}\n$$\n\n\nThese equations allow to describe very precisely how an action potential is created from external currents.\n\n\n```{figure} ../img/hodgkinhuxley-data.png\n---\nwidth: 100%\n---\nAction potential for a Hodgkin-Huxley neuron."
  },
  {
    "objectID": "notes/1.3-Neurons.html#spiking-neurons",
    "href": "notes/1.3-Neurons.html#spiking-neurons",
    "title": "3  Neurons",
    "section": "3.2 Spiking neurons",
    "text": "3.2 Spiking neurons\nAs action potentials are stereotypical, it is a waste of computational resources to model their generation precisely. What actually matters are the sub-threshold dynamics, i.e. what happens before the spike is emitted.\nThe leaky integrate-and-fire (LIF; Lapicque, 1907) neuron integrates its input current and emits a spike if the membrane potential exceeds a threshold.\n\\[\n    C \\, \\frac{dv}{dt} = - g_L \\, (v - V_L) + I\n\\]\n\\[\n    \\text{if} \\; v > V_T \\; \\text{emit a spike and reset.}\n\\]\n\n\n\n```{figure} ../img/LIF-data.png\n\n\n\n\nwidth: 70%\n\n\n\nSpike emission for a LIF neuron.\n\nOther well-known spiking neuron models include:\n\n* Izhikevich quadratic IF {cite}`Izhikevich2003`, using a quadratic function of the membrane potential and an adaptation variable $u$.\n\n$$\n    \\frac{dv}{dt} = 0.04 \\, v^2 + 5 \\, v + 140 - u + I \n$$\n$$\n    \\frac{du}{dt} = a \\, (b \\, v - u)\n$$\n\n* Adaptive exponential IF (AdEx, {cite}`Brette2005`), using an exponential function.\n\n$$\n\\begin{aligned}\n    C \\, \\frac{dv}{dt} = -g_L \\ (v - E_L) + & g_L \\, \\Delta_T \\, \\exp(\\frac{v - v_T}{\\Delta_T}) \\\\\n                                            & + I - w\n\\end{aligned}\n$$\n$$\n    \\tau_w \\, \\frac{dw}{dt} = a \\, (v - E_L) - w\n$$\n\n```{figure} ../img/LIF-Izhi-AdEx.png\n---\nwidth: 100%\n---\nDifferent subthreshold dynamics between the LIF, Izhikevich and AdEx neuron models.\nContrary to the simple LIF model, these realistic neuron models can reproduce a variety of dynamics, as biological neurons do not all respond the same to an input current. Some fire regularly, some slow down with time, while others emit bursts of spikes. Modern spiking neuron models allow to recreate these variety of dynamics by changing a few parameters.\n\n\n\n```{figure} ../img/adex.png\n\n\n\n\nwidth: 100%\n\n\n\nDifferent parameters of the AdEx neuron model produce different spiking patterns.\n\n\n## Rate-coded neurons\n\n<div class='embed-container'><iframe src='https://www.youtube.com/embed/AFzYj1VUnCg' frameborder='0' allowfullscreen></iframe></div>\n\n\nAt the population level, interconnected networks of spiking neurons tend to fire synchronously (code redundancy). What if the important information was not the precise spike timings, but the **firing rate** of a small population? The instantaneous firing rate is defined in Hz (number of spikes per second). It can be estimated by an histogram of the spikes emitted by a network of similar neurons, or by repeating the same experiment multiple times for a single neuron. One can also build neural models that directly model the **firing rate** of (a population of) neuron(s): the **rate-coded** neuron.\n\n```{figure} ../img/ratecoded-izhikevich.png\n---\nwidth: 60%\n---\nThe spiking pattern (raster plot) of a population of interconnected neurons can be approximated by its mean firing rate.\nA rate-coded neuron is represented by two time-dependent variables:\n\nThe “membrane potential” \\(v(t)\\) which evolves over time using an ODE.\n\n\\[\n    \\tau \\, \\frac{d v(t)}{dt} + v(t) = \\sum_{i=1}^d w_{i, j} \\, r_i(t) + b\n\\]\n\nThe firing rate \\(r(t)\\) which transforms the membrane potential into a single continuous value using a transfer function or activation function.\n\n\\[\n    r(t) = f(v(t))\n\\]\n\n\n\n```{figure} ../img/ratecoded-neuron.svg\n\n\n\n\nwidth: 70%\n\n\n\nRate-coded neuron.\n\nThe membrane potential uses a weighted sum of inputs (the firing rates $r_i(t)$ of other neurons) by multiplying each rate with a **weight** $w_i$ and adds a constant value $b$ (the **bias**). The activation function can be any non-linear function, usually making sure that the firing rate is positive.\n\n```{figure} ../img/ratecoded-simple.png\n---\nwidth: 70%\n---\nFiring rate of a rate-coded neuron for a step input.\nRemarks on ODEs\nLet’s consider a simple rate-coded neuron taking a step signal \\(I(t)\\) as input:\n\\[\n    \\tau \\, \\frac{d v(t)}{dt} + v(t) = I(t)\n\\]\n\\[\n    r(t) = (v(t))^+\n\\]\nThe “speed” of \\(v(t)\\) is given by its temporal derivative:\n\\[\n    \\frac{d v(t)}{dt} = \\frac{I(t) - v(t)}{\\tau}\n\\]\nWhen \\(v(t)\\) is quite different from \\(I(t)\\), the membrane potential “accelerates” to reduce the difference. When \\(v(t)\\) is similar to \\(I(t)\\), the membrane potential stays constant.\n\n\n\n```{figure} ../img/ratecoded-simple-multiple.png\n\n\n\n\nwidth: 70%\n\n\n\nThe time constant \\(\\tau\\) of a rate-coded neuron influences the speed at which it reacts to inputs.\n\nThe membrane potential follows an exponential function which tries to \"match\" its input with a speed determined by the **time constant** $\\tau$. The time constant $\\tau$ determines how fast the rate-coded neuron matches its inputs. Biological neurons have time constants between 5 and 30 ms depending on the cell type.\n\nThere exists a significant number of transfer functions that can be used:\n\n```{figure} ../img/ratecoded-transferfunctions.png\n---\nwidth: 70%\n---\nTypical transfer functions used in neural networks include the rectifier (ReLU), piece-wise linear, sigmoid (or logistic) and tanh functions..\nWhen using the rectifier activation function (ReLU):\n\\[\n    f(x) = \\max(0, x)\n\\]\nthe membrane potential \\(v(t)\\) can take any value, but the firing rate \\(r(t)\\) is only positive.\n\n\n\n```{figure} ../img/ratecoded-simple2.png\n\n\n\n\nwidth: 80%\n\n\n\nThe rectifier function only keeps the positive part of the membrane potential.\n\nWhen using the logistic (or sigmoid) activation function:\n\n$$\n    f(x) = \\frac{1}{1 + \\exp(-x)}\n$$\n\nthe firing rate $r(t)$ is bounded between 0 and 1, but responds for negative membrane potentials.\n\n```{figure} ../img/ratecoded-simple3.png\n---\nwidth: 80%\n---\nThe sigmoid/logistic function bounds the firing rate between 0 and 1, even if the membrane potential is negative."
  },
  {
    "objectID": "notes/1.3-Neurons.html#artificial-neurons",
    "href": "notes/1.3-Neurons.html#artificial-neurons",
    "title": "3  Neurons",
    "section": "3.3 Artificial neurons",
    "text": "3.3 Artificial neurons\n\n\n\n\nBy omitting the dynamics of the rate-coded neuron, one obtains the very simple artificial neuron (McCulloch and Pitts, 1943):\n\n\n\n```{figure} ../img/artificialneuron.svg\n\n\n\n\nwidth: 70%\n\n\n\nArtificial neuron.\n\nAn artificial neuron sums its inputs $x_1, \\ldots, x_d$ by multiplying them with weights $w_1, \\ldots, w_d$, adds a bias $b$ and transforms the result into an output $y$ using an activation function $f$.\n\n$$\n    y = f( \\sum_{i=1}^d w_i \\, x_i + b)\n$$\n\n\nThe output $y$ directly reflects the input, without temporal integration. The weighted sum of inputs + bias $\\sum_{i=1}^d w_i \\, x_i + b$ is called the **net activation**. \n\nThis overly simplified neuron model is the basic unit of the **artificial neural networks** (ANN) used in machine learning / deep learning.\n\n**Artificial neurons and hyperplanes**\n\nLet's consider an artificial neuron with only two inputs $x_1$ and $x_2$.\n\nThe net activation $w_1 \\, x_1 + w_2 \\, x_2 + b$ is the equation of a line in the space $(x_1, x_2)$. \n\n$$\n    w_1 \\, x_1 + w_2 \\, x_2 + b = 0 \\Leftrightarrow x_2 = - \\frac{w_1}{w_2} \\, x_1 - \\frac{b}{w_2}\n$$\n\n\n\n```{figure} ../img/artificialneuron-simple.png\n---\nwidth: 50%\n---\nThe net activation represents an hyperplane in 2D.\nThe net activation is a line in 2D, a plane in 3D, etc. Generally, the net activation describes an hyperplane in the input space with \\(d\\) dimensions \\((x_1, x_2, \\ldots, x_d)\\). An hyperplane has one dimension less than the space.\n\n\n\n```{figure} ../img/hyperplane.gif\n\n\n\n\nwidth: 50%\n\n\n\nHyperplane in 3D. Source: https://newvitruvian.com/explore/vector-planes/#gal_post_7186_nonzero-vector.gif\n\n\nWe can write the net activation using a **weight vector** $\\mathbf{w}$ and a **bias** $b$:\n\n$$\n    \\sum_{i=1}^d w_i \\, x_i + b  = \\langle\\mathbf{w} \\cdot \\mathbf{x} \\rangle + b\n$$\n\nwith:\n\n$$\n    \\mathbf{w} = \\begin{bmatrix} w_1 \\\\ w_2 \\\\ \\ldots \\\\ w_d \\end{bmatrix} \\qquad \\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\ldots \\\\ x_d \\end{bmatrix}\n$$\n\n$\\langle \\cdot \\rangle$ is the **dot product** (aka inner product, scalar product) between the **input vector** $\\mathbf{x}$ and the weight vector $\\mathbf{w}$.\n\nThe weight vector is orthogonal to the hyperplane $(\\mathbf{w}, b)$ and defines its orientation. $b$ is the \"distance\" between the hyperplane and the origin. The hyperplane separates the input space into two parts:\n\n* $\\langle\\mathbf{w} \\cdot \\mathbf{x} \\rangle + b > 0$ for all points $\\mathbf{x}$ **above** the hyperplane.\n\n* $\\langle\\mathbf{w} \\cdot \\mathbf{x} \\rangle + b < 0$ for all points $\\mathbf{x}$ **below** the hyperplane.\n\nBy looking at the **sign** of the net activation, we can separate the input space into two classes. This will be the main principle of **linear classification**.\n\n\n```{figure} ../img/projection.svg\n---\nwidth: 80%\n---\nThe sign of the projection of an input $\\mathbf{x}$ on the hyperplane tells whether the input is above or below the hyperplane."
  },
  {
    "objectID": "notes/2.1-Optimization.html",
    "href": "notes/2.1-Optimization.html",
    "title": "4  Optimization",
    "section": "",
    "text": "Slides: pdf"
  },
  {
    "objectID": "notes/2.1-Optimization.html#analytic-optimization",
    "href": "notes/2.1-Optimization.html#analytic-optimization",
    "title": "4  Optimization",
    "section": "4.1 Analytic optimization",
    "text": "4.1 Analytic optimization\n\n\n\n\nMachine learning is all about optimization:\n\nSupervised learning minimizes the error between the prediction and the data.\nUnsupervised learning maximizes the fit between the model and the data\nReinforcement learning maximizes the collection of rewards.\n\nThe function to be optimized is called the objective function, cost function or loss function. ML searches for the value of free parameters which optimize the objective function on the data set. The simplest optimization method is the gradient descent (or ascent) method.\nThe easiest method to find the optima of a function \\(f(x)\\) is to look where its first-order derivative is equal to 0:\n\\[\n    x^* = \\min_x f(x) \\Leftrightarrow f'(x^*) = 0 \\; \\text{and} \\; f''(x^*) > 0\n\\]\n\\[\n    x^* = \\max_x f(x) \\Leftrightarrow f'(x^*) = 0 \\; \\text{and} \\; f''(x^*) < 0\n\\]\nThe sign of the second order derivative tells us whether it is a maximum or minimum. There can be multiple minima or maxima (or none) depending on the function. The “best” minimum (with the lowest value among all minima) is called the global minimum. The others are called local minima.\n\n\n\n```{figure} ../img/localminimum.png\n\n\n\n\nwidth: 60%\n\n\n\nFunctions (may) have one global minimum but several local minima.\n\n\n**Multivariate functions**\n\nA multivariate function is a function of more than one variable, e.g.  $f(x, y)$. A point $(x^*, y^*)$ is an optimum of $f$ if all partial derivatives are zero:\n\n$$\n    \\begin{cases}\n        \\dfrac{\\partial f(x^*, y^*)}{\\partial x} = 0 \\\\\n        \\dfrac{\\partial f(x^*, y^*)}{\\partial y} = 0 \\\\\n    \\end{cases}\n$$\n\nThe vector of partial derivatives is called the **gradient of the function**:\n\n$$\n    \\nabla_{x, y} \\, f(x^*, y^*) = \\begin{bmatrix} \\dfrac{\\partial f(x^*, y^*)}{\\partial x} \\\\ \\dfrac{\\partial f(x^*, y^*)}{\\partial y} \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\n$$\n\n```{figure} ../img/optimization-example-multivariate.png\n---\nwidth: 80%\n---\nMultivariate optimization of $f(x, y) = (x - 1)^2 + y^2 + 1$. The minimum is in $(1, 0)$."
  },
  {
    "objectID": "notes/2.1-Optimization.html#gradient-descent",
    "href": "notes/2.1-Optimization.html#gradient-descent",
    "title": "4  Optimization",
    "section": "4.2 Gradient descent",
    "text": "4.2 Gradient descent\n\n\n\n\nIn machine learning, we generally do not have access to the analytical form of the objective function. We can not therefore get its derivative and search where it is 0. However, we have access to its value (and derivative) for certain values, for example:\n\\[\n    f(0, 1) = 2 \\qquad f'(0, 1) = -1.5\n\\]\nWe can “ask” the model for as many values as we want, but we never get its analytical form. For most useful problems, the function would be too complex to differentiate anyway.\n\n\n\n```{figure} ../img/derivative-approx.png\n\n\n\n\nwidth: 60%\n\n\n\nEuler method: the derivative of a function is the slope of its tangent.\n\nLet's remember the definition of the derivative of a function. The derivative $f'(x)$ is defined by the slope of the tangent of the function:\n\n$$\n    f'(x) = \\lim_{h \\to 0} \\frac{f(x + h) - f(x)}{x + h - x} = \\lim_{h \\to 0} \\frac{f(x + h) - f(x)}{h}\n$$\n\nIf we take $h$ small enough, we have the following approximation:\n\n$$\n    f(x + h) - f(x) \\approx h \\, f'(x)\n$$\n\nIf we want $x+h$ to be closer to the minimum than $x$, we want:\n\n$$\n    f(x + h) < f(x)\n$$\n\nor:\n\n$$\n    f(x + h) - f(x) < 0\n$$\n\nWe therefore want that:\n\n$$\n    h \\, f'(x) < 0\n$$\n\nThe **change** $h$ in the value of $x$ must have the opposite sign of $f'(x)$ in order to get closer to the minimum. If the function is increasing in $x$, the minimum is smaller (to the left) than $x$. If the function is decreasing in $x$, the minimum is bigger than $x$ (to the right).\n\n\n**Gradient descent** (GD) is a first-order method to iteratively find the minimum of a function $f(x)$. It starts with a random estimate $x_0$ and iteratively changes its value so that it becomes closer to the minimum.\n\n```{figure} ../img/gradient.png\n---\nwidth: 70%\n---\nGradient descent iteratively modifies the estimate $x_n$ in the opposite direction of the derivative.\nIt creates a series of estimates \\([x_0, x_1, x_2, \\ldots]\\) that converge to a local minimum of \\(f\\). Each element of the series is calculated based on the previous element and the derivative of the function in that element:\n\\[\n    x_{n+1} = x_n + \\Delta x =  x_n - \\eta \\, f'(x_n)\n\\]\nIf the function is locally increasing (resp. decreasing), the new estimate should be smaller (resp. bigger) than the previous one. \\(\\eta\\) is a small parameter between 0 and 1 called the learning rate that controls the speed of convergence (more on that later).\nGradient descent algorithm:\n\nWe start with an initially wrong estimate of \\(x\\): \\(x_0\\)\nfor \\(n \\in [0, \\infty]\\):\n\nWe compute or estimate the derivative of the loss function in \\(x_{n}\\): \\(f'(x_{n})\\)\nWe compute a new value \\(x_{n+1}\\) for the estimate using the gradient descent update rule:\n\n\\[\n      \\Delta x = x_{n+1} - x_n =  - \\eta \\, f'(x_n)\n  \\]\n\nThere is theoretically no end to the GD algorithm: we iterate forever and always get closer to the minimum. The algorithm can be stopped when the change \\(\\Delta x\\) is below a threshold.\n\n\n\n```{figure} ../img/gradient-descent-animation.gif\n\n\n\n\nwidth: 80%\n\n\n\nVisualization of Gradient Descent on a quadratic function. Notice how the speed of convergence slows down when approaching the minimum.\n\n\nGradient descent can be applied to multivariate functions:\n\n$$\n    \\min_{x, y, z} \\qquad f(x, y, z)\n$$\n\nEach variable is updated independently using partial derivatives:\n\n$$\n    \\Delta x = x_{n+1} - x_{n} = - \\eta \\, \\frac{\\partial f(x_n, y_n, z_n)}{\\partial x}\n$$\n$$\n    \\Delta y = y_{n+1} - y_{n} = - \\eta \\, \\frac{\\partial f(x_n, y_n, z_n)}{\\partial y}\n$$\n$$\n    \\Delta z = z_{n+1} - z_{n} = - \\eta \\, \\frac{\\partial f(x_n, y_n, z_n)}{\\partial z}\n$$\n\nWe can also use the vector notation to use the **gradient operator**:\n\n$$\n    \\mathbf{x}_n = \\begin{bmatrix} x_n \\\\ y_n \\\\ z_n \\end{bmatrix} \\quad \\text{and} \\quad \\nabla_\\mathbf{x} f(\\mathbf{x}) = \\begin{bmatrix} \\frac{\\partial f(x, y, z)}{\\partial x} \\\\ \\frac{\\partial f(x, y, z)}{\\partial y} \\\\ \\frac{\\partial f(x, y, z)}{\\partial z} \\end{bmatrix}\n    \\qquad \\rightarrow \\qquad \\Delta \\mathbf{x} = - \\eta \\, \\nabla_\\mathbf{x} f(\\mathbf{x}_n)\n$$\n\nThe change in the estimation is in the **opposite direction of the gradient**, hence the name **gradient descent**.\n\n\n```{figure} ../img/gradient-descent-animation-multivariate.gif\n---\nwidth: 100%\n---\nVisualization of Gradient Descent on a multivariate function in 2 dimensions.\nThe choice of the learning rate \\(\\eta\\) is critical:\n\nIf it is too small, the algorithm will need a lot of iterations to converge.\nIf it is too big, the algorithm can oscillate around the desired values without ever converging.\n\n\n\n\n```{figure} ../img/gradient-descent-learningrate.gif\n\n\n\n\nwidth: 100%\n\n\n\nInfluence of the learning on convergence: too small (red) and it takes forever, too high (green) and convergence is unstable. Finding its optimal value (blue) is hard as it depends on the function itself.\n\nGradient descent is not optimal: it always finds a local minimum, but there is no guarantee that it is the global minimum. The found solution depends on the initial choice of $x_0$. If you initialize the parameters near to the global minimum, you are lucky. But how? This will be a big issue in neural networks.\n\n\n## Regularization\n\n<div class='embed-container'><iframe src='https://www.youtube.com/embed/LI5ExC4d9Js' frameborder='0' allowfullscreen></iframe></div>\n\n\n### L2 - Regularization\n\nMost of the time, there are many minima to a function, if not an infinity. As GD only converges to the \"closest\" local minimum, you are never sure that you get a good solution. Consider the following function:\n\n$$\n    f(x, y) = (x -1)^2\n$$\n\nAs it does not depend on $y$, whatever initial value $y_0$ will be considered as a solution. As we will see later, this is something we do not want.\n\n\n```{figure} ../img/gradient-descent-animation-regularization1.gif\n---\nwidth: 100%\n---\nFunction with an infinity of minima: as long as $x=1$, each point on the vertical line is a minimum.\nTo obtain a single solution, we may want to put the additional constraint that both \\(x\\) and \\(y\\) should be as small as possible. One possibility is to also minimize the Euclidian norm (or L2-norm) of the vector \\(\\mathbf{x} = [x, y]\\).\n\\[\n    \\min_{x, y} ||\\mathbf{x}||^2 = x^2 + y^2\n\\]\nNote that this objective is in contradiction with the original objective: \\((0, 0)\\) minimizes the norm, but not the function \\(f(x, y)\\). We construct a new function as the sum of \\(f(x, y)\\) and the norm of \\(\\mathbf{x}\\), weighted by the regularization parameter \\(\\lambda\\):\n\\[\n    \\mathcal{L}(x, y) = f(x, y) + \\lambda \\, (x^2 + y^2)\n\\]\nFor a fixed value of \\(\\lambda\\) (for example 0.1), we now minimize using gradient descent this new loss function. To do that, we just need to compute its gradient:\n\\[\n    \\nabla_{x, y} \\, \\mathcal{L}(x, y) = \\begin{bmatrix} \\frac{\\partial f(x, y)}{\\partial x} + 2\\, \\lambda \\, x \\\\ \\frac{\\partial f(x, y)}{\\partial y} + 2\\, \\lambda \\, y \\end{bmatrix}\n\\]\nand apply gradient descent iteratively:\n\\[\n    \\Delta \\begin{bmatrix} x \\\\ y \\end{bmatrix} = - \\eta \\, \\nabla_{x, y} \\, \\mathcal{L}(x, y) = - \\eta \\, \\begin{bmatrix} \\frac{\\partial f(x, y)}{\\partial x} + 2\\, \\lambda \\, x \\\\ \\frac{\\partial f(x, y)}{\\partial y} + 2\\, \\lambda \\, y \\end{bmatrix}\n\\]\n\n\n\n```{figure} ../img/gradient-descent-animation-regularization2.gif\n\n\n\n\nwidth: 100%\n\n\n\nGradient descent with L2 regularization, using \\(\\lambda = 0.1\\).\n\nYou may notice that the result of the optimization is a bit off, it is not exactly $(1, 0)$. This is because we do not optimize $f(x, y)$ directly, but $\\mathcal{L}(x, y)$. Let's have a look at the landscape of the loss function:\n\n```{figure} ../img/gradient-descent-animation-regularization3.gif\n---\nwidth: 100%\n---\nLandscape of the loss function $\\mathcal{L}(x, y) = f(x, y) + \\lambda \\, (x^2 + y^2)$ with $\\lambda = 0.1$.\nThe optimization with GD indeed works, it is just that the function is different. The constraint on the Euclidian norm “attracts” or “distorts” the function towards \\((0, 0)\\). This may seem counter-intuitive, but we will see with deep networks that we can live with it. Let’s now look at what happens when we increase \\(\\lambda\\) to 5:\n\n\n\n```{figure} ../img/gradient-descent-animation-regularization4.gif\n\n\n\n\nwidth: 100%\n\n\n\nGradient descent with L2 regularization, using \\(\\lambda = 5\\).\n\n```{figure} ../img/gradient-descent-animation-regularization5.gif\n---\nwidth: 100%\n---\nLandscape of the loss function $\\mathcal{L}(x, y) = f(x, y) + \\lambda \\, (x^2 + y^2)$ with $\\lambda = 5$.\nNow the result of the optimization is totally wrong: the constraint on the norm completely dominates the optimization process.\n\\[\n    \\mathcal{L}(x, y) = f(x, y) + \\lambda \\, (x^2 + y^2)\n\\]\n\\(\\lambda\\) controls which of the two objectives, \\(f(x, y)\\) or \\(x^2 + y^2\\), has the priority:\n\nWhen \\(\\lambda\\) is small, \\(f(x, y)\\) dominates and the norm of \\(\\mathbf{x}\\) can be anything.\nWhen \\(\\lambda\\) is big, \\(x^2 + y^2\\) dominates, the result will be very small but \\(f(x, y)\\) will have any value.\n\nThe right value for \\(\\lambda\\) is hard to find. We will see later methods to experimentally find its most adequate value.\nRegularization is a form of **constrained optimization**. What we actually want to solve is the constrained optimization problem:\n\n$$\n    \\min_{x, y} \\qquad f(x, y) \\\\\n    \\text{so that} \\qquad x^2 + y^2 < \\delta\n$$\n\ni.e. minimize $f(x, y)$ while keeping the norm of $[x, y]$ below a threshold $\\delta$. **Lagrange optimization** (technically KKT optimization; see the course Introduction to AI) allows to solve that problem by searching the minimum of the generalized Lagrange function:\n\n$$\n    \\mathcal{L}(x, y, \\lambda) = f(x, y) + \\lambda \\, (x^2 + y^2 - \\delta)\n$$\n\nRegularization is a special case of Lagrange optimization, as it considers $\\lambda$ to be fixed, while it is an additional variable in Lagrange optimization. When differentiating this function, $\\delta$ disappears anyway, so it is equivalent to our regularized loss function.\n\n4.2.1 L1 - Regularization\nAnother form of regularization is L1 - regularization using the L1-norm (absolute values):\n\\[\n    \\mathcal{L}(x, y) = f(x, y) + \\lambda \\, (|x| + |y|)\n\\]\nIts gradient only depend on the sign of \\(x\\) and \\(y\\):\n\\[\n    \\nabla_{x, y} \\, \\mathcal{L}(x, y) = \\begin{bmatrix} \\frac{\\partial f(x, y)}{\\partial x} + \\lambda \\, \\text{sign}(x) \\\\ \\frac{\\partial f(x, y)}{\\partial y} + \\lambda \\, \\text{sign}(y) \\end{bmatrix}\n\\]\nIt tends to lead to sparser value of \\((x, y)\\), i.e. either \\(x\\) or \\(y\\) will be close or equal to 0.\n\n\n\n```{figure} ../img/gradient-descent-animation-regularization6.gif\n\n\n\n\nwidth: 100%\n\n\n\nGradient descent with L1 regularization, using \\(\\lambda = 0.1\\). ```\nBoth L1 and L2 regularization can be used in neural networks depending on the desired effect."
  },
  {
    "objectID": "notes/2.2-LinearRegression.html",
    "href": "notes/2.2-LinearRegression.html",
    "title": "5  Linear regression",
    "section": "",
    "text": "Slides: pdf"
  },
  {
    "objectID": "notes/2.2-LinearRegression.html#linear-regression",
    "href": "notes/2.2-LinearRegression.html#linear-regression",
    "title": "5  Linear regression",
    "section": "5.1 Linear regression",
    "text": "5.1 Linear regression\n\n\n\n\n\n\n\n```{figure} ../img/regression-animation2.png\n\n\n\n\nwidth: 70%\n\n\n\nSimple linear regression. \\(x\\) is the input, \\(y\\) the output. The data is represented by blue dots, the model by the black line.\n\nLet's consider a training set of N examples $\\mathcal{D} = (x_i, t_i)_{i=1..N}$. In **linear regression**, we want to learn a linear model (hypothesis) $y$ that is linearly dependent on the input $x$:\n\n$$\n    y = f_{w, b}(x) = w \\, x + b\n$$\n\nThe **free parameters** of the model are the slope $w$ and the intercept $b$. This model corresponds to a single **artificial neuron** with output $y$, having one input $x$, one weight $w$, one bias $b$ and a **linear** activation function $f(x) = x$.\n\n\n```{figure} ../img/artificialneuron.svg\n---\nwidth: 60%\n---\nArtificial neuron with multiple inputs.\nThe goal of the linear regression (or least mean squares - LMS) is to minimize the mean square error (mse) between the targets and the predictions. This loss function is defined as the mathematical expectation of the quadratic error over the training set:\n\\[\n    \\mathcal{L}(w, b) =  \\mathbb{E}_{x_i, t_i \\in \\mathcal{D}} [ (t_i - y_i )^2 ]\n\\]\nAs the training set is finite and the samples i.i.d, we can simply replace the expectation by an average over the training set:\n\\[\n    \\mathcal{L}(w, b) = \\frac{1}{N} \\, \\sum_{i=1}^{N} (t_i - y_i )^2\n\\]\nThe minimum of the mse is achieved when the prediction \\(y_i = f_{w, b}(x_i)\\) is equal to the true value \\(t_i\\) for all training examples. In other words, we want to minimize the residual error of the model on the data. It is not always possible to obtain the global minimum (0) as the data may be noisy, but the closer, the better.\n\n\n\n```{figure} ../img/regression-animation-mse-dual.png\n\n\n\n\nwidth: 100%\n\n\n\nA good fit to the data is when the prediction \\(y_i\\) (on the line) is close to the data \\(t_i\\) for all training examples.\n\n### Least Mean Squares\n\nWe search for $w$ and $b$ which minimize the mean square error:\n\n$$\n    \\mathcal{L}(w, b) = \\frac{1}{N} \\, \\sum_{i=1}^{N} (t_i - y_i )^2\n$$\n\nWe will apply **gradient descent** to iteratively modify estimates of $w$ and $b$:\n\n$$\n    \\Delta w = - \\eta \\, \\frac{\\partial \\mathcal{L}(w, b)}{\\partial w}\n$$\n$$\n    \\Delta b = - \\eta \\, \\frac{\\partial \\mathcal{L}(w, b)}{\\partial b}\n$$\n\nLet's search for the partial derivative of the mean square error with respect to $w$:\n\n$$\n    \\frac{\\partial \\mathcal{L}(w, b)}{\\partial w} = \\frac{\\partial}{\\partial w} [\\frac{1}{N} \\, \\sum_{i=1}^{N} (t_i - y_i )^2]\n$$\n\nPartial derivatives are linear, so the derivative of a sum is the sum of the derivatives:\n\n$$\n    \\frac{\\partial \\mathcal{L}(w, b)}{\\partial w} = \\frac{1}{N} \\, \\sum_{i=1}^{N} \\frac{\\partial}{\\partial w} (t_i - y_i )^2\n$$\n\nThis means we can compute a gradient for each training example instead of for the whole training set (see later the distinction batch/online):\n\n$$\n    \\frac{\\partial \\mathcal{L}(w, b)}{\\partial w} = \\frac{1}{N} \\, \\sum_{i=1}^{N} \\frac{\\partial}{\\partial w} \\mathcal{l}_i(w, b)\n    \\qquad \\text{with} \\qquad \\mathcal{l}_i(w, b) = (t_i - y_i )^2\n$$\n\nThe individual loss $\\mathcal{l}_i(w, b) = (t_i - y_i )^2$ is the composition of two functions:\n\n* a square error function $g_i(y_i) = (t_i - y_i)^2$.\n\n* the prediction $y_i = f_{w, b}(x_i) = w \\, x_i + b$.\n\nThe **chain rule** tells us how to derive such composite functions:\n\n$$\n    \\frac{ d f(g(x))}{dx} = \\frac{ d f(g(x))}{d g(x)} \\times \\frac{ d g(x)}{dx} = \\frac{ d f(y)}{dy} \\times \\frac{ d g(x)}{dx}\n$$\n\nThe first derivative considers $g(x)$ to be a single variable. Applied to our problem, this gives:\n\n$$\n     \\frac{\\partial}{\\partial w} \\mathcal{l}_i(w, b) =  \\frac{\\partial g_i(y_i)}{\\partial y_i} \\times  \\frac{\\partial y_i}{\\partial w}\n$$\n\nThe square error function $g_i(y) = (t_i - y)^2$ is easy to differentiate w.r.t $y$:\n\n$$\n    \\frac{\\partial g_i(y_i)}{\\partial y_i} = - 2 \\, (t_i - y_i)\n$$\n\nThe prediction $y_i = w \\, x_i + b$ also w.r.t $w$ and $b$:\n\n$$\n   \\frac{\\partial  y_i}{\\partial w} = x_i\n$$\n\n$$\n   \\frac{\\partial  y_i}{\\partial b} = 1\n$$\n\nThe partial derivative of the individual loss is:\n\n$$\n    \\frac{\\partial \\mathcal{l}_i(w, b)}{\\partial w} = - 2 \\, (t_i - y_i) \\, x_i\n$$\n\n$$\n    \\frac{\\partial \\mathcal{l}_i(w, b)}{\\partial b} = - 2 \\, (t_i - y_i)\n$$\n\nThis gives us:\n\n$$\n    \\frac{\\partial \\mathcal{L}(w, b)}{\\partial w} = - \\frac{2}{N} \\sum_{i=1}^{N} (t_i - y_i) \\, x_i\n$$\n\n$$\n    \\frac{\\partial \\mathcal{L}(w, b)}{\\partial b} = - \\frac{2}{N} \\sum_{i=1}^{N} (t_i - y_i)\n$$\n\nGradient descent is then defined by the learning rules (absorbing the 2 in $\\eta$):\n\n$$\n    \\Delta w = \\eta \\, \\frac{1}{N} \\sum_{i=1}^{N} (t_i - y_i) \\, x_i\n$$\n\n$$\n    \\Delta b = \\eta \\, \\frac{1}{N} \\sum_{i=1}^{N} (t_i - y_i)\n$$\n\n\n**Least Mean Squares** (LMS) or Ordinary Least Squares (OLS) is a **batch** algorithm: the parameter changes are computed over the whole dataset.\n\n$$\n    \\Delta w = \\eta \\, \\frac{1}{N} \\sum_{i=1}^{N} (t_i - y_i) \\, x_i\n$$\n$$\n    \\Delta b = \\eta \\, \\frac{1}{N} \\sum_{i=1}^{N} (t_i - y_i)\n$$\n\nThe parameter changes have to be applied multiple times (**epochs**) in order for the parameters to converge. One can stop when the parameters do not change much, or after a fixed number of epochs.\n\n\n```{admonition} LMS algorithm\n\n* $w=0 \\quad;\\quad b=0$\n\n* **for** M epochs:\n\n    * $dw=0 \\quad;\\quad db=0$\n\n    * **for** each sample $(x_i, t_i)$:\n\n        * $y_i = w \\, x_i + b$\n\n        * $dw = dw + (t_i - y_i) \\, x_i$\n\n        * $db = db + (t_i - y_i)$\n\n    * $\\Delta w = \\eta \\, \\frac{1}{N} dw$\n\n    * $\\Delta b = \\eta \\, \\frac{1}{N} db$\n\n\n\n```{figure} ../img/regression-animation.gif\n\n\n\n\nwidth: 70%\n\n\n\nVisualization of least mean squares applied to a simple regression problem with \\(\\eta=0.1\\). Each step of the animation corresponds to one epoch (iteration over the training set).\n\nDuring learning, the **mean square error** (mse) decreases with the number of epochs but does not reach zero because of the noise in the data.\n\n\n```{figure} ../img/regression-animation-loss.png\n---\nwidth: 70%\n---\nEvolution of the loss function during training.\n\n5.1.1 Delta learning rule\nLMS is very slow, because it changes the weights only after the whole training set has been evaluated. It is also possible to update the weights immediately after each example using the delta learning rule, which is the online version of LMS:\n\\[\\Delta w = \\eta \\, (t_i - y_i) \\, x_i\\]\n\\[\\Delta b = \\eta \\, (t_i - y_i)\\]\n```{admonition} Delta learning rule\n\n\\(w=0 \\quad;\\quad b=0\\)\nfor M epochs:\n\nfor each sample \\((x_i, t_i)\\):\n\n\\(y_i = w \\, x_i + b\\)\n\\(\\Delta w = \\eta \\, (t_i - y_i ) \\, x_i\\)\n\\(\\Delta b = \\eta \\, (t_i - y_i)\\) ```\n\n\n\nThe batch version is more stable, but the online version is faster: the weights have already learned something when arriving at the end of the first epoch. Note that the loss function is slightly higher at the end of learning (see Exercise 3 for a deeper discussion).\n\n\n\n```{figure} ../img/regression-animation-online.gif\n\n\n\n\nwidth: 70%\n\n\n\nVisualization of the delta learning rule applied to a simple regression problem with \\(\\eta = 0.1\\). Each step of the animation corresponds to one epoch (iteration over the training set).\n\n```{figure} ../img/regression-animation-online-loss.png\n---\nwidth: 70%\n---\nEvolution of the loss function during training. With the same learning rate, the delta learning rule converges much faster but reaches a poorer minimum. Lowering the learning rate slows down learning but reaches a better minimum."
  },
  {
    "objectID": "notes/2.2-LinearRegression.html#multiple-linear-regression",
    "href": "notes/2.2-LinearRegression.html#multiple-linear-regression",
    "title": "5  Linear regression",
    "section": "5.2 Multiple linear regression",
    "text": "5.2 Multiple linear regression\n\n\n\n\nThe key idea of linear regression (one input \\(x\\), one output \\(y\\)) can be generalized to multiple inputs and outputs.\nMultiple Linear Regression (MLR) predicts several output variables based on several explanatory variables:\n\\[\n\\begin{cases}\ny_1 = w_1 \\, x_1 + w_2 \\, x_2 + b_1\\\\\n\\\\\ny_2 = w_3 \\, x_1 + w_3 \\, x_2 + b_2\\\\\n\\end{cases}\n\\]\n`````{admonition} Example: fuel consumption and CO2 emissions\nLet’s suppose you have 13971 measurements in some Excel file, linking engine size, number of cylinders, fuel consumption and CO2 emissions of various cars. You want to predict fuel consumption and CO2 emissions when you know the engine size and the number of cylinders.\n:header-rows: 1\n:name: example-table\n\n* - Engine size\n  - Cylinders\n  - Fuel consumption\n  - CO2 emissions\n* - 2\n  - 4\n  - 8.5\n  - 196\n* - 2.4\n  - 4\n  - 9.6\n  - 221\n* - 1.5\n  - 4\n  - 5.9\n  - 136\n* - 3.5\n  - 6\n  - 11\n  - 255\n* - ...\n  - ...\n  - ...\n  - ...\n\n\n\n```{figure} ../img/MLR-example-data.png\n\n\n\n\nwidth: 90%\n\n\n\nCO2 emissions and fuel consumption depend almost linearly on the engine size and number of cylinders.\n\n```{figure} ../img/MLR-example-data-3d.png\n---\nwidth: 100%\n---\nCO2 emissions and fuel consumption depend almost linearly on the engine size and number of cylinders.\nWe can notice that the output variables seem to linearly depend on the inputs. Noting the input variables \\(x_1\\), \\(x_2\\) and the output ones \\(y_1\\), \\(y_2\\), we can define our problem as a multiple linear regression:\n\\[\n\\begin{cases}\ny_1 = w_1 \\, x_1 + w_2 \\, x_2 + b_1\\\\\n\\\\\ny_2 = w_3 \\, x_1 + w_3 \\, x_2 + b_2\\\\\n\\end{cases}\n\\]\nand solve it using the least mean squares method by minimizing the mse between the model and the data.\n\n\n\n```{figure} ../img/MLR-example-fit-3d.png\n\n\n\n\nwidth: 100%\n\n\n\nThe result of MLR is a plane in the input space.\n\n````{note}\nUsing the Python library `scikit-learn` (<https://scikit-learn.org>), this is done in two lines of code:\n\n```python\nfrom sklearn.linear_model import LinearRegression\nreg = LinearRegression().fit(X, y)\n\nThe system of equations:\n\\[\n\\begin{cases}\ny_1 = w_1 \\, x_1 + w_2 \\, x_2 + b_1\\\\\n\\\\\ny_2 = w_3 \\, x_1 + w_4 \\, x_2 + b_2\\\\\n\\end{cases}\n\\]\ncan be put in a matrix-vector form:\n\\[\n    \\begin{bmatrix} y_1 \\\\ y_2 \\\\\\end{bmatrix} = \\begin{bmatrix} w_1 & w_2 \\\\ w_3 & w_4 \\\\\\end{bmatrix} \\times \\begin{bmatrix} x_1 \\\\ x_2 \\\\\\end{bmatrix} + \\begin{bmatrix} b_1 \\\\ b_2 \\\\\\end{bmatrix}\n\\]\nWe simply create the corresponding vectors and matrices:\n\\[\n    \\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\\\end{bmatrix} \\qquad \\mathbf{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\\\end{bmatrix} \\qquad \\mathbf{t} = \\begin{bmatrix} t_1 \\\\ t_2 \\\\\\end{bmatrix} \\qquad \\mathbf{b} = \\begin{bmatrix} b_1 \\\\ b_2 \\\\\\end{bmatrix} \\qquad W = \\begin{bmatrix} w_1 & w_2 \\\\ w_3 & w_4 \\\\\\end{bmatrix}\n\\]\n\\(\\mathbf{x}\\) is the input vector, \\(\\mathbf{y}\\) is the output vector, \\(\\mathbf{t}\\) is the target vector. \\(W\\) is called the weight matrix and \\(\\mathbf{b}\\) the bias vector.\nThe model is now defined by:\n\\[\n    \\mathbf{y} = f_{W, \\mathbf{b}}(\\mathbf{x}) = W \\times \\mathbf{x} + \\mathbf{b}\n\\]\nThe problem is exactly the same as before, except that we use vectors and matrices instead of scalars: \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) can have any number of dimensions, the same procedure will apply. This corresponds to a linear neural network (or linear perceptron), with one output neuron per predicted value \\(y_i\\) using the linear activation function.\n\n\n\n```{figure} ../img/linearperceptron.svg\n\n\n\n\nwidth: 60%\n\n\n\nA linear perceptron is a single layer of artificial neurons. The output vector \\(\\mathbf{y}\\) is compared to the ground truth vector \\(\\mathbf{t}\\) using the mse loss.\n\n\nThe mean square error still needs to be a scalar in order to be minimized. We can define it as the squared norm of the error **vector**:\n\n$$\n    \\min_{W, \\mathbf{b}} \\, \\mathcal{L}(W, \\mathbf{b}) = \\mathbb{E}_\\mathcal{D} [ ||\\mathbf{t} - \\mathbf{y}||^2 ] = \\mathbb{E}_\\mathcal{D} [ ((t_1 - y_1)^2 + (t_2 - y_2)^2) ]\n$$\n\nIn order to apply gradient descent, one needs to calculate partial derivatives w.r.t the weight matrix $W$ and the bias vector $\\mathbf{b}$, i.e. **gradients**:\n\n$$\n    \\begin{cases}\n    \\Delta W = - \\eta \\, \\nabla_W \\, \\mathcal{L}(W, \\mathbf{b}) \\\\\n    \\\\\n    \\Delta \\mathbf{b} = - \\eta \\, \\nabla_\\mathbf{b}  \\mathcal{L}(W, \\mathbf{b}) \\\\\n    \\end{cases}\n$$\n\n\n\n```{note}\nSome more advanced linear algebra becomes important to know how to compute these gradients:\n\n<https://web.stanford.edu/class/cs224n/readings/gradient-notes.pdf>\nWe search the minimum of the mse loss function:\n\\[\n    \\min_{W, \\mathbf{b}} \\, \\mathcal{L}(W, \\mathbf{b}) = \\mathbb{E}_\\mathcal{D} [ ||\\mathbf{t} - \\mathbf{y}||^2 ] \\approx \\frac{1}{N} \\, \\sum_{i=1}^N ||\\mathbf{t}_i - \\mathbf{y}_i||^2 = \\frac{1}{N} \\, \\sum_{i=1}^N \\mathcal{l}_i(W, \\mathbf{b})\n\\]\nThe individual loss function \\(\\mathcal{l}_i(W, \\mathbf{b})\\) is the squared \\(\\mathcal{L}^2\\)-norm of the error vector, what can be expressed as a dot product or a vector multiplication:\n\\[\n    \\mathcal{l}_i(W, \\mathbf{b}) = ||\\mathbf{t}_i - \\mathbf{y}_i||^2 = \\langle \\mathbf{t}_i - \\mathbf{y}_i \\cdot \\mathbf{t}_i - \\mathbf{y}_i \\rangle = (\\mathbf{t}_i - \\mathbf{y}_i)^T \\times (\\mathbf{t}_i - \\mathbf{y}_i)\n\\]\nRemember:\n\n$$\\mathbf{x}^T \\times \\mathbf{x} = \\begin{bmatrix} x_1 & x_2 & \\ldots & x_n \\end{bmatrix} \\times \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix} = x_1 \\, x_1 + x_2 \\, x_2 + \\ldots + x_n \\, x_n = \\langle \\mathbf{x} \\cdot \\mathbf{x} \\rangle = ||\\mathbf{x}||^2_2$$\nThe chain rule tells us in principle that:\n\\[\\nabla_{W} \\, \\mathcal{l}_i(W, \\mathbf{b}) = \\nabla_{\\mathbf{y}_i} \\, \\mathcal{l}_i(W, \\mathbf{b}) \\times \\nabla_{W} \\, \\mathbf{y}_i\\]\nThe gradient w.r.t the output vector \\(\\mathbf{y}_i\\) is quite easy to obtain, as it a quadratic function of \\(\\mathbf{t}_i - \\mathbf{y}_i\\):\n\\[\\nabla_{\\mathbf{y}_i} \\, \\mathcal{l}_i(W, \\mathbf{b}) = \\nabla_{\\mathbf{y}_i} \\, (\\mathbf{t}_i - \\mathbf{y}_i)^T \\times (\\mathbf{t}_i - \\mathbf{y}_i)\\]\nThe proof relies on product differentiation \\((f\\times g)' = f' \\, g + f \\, g'\\):\n\\[\\begin{aligned}\n    \\nabla_{\\mathbf{y}_i} \\, (\\mathbf{t}_i - \\mathbf{y}_i)^T \\times (\\mathbf{t}_i - \\mathbf{y}_i) & = ( \\nabla_{\\mathbf{y}_i} \\, (\\mathbf{t}_i - \\mathbf{y}_i) ) \\times (\\mathbf{t}_i - \\mathbf{y}_i) + (\\mathbf{t}_i - \\mathbf{y}_i) \\times \\nabla_{\\mathbf{y}_i} \\, (\\mathbf{t}_i - \\mathbf{y}_i)  \\\\\n    &\\\\\n    &= - (\\mathbf{t}_i - \\mathbf{y}_i) - (\\mathbf{t}_i - \\mathbf{y}_i) \\\\\n    &\\\\\n    &= - 2 \\, (\\mathbf{t}_i - \\mathbf{y}_i) \\\\\n\\end{aligned}\n\\]\nWe use the properties $\\nabla_{\\mathbf{x}}\\,  \\mathbf{x}^T \\times \\mathbf{z} = \\mathbf{z}$ and $\\nabla_{\\mathbf{z}} \\, \\mathbf{x}^T \\times \\mathbf{z} = \\mathbf{x}$ to get rid of the transpose.\nThe “problem” is when computing \\(\\nabla_{W} \\, \\mathbf{y}_i = \\nabla_{W} \\, (W \\times \\mathbf{x}_i + \\mathbf{b})\\): * \\(\\mathbf{y}_i\\) is a vector and \\(W\\) a matrix. * \\(\\nabla_{W} \\, \\mathbf{y}_i\\) is then a Jacobian (matrix), not a gradient (vector).\nIntuitively, differentiating \\(W \\times \\mathbf{x}_i + \\mathbf{b}\\) w.r.t \\(W\\) should return \\(\\mathbf{x}_i\\), but it is a vector, not a matrix…\nActually, only the gradient (or Jacobian) of \\(\\mathcal{l}_i(W, \\mathbf{b})\\) w.r.t \\(W\\) should be a matrix of the same size as \\(W\\) so that we can apply gradient descent:\n\\[\\Delta W = - \\eta \\, \\nabla_W \\, \\mathcal{L}(W, \\mathbf{b})\\]\nWe already know that:\n\\[\\nabla_{W} \\, \\mathcal{l}_i(W, \\mathbf{b}) = - 2\\, (\\mathbf{t}_i - \\mathbf{y}_i) \\times \\nabla_{W} \\, \\mathbf{y}_i\\]\nIf \\(\\mathbf{x}_i\\) has \\(n\\) elements and \\(\\mathbf{y}_i\\) \\(m\\) elements, \\(W\\) is a \\(m \\times n\\) matrix.\nRemember the outer product between two vectors:\n\n$$\n\\mathbf{u} \\times \\mathbf{v}^\\textsf{T} =\n  \\begin{bmatrix}u_1 \\\\ u_2 \\\\ u_3 \\\\ u_4\\end{bmatrix}\n    \\begin{bmatrix}v_1 & v_2 & v_3\\end{bmatrix} =\n  \\begin{bmatrix}\n    u_1v_1 & u_1v_2 & u_1v_3 \\\\\n    u_2v_1 & u_2v_2 & u_2v_3 \\\\\n    u_3v_1 & u_3v_2 & u_3v_3 \\\\\n    u_4v_1 & u_4v_2 & u_4v_3\n  \\end{bmatrix}.\n$$\nIt is easy to see that the outer product between \\((\\mathbf{t}_i - \\mathbf{y}_i)\\) and \\(\\mathbf{x}_i\\) gives a \\(m \\times n\\) matrix:\n\\[\n    \\nabla_W \\, \\mathcal{l}_i(W, \\mathbf{b}) = - 2 \\, (\\mathbf{t}_i - \\mathbf{y}_i) \\times \\mathbf{x}_i^T\\\\\n\\]\nLet’s prove it element per element on a small matrix:\n\\[\n    \\mathbf{y} = W \\times \\mathbf{x} + \\mathbf{b}\n\\]\n\\[\n    \\begin{bmatrix} y_1 \\\\ y_2 \\\\\\end{bmatrix} = \\begin{bmatrix} w_1 & w_2 \\\\ w_3 & w_4 \\\\\\end{bmatrix} \\times \\begin{bmatrix} x_1 \\\\ x_2 \\\\\\end{bmatrix} + \\begin{bmatrix} b_1 \\\\ b_2 \\\\\\end{bmatrix}\n\\]\n\\[\n\\mathcal{l}(W, \\mathbf{b}) = (\\mathbf{t} - \\mathbf{y})^T \\times (\\mathbf{t} - \\mathbf{y}) = \\begin{bmatrix} t_1 - y_1 & t_2 - y_2 \\\\\\end{bmatrix} \\times \\begin{bmatrix} t_1 - y_1 \\\\ t_2 - y_2 \\\\\\end{bmatrix} = (t_1 - y_1)^2 + (t_2 - y_2)^2\n\\]\nThe Jacobian w.r.t \\(W\\) can be explicitly formed using partial derivatives:\n\\[\n\\nabla_W \\, \\mathcal{l}(W, \\mathbf{b}) = \\begin{bmatrix}\n\\dfrac{\\partial \\mathcal{l}(W, \\mathbf{b})}{\\partial w_1} & \\dfrac{\\partial \\mathcal{l}(W, \\mathbf{b})}{\\partial w_2} \\\\ \\dfrac{\\partial \\mathcal{l}(W, \\mathbf{b})}{\\partial w_3} & \\dfrac{\\partial \\mathcal{l}(W, \\mathbf{b})}{\\partial w_4} \\\\\n\\end{bmatrix}\n= \\begin{bmatrix}\n-2 \\, (t_1 - y_1) \\, x_1 & -2 \\, (t_1 - y_1) \\, x_2 \\\\ -2 \\, (t_2 - y_2) \\, x_1 & -2 \\, (t_2 - y_2) \\, x_2 \\\\\n\\end{bmatrix}\n\\]\nWe can rearrange this matrix as an outer product:\n\\[\n\\nabla_W \\, \\mathcal{l}(W, \\mathbf{b}) = -2 \\, \\begin{bmatrix}\nt_1 - y_1  \\\\  t_2 - y_2 \\\\\n\\end{bmatrix} \\times \\begin{bmatrix}\nx_1 & x_2 \\\\\n\\end{bmatrix}\n= - 2 \\, (\\mathbf{t} - \\mathbf{y}) \\times \\mathbf{x}^T\n\\]\nMultiple linear regression\n\nBatch version:\n\n\\[\\begin{cases}\n    \\Delta W = \\eta \\, \\dfrac{1}{N} \\sum_{i=1}^N \\, (\\mathbf{t}_i - \\mathbf{y}_i ) \\times \\mathbf{x}_i^T \\\\\n    \\\\\n    \\Delta \\mathbf{b} = \\eta \\, \\dfrac{1}{N} \\sum_{i=1}^N \\, (\\mathbf{t}_i - \\mathbf{y}_i) \\\\\n\\end{cases}\\]\n\nOnline version (delta learning rule):\n\n\\[\\begin{cases}\n    \\Delta W = \\eta \\, (\\mathbf{t}_i - \\mathbf{y}_i ) \\times \\mathbf{x}_i^T \\\\\n    \\\\\n    \\Delta \\mathbf{b} = \\eta \\, (\\mathbf{t}_i - \\mathbf{y}_i) \\\\\n\\end{cases}\\]\nThe matrix-vector notation is completely equivalent to having one learning rule per parameter:\n\\[\n\\begin{cases}\n    \\Delta w_1 = \\eta \\, (t_1 - y_1) \\, x_1 \\\\\n    \\Delta w_2 = \\eta \\, (t_1 - y_1) \\, x_2 \\\\\n    \\Delta w_3 = \\eta \\, (t_2 - y_2) \\, x_1 \\\\\n    \\Delta w_4 = \\eta \\, (t_2 - y_2) \\, x_2 \\\\\n\\end{cases}\n\\qquad\n\\begin{cases}\n    \\Delta b_1 = \\eta \\, (t_1 - y_1) \\\\\n    \\Delta b_2 = \\eta \\, (t_2 - y_2) \\\\\n\\end{cases}\n\\]\nThe delta learning rule is always of the form: $\\Delta w$ = eta * error * input. Biases have an input of 1."
  },
  {
    "objectID": "notes/2.2-LinearRegression.html#logistic-regression",
    "href": "notes/2.2-LinearRegression.html#logistic-regression",
    "title": "5  Linear regression",
    "section": "5.3 Logistic regression",
    "text": "5.3 Logistic regression\n\n\n\n\nLet’s suppose we want to perform a regression, but where the outputs \\(t_i\\) are bounded between 0 and 1. We could use a logistic (or sigmoid) function instead of a linear function in order to transform the input into an output:\n\\[\n    y = \\sigma(w \\, x + b )  = \\displaystyle\\frac{1}{1+\\exp(-w \\, x - b )}\n\\]\n\n\n\n```{figure} ../img/sigmoid.png\n\n\n\n\nwidth: 60%\n\n\n\nLogistic or sigmoid function \\(\\sigma(x)=\\displaystyle\\frac{1}{1+\\exp(-x)}\\).\n\nBy definition of the logistic function, the prediction $y$ will be bounded between 0 and 1, what matches the targets $t$. Let's now apply gradient descent on the **mse** loss using this new model. The individual loss will be:\n\n$$l_i(w, b) = (t_i - \\sigma(w \\, x_i + b) )^2 $$\n\nThe partial derivative of the individual loss is easy to find using the chain rule:\n\n$$\n\\begin{aligned}\n    \\displaystyle\\frac{\\partial l_i(w, b)}{\\partial w}\n        &= 2 \\, (t_i - y_i)  \\, \\frac{\\partial}{\\partial w}  (t_i - \\sigma(w \\, x_i + b ))\\\\\n        &\\\\\n        &= - 2 \\, (t_i - y_i) \\, \\sigma'(w \\, x_i + b ) \\,  x_i \\\\\n\\end{aligned}\n$$\n\nThe non-linear transfer function $\\sigma(x)$ therefore adds its derivative into the gradient:\n\n$$\n    \\Delta w = \\eta \\, (t_i - y_i) \\, \\sigma'(w \\, x_i + b ) \\, x_i\n$$\n\nThe logistic function $\\sigma(x)=\\frac{1}{1+\\exp(-x)}$ has the nice property that its derivative can be expressed easily:\n\n$$\n    \\sigma'(x) = \\sigma(x) \\, (1 - \\sigma(x) )\n$$\n\n\n\n\n```{note}\nHere is the proof using the fact that the derivative of $\\displaystyle\\frac{1}{f(x)}$ is $\\displaystyle\\frac{- f'(x)}{f^2(x)}$ :\n\n$$\\begin{aligned}\n    \\sigma'(x) & = \\displaystyle\\frac{-1}{(1+\\exp(-x))^2} \\, (- \\exp(-x)) \\\\\n    &\\\\\n    &= \\frac{1}{1+\\exp(-x)} \\times \\frac{\\exp(-x)}{1+\\exp(-x)}\\\\\n    &\\\\\n    &= \\frac{1}{1+\\exp(-x)} \\times \\frac{1 + \\exp(-x) - 1}{1+\\exp(-x)}\\\\\n    &\\\\\n    &= \\frac{1}{1+\\exp(-x)} \\times (1 - \\frac{1}{1+\\exp(-x)})\\\\\n    &\\\\\n    &= \\sigma(x) \\, (1 - \\sigma(x) )\\\\\n\\end{aligned}\n$$\nThe delta learning rule for the logistic regression model is therefore easy to obtain:\n\\[\n\\begin{cases}\n    \\Delta w = \\eta \\, (t_i - y_i) \\, y_i \\, ( 1 - y_i ) \\, x_i \\\\\n\\\\\n    \\Delta b = \\eta \\, (t_i - y_i) \\, y_i \\, ( 1 - y_i ) \\\\\n\\end{cases}\n\\]\nGeneralized form of the delta learning rule\n\n\n\n```{figure} ../img/artificialneuron.svg\n\n\n\n\nwidth: 60%\n\n\n\nArtificial neuron with multiple inputs.\n\nFor a linear perceptron with parameters $W$ and $\\mathbf{b}$ and any activation function $f$:\n\n$$\n    \\mathbf{y} = f(W \\times \\mathbf{x} + \\mathbf{b} )  \n$$\n\nand the **mse** loss function:\n\n$$\n    \\mathcal{L}(W, \\mathbf{b}) = \\mathbb{E}_{\\mathcal{D}}[||\\mathbf{t} - \\mathbf{y}||^2]\n$$\n\nthe **delta learning rule** has the form:\n\n$$\n\\begin{cases}\n    \\Delta W = \\eta \\, (\\mathbf{t} - \\mathbf{y}) \\times f'(W \\times \\mathbf{x} + \\mathbf{b}) \\times \\mathbf{x}^T \\\\\n\\\\\n    \\Delta \\mathbf{b} = \\eta \\, (\\mathbf{t} - \\mathbf{y}) \\times f'(W \\times \\mathbf{x} + \\mathbf{b}) \\\\\n\\end{cases}\n$$\n\n\nIn the linear case, $f'(x) = 1$. One can use any non-linear function, e.g hyperbolic tangent tanh(), ReLU, etc. Transfer functions are chosen for neural networks so that we can compute their derivative easily.\n\n\n## Polynomial regression\n\n<div class='embed-container'><iframe src='https://www.youtube.com/embed/a6sQgJovhzU' frameborder='0' allowfullscreen></iframe></div>\n\n```{figure} ../img/polynomialregression.png\n---\nwidth: 60%\n---\nPolynomial regression.\nThe functions underlying real data are rarely linear plus some noise around the ideal value. In the figure above, the input/output function is better modeled by a second-order polynomial:\n\\[y = f_{\\mathbf{w}, b}(x) = w_1 \\, x + w_2 \\, x^2 +b\\]\nWe can transform the input into a vector of coordinates:\n\\[\\mathbf{x} = \\begin{bmatrix} x \\\\ x^2 \\\\ \\end{bmatrix} \\qquad \\mathbf{w} = \\begin{bmatrix} w_1 \\\\ w_2 \\\\ \\end{bmatrix}\\]\nThe problem becomes:\n\\[y = \\langle \\mathbf{w} . \\mathbf{x} \\rangle + b = \\sum_j w_j \\, x_j + b\\]\nWe can simply apply multiple linear regression (MLR) to find \\(\\mathbf{w}\\) and b:\n\\[\\begin{cases}\n\\Delta \\mathbf{w} =  \\eta \\, (t - y) \\, \\mathbf{x}\\\\\n\\\\\n\\Delta b =  \\eta \\, (t - y)\\\\\n\\end{cases}\\]\nThis generalizes to polynomials of any order \\(p\\):\n\\[y = f_{\\mathbf{w}, b}(x) = w_1 \\, x + w_2 \\, x^2 + \\ldots + w_p \\, x^p + b\\]\nWe create a vector of powers of \\(x\\):\n\\[\\mathbf{x} = \\begin{bmatrix} x \\\\ x^2 \\\\ \\ldots \\\\ x^p \\end{bmatrix} \\qquad \\mathbf{w} = \\begin{bmatrix} w_1 \\\\ w_2 \\\\ \\ldots \\\\ w_p \\end{bmatrix}\\]\nad apply multiple linear regression (MLR) to find \\(\\mathbf{w}\\) and b:\n\\[\\begin{cases}\n\\Delta \\mathbf{w} =  \\eta \\, (t - y) \\, \\mathbf{x}\\\\\n\\\\\n\\Delta b =  \\eta \\, (t - y)\\\\\n\\end{cases}\\]\nNon-linear problem solved! The only unknown is which order for the polynomial matches best the data. One can perform regression with any kind of parameterized function using gradient descent."
  },
  {
    "objectID": "notes/2.2-LinearRegression.html#a-bit-of-learning-theory",
    "href": "notes/2.2-LinearRegression.html#a-bit-of-learning-theory",
    "title": "5  Linear regression",
    "section": "5.4 A bit of learning theory",
    "text": "5.4 A bit of learning theory\n\n\n\n\nBefore going further, let’s think about what we have been doing so far. We had a bunch of data samples \\(\\mathcal{D} = (\\mathbf{x}_i, t_i)_{i=1..N}\\) (the training set). We decided to apply a (linear) model on it:\n\\[y_i = \\langle \\mathbf{w} . \\mathbf{x}_i \\rangle + b\\]\nWe then minimized the mean square error (mse) on that training set using gradient descent:\n\\[\n    \\mathcal{L}(w, b) = \\mathbb{E}_{\\mathbf{x}, t \\in \\mathcal{D}} [(t_i - y_i )^2]\n\\]\nAt the end of learning, we can measure the residual error of the model on the data:\n\\[\n    \\epsilon_\\mathcal{D} = \\frac{1}{N} \\, \\sum_{i=1}^{N} (t_i - y_i )^2\n\\]\nWe get a number, for example 0.04567. Is that good?\nThe mean square error mse is not very informative, as its value depends on how the outputs are scaled: multiply the targets and prediction by 10 and the mse is 100 times higher.\n\n\n\n```{figure} ../img/regression-animation-mse-dual.png\n\n\n\n\nwidth: 100%\n\n\n\nThe residual error measures the quality of the fit, but it is sensible to the scaling of the outputs.\n\n\nThe **coefficient of determination** $R^2$ is a rescaled variant of the mse comparing the variance of the residuals to the variance of the data around its mean $\\hat{t}$:\n\n$$\n    R^2 = 1 - \\frac{\\text{Var}(\\text{residuals})}{\\text{Var}(\\text{data})} = 1 - \\frac{\\sum_{i=1}^N (t_i- y_i)^2}{\\sum_{i=1}^N (t_i - \\hat{t})^2}\n$$\n\n$R^2$ should be as close from 1 as possible. For example, if $R^2 = 0.8$, we can say that the **model explains 80% of the variance of the data**.  \n\n```{figure} ../img/r2.png\n---\nwidth: 100%\n---\nThe coefficient of determination compares the variance of the residuals to the variance of the data. Source: <https://towardsdatascience.com/introduction-to-linear-regression-in-python-c12a072bedf0>\n\n5.4.1 Sensibility to outliers\nSuppose we have a training set with one outlier (bad measurement, bad luck, etc).\n\n\n\n```{figure} ../img/regression-outlier.png\n\n\n\n\nwidth: 70%\n\n\n\nLinear data with one outlier.\n\nLMS would find the minimum of the mse, but it is clearly a bad fit for most points.\n\n```{figure} ../img/regression-outlier-fit.png\n---\nwidth: 70%\n---\nLMS is attracted by the outlier, leading to a bad prediction for all points.\nThis model feels much better, but its residual mse is actually higher…\n\n\n\n```{figure} ../img/regression-outlier-fit-corrected.png\n\n\n\n\nwidth: 70%\n\n\n\nBy ignoring the outlier, the prediction would be correct for most points.\n\nLet's visualize polynomial regression with various orders of the polynomial on a small dataset.\n\n\n```{figure} ../img/polynomialregression-animation.gif\n---\nwidth: 70%\n---\nPolynomial regression with various orders.\nWhen only looking at the residual mse on the training data, one could think that the higher the order of the polynomial, the better. But it is obvious that the interpolation quickly becomes very bad when the order is too high. A complex model (with a lot of parameters) is useless for predicting new values. We actually do not care about the error on the training set, but about generalization.\n\n\n\n```{figure} ../img/polynomialregression-mse.png\n\n\n\n\nwidth: 70%\n\n\n\nResidual mse of polynomial regression depending on the order of the polynomial.\n\n### Cross-validation\n\nLet’s suppose we dispose of $m$ models $\\mathcal{M} = \\{ M_1, ..., M_m\\}$ that could be used to fit (or classify) some data $\\mathcal{D} = \\{\\mathbf{x}_i, t_i\\}_{i=1}^N$. Such a class could be the ensemble of polynomes with different orders, different algorithms (NN, SVM) or the same algorithm with different values for the hyperparameters (learning rate, regularization parameters...).\n\nThe naive and **wrong** method to find the best hypothesis would be:\n\n```{admonition} Do not do this!\n- For all models $M_i$:\n\n    - Train $M_i$ on $\\mathcal{D}$ to obtain an hypothesis $h_i$.\n\n    - Compute the training error $\\epsilon_\\mathcal{D}(h_i)$ of $h_i$ on $\\mathcal{D}$ :\n\n    $$\n        \\epsilon_\\mathcal{D}(h_i) =  \\mathbb{E}_{(\\mathbf{x}, t) \\in \\mathcal{D}} [(h_i(\\mathbf{x}) - t)^2]\n    $$\n\n- Select the hypothesis $h_{i}^*$ with the minimal training error : $h_{i}^* = \\text{argmin}_{h_i \\in \\mathcal{M}} \\quad \\epsilon_\\mathcal{D}(h_i)$\nThis method leads to overfitting, as only the training error is used.\nThe solution is randomly take some samples out of the training set to form the test set. Typical values are 20 or 30 % of the samples in the test set.\n\nTrain the model on the training set (70% of the data).\nTest the performance of the model on the test set (30% of the data).\n\n\n\n\n```{figure} ../img/polynomialregression-traintest.png\n\n\n\n\nwidth: 70%\n\n\n\nPolynomial data split in a training set and a test set.\n\nThe test performance will better measure how well the model generalizes to new examples.\n\n\n```{admonition} Simple hold-out cross-validation\n\n* Split the training data $\\mathcal{D}$ into $\\mathcal{S}_{\\text{train}}$ and $\\mathcal{S}_{\\text{test}}$.\n\n* For all models $M_i$:\n\n    * Train $M_i$ on $\\mathcal{S}_{\\text{train}}$ to obtain an hypothesis $h_i$.\n\n    * Compute the empirical error $\\epsilon_{\\text{test}}(h_i)$ of $h_i$ on $\\mathcal{S}_{\\text{test}}$ :\n\n    $$\\epsilon_{\\text{test}}(h_i) = \\mathbb{E}_{(\\mathbf{x}, t) \\in  \\mathcal{S}_{\\text{test}}} [(h_i(\\mathbf{x}) - t)^2]$$\n\n* Select the hypothesis $h_{i}^*$ with the minimal empirical error : $h_{i}^* = \\text{argmin}_{h_i \\in \\mathcal{M}} \\quad \\epsilon_{\\text{test}}(h_i)$\nThe disadvantage of simple hold-out cross-validation is that 20 or 30% of the data is wasted and not used for learning. It may be a problem when data is rare or expensive.\nk-fold cross-validation allows a more efficient use os the available data and a better measure of the generalization error. The idea is to build several different training/test sets with the same data, train and test each model repeatedly on each partition and choose the hypothesis that works best on average.\n\n\n\n```{figure} ../img/kfold.jpg\n\n\n\n\nwidth: 70%\n\n\n\nk-fold cross-validation. Source https://upload.wikimedia.org/wikipedia/commons/1/1c/K-fold_cross_validation_EN.jpg\n\n```{admonition} k-fold cross-validation\n* Randomly split the data $\\mathcal{D}$ into $k$ subsets of $\\frac{N}{k}$ examples $\\{ \\mathcal{S}_{1}, \\dots , \\mathcal{S}_{k}\\}$\n\n* For all models $M_i$:\n\n    * For all $k$ subsets $\\mathcal{S}_j$:\n\n        * Train $M_i$ on $\\mathcal{D} - \\mathcal{S}_j$ to obtain an hypothesis $h_{ij}$\n\n        * Compute the empirical error $\\epsilon_{\\mathcal{S}_j}(h_{ij})$ of $h_{ij}$ on $\\mathcal{S}_j$\n\n    * The empirical error of the model $M_i$ on $\\mathcal{D}$ is the average of empirical errors made on $(\\mathcal{S}_j)_{j=1}^{k}$\n        \n        $$\n            \\epsilon_{\\mathcal{D}} (M_i) = \\frac{1}{k} \\cdot \\sum_{j=1}^{k} \\epsilon_{\\mathcal{S}_j}(h_{ij})\n        $$\n* Select the model $M_{i}^*$ with the minimal empirical error on $\\mathcal{D}$.\nIn general, you can take \\(k=10\\) partitions. The extreme case is to take \\(k=N\\) partition, i.e. the test set has only one sample each time: leave-one-out cross-validation. k-fold cross-validation works well, but needs a lot of repeated learning.\n\n\n5.4.2 Underfitting - overfitting\nWhile the training mse always decrease with more complex models, the test mse increases after a while. This is called overfitting: learning by heart the data without caring about generalization. The two curves suggest that we should chose a polynomial order between 2 and 9.\n\n\n\n```{figure} ../img/polynomialregression-mse-traintest.png\n\n\n\n\nwidth: 90%\n\n\n\nTraining and test mse of polynomial regression.\n\n\nA model not complex enough for the data will **underfit**: its training error is high. A model too complex for the data will **overfit**: its test error is high. In between, there is the right complexity for the model: it learns the data correctly but does not overfit. \n\n```{figure} ../img/underfitting-overfitting.png\n---\nwidth: 100%\n---\nUnderfitting and overfitting.\nWhat does complexity mean? In polynomial regression, the complexity is related to the order of the polynomial, i.e. the number of coefficients to estimate:\n\\[y = f_{\\mathbf{w}, b}(x) = \\sum_{k=1}^p w_k \\, x^k + b\\]\n\\[\\mathbf{x} = \\begin{bmatrix} x \\\\ x^2 \\\\ \\ldots \\\\ x^p \\end{bmatrix} \\qquad \\mathbf{w} = \\begin{bmatrix} w_1 \\\\ w_2 \\\\ \\ldots \\\\ w_p \\end{bmatrix}\\]\nA polynomial of order \\(p\\) has \\(p+1\\) unknown parameters (free parameters): the \\(p\\) weights and the bias. Generally, the complexity of a model relates to its number of free parameters:\n\nThe more free parameters, the more complex the model is, the more likely it will overfit.\n\nUnder-/Over-fitting relates to the statistical concept of bias-variance trade-off. The bias is the training error that the hypothesis would make if the training set was infinite (accuracy, flexibility of the model): a model with high bias is underfitting. The variance is the error that will be made by the hypothesis on new examples taken from the same distribution (spread, the model is correct on average, but not for individual samples): a model with high variance is overfitting.\n\n\n\n```{figure} ../img/biasvariance3.png\n\n\n\n\nwidth: 80%\n\n\n\nBias and variance of an estimator. Source: http://scott.fortmann-roe.com/docs/BiasVariance.html\n\nThe bias decreases when the model becomes complex; the variance increases when the model becomes complex. The **generalization error** is a combination of the bias and variance:\n\n$$\n    \\text{generalization error} = \\text{bias}^2 + \\text{variance}\n$$\n\nWe search for the model with the **optimum complexity** realizing the trade-off between bias and variance. It is better to have a model with a slightly higher bias (training error) but with a smaller variance (generalization error).\n\n```{figure} ../img/biasvariance2.png\n---\nwidth: 80%\n---\nThe optimal complexity of an algorithm is a trade-off between bias and variance. Source: <http://scott.fortmann-roe.com/docs/BiasVariance.html>"
  },
  {
    "objectID": "notes/2.2-LinearRegression.html#regularized-regression",
    "href": "notes/2.2-LinearRegression.html#regularized-regression",
    "title": "5  Linear regression",
    "section": "5.5 Regularized regression",
    "text": "5.5 Regularized regression\n\n\n\n\nLinear regression can either underfit or overfit depending on the data.\n\n\n\n```{figure} ../img/underfitting-overfitting-linear.png\n\n\n\n\nwidth: 60%\n\n\n\nLinear regression underfits non-linear data.\n\n```{figure} ../img/regression-outlier-fit.png\n---\nwidth: 60%\n---\nLinear regression overfits outliers.\nWhen linear regression underfits (both training and test errors are high), the data is not linear: we need to use a neural network. When linear regression overfits (the test error is higher than the training error), we would like to decrease its complexity.\nThe problem is that the number of free parameters in linear regression only depends on the number of inputs (dimensions of the input space).\n\\[\n    y = \\sum_{i=1}^d w_i \\, x_i + b\n\\]\nFor \\(d\\) inputs, there are \\(d+1\\) free parameters: the \\(d\\) weights and the bias.\nWe must find a way to reduce the complexity of the linear regression without changing the number of parameters, which is impossible. The solution is to constrain the values that the parameters can take: regularization. Regularization reduces the variance at the cost of increasing the bias.\n\n5.5.1 L2 regularization - Ridge regression\nUsing L2 regularization for linear regression leads to the Ridge regression algorithm. The individual loss function is defined as:\n\\[\n    \\mathcal{l}_i(\\mathbf{w}, b) = (t_i - y_i)^2 + \\lambda \\, ||\\mathbf{w}||^2\n\\]\nThe first part of the loss function is the classical mse on the training set: its role is to reduce the bias. The second part minimizes the L2 norm of the weight vector (or matrix), reducing the variance:\n\\[\n    ||\\mathbf{w}||^2 = \\sum_{i=1}^d w_i^2\n\\]\nDeriving the regularized delta learning rule is straightforward:\n\\[\n    \\Delta w_i = \\eta \\, ((t_i - y_i) \\ x_i - \\lambda \\, w_i)\n\\]\nRidge regression is also called weight decay: even if there is no error, all weights will decay to 0.\n\n\n\n```{figure} ../img/ridge-effect.png\n\n\n\n\nwidth: 60%\n\n\n\nRidge regression finds the smallest value for the weights that minimize the mse. Source: https://www.mlalgorithms.org/articles/l1-l2-regression/\n\n\n### L1 regularization - LASSO regression\n\nUsing **L1 regularization** for linear regression leads to the **LASSO regression** algorithm (least absolute shrinkage and selection operator). The individual loss function is defined as:\n\n$$\n    \\mathcal{l}_i(\\mathbf{w}, b) =  (t_i - y_i)^2 + \\lambda \\, |\\mathbf{w}|\n$$\n\nThe second part minimizes this time the L1 norm of the weight vector, i.e. its absolute value:\n\n$$\n    |\\mathbf{w}| = \\sum_{i=1}^d |w_i|\n$$\n\nRegularized delta learning rule with LASSO:\n\n$$\n    \\Delta w_i = \\eta \\, ((t_i - y_i) \\ x_i - \\lambda \\, \\text{sign}(w_i))\n$$\n\n**Weight decay** does not depend on the value of the weight, only its sign. Weights can decay very fast to 0.\n\n```{figure} ../img/lasso-effect.png\n---\nwidth: 60%\n---\nLASSO regression tries to set as many weight to 0 as possible (sparse code). Source: <https://www.mlalgorithms.org/articles/l1-l2-regression/>\nBoth methods depend on the regularization parameter \\(\\lambda\\). Its value determines how important the regularization term should. Regularization introduce a bias, as the solution found is not the minimum of the mse, but reduces the variance of the estimation, as small weights are less sensible to noise.\nLASSO allows feature selection: features with a zero weight can be removed from the training set.\n\n\n\n```{figure} ../img/linearregression-withoutregularization.png\n\n\n\n\nwidth: 80%\n\n\n\nLinear regression tends to assign values to all weights. Source: https://www.analyticsvidhya.com/blog/2017/06/a-comprehensive-guide-for-linear-ridge-and-lasso-regression/\n\n```{figure} ../img/linearregression-withregularization.png\n---\nwidth: 80%\n---\nLASSO regression tries to set as many weights to 0 as possible (sparse code). Source: <https://www.analyticsvidhya.com/blog/2017/06/a-comprehensive-guide-for-linear-ridge-and-lasso-regression/>\n\n\n5.5.2 L1+L2 regularization - ElasticNet\nAn ElasticNet is a linear regression using both L1 and L2 regression:\n\\[\n    \\mathcal{l}_i(\\mathbf{w}, b) =  (t_i - y_i)^2 + \\lambda_1 \\, |\\mathbf{w}| + \\lambda_2 \\, ||\\mathbf{w}||^2\n\\]\nIt combines the advantages of Ridge and LASSO, at the cost of having now two regularization parameters to determine."
  },
  {
    "objectID": "notes/2.3-LinearClassification.html",
    "href": "notes/2.3-LinearClassification.html",
    "title": "6  Linear classification",
    "section": "",
    "text": "Slides: pdf"
  },
  {
    "objectID": "notes/2.3-LinearClassification.html#hard-linear-classification",
    "href": "notes/2.3-LinearClassification.html#hard-linear-classification",
    "title": "6  Linear classification",
    "section": "6.1 Hard linear classification",
    "text": "6.1 Hard linear classification\n\n\n\n\nThe training data \\(\\mathcal{D}\\) is composed of \\(N\\) examples \\((\\mathbf{x}_i, t_i)_{i=1..N}\\) , with a d-dimensional input vector \\(\\mathbf{x}_i \\in \\Re^d\\) and a binary output \\(t_i \\in \\{-1, +1\\}\\). The data points where \\(t = + 1\\) are called the positive class, the other the negative class.\n\n\n\n```{figure} ../img/classification-animation1.png\n\n\n\n\nwidth: 80%\n\n\n\nBinary linear classification of 2D data.\n\nFor example, the inputs $\\mathbf{x}_i$ can be images (one dimension per pixel) and the positive class corresponds to cats ($t_i = +1$), the negative class to dogs ($t_i = -1$).\n\n```{figure} ../img/cats-dogs.jpg\n---\nwidth: 100%\n---\nBinary linear classification of cats vs. dogs images. Source: <http://adilmoujahid.com/posts/2016/06/introduction-deep-learning-python-caffe>\nWe want to find the hyperplane \\((\\mathbf{w}, b)\\) of \\(\\Re^d\\) that correctly separates the two classes.\n\n\n\n```{figure} ../img/classification-animation2.png\n\n\n\n\nwidth: 80%\n\n\n\nThe hyperplane separates the input space into two regions.\n\nFor a point $\\mathbf{x} \\in \\mathcal{D}$, $\\langle \\mathbf{w} \\cdot \\mathbf{x} \\rangle +b$  is the projection of $\\mathbf{x}$  onto the hyperplane $(\\mathbf{w}, b)$.\n\n* If $\\langle \\mathbf{w} \\cdot \\mathbf{x} \\rangle +b > 0$, the point is\n    above the hyperplane.\n\n* If $\\langle \\mathbf{w} \\cdot \\mathbf{x} \\rangle +b < 0$, the point is\n    below the hyperplane.\n\n* If $\\langle \\mathbf{w} \\cdot \\mathbf{x} \\rangle +b = 0$, the point is\n    on the hyperplane.\n\n```{figure} ../img/projection.svg\n---\nwidth: 80%\n---\nProjection on an hyperplane.\nBy looking at the sign of \\(\\langle \\mathbf{w} \\cdot \\mathbf{x} \\rangle +b\\), we can predict the class.\n\\[\\text{sign}(x) = \\begin{cases} +1 \\; \\text{if} \\; x \\geq 0 \\\\ -1 \\; \\text{if} \\; x < 0 \\\\ \\end{cases}\\]\nBinary linear classification can therefore be made by a single artificial neuron using the sign transfer function.\n\\[\ny = f_{\\mathbf{w}, b} (\\mathbf{x}) = \\text{sign} ( \\langle \\mathbf{w} \\cdot \\mathbf{x} \\rangle +b )  = \\text{sign} ( \\sum_{j=1}^d w_j \\, x_j +b )\n\\]\n\\(\\mathbf{w}\\) is the weight vector and \\(b\\) is the bias.\nLinear classification is the process of finding an hyperplane \\((\\mathbf{w}, b)\\) that correctly separates the two classes. If such an hyperplane can be found, the training set is said linearly separable. Otherwise, the problem is non-linearly separable and other methods have to be applied (MLP, SVM…).\n\n\n\n```{figure} ../img/linearlyseparable.png\n\n\n\n\nwidth: 100%\n\n\n\nLinearly and non-linearly spearable datasets.\n\n### Perceptron algorithm\n\nThe Perceptron algorithm tries to find the weights and biases minimizing the **mean square error** (*mse*) or **quadratic loss**:\n\n$$\\mathcal{L}(\\mathbf{w}, b) = \\mathbb{E}_\\mathcal{D} [(t_i - y_i)^2] \\approx \\frac{1}{N} \\, \\sum_{i=1}^{N} (t_i - y_i)^2$$\n\nWhen the prediction $y_i$ is the same as the data $t_i$ for all examples in the training set (perfect classification), the mse is minimal and equal to 0. We can apply gradient descent to find this minimum.\n\n$$\n    \\Delta \\mathbf{w} = - \\eta \\, \\nabla_\\mathbf{w} \\, \\mathcal{L}(\\mathbf{w}, b)\n$$\n\n$$\n    \\Delta b = - \\eta \\, \\nabla_b \\, \\mathcal{L}(\\mathbf{w}, b)\n$$\n\nLet's search for the partial derivative of the quadratic error function with respect to the weight vector:\n\n$$\n    \\nabla_\\mathbf{w} \\, \\mathcal{L}(\\mathbf{w}, b) = \\nabla_\\mathbf{w} \\,  \\frac{1}{N} \\, \\sum_{i=1}^{N} (t_i - y_i )^2 = \\frac{1}{N} \\, \\sum_{i=1}^{N} \\nabla_\\mathbf{w} \\,  (t_i - y_i )^2 = \\frac{1}{N} \\, \\sum_{i=1}^{N} \\nabla_\\mathbf{w} \\,  \\mathcal{l}_i (\\mathbf{w}, b)\n$$\n\n\nEverything is similar to linear regression until we get:\n\n$$\n    \\nabla_\\mathbf{w} \\,  \\mathcal{l}_i (\\mathbf{w}, b) = - 2 \\, (t_i - y_i) \\, \\nabla_\\mathbf{w} \\, \\text{sign}( \\langle \\mathbf{w} \\cdot \\mathbf{x}_i \\rangle +b)\n$$\n\nIn order to continue with the chain rule, we would need to differentiate $\\text{sign}(x)$.\n\n$$\n    \\nabla_\\mathbf{w} \\,  \\mathcal{l}_i (\\mathbf{w}, b) = - 2 \\, (t_i - y_i) \\, \\text{sign}'( \\langle \\mathbf{w} \\cdot \\mathbf{x}_i \\rangle +b) \\,  \\mathbf{x}_i \n$$\n\nBut the sign function is **not** differentiable... We will simply pretend that the sign() function is linear, with a derivative of 1:\n\n$$\n    \\nabla_\\mathbf{w} \\,  \\mathcal{l}_i (\\mathbf{w}, b) = - 2 \\, (t_i - y_i) \\,   \\mathbf{x}_i \n$$\n\nThe update rule for the weight vector $\\mathbf{w}$ and the bias $b$ is therefore the same as in linear regression:\n\n$$\n    \\Delta \\mathbf{w} =  \\eta \\, \\frac{1}{N} \\, \\sum_{i=1}^{N} (t_i - y_i) \\, \\mathbf{x}_i\n$$\n\n$$\n    \\Delta b = \\eta \\, \\frac{1}{N} \\, \\sum_{i=1}^{N} (t_i - y_i )\n$$\n\nBy applying gradient descent on the quadratic error function, one obtains the following algorithm:\n\n```{admonition} Batch perceptron\n\n* **for** $M$ epochs:\n\n    * $\\mathbf{dw} = 0 \\qquad db = 0$\n\n    * **for** each sample $(\\mathbf{x}_i, t_i)$:\n\n        * $y_i =  \\text{sign}( \\langle \\mathbf{w} \\cdot \\mathbf{x}_i \\rangle + b)$\n\n        * $\\mathbf{dw} = \\mathbf{dw} + (t_i - y_i) \\, \\mathbf{x}_i$\n\n        * $db = db + (t_i - y_i)$\n\n    * $\\Delta \\mathbf{w} = \\eta \\, \\frac{1}{N} \\, \\mathbf{dw}$\n\n    * $\\Delta b = \\eta \\, \\frac{1}{N} \\, db$\nThis is called the batch version of the Perceptron algorithm. If the data is linearly separable and \\(\\eta\\) is well chosen, it converges to the minimum of the mean square error.\n\n\n\n```{figure} ../img/classification-animation.gif\n\n\n\n\nwidth: 80%\n\n\n\nBatch perceptron algorithm.\n\nThe **Perceptron algorithm** was invented by the psychologist Frank Rosenblatt in 1958. It was the first algorithmic neural network able to learn linear classification.\n\n```{admonition} Online perceptron algorithm\n\n* **for** $M$ epochs:\n\n    * **for** each sample $(\\mathbf{x}_i, t_i)$:\n\n        * $y_i =  \\text{sign}( \\langle \\mathbf{w} \\cdot \\mathbf{x}_i \\rangle + b)$\n\n        * $\\Delta \\mathbf{w} = \\eta \\, (t_i - y_i) \\, \\mathbf{x}_i$\n\n        * $\\Delta b = \\eta \\, (t_i - y_i)$\nThis algorithm iterates over all examples of the training set and applies the delta learning rule to each of them immediately, not at the end on the whole training set. One could check whether there are still classification errors on the training set at the end of each epoch and stop the algorithm. The delta learning rule depends as always on the learning rate \\(\\eta\\), the error made by the prediction (\\(t_i - y_i\\)) and the input \\(\\mathbf{x}_i\\).\n\n\n\n```{figure} ../img/classification-animation-online.gif\n\n\n\n\nwidth: 80%\n\n\n\nOnline perceptron algorithm.\n\n### Stochastic Gradient descent\n\nThe mean square error is defined as the **expectation** over the data:\n\n$$\\mathcal{L}(\\mathbf{w}, b) = \\mathbb{E}_\\mathcal{D} [(t_i - y_i)^2]$$\n\n**Batch learning** uses the whole training set as samples to estimate the mse:\n\n$$\\mathcal{L}(\\mathbf{w}, b) \\approx \\frac{1}{N} \\, \\sum_{i=1}^{N} (t_i - y_i)^2$$\n\n\n$$\n    \\Delta \\mathbf{w} = \\eta \\, \\frac{1}{N} \\sum_{i=1}^{N} (t_i - y_i ) \\, \\mathbf{x_i}\n$$\n\n**Online learning** uses a single sample to estimate the mse:\n\n$$\\mathcal{L}(\\mathbf{w}, b) \\approx (t_i - y_i)^2$$\n\n\n$$\n    \\Delta \\mathbf{w} = \\eta \\, (t_i - y_i) \\, \\mathbf{x_i}\n$$\n\nBatch learning has less bias (central limit theorem) and is less sensible to noise in the data, but is very slow. Online learning converges faster, but can be instable and overfits (high variance). \n\nIn practice, we use a trade-off between batch and online learning called **Stochastic Gradient Descent (SGD)** or **Minibatch Gradient Descent**.\n\nThe training set is randomly split at each epoch into small chunks of data (a **minibatch**, usually 32 or 64 examples) and the batch learning rule is applied on each chunk.\n\n$$\n    \\Delta \\mathbf{w} = \\eta \\, \\frac{1}{K} \\sum_{i=1}^{K} (t_i - y_i) \\, \\mathbf{x_i}\n$$\n\nIf the **batch size** is well chosen, SGD is as stable as batch learning and as fast as online learning. The minibatches are randomly selected at each epoch (i.i.d).\n\n\n\n```{note}\nOnline learning is a stochastic gradient descent with a batch size of 1."
  },
  {
    "objectID": "notes/2.3-LinearClassification.html#maximum-likelihood-estimation",
    "href": "notes/2.3-LinearClassification.html#maximum-likelihood-estimation",
    "title": "6  Linear classification",
    "section": "6.2 Maximum Likelihood Estimation",
    "text": "6.2 Maximum Likelihood Estimation\n\n\n\n\nLet’s consider \\(N\\) samples \\(\\{x_i\\}_{i=1}^N\\) independently taken from a normal distribution \\(X\\). The probability density function (pdf) of a normal distribution is:\n\\[\n    f(x ; \\mu, \\sigma) =  \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\, \\exp{- \\frac{(x - \\mu)^2}{2\\sigma^2}}\n\\]\nwhere \\(\\mu\\) is the mean of the distribution and \\(\\sigma\\) its standard deviation.\n\n\n\n```{figure} ../img/MLE2.png\n\n\n\n\nwidth: 80%\n\n\n\nNormal distributions with different parameters \\(\\mu\\) and \\(\\sigma\\) explain the data with different likelihoods.\n\nThe problem is to find the values of $\\mu$ and $\\sigma$ which explain best the observations $\\{x_i\\}_{i=1}^N$.\n\nThe idea of MLE is to maximize the joint density function for all observations. This function is expressed by the **likelihood function**:\n\n$$\n    L(\\mu, \\sigma) = P( x ; \\mu , \\sigma  )  = \\prod_{i=1}^{N} f(x_i ; \\mu, \\sigma )\n$$\n\nWhen the pdf takes high values for all samples, it is quite likely that the samples come from this particular distribution. The likelihood function reflects the probability that the parameters $\\mu$ and $\\sigma$ explain the observations $\\{x_i\\}_{i=1}^N$.\n\nWe therefore search for the values $\\mu$ and $\\sigma$ which **maximize** the likelihood function.\n\n$$\n    \\text{max}_{\\mu, \\sigma} \\quad L(\\mu, \\sigma) = \\prod_{i=1}^{N} f(x_i ; \\mu, \\sigma )\n$$\n\n\nFor the normal distribution, the likelihood function is:\n\n$$\n\\begin{aligned}\n    L(\\mu, \\sigma) & = \\prod_{i=1}^{N} f(x_i ; \\mu, \\sigma ) \\\\\n                   & = \\prod_{i=1}^{N} \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\, \\exp{- \\frac{(x_i - \\mu)^2}{2\\sigma^2}}\\\\\n                   & =  (\\frac{1}{\\sqrt{2\\pi \\sigma^2}})^N \\, \\prod_{i=1}^{N} \\exp{- \\frac{(x_i - \\mu)^2}{2\\sigma^2}}\\\\\n                   & =  (\\frac{1}{\\sqrt{2\\pi \\sigma^2}})^N \\, \\exp{- \\frac{\\sum_{i=1}^{N}(x_i - \\mu)^2}{2\\sigma^2}}\\\\\n\\end{aligned}\n$$\n\nTo find the maximum of $L(\\mu, \\sigma)$, we need to search where the gradient is equal to zero:\n\n$$\n\\begin{cases}\n    \\dfrac{\\partial L(\\mu, \\sigma)}{\\partial \\mu} = 0 \\\\\n    \\dfrac{\\partial L(\\mu, \\sigma)}{\\partial \\sigma} = 0 \\\\\n\\end{cases}\n$$\n\nThe likelihood function is complex to differentiate, so we consider its logarithm $l(\\mu, \\sigma) = \\log(L(\\mu, \\sigma))$ which has a maximum for the same value of $(\\mu, \\sigma)$ as the log function is monotonic.\n\n$$\n\\begin{aligned}\n    l(\\mu, \\sigma) & = \\log(L(\\mu, \\sigma)) \\\\\n                   & =  \\log \\left((\\frac{1}{\\sqrt{2\\pi \\sigma^2}})^N \\, \\exp{- \\frac{\\sum_{i=1}^{N}(x_i - \\mu)^2}{2\\sigma^2}} \\right)\\\\\n                   & =  - \\frac{N}{2} \\log (2\\pi \\sigma^2) - \\frac{\\sum_{i=1}^{N}(x_i - \\mu)^2}{2\\sigma^2}\\\\\n\\end{aligned}\n$$\n\n$l(\\mu, \\sigma)$ is called the **log-likelihood** function. The maximum of the log-likelihood function respects:\n\n$$\n\\begin{aligned}\n    \\frac{\\partial l(\\mu, \\sigma)}{\\partial \\mu} & = \\frac{\\sum_{i=1}^{N}(x_i - \\mu)}{\\sigma^2} = 0 \\\\\n    \\frac{\\partial l(\\mu, \\sigma)}{\\partial \\sigma} & = - \\frac{N}{2} \\frac{4 \\pi \\sigma}{2 \\pi \\sigma^2} + \\frac{\\sum_{i=1}^{N}(x_i - \\mu)^2}{\\sigma^3} \\\\\n                                                    & = - \\frac{N}{\\sigma} + \\frac{\\sum_{i=1}^{N}(x_i - \\mu)^2}{\\sigma^3} = 0\\\\\n\\end{aligned}\n$$\n\nWe obtain:\n\n$$\n    \\mu = \\frac{1}{N} \\sum_{i=1}^{N} x_i  \\qquad\\qquad    \\sigma^2 = \\frac{1}{N} \\sum_{i=1}^{N}(x_i - \\mu)^2\n$$\n\nUnsurprisingly, the mean and variance of the normal distribution which best explains the data are the mean and variance of the data...\n\nThe same principle can be applied to estimate the parameters of any distribution: normal, exponential, Bernouilli, Poisson, etc... When a machine learning method has an probabilistic interpretation (i.e. it outputs probabilities), MLE can be used to find its parameters. One can use global optimization like here, or gradient descent to estimate the parameters iteratively.\n\n\n## Soft linear classification : Logistic regression\n\n<div class='embed-container'><iframe src='https://www.youtube.com/embed/_Zc-k9pXVvE' frameborder='0' allowfullscreen></iframe></div>\n\nIn logistic regression, we want to perform a regression, but where the targets $t_i$ are bounded betwen 0 and 1. We can use a logistic function instead of a linear function in order to transform the net activation into an output:\n\n$$\n\\begin{aligned}\n    y = \\sigma(w \\, x + b )  = \\frac{1}{1+\\exp(-w \\, x - b )}\n\\end{aligned}\n$$\n\nLogistic regression can be used in binary classification if we consider $y = \\sigma(w \\, x + b )$ as the probability that the example belongs to the positive class ($t=1$).\n\n$$\n    P(t = 1 | x; w, b) = y ; \\qquad P(t = 0 | x; w, b) = 1 - y\n$$\n\nThe output $t$ therefore comes from a Bernouilli distribution $\\mathcal{B}$ of parameter $p = y = f_{w, b}(x)$. The probability density function (pdf) is:\n\n$$f(t | x; w, b) = y^t \\, (1- y)^{1-t}$$\n\n\nIf we consider our training samples $(x_i, t_i)$ as independently taken from this distribution, our task is to find the parameterized distribution that best explains the data, which means to find the parameters $w$ and $b$ maximizing the **likelihood** that the samples $t$ come from a Bernouilli distribution when $x$, $w$  and $b$ are given. We only need to apply **Maximum Likelihood Estimation** (MLE) on this Bernouilli distribution!\n\nThe likelihood function for logistic regression is :\n\n$$\n\\begin{aligned}\n    L( w, b) &= P( t | x; w,  b )  = \\prod_{i=1}^{N} f(t_i | x_i;  w,  b ) \\\\\n    &= \\prod_{i=1}^{N}  y_i^{t_i} \\, (1- y_i)^{1-t_i}\n\\end{aligned}\n$$\n\nThe likelihood function is quite hard to differentiate, so we take the **log-likelihood** function:\n\n$$\n\\begin{aligned}\n    l( w, b) &= \\log L( w, b) \\\\\n    &=  \\sum_{i=1}^{N} [t_i \\, \\log y_i + (1 - t_i) \\, \\log( 1- y_i)]\\\\\n\\end{aligned}\n$$\n\nor even better: the **negative log-likelihood** which will be minimized using gradient descent:\n\n$$\n    \\mathcal{L}( w, b) =  - \\sum_{i=1}^{N} [t_i \\, \\log y_i + (1 - t_i) \\, \\log( 1- y_i)]\n$$\n\nWe then search for the minimum of the negative log-likelihood function by computing its gradient (here for a single sample):\n\n$$\n\\begin{aligned}\n    \\frac{\\partial \\mathcal{l}_i(w, b)}{\\partial w}\n        &= -\\frac{\\partial}{\\partial w} [ t_i \\, \\log y_i + (1 - t_i) \\, \\log( 1- y_i) ] \\\\\n        &= - t_i \\, \\frac{\\partial}{\\partial w} \\log y_i - (1 - t_i) \\, \\frac{\\partial}{\\partial w}\\log( 1- y_i) \\\\\n        &= - t_i \\, \\frac{\\frac{\\partial}{\\partial w} y_i}{y_i} - (1 - t_i) \\, \\frac{\\frac{\\partial}{\\partial w}( 1- y_i)}{1- y_i} \\\\\n        &= - t_i \\, \\frac{y_i \\, (1 - y_i) \\, x_i}{y_i} + (1 - t_i) \\, \\frac{y_i \\, (1-y_i) \\, x_i}{1 - y_i}\\\\\n        &= - ( t_i - y_i ) \\, x_i\\\\\n\\end{aligned}\n$$\n\nWe obtain the same gradient as the linear perceptron, but with a non-linear output function! Logistic regression is therefore a regression method used for classification. It uses a non-linear transfer function $\\sigma(x)=\\frac{1}{1+\\exp(-x)}$ applied on the net activation:\n\n$$\n    y_i = \\sigma(\\langle \\mathbf{w} \\cdot \\mathbf{x}_i \\rangle + b )\n$$\n\nThe continuous output $y$ is interpreted as the probability of belonging to the positive class.\n\n$$\n   P(t_i = 1 | \\mathbf{x}_i; \\mathbf{w}, b) = y_i ; \\qquad P(t_i = 0 | \\mathbf{x}_i; \\mathbf{w}, b) = 1 - y_i\n$$\n\nWe minimize the **negative log-likelihood** loss function:\n\n$$\n    \\mathcal{L}(\\mathbf{w}, b) =  - \\sum_{i=1}^{N} [t_i \\, \\log y_i + (1 - t_i) \\, \\log( 1- y_i)]\n$$\n\nGradient descent leads to the delta learning rule, using the class as a target and the probability as a prediction:\n\n$$\n    \\begin{cases}\n    \\Delta \\mathbf{w} = \\eta \\, ( t_i - y_i ) \\, \\mathbf{x}_i \\\\\n    \\\\\n    \\Delta b = \\eta \\, ( t_i - y_i ) \\\\\n    \\end{cases}\n$$\n\n```{admonition} Logistic regression\n\n* $\\mathbf{w} = 0 \\qquad b = 0$\n\n* **for** $M$ epochs:\n\n    * **for** each sample $(\\mathbf{x}_i, t_i)$:\n\n        * $y_i =  \\sigma( \\langle \\mathbf{w} \\cdot \\mathbf{x}_i \\rangle  + b)$\n\n        * $\\Delta \\mathbf{w} = \\eta \\, (t_i - y_i) \\, \\mathbf{x}_i$\n\n        * $\\Delta b = \\eta \\, (t_i - y_i)$\nLogistic regression works just like linear classification, except in the way the prediction is done. To know to which class \\(\\mathbf{x}_i\\) belongs, simply draw a random number between 0 and 1:\n\nif it is smaller than \\(y_i\\) (probability \\(y_i\\)), it belongs to the positive class.\nif it is bigger than \\(y_i\\) (probability \\(1-y_i\\)), it belongs to the negative class.\n\nAlternatively, you can put a hard limit at 0.5:\n\nif \\(y_i > 0.5\\) then the class is positive.\nif \\(y_i < 0.5\\) then the class is negative.\n\n\n\n\n```{figure} ../img/logisticregression-animation.gif\n\n\n\n\nwidth: 80%\n\n\n\nLogistic regression for soft classification. The confidence scores tells how certain the classification is. ```\nLogistic regression also provides a confidence score: the closer \\(y\\) is from 0 or 1, the more confident we can be that the classification is correct. This is particularly important in safety critical applications: if you detect the positive class but with a confidence of 0.51, you should perhaps not trust the prediction. If the confidence score is 0.99, you can probably trust the prediction."
  },
  {
    "objectID": "notes/2.4-Multiclassification.html",
    "href": "notes/2.4-Multiclassification.html",
    "title": "7  Multi-class classification",
    "section": "",
    "text": "Slides: pdf"
  },
  {
    "objectID": "notes/2.4-Multiclassification.html#multi-class-classification",
    "href": "notes/2.4-Multiclassification.html#multi-class-classification",
    "title": "7  Multi-class classification",
    "section": "7.1 Multi-class classification",
    "text": "7.1 Multi-class classification\nCan we perform multi-class classification using the previous methods when \\(t \\in \\{A, B, C\\}\\) instead of \\(t = +1\\) or \\(-1\\)? There are two main solutions:\n\nOne-vs-All (or One-vs-the-rest): one trains simultaneously a binary (linear) classifier for each class. The examples belonging to this class form the positive class, all others are the negative class:\n\nA vs. B and C\nB vs. A and C\nC vs. A and B\n\n\nIf multiple classes are predicted for a single example, ones needs a confidence level for each classifier saying how sure it is of its prediction.\n\nOne-vs-One: one trains a classifier for each pair of class:\n\nA vs. B\nB vs. C\nC vs. A\n\n\nA majority vote is then performed to find the correct class.\n\n\n\n```{figure} ../img/pixelspace.jpg\n\n\n\n\nwidth: 80%\n\n\n\nExample of One-vs-All classification: one binary classifier per class. Source http://cs231n.github.io/linear-classify\n\n\n## Softmax linear classifier\n\nSuppose we have $C$ classes (dog vs. cat vs. ship vs...). The One-vs-All scheme involves $C$ binary classifiers $(\\mathbf{w}_i, b_i)$, each with a weight vector and a bias, working on the same input $\\mathbf{x}$.\n\n$$y_i = f(\\langle \\mathbf{w}_i \\cdot \\mathbf{x} \\rangle + b_i)$$\n\nPutting all neurons together, we obtain a **linear perceptron** similar to multiple linear regression:\n\n$$\n    \\mathbf{y} = f(W \\times \\mathbf{x} + \\mathbf{b})\n$$\n\nThe $C$ weight vectors form a $d\\times C$ **weight matrix** $W$, the biases form a vector $\\mathbf{b}$.\n\n```{figure} ../img/imagemap.jpg\n---\nwidth: 100%\n---\nLinear perceptron for images. The output is the logit score. Source <http://cs231n.github.io/linear-classify>\nThe net activations form a vector \\(\\mathbf{z}\\):\n\\[\n    \\mathbf{z} = f_{W, \\mathbf{b}}(\\mathbf{x}) = W \\times \\mathbf{x} + \\mathbf{b}\n\\]\nEach element \\(z_j\\) of the vector \\(\\mathbf{z}\\) is called the logit score of the class: the higher the score, the more likely the input belongs to this class. The logit scores are not probabilities, as they can be negative and do not sum to 1.\n\n7.1.1 One-hot encoding\nHow do we represent the ground truth \\(\\mathbf{t}\\) for each neuron? The target vector \\(\\mathbf{t}\\) is represented using one-hot encoding. The binary vector has one element per class: only one element is 1, the others are 0. Example:\n\\[\n    \\mathbf{t} = [\\text{cat}, \\text{dog}, \\text{ship}, \\text{house}, \\text{car}] = [0, 1, 0, 0, 0]\n\\]\nThe labels can be seen as a probability distribution over the training set, in this case a multinomial distribution (a dice with \\(C\\) sides). For a given image \\(\\mathbf{x}\\) (e.g. a picture of a dog), the conditional pmf is defined by the one-hot encoded vector \\(\\mathbf{t}\\):\n\\[P(\\mathbf{t} | \\mathbf{x}) = [P(\\text{cat}| \\mathbf{x}), P(\\text{dog}| \\mathbf{x}), P(\\text{ship}| \\mathbf{x}), P(\\text{house}| \\mathbf{x}), P(\\text{car}| \\mathbf{x})] = [0, 1, 0, 0, 0]\\]\n\n\n\n```{figure} ../img/softmax-transformation.png\n\n\n\n\nwidth: 100%\n\n\n\nThe logit scores \\(\\mathbf{z}\\) cannot be compared to the targets \\(\\mathbf{t}\\): we need to transform them into a probability distribution \\(\\mathbf{y}\\).\n\nWe need to transform the logit score $\\mathbf{z}$ into a **probability distribution** $P(\\mathbf{y} | \\mathbf{x})$ that should be as close as possible from $P(\\mathbf{t} | \\mathbf{x})$.\n\n### Softmax activation\n\nThe **softmax** operator makes sure that the sum of the outputs $\\mathbf{y} = \\{y_i\\}$ over all classes is 1.\n\n$$\n    y_j = P(\\text{class = j} | \\mathbf{x}) = \\mathcal{S}(z_j) = \\frac{\\exp(z_j)}{\\sum_k \\exp(z_k)}\n$$\n\n```{figure} ../img/softmax-comp.png\n---\nwidth: 100%\n---\nSoftmax activation transforms the logit score into a probability distribution. Source <http://cs231n.github.io/linear-classify>\nThe higher \\(z_j\\), the higher the probability that the example belongs to class \\(j\\). This is very similar to logistic regression for soft classification, except that we have multiple classes.\n\n\n7.1.2 Cross-entropy loss function\nWe cannot use the mse as a loss function, as the softmax function would be hard to differentiate:\n\\[\n    \\text{mse}(W, \\mathbf{b}) = \\sum_j (t_{j} - \\frac{\\exp(z_j)}{\\sum_k \\exp(z_k)})^2\n\\]\nWe actually want to minimize the statistical distance netween two distributions:\n\nThe model outputs a multinomial probability distribution \\(\\mathbf{y}\\) for an input \\(\\mathbf{x}\\): \\(P(\\mathbf{y} | \\mathbf{x}; W, \\mathbf{b})\\).\nThe one-hot encoded classes also come from a multinomial probability distribution \\(P(\\mathbf{t} | \\mathbf{x})\\).\n\nWe search which parameters \\((W, \\mathbf{b})\\) make the two distributions \\(P(\\mathbf{y} | \\mathbf{x}; W, \\mathbf{b})\\) and \\(P(\\mathbf{t} | \\mathbf{x})\\) close. The training data \\(\\{\\mathbf{x}_i, \\mathbf{t}_i\\}\\) represents samples from \\(P(\\mathbf{t} | \\mathbf{x})\\). \\(P(\\mathbf{y} | \\mathbf{x}; W, \\mathbf{b})\\) is a good model of the data when the two distributions are close, i.e. when the negative log-likelihood of each sample under the model is small.\n\n\n\n```{figure} ../img/crossentropy.svg\n\n\n\n\nwidth: 100%\n\n\n\nCross-entropy between two distributions \\(X\\) and \\(Y\\): are samples of \\(X\\) likely under \\(Y\\)?\n\nFor an input $\\mathbf{x}$, we minimize the **cross-entropy** between the target distribution and the predicted outputs:\n\n$$\n    \\mathcal{l}(W, \\mathbf{b}) = \\mathcal{H}(\\mathbf{t} | \\mathbf{x}, \\mathbf{y} | \\mathbf{x}) =  \\mathbb{E}_{t \\sim P(\\mathbf{t} | \\mathbf{x})} [ - \\log P(\\mathbf{y} = t | \\mathbf{x})]\n$$\n\nThe cross-entropy samples from $\\mathbf{t} | \\mathbf{x}$:\n\n$$\n    \\mathcal{l}(W, \\mathbf{b}) = \\mathcal{H}(\\mathbf{t} | \\mathbf{x}, \\mathbf{y} | \\mathbf{x}) =  \\mathbb{E}_{t \\sim P(\\mathbf{t} | \\mathbf{x})} [ - \\log P(\\mathbf{y} = t | \\mathbf{x})]\n$$\n\nFor a given input $\\mathbf{x}$, $\\mathbf{t}$ is non-zero only for the correct class $t^*$, as $\\mathbf{t}$ is a one-hot encoded vector $[0, 1, 0, 0, 0]$:\n\n$$\n    \\mathcal{l}(W, \\mathbf{b}) =  - \\log P(\\mathbf{y} = t^* | \\mathbf{x})\n$$\n\nIf we note $j^*$ the index of the correct class $t^*$, the cross entropy is simply:\n\n$$\n    \\mathcal{l}(W, \\mathbf{b}) =  - \\log y_{j^*}\n$$\n\nAs only one element of $\\mathbf{t}$ is non-zero, the cross-entropy is the same as the **negative log-likelihood** of the prediction for the true label:\n\n$$\n    \\mathcal{l}(W, \\mathbf{b}) =  - \\log y_{j^*}\n$$\n\nThe minimum of $- \\log y$ is obtained when $y =1$: We want to classifier to output a probability 1 for the true label. Because of the softmax activation function, the probability for the other classes should become closer from 0.\n\n$$\n    y_j = P(\\text{class = j}) = \\frac{\\exp(z_j)}{\\sum_k \\exp(z_k)}\n$$\n\nMinimizing the cross-entropy / negative log-likelihood pushes the output distribution $\\mathbf{y} | \\mathbf{x}$ to be as close as possible to the target distribution $\\mathbf{t} | \\mathbf{x}$.\n\n```{figure} ../img/crossentropy-animation.gif\n---\nwidth: 100%\n---\nMinimizing the cross-entropy between $\\mathbf{t} | \\mathbf{x}$ and $\\mathbf{y} | \\mathbf{x}$ makes them similar.\nAs \\(\\mathbf{t}\\) is a binary vector \\([0, 1, 0, 0, 0]\\), the cross-entropy / negative log-likelihood can also be noted as the dot product between \\(\\mathbf{t}\\) and \\(\\log \\mathbf{y}\\):\n\\[\n    \\mathcal{l}(W, \\mathbf{b}) = - \\langle \\mathbf{t} \\cdot \\log \\mathbf{y} \\rangle = - \\sum_{j=1}^C t_j \\, \\log y_j =  - \\log y_{j^*}\n\\]\nThe cross-entropy loss function is then the expectation over the training set of the individual cross-entropies:\n\\[\n    \\mathcal{L}(W, \\mathbf{b}) = \\mathbb{E}_{\\mathbf{x}, \\mathbf{t} \\sim \\mathcal{D}} [- \\langle \\mathbf{t} \\cdot \\log \\mathbf{y} \\rangle ] \\approx \\frac{1}{N} \\sum_{i=1}^N - \\langle \\mathbf{t}_i \\cdot \\log \\mathbf{y}_i \\rangle\n\\]\nThe nice thing with the cross-entropy loss function, when used on a softmax activation function, is that the partial derivative w.r.t the logit score \\(\\mathbf{z}\\) is simple:\n\\[\n\\begin{split}\n    \\frac{\\partial {l}(W, \\mathbf{b})}{\\partial z_i} & = - \\sum_j \\frac{\\partial}{\\partial z_i}  t_j \\log(y_j)=\n- \\sum_j t_j \\frac{\\partial \\log(y_j)}{\\partial z_i} = - \\sum_j t_j \\frac{1}{y_j} \\frac{\\partial y_j}{\\partial z_i} \\\\\n& = - \\frac{t_i}{y_i} \\frac{\\partial y_i}{\\partial z_i} - \\sum_{j \\neq i}^C \\frac{t_j}{y_j} \\frac{\\partial y_j}{\\partial z_i}\n= - \\frac{t_i}{y_i} y_i (1-y_i) - \\sum_{j \\neq i}^C \\frac{t_j}{y_i} (-y_j \\, y_i) \\\\\n& = - t_i + t_i \\, y_i + \\sum_{j \\neq i}^C t_j \\, y_i = - t_i + \\sum_{j = 1}^C t_j y_i\n= -t_i + y_i \\sum_{j = 1}^C t_j \\\\\n& = - (t_i - y_i)\n\\end{split}\n\\]\ni.e. the same as with the mse in linear regression! Refer https://peterroelants.github.io/posts/cross-entropy-softmax/ for more explanations on the proof.\nWhen differentiating a softmax probability $y_j = \\dfrac{\\exp(z_j)}{\\sum_k \\exp(z_k)}$ w.r.t a logit score $z_i$, i.e. $\\dfrac{\\partial y_j}{\\partial z_i}$, we need to consider two cases:\n\n* If $i=j$, $\\exp(z_i)$ appears both at the numerator and denominator of $\\frac{\\exp(z_i)}{\\sum_k \\exp(z_k)}$. The product rule $(f\\times g)' = f'\\, g + f \\, g'$ gives us:\n\n$$\\begin{aligned}\n\\dfrac{\\partial \\log(y_i)}{\\partial z_i} &= \\dfrac{\\exp(z_i)}{\\sum_k \\exp(z_k)} + \\exp(z_i) \\, \\dfrac{- \\exp(z_i)}{(\\sum_k \\exp(z_k))^2} \\\\\n&= \\dfrac{\\exp(z_i)  \\, \\sum_k \\exp(z_k) - \\exp(z_i)^2}{(\\sum_k \\exp(z_k))^2} \\\\\n&= \\dfrac{\\exp(z_i)}{\\sum_k \\exp(z_k)} \\, (1- \\dfrac{\\exp(z_i)}{\\sum_k \\exp(z_k)})\\\\\n&= y_i \\, (1 - y_i)\\\\\n\\end{aligned}\n$$\n\nThis is similar to the derivative of the logistic function.\n\n* If $i \\neq j$, $z_i$ only appears at the denominator, so we only need the chain rule:\n\n$$\\begin{aligned}\n\\dfrac{\\partial \\log(y_j)}{\\partial z_i} &= - \\exp(z_j) \\, \\dfrac{\\exp(z_i)}{(\\sum_k \\exp(z_k))^2} \\\\\n&= - \\dfrac{\\exp(z_i)}{\\sum_k \\exp(z_k)} \\, \\dfrac{\\exp(z_j)}{\\sum_k \\exp(z_k)} \\\\\n&= - y_i \\, y_j \\\\\n\\end{aligned}\n$$\n\nUsing the vector notation, we get:\n\\[\n    \\frac{\\partial \\mathcal{l}(W, \\mathbf{b})}{\\partial \\mathbf{z}} =  -  (\\mathbf{t} - \\mathbf{y} )\n\\]\nAs:\n\\[\n    \\mathbf{z} = W \\times \\mathbf{x} + \\mathbf{b}\n\\]\nwe can obtain the partial derivatives:\n\\[\n\\begin{cases}\n    \\dfrac{\\partial \\mathcal{l}(W, \\mathbf{b})}{\\partial W} = \\dfrac{\\partial \\mathcal{l}(W, \\mathbf{b})}{\\partial \\mathbf{z}} \\times \\dfrac{\\partial \\mathbf{z}}{\\partial W} = - (\\mathbf{t} - \\mathbf{y} ) \\times \\mathbf{x}^T \\\\\n    \\\\\n    \\dfrac{\\partial \\mathcal{l}(W, \\mathbf{b})}{\\partial \\mathbf{b}} = \\dfrac{\\partial \\mathcal{l}(W, \\mathbf{b})}{\\partial \\mathbf{z}} \\times \\dfrac{\\partial \\mathbf{z}}{\\partial \\mathbf{b}} = - (\\mathbf{t} - \\mathbf{y} ) \\\\\n\\end{cases}\n\\]\nSo gradient descent leads to the delta learning rule:\n\\[\n\\begin{cases}\n    \\Delta W = \\eta \\,  (\\mathbf{t} - \\mathbf{y} ) \\times \\mathbf{x}^T \\\\\n    \\\\\n    \\Delta \\mathbf{b} = \\eta \\,  (\\mathbf{t} - \\mathbf{y} ) \\\\\n\\end{cases}\n\\]\n```{admonition} Softmax linear classifier\n\n\nWe first compute the logit scores \\(\\mathbf{z}\\) using a linear layer:\n\n\\[\n    \\mathbf{z} = W \\times \\mathbf{x} + \\mathbf{b}\n\\]\n\nWe turn them into probabilities \\(\\mathbf{y}\\) using the softmax activation function:\n\n\\[\n    y_j = \\frac{\\exp(z_j)}{\\sum_k \\exp(z_k)}\n\\]\n\nWe minimize the cross-entropy / negative log-likelihood on the training set:\n\n\\[\n    \\mathcal{L}(W, \\mathbf{b}) = \\mathbb{E}_{\\mathbf{x}, \\mathbf{t} \\sim \\mathcal{D}} [ - \\langle \\mathbf{t} \\cdot \\log \\mathbf{y} \\rangle]\n\\]\nwhich simplifies into the delta learning rule:\n\\[\n\\begin{cases}\n    \\Delta W = \\eta \\,  (\\mathbf{t} - \\mathbf{y} ) \\times \\mathbf{x}^T \\\\\n    \\\\\n    \\Delta \\mathbf{b} = \\eta \\,  (\\mathbf{t} - \\mathbf{y} ) \\\\\n\\end{cases}\n\\] ```\n\n\n7.1.3 Comparison of linear classification and regression\nClassification and regression differ in the nature of their outputs: in classification they are discrete, in regression they are continuous values. However, when trying to minimize the mismatch between a model \\(\\mathbf{y}\\) and the real data \\(\\mathbf{t}\\), we have found the same delta learning rule:\n\\[\n\\begin{cases}\n    \\Delta W = \\eta \\,  (\\mathbf{t} - \\mathbf{y} ) \\times \\mathbf{x}^T \\\\\n    \\\\\n    \\Delta \\mathbf{b} = \\eta \\,  (\\mathbf{t} - \\mathbf{y} ) \\\\\n\\end{cases}\n\\]\nRegression and classification are in the end the same problem for us. The only things that needs to be adapted is the activation function of the output and the loss function:\n\nFor regression, we use regular activation functions and the mean square error (mse):\n\\[\n  \\mathcal{L}(W, \\mathbf{b}) = \\mathbb{E}_{\\mathbf{x}, \\mathbf{t} \\in \\mathcal{D}} [ ||\\mathbf{t} - \\mathbf{y}||^2 ]\n  \\]\nFor classification, we use the softmax activation function and the cross-entropy (negative log-likelihood) loss function:\n\\[\\mathcal{L}(W, \\mathbf{b}) = \\mathbb{E}_{\\mathbf{x}, \\mathbf{t} \\sim \\mathcal{D}} [ - \\langle \\mathbf{t} \\cdot \\log \\mathbf{y} \\rangle]\\]"
  },
  {
    "objectID": "notes/2.4-Multiclassification.html#multi-label-classification",
    "href": "notes/2.4-Multiclassification.html#multi-label-classification",
    "title": "7  Multi-class classification",
    "section": "7.2 Multi-label classification",
    "text": "7.2 Multi-label classification\nWhat if there is more than one label on the image? The target vector \\(\\mathbf{t}\\) does not represent a probability distribution anymore:\n\\[\n    \\mathbf{t} = [\\text{cat}, \\text{dog}, \\text{ship}, \\text{house}, \\text{car}] = [1, 1, 0, 0, 0]\n\\]\nNormalizing the vector does not help: it is not a dog or a cat, it is a dog and a cat.\n\\[\n    \\mathbf{t} = [\\text{cat}, \\text{dog}, \\text{ship}, \\text{house}, \\text{car}] = [0.5, 0.5, 0, 0, 0]\n\\]\nFor multi-label classification, we can simply use the logistic activation function for the output neurons:\n\\[\n    \\mathbf{y} = \\sigma(W \\times \\mathbf{x} + \\mathbf{b})\n\\]\nThe outputs are between 0 and 1, but they do not sum to one. Each output neuron performs a logistic regression for soft classification on their class:\n\\[y_j = P(\\text{class} = j | \\mathbf{x})\\]\nEach output neuron \\(y_j\\) has a binary target \\(t_j\\) (one-vs-the-rest) and has to minimize the negative log-likelihood:\n\\[\n\\mathcal{l}_j(W, \\mathbf{b}) =  - t_j \\, \\log y_j + (1 - t_j) \\, \\log( 1- y_j)\n\\]\nThe binary cross-entropy loss for the whole network is the sum of the negative log-likelihood for each class:\n\\[\n\\mathcal{L}(W, \\mathbf{b}) =  \\mathbb{E}_{\\mathcal{D}} [- \\sum_{j=1}^C t_j \\, \\log y_j + (1 - t_j) \\, \\log( 1- y_j)]\n\\]"
  },
  {
    "objectID": "notes/2.5-LearningTheory.html",
    "href": "notes/2.5-LearningTheory.html",
    "title": "8  Learning theory",
    "section": "",
    "text": "Slides: pdf"
  },
  {
    "objectID": "notes/2.5-LearningTheory.html#error-measurements",
    "href": "notes/2.5-LearningTheory.html#error-measurements",
    "title": "8  Learning theory",
    "section": "8.1 Error measurements",
    "text": "8.1 Error measurements\n\n\n\n\nThe training error is the error made on the training set. It is easy to measure for classification as the number of misclassified examples divided by the total number of examples.\n\\[\n    \\epsilon_\\mathcal{D} = \\frac{\\text{# misclassifications}}{\\text{# examples}}\n\\]\nThe training error is totally irrelevant on usage: reading the training set has a training error of 0%. What matters is the generalization error, which is the error that will be made on new examples (not used during learning). It is much harder to measure (potentially infinite number of new examples, what is the correct answer?). The generalization error is often approximated by the empirical error: one keeps a number of training examples out of the learning phase and one tests the performance on them.\nClassification errors can also depend on the class:\n\nFalse Positive errors (FP, false alarm, type I) is when the classifier predicts a positive class for a negative example.\nFalse Negative errors (FN, miss, type II) is when the classifier predicts a negative class for a positive example.\nTrue Positive (TP) and True Negative (TN) are correctly classified examples.\n\nIs it better to fail to detect a cancer (FN) or to incorrectly predict one (FP)?\nSome other metrics:\n\nAccuracy (1 - error)\n\n\\[\n    \\text{acc} = \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{FP} + \\text{TN} + \\text{FN}}\n\\]\n\nRecall (hit rate, sensitivity)\n\n\\[\n    R = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\n\\]\n\nPrecision (specificity)\n\n\\[\n    P = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}\n\\]\n\nF1 score = harmonic mean of precision and recall\n\n\\[\n    \\text{F1} = \\frac{2\\, P \\, R}{P + R}\n\\]\nFor multiclass classification problems, the confusion matrix tells how many examples are correctly classified and where confusion happens. One axis is the predicted class, the other is the target class. Each element of the matrix tells how many examples are classified or misclassified. The matrix should be as diagonal as possible.\n\n\n\n```{figure} ../img/confusionmatrix.png\n\n\n\n\nwidth: 50%\n\n\n\nConfusion matrix.\n\n\n\n\n````{note}\nUsing `scikit-learn`:\n\n```python\nfrom sklearn.metrics import confusion_matrix\n\nm = confusion_matrix(t, y)\n````"
  },
  {
    "objectID": "notes/2.5-LearningTheory.html#cross-validation",
    "href": "notes/2.5-LearningTheory.html#cross-validation",
    "title": "8  Learning theory",
    "section": "8.2 Cross-validation",
    "text": "8.2 Cross-validation\n\n\n\n```{figure} ../img/underfitting-overfitting.png\n\n\n\n\nwidth: 100%\n\n\n\nOverfitting in regression.\n\n```{figure} ../img/underfitting-overfitting-classification.png\n---\nwidth: 100%\n---\nOverfitting in classification.\nIn classification too, cross-validation has to be used to prevent overfitting. The classifier is trained on the training set and tested on the test set. Optionally, a third validation set can be used to track overfitting during training.\n\n\n\n```{figure} ../img/validationset.svg\n\n\n\n\nwidth: 80%\n\n\n\nTraining, validation and test sets. Source: https://developers.google.com/machine-learning/crash-course/validation/another-partition\n\n```{note}\nBeware: the test data must come from the same distribution as the training data, otherwise it makes no sense."
  },
  {
    "objectID": "notes/2.5-LearningTheory.html#vapnik-chervonenkis-dimension",
    "href": "notes/2.5-LearningTheory.html#vapnik-chervonenkis-dimension",
    "title": "8  Learning theory",
    "section": "8.3 Vapnik-Chervonenkis dimension",
    "text": "8.3 Vapnik-Chervonenkis dimension\n\n\n\n\nHow many data examples can be correctly classified by a linear model in \\(\\Re^d\\)? In \\(\\Re^2\\), all dichotomies of three non-aligned examples can be correctly classified by a linear model (\\(y = w_1 \\, x_1 + w_2 \\, x_2 + b\\)).\n\n\n\n```{figure} ../img/vc4.png\n\n\n\n\nwidth: 60%\n\n\n\nA linear classifier in 2D can classify any configuration of three points.\n\nHowever, there exists sets of four examples in $\\Re^2$ which can NOT be correctly classified by a linear model, i.e. they are not linearly separable.\n\n```{figure} ../img/vc6.png\n---\nwidth: 60%\n---\nThere exists configurations of four points in 2D that cannot be linearly classified.\nThe XOR function in \\(\\Re^2\\) is for example not linearly separable, i.e. the Perceptron algorithm can not converge.\nThe probability that a set of 3 (non-aligned) points in \\(\\Re^2\\) is linearly separable is 1, but the probability that a set of four points is linearly separable is smaller than 1 (but not zero). When a class of hypotheses \\(\\mathcal{H}\\) can correctly classify all points of a training set \\(\\mathcal{D}\\), we say that \\(\\mathcal{H}\\) shatters \\(\\mathcal{D}\\).\nThe Vapnik-Chervonenkis dimension \\(\\text{VC}_\\text{dim} (\\mathcal{H})\\) of an hypothesis class \\(\\mathcal{H}\\) is defined as the maximal number of training examples that \\(\\mathcal{H}\\) can shatter. We saw that in \\(\\Re^2\\), this dimension is 3:\n\\[\\text{VC}_\\text{dim} (\\text{Linear}(\\Re^2) ) = 3\\]\nThis can be generalized to linear classifiers in \\(\\Re^d\\):\n\\[\\text{VC}_\\text{dim} (\\text{Linear}(\\Re^d) ) = d+1\\]\nThis corresponds to the number of free parameters of the linear classifier: \\(d\\) parameters for the weight vector, 1 for the bias. Given any set of \\((d+1)\\) examples in \\(\\Re^d\\), there exists a linear classifier able to classify them perfectly. For other types of (non-linear) hypotheses, the VC dimension is generally proportional to the number of free parameters, but regularization reduces the VC dimension of the classifier.\n```{admonition} Vapnik-Chervonenkis theorem\nThe generalization error \\(\\epsilon(h)\\) of an hypothesis \\(h\\) taken from a class \\(\\mathcal{H}\\) of finite VC dimension and trained on \\(N\\) samples of \\(\\mathcal{S}\\) is bounded by the sum of the training error \\(\\hat{\\epsilon}_{\\mathcal{S}}(h)\\) and the VC complexity term:\n\\[\n    \\epsilon(h) \\leq \\hat{\\epsilon}_{\\mathcal{S}}(h) + \\sqrt{\\frac{\\text{VC}_\\text{dim} (\\mathcal{H}) \\cdot (1 + \\log(\\frac{2\\cdot N}{\\text{VC}_\\text{dim} (\\mathcal{H})})) - \\log(\\frac{\\delta}{4})}{N}}\n\\]\nwith probability \\(1-\\delta\\), if \\(\\text{VC}_\\text{dim} (\\mathcal{H}) << N\\).\nVapnik, Vladimir (2000). The nature of statistical learning theory. Springer.\n\n## Structural risk minimization\n\n```{figure} ../img/srm.png\n---\nwidth: 60%\n---\nStructural risk minimization.\nThe generalization error increases with the VC dimension, while the training error decreases. Structural risk minimization is an alternative method to cross-validation. The VC dimensions of various classes of hypothesis are already known (~ number of free parameters). The VC bounds tells how many training samples are needed by a given hypothesis class in order to obtain a satisfying generalization error.\n\\[\\epsilon(h) \\leq \\hat{\\epsilon}_{\\mathcal{S}(h)} + \\sqrt{\\frac{\\text{VC}_\\text{dim} (\\mathcal{H}) \\cdot (1 + \\log(\\frac{2\\cdot N}{\\text{VC}_\\text{dim} (\\mathcal{H})})) - \\log(\\frac{\\delta}{4})}{N}}\\]\nThe more complex the model, the more training data you will need to get a good generalization error!\nRule of thumb:\n\\[\n        \\epsilon(h) \\approx \\frac{\\text{VC}_\\text{dim} (\\mathcal{H})}{N}\n\\]\nA learning algorithm should only try to minimize the training error, as the VC complexity term only depends on the model. This term is only an upper bound: most of the time, the real bound is usually 100 times smaller.\nThe VC dimension of linear classifiers in \\(\\Re^d\\) is:\n\\[\\text{VC}_\\text{dim} (\\text{Linear}(\\Re^d) ) = d+1\\]\nGiven any set of \\((d+1)\\) examples in \\(\\Re^d\\), there exists a linear classifier able to classify them perfectly. For \\(N >> d\\) the probability of having training errors becomes huge (the data is generally not linearly separable).\n\nIf we project the input data onto a space with sufficiently high dimensions, it becomes then possible to find a linear classifier on this projection space that is able to classify the data!\n\nHowever, if the space has too many dimensions, the VC dimension will increase and the generalization error will increase. This is the basic principle of all non-linear ML methods: multi-layer perceptron, radial-basis-function networks, support-vector machines…"
  },
  {
    "objectID": "notes/2.5-LearningTheory.html#feature-space",
    "href": "notes/2.5-LearningTheory.html#feature-space",
    "title": "8  Learning theory",
    "section": "8.4 Feature space",
    "text": "8.4 Feature space\n\n\n\n\n{admonition} Cover's theorem on the separability of patterns (1965) A complex pattern-classification problem, cast in a high dimensional space non-linearly, is more likely to be linearly separable than in a low-dimensional space, provided that the space is not densely populated.*\n\n\n\n```{figure} ../img/featurespace.png\n\n\n\n\nwidth: 60%\n\n\n\nProjection to a feature space.\n\nThe highly dimensional space where the input data is projected is called the **feature space** When the number of dimensions of the feature space increases, the training error decreases (the pattern is more likely linearly separable) but the generalization error increases (the VC dimension increases).\n\n### Polynomial features\n\nFor the polynomial regression of order $p$:\n\n$$y = f_{\\mathbf{w}, b}(x) = w_1 \\, x + w_2 \\, x^2 + \\ldots + w_p \\, x^p + b$$\n\nthe vector $\\mathbf{x} = \\begin{bmatrix} x \\\\ x^2 \\\\ \\ldots \\\\ x^p \\end{bmatrix}$ defines a feature space for  the input $x$. The elements of the feature space are called **polynomial features**. We can define polynomial features of more than one variable, e.g. $x^2 \\, y$, $x^3 \\, y^4$, etc. We then apply multiple **linear** regression (MLR) on the polynomial feature space to find the parameters:\n\n$$\\Delta \\mathbf{w} =  \\eta \\, (t - y) \\, \\mathbf{x}$$\n\n### Radial-basis function networks\n\n```{figure} ../img/rbf.png\n---\nwidth: 80%\n---\nRadial basis function network. Source: <https://mccormickml.com/2013/08/15/radial-basis-function-network-rbfn-tutorial/>\nRadial-basis function (RBF) networks samples a subset of \\(K\\) training examples and form the feature space using a gaussian kernel:\n\\[\\phi(\\mathbf{x}) = \\begin{bmatrix} \\varphi(\\mathbf{x} - \\mathbf{x}_1) \\\\ \\varphi(\\mathbf{x} - \\mathbf{x}_2) \\\\ \\ldots \\\\ \\varphi(\\mathbf{x} - \\mathbf{x}_K) \\end{bmatrix}\\]\nwith \\(\\varphi(\\mathbf{x} - \\mathbf{x}_i) = \\exp - \\beta \\, ||\\mathbf{x} - \\mathbf{x}_i||^2\\) decreasing with the distance between the vectors.\n\n\n\n```{figure} ../img/rbf2.png\n\n\n\n\nwidth: 80%\n\n\n\nSelection of protypes among the training data. Source: https://mccormickml.com/2013/08/15/radial-basis-function-network-rbfn-tutorial/\n\n```{figure} ../img/rbf4.png\n---\nwidth: 80%\n---\nGaussian kernel. Source: <https://mccormickml.com/2013/08/15/radial-basis-function-network-rbfn-tutorial/>\nBy applying a linear classification algorithm on the RBF feature space:\n\\[\\mathbf{y} = f(W \\times \\phi(\\mathbf{x}) + \\mathbf{b})\\]\nwe obtain a smooth non-linear partition of the input space. The width of the gaussian kernel allows distance-based generalization.\n\n\n\n```{figure} ../img/rbf3.png\n\n\n\n\nwidth: 80%\n\n\n\nRBF networks learn linearly smooth transitions between the training examples. Source: https://mccormickml.com/2013/08/15/radial-basis-function-network-rbfn-tutorial/\n\n### Kernel perceptron\n\nWhat happens during online Perceptron learning?\n\n```{admonition} Primal form of the online Perceptron algorithm\n\n* **for** $M$ epochs:\n\n    * **for** each sample $(\\mathbf{x}_i, t_i)$:\n\n        * $y_i =  \\text{sign}( \\langle \\mathbf{w} \\cdot \\mathbf{x}_i \\rangle + b)$\n\n        * $\\Delta \\mathbf{w} = \\eta \\, (t_i - y_i) \\, \\mathbf{x}_i$\n\n        * $\\Delta b = \\eta \\, (t_i - y_i)$\nIf an example \\(\\mathbf{x}_i\\) is correctly classified (\\(y_i = t_i\\)), the weight vector does not change.\n\\[\\mathbf{w} \\leftarrow \\mathbf{w}\\]\nIf an example \\(\\mathbf{x}_i\\) is miscorrectly classified (\\(y_i \\neq t_i\\)), the weight vector is increased from \\(t_i \\, \\mathbf{x}_i\\).\n\\[\\mathbf{w} \\leftarrow \\mathbf{w} + 2 \\, \\eta \\, t_i \\, \\mathbf{x}_i\\]\nIf you initialize the weight vector to 0, its final value will therefore be a linear combination of the input samples:\n\\[\\mathbf{w} = \\sum_{i=1}^N \\alpha_i \\, t_i \\, \\mathbf{x}_i\\]\nThe coefficients \\(\\alpha_i\\) represent the embedding strength of each example, i.e. how often they were misclassified.\nWith \\(\\mathbf{w} = \\sum_{i=1}^N \\alpha_i \\, t_i \\, \\mathbf{x}_i\\), the prediction for an input \\(\\mathbf{x}\\) only depends on the training samples and their \\(\\alpha_i\\) value:\n\\[y =  \\text{sign}( \\sum_{i=1}^N \\alpha_i \\, t_i \\, \\langle \\mathbf{x}_i \\cdot \\mathbf{x} \\rangle)\\]\nTo make a prediction \\(y\\), we need the dot product between the input \\(\\mathbf{x}\\) and all training examples \\(\\mathbf{x}_i\\). We ignore the bias here, but it can be added back.\n```{admonition} Dual form of the online Perceptron algorithm\n\nfor \\(M\\) epochs:\n\nfor each sample \\((\\mathbf{x}_i, t_i)\\):\n\n\\(y_i = \\text{sign}( \\sum_{j=1}^N \\alpha_j \\, t_j \\, \\langle \\mathbf{x}_j \\cdot \\mathbf{x}_i \\rangle)\\)\nif \\(y_i \\neq t_i\\) :\n\n\\(\\alpha_i \\leftarrow \\alpha_i + 1\\) ```\n\n\n\n\nThis dual form of the Perceptron algorithm is strictly equivalent to its primal form. It needs one parameter \\(\\alpha_i\\) per training example instead of a weight vector (\\(N >> d\\)), but relies on dot products between vectors.\nWhy is it interesting to have an algorithm relying on dot products? You can project the inputs \\(\\mathbf{x}\\) to a feature space \\(\\phi(\\mathbf{x})\\) and apply the same algorithm:\n\\[y =  \\text{sign}( \\sum_{i=1}^N \\alpha_i \\, t_i \\, \\langle \\phi(\\mathbf{x}_i) \\cdot \\phi(\\mathbf{x}) \\rangle)\\]\nBut you do not need to compute the dot product in the feature space, all you need to know is its result.\n\\[K(\\mathbf{x}_i, \\mathbf{x}) = \\langle \\phi(\\mathbf{x}_i) \\cdot \\phi(\\mathbf{x}) \\rangle\\]\n\nKernel trick: A kernel \\(K(\\mathbf{x}, \\mathbf{z})\\) allows to compute the dot product between the feature space representation of two vectors without ever computing these representations!\n\nLet’s consider the quadratic kernel in \\(\\Re^3\\):\n\\[\\begin{eqnarray*}\n\\forall (\\mathbf{x}, \\mathbf{z}) \\in \\Re^3 \\times \\Re^3 && \\\\\n&& \\\\\n  K(\\mathbf{x}, \\mathbf{z}) &=& ( \\langle \\mathbf{x} \\cdot  \\mathbf{z} \\rangle)^2 \\\\\n                                            &=&  (\\sum_{i=1}^3 x_i \\cdot z_i) \\cdot (\\sum_{j=1}^3 x_j \\cdot z_j) \\\\\n                                            &=&  \\sum_{i=1}^3 \\sum_{j=1}^3 (x_i \\cdot x_j) \\cdot ( z_i \\cdot z_j) \\\\\n                                            &=&  \\langle \\phi(\\mathbf{x}) \\cdot \\phi(\\mathbf{z}) \\rangle\n\\end{eqnarray*}\\]\nwith:\n\\[\n  \\phi(\\mathbf{x}) = \\begin{bmatrix}\n                            x_1 \\cdot x_1 \\\\\n                            x_1 \\cdot x_2 \\\\\n                            x_1 \\cdot x_3 \\\\\n                            x_2 \\cdot x_1 \\\\\n                            x_2 \\cdot x_2 \\\\\n                            x_2 \\cdot x_3 \\\\\n                            x_3 \\cdot x_1 \\\\\n                            x_3 \\cdot x_2 \\\\\n                            x_3 \\cdot x_3 \\end{bmatrix}\n\\]\nThe quadratic kernel implicitely transforms an input space with three dimensions into a feature space of 9 dimensions.\nMore generally, the polynomial kernel in \\(\\Re^d\\) of degree \\(p\\):\n\\[\n\\begin{align*}\n\\forall (\\mathbf{x}, \\mathbf{z}) \\in \\Re^d \\times \\Re^d \\qquad  K(\\mathbf{x}, \\mathbf{z}) &= ( \\langle \\mathbf{x} \\cdot  \\mathbf{z} \\rangle)^p \\\\\n                                            &=  \\langle \\phi(\\mathbf{x}) \\cdot \\phi(\\mathbf{z}) \\rangle\n\\end{align*}\n\\]\ntransforms the input from a space with \\(d\\) dimensions into a feature space of \\(d^p\\) dimensions.\nWhile the inner product in the feature space would require \\(O(d^p)\\) operations, the calculation of the kernel directly in the input space only requires \\(O(d)\\) operations. This is called the kernel trick: when a linear algorithm only relies on the dot product between input vectors, it can be safely projected into a higher dimensional feature space through a kernel function, without increasing too much its computational complexity, and without ever computing the values in the feature space.\nThe kernel perceptron is the dual form of the Perceptron algorithm using a kernel:\n```{admonition} Kernel perceptron\n\nfor \\(M\\) epochs:\n\nfor each sample \\((\\mathbf{x}_i, t_i)\\):\n\n\\(y_i = \\text{sign}( \\sum_{j=1}^N \\alpha_j \\, t_j \\, K(\\mathbf{x}_j, \\mathbf{x}_i))\\)\nif \\(y_i \\neq t_i\\) :\n\n\\(\\alpha_i \\leftarrow \\alpha_i + 1\\) ```\n\n\n\n\nDepending on the kernel, the implicit dimensionality of the feature space can even be infinite! Some kernels:\n\nLinear kernel: dimension of the feature space = \\(d\\).\n\n\\[\nK(\\mathbf{x},\\mathbf{z}) = \\langle \\mathbf{x} \\cdot \\mathbf{z} \\rangle\n\\]\n\nPolynomial kernel: dimension of the feature space = \\(d^p\\).\n\n\\[\nK(\\mathbf{x},\\mathbf{z}) = (\\langle \\mathbf{x} \\cdot \\mathbf{z} \\rangle)^p\n\\]\n\nGaussian kernel (or RBF kernel): dimension of the feature space= \\(\\infty\\).\n\n\\[\nK(\\mathbf{x},\\mathbf{z}) = \\exp(-\\frac{\\| \\mathbf{x} - \\mathbf{z} \\|^2}{2\\sigma^2})\n\\]\n\nHyperbolic tangent kernel: dimension of the feature space = \\(\\infty\\)\n\n\\[\nk(\\mathbf{x},\\mathbf{z})=\\tanh(\\langle \\kappa \\mathbf{x} \\cdot \\mathbf{z} \\rangle +c)\n\\]\nIn practice, the choice of the kernel family depends more on the nature of data (text, image…) and its distribution than on the complexity of the learning problem. RBF kernels tend to “group” positive examples together. Polynomial kernels are more like “distorted” hyperplanes. Kernels have parameters (\\(p\\), \\(\\sigma\\)…) which have to found using cross-validation.\n\n\n\n```{figure} ../img/kernels.png\n\n\n\n\nwidth: 100%\n\n\n\nDifferent kernels lead to different decision functions. Source: http://beta.cambridgespark.com/courses/jpm/05-module.html\n\n### Support vector machines\n\n**Support vector machines** (SVM) extend the idea of a kernel perceptron using a different linear learning algorithm, the maximum margin classifier. Using Lagrange optimization and regularization, the maximal margin classifer tries to maximize the \"safety zone\" (geometric margin) between the classifier and the training examples. It also tries to reduce the number of non-zero $\\alpha_i$ coefficients to keep the complexity of the classifier bounded, thereby improving the generalization:\n\n$$\n\\mathbf{y} = \\text{sign}(\\sum_{i=1}^{N_{SV}} \\alpha_i \\, t_i \\, K(\\mathbf{x}_i, \\mathbf{x}) + b)\n$$\n\n\n```{figure} ../img/supportvectors.svg\n---\nwidth: 60%\n---\nSupport-vectors are the closest examples to the hyperplane. They have a non-zero $\\alpha$ coefficient.\nCoupled with a good kernel, a SVM can efficiently solve non-linear classification problems without overfitting. SVMs were the weapon of choice before the deep learning era, which deals better with huge datasets."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Chollet, François. 2017. Deep Learning with\nPython. Manning publications.\n\n\nGerstner, Wulfram, Werner Kistler, Richard Naud, and Liam Paninski.\n2014. Neuronal Dynamics - a Neuroscience Textbook.\nCambridge University Press.\n\n\nGoodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. Deep\nLearning. MIT Press.\n\n\nHaykin, Simon S. 2009. Neural Networks and\nLearning Machines, 3rd Edition.\nPearson."
  }
]