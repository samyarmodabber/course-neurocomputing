<!DOCTYPE html>
<html lang="en"><head>
<script src="8.2-Beyond_files/libs/clipboard/clipboard.min.js"></script>
<script src="8.2-Beyond_files/libs/quarto-html/tabby.min.js"></script>
<script src="8.2-Beyond_files/libs/quarto-html/popper.min.js"></script>
<script src="8.2-Beyond_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="8.2-Beyond_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="8.2-Beyond_files/libs/quarto-html/quarto-html.min.css" rel="stylesheet" data-mode="light">
<link href="8.2-Beyond_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.2.269">

  <meta name="author" content="Julien Vitay">
  <title>Neurocomputing</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="8.2-Beyond_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="8.2-Beyond_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
      }
    pre.numberSource { margin-left: 3em;  padding-left: 4px; }
    div.sourceCode
      { color: #24292e;  }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #24292e; } /* Normal */
    code span.al { color: #ff5555; font-weight: bold; } /* Alert */
    code span.an { color: #6a737d; } /* Annotation */
    code span.at { color: #d73a49; } /* Attribute */
    code span.bn { color: #005cc5; } /* BaseN */
    code span.bu { color: #d73a49; } /* BuiltIn */
    code span.cf { color: #d73a49; } /* ControlFlow */
    code span.ch { color: #032f62; } /* Char */
    code span.cn { color: #005cc5; } /* Constant */
    code span.co { color: #6a737d; } /* Comment */
    code span.cv { color: #6a737d; } /* CommentVar */
    code span.do { color: #6a737d; } /* Documentation */
    code span.dt { color: #d73a49; } /* DataType */
    code span.dv { color: #005cc5; } /* DecVal */
    code span.er { color: #ff5555; text-decoration: underline; } /* Error */
    code span.ex { color: #d73a49; font-weight: bold; } /* Extension */
    code span.fl { color: #005cc5; } /* Float */
    code span.fu { color: #6f42c1; } /* Function */
    code span.im { color: #032f62; } /* Import */
    code span.in { color: #6a737d; } /* Information */
    code span.kw { color: #d73a49; } /* Keyword */
    code span.op { color: #24292e; } /* Operator */
    code span.ot { color: #6f42c1; } /* Other */
    code span.pp { color: #d73a49; } /* Preprocessor */
    code span.re { color: #6a737d; } /* RegionMarker */
    code span.sc { color: #005cc5; } /* SpecialChar */
    code span.ss { color: #032f62; } /* SpecialString */
    code span.st { color: #032f62; } /* String */
    code span.va { color: #e36209; } /* Variable */
    code span.vs { color: #032f62; } /* VerbatimString */
    code span.wa { color: #ff5555; } /* Warning */
  </style>
  <link rel="stylesheet" href="8.2-Beyond_files/libs/revealjs/dist/theme/quarto.css" id="theme">
  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">
  <link href="8.2-Beyond_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="8.2-Beyond_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="8.2-Beyond_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="8.2-Beyond_files/libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="8.2-Beyond_files/libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="8.2-Beyond_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-captioned.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-captioned) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-captioned.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-captioned .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-captioned .callout-caption  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-captioned.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-captioned.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-caption {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-caption {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-caption {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-captioned .callout-body > .callout-content > :last-child {
    margin-bottom: 0.5rem;
  }

  .callout.callout-captioned .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-captioned) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-caption {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-caption {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-caption {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-caption {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-caption {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" data-background-image="img/tuc-new-large.png" data-background-opacity="1" data-background-position="top" data-background-size="30%" class="quarto-title-block center">
  <h1 class="title">Neurocomputing</h1>
  <p class="subtitle">Beyond deep learning</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Julien Vitay 
</div>
        <p class="quarto-title-affiliation">
            Professur für Künstliche Intelligenz - Fakultät für Informatik
          </p>
    </div>
</div>

</section>
<section id="towards-biological-deep-learning" class="title-slide slide level1 center">
<h1>1 - Towards biological deep learning?</h1>

</section>

<section id="the-credit-assignment-problem" class="title-slide slide level1 center">
<h1>The credit assignment problem</h1>

<img data-src="img/creditassignment1.png" class="r-stretch quarto-figure-center"><p class="caption">Source: <a href="https://simons.berkeley.edu/sites/default/files/docs/9574/backpropagationanddeeplearninginthebrain-timothylillicrap.pdf" class="uri">https://simons.berkeley.edu/sites/default/files/docs/9574/backpropagationanddeeplearninginthebrain-timothylillicrap.pdf</a></p></section>

<section id="backpropagation-is-not-biologically-plausible" class="title-slide slide level1 center">
<h1>Backpropagation is not biologically plausible</h1>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li>Backpropagation solves the credit assignment problem by transmitting the error gradient <strong>backwards</strong> through the weights (<span class="math inline">\sim</span> synapses).</li>
</ul>
<p><span class="math display">\Delta W_0 = \eta \, (\mathbf{t} - \mathbf{y}) \times W_1 \times \mathbf{x}^T</span></p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/creditassignment2.png" style="width:80.0%"></p>
<p></p><figcaption>Source: <a href="https://simons.berkeley.edu/sites/default/files/docs/9574/backpropagationanddeeplearninginthebrain-timothylillicrap.pdf" class="uri">https://simons.berkeley.edu/sites/default/files/docs/9574/backpropagationanddeeplearninginthebrain-timothylillicrap.pdf</a></figcaption><p></p>
</figure>
</div>
<ul>
<li>But information only goes in one direction in the brain: from the presynaptic neuron to the postsynaptic one.</li>
</ul>
</div><div class="column" style="width:50%;">
<ul>
<li>A synapse does know not the weight of other synapses and cannot transmit anything backwards.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/chemicalsynapse.jpg" style="width:80.0%"></p>
</figure>
</div>
</div>
</div>
</section>

<section id="feedback-alignment" class="title-slide slide level1 center">
<h1>Feedback alignment</h1>
<div class="columns">
<div class="column" style="width:45%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/feedbackalignment1.png" style="width:80.0%"></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/feedbackalignment2.png" style="width:80.0%"></p>
</figure>
</div>
</div><div class="column" style="width:55%;">
<ul>
<li><p>An alternative mechanism consists of backpropagating the error through another set of <strong>feedback weights</strong>.</p></li>
<li><p>Feedback connections are ubiquitous in the brain, especially in the neocortex.</p></li>
<li><p>The feedback weights do not need to learn: they can stay random.</p></li>
<li><p>The mechanism only works for small networks on MNIST until now.</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/feedbackalignment3.png"></p>
</figure>
</div>
</div>
</div>
<div class="footer">
<p>Lillicrap et al.&nbsp;(2016). Random synaptic feedback weights support error backpropagation for deep learning. Nat Commun 7, 1–10. doi:10.1038/ncomms13276.</p>
</div>
</section>

<section id="predictive-coding" class="title-slide slide level1 center">
<h1>Predictive coding</h1>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li><p>Another alternative is <strong>predictive coding</strong> (Rao and Ballard, 1999), where the role of each layer is to predict the activity of the previous layer by learning a <strong>predictive model</strong> and computing a <strong>prediction error</strong>.</p></li>
<li><p>The brain does not process its sensory inputs in a purely feedforward manner, it compares it to its own predictions or <strong>expectations</strong>: you perceive only what you cannot predict.</p></li>
<li><p>In a hierarchical predictive network, each layer is composed of error and prediction neurons.</p></li>
<li><p>All learning rules are local, no need for backpropagation. Problem: very slow…</p></li>
</ul>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/predictivecoding.png"></p>
<p></p><figcaption>Source: <a href="https://doi.org/10.1038/s41593-018-0200-7" class="uri">https://doi.org/10.1038/s41593-018-0200-7</a></figcaption><p></p>
</figure>
</div>
</div>
</div>
<div class="footer">
<p>Rao &amp; Ballard (1999). Predictive coding in the visual cortex: A functional interpretation of some extra-classical receptive-field effects. Nature Neuroscience, 2(1) doi:10.1038/4580</p>
</div>
</section>

<section id="active-inference" class="title-slide slide level1 center">
<h1>Active inference</h1>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li>More generally, one can understand brain behavior as:</li>
</ul>
<ol type="1">
<li>learning a generative model of the world, i.e.&nbsp;predicting what is going to happen next.</li>
<li>minimizing the surprise / uncertainty, i.e.&nbsp;acting in order to improve the model and reach desirable outcomes (rewards).</li>
</ol>
<ul>
<li>Active inference proposes that the brain minimizes its <strong>free energy</strong> through action selection, perception and learning:</li>
</ul>
<p><span class="math display">
\mathcal{F} = D_\text{KL}[ p(x) || q(x)] + H[q(x)]
</span></p>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/activeinference2.jpg"></p>
<p></p><figcaption>Source: <a href="https://www.frontiersin.org/articles/10.3389/frobt.2018.00021" class="uri">https://www.frontiersin.org/articles/10.3389/frobt.2018.00021</a></figcaption><p></p>
</figure>
</div>
</div>
</div>
<ul>
<li>Although active inference is mostly a framework about Bayesian statistics and neuroscience, deep neural network implementations (using predictive coding networks) start to appear, paving the way for the next-gen AI.</li>
</ul>
</section>

<section id="deep-learning-architectures-are-way-too-simple-and-unidirectional" class="title-slide slide level1 center">
<h1>Deep learning architectures are way too simple and unidirectional</h1>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/fellemanvanessen.png"></p>
</figure>
</div>
</div><div class="column" style="width:50%;">
<ul>
<li><p>Deep learning architectures are mostly unidirectional, from the input to the output, without feedback connections.</p></li>
<li><p>The brain is totally differently organized: a big “mess” of interconnected areas processing everything in parallel.</p></li>
<li><p>The figure on the left is only for vision, and only for the cerebral cortex: the thalamus, basal ganglia, hippocampus, cerebellum, etc, create additional shortcuts.</p></li>
<li><p>Is the complex structure of the brain just a side effect of evolution, or is it the only possible solution?</p></li>
<li><p><strong>Inductive bias</strong>: the choice of the architecture constrains the functions it can perform / learn.</p></li>
</ul>
</div>
</div>
<div class="footer">
<p>Felleman, D. J., and Van Essen, D. C. (1991). Distributed hierarchical processing in the primate cerebral cortex. Cereb. Cortex 1, 1–47. doi:10.1093/cercor/1.1.1.</p>
</div>
</section>

<section id="neural-dynamics" class="title-slide slide level1 center">
<h1>2 - Neural dynamics</h1>

</section>

<section id="biological-neurons-have-dynamics" class="title-slide slide level1 center">
<h1>Biological neurons have dynamics</h1>
<ul>
<li>The <strong>artificial neuron</strong> has no dynamics, it is a simple mathematical function:</li>
</ul>
<p><span class="math display">
    y = f( \sum_{i=1}^d w_i \, x_i + b)
</span></p>
<ul>
<li><p>If you do not change the inputs to an artificial neuron, its output won’t change.</p></li>
<li><p>Time does not exist, even in a LSTM: the only temporal variable is the frequency at which inputs are set.</p></li>
</ul>

<img data-src="img/artificialneuron.svg" class="r-stretch quarto-figure-center"></section>

<section id="biological-neurons-have-dynamics-1" class="title-slide slide level1 center">
<h1>Biological neurons have dynamics</h1>
<ul>
<li><p>Biological neurons have <strong>dynamics</strong>:</p>
<ul>
<li><p>They adapt their firing rate to constant inputs.</p></li>
<li><p>They continue firing after an input disappears.</p></li>
<li><p>They fire even in the absence of inputs (tonic firing).</p></li>
</ul></li>
<li><p>These dynamics are essential to information processing in recurrent populations.</p></li>
</ul>
<div class="columns">
<div class="column" style="width:55%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/adex.png"></p>
</figure>
</div>
</div><div class="column" style="width:45%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/ratecoded-izhikevich.png"></p>
</figure>
</div>
</div>
</div>
</section>

<section id="reservoir-computing" class="title-slide slide level1 center">
<h1>Reservoir computing</h1>
<ul>
<li><p>The concept of <strong>Reservoir Computing</strong> (RC) was developed simultaneously by two researchers at the beginning of the 2000s.</p></li>
<li><p>Herbert Jaeger (Bremen) introduced <strong>echo-state networks</strong> (ESN) using rate-coded neurons.</p></li>
<li><p>Wolfgang Maass (TU Graz) introduced <strong>liquid state machines</strong> (LSM) using spiking neurons.</p></li>
</ul>

<img data-src="img/rc-network.jpg" style="width:60.0%" class="r-stretch quarto-figure-center"><div class="callout callout-tip no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>Jaeger, H. (2001). The “echo state” approach to analysing and training recurrent neural networks. Technical Report.</p>
<p>Maass, W., Natschläger, T., and Markram, H. (2002). Real-time computing without stable states: a new framework for neural computation based on perturbations. Neural computation 14, 2531–60. doi:10.1162/089976602760407955.</p>
</div>
</div>
</div>
</section>

<section id="echo-state-networks" class="title-slide slide level1 center">
<h1>Echo-state networks</h1>
<ul>
<li>An ESN is a set of <strong>recurrent</strong> units (sparsely connected) exhibiting complex spatiotemporal dynamics.</li>
</ul>

<img data-src="img/RC-principle2.png" style="width:60.0%" class="r-stretch quarto-figure-center"><ul>
<li>Rate-coded neurons in the reservoir integrate inputs and recurrent connections using an ODE:</li>
</ul>
<p><span class="math display">
    \tau \, \frac{dx_j(t)}{dt} + x_j(t) = \sum_i W^\text{IN}_{ij} \, I_i(t) + \sum_i W_{ij} \, r_i(t)
</span></p>
<ul>
<li>The output of a neuron uses the tanh function (between -1 and 1):</li>
</ul>
<p><span class="math display">
    r_j(t) = \tanh(x_j(t))
</span></p>
<div class="footer">
<p>Tanaka et al.&nbsp;(2019). Recent advances in physical reservoir computing: A review. Neural Networks 115, 100–123. doi:10.1016/j.neunet.2019.03.005.</p>
</div>
</section>

<section id="echo-state-networks-1" class="title-slide slide level1 center">
<h1>Echo-state networks</h1>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li><strong>Readout neurons</strong> (or output neurons) transform linearly the activity in the reservoir:</li>
</ul>
<p><span class="math display">
    z_k(t) = \sum_j W^\text{OUT}_{jk} \, r_j(t)
</span></p>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/RC-principle2.png"></p>
</figure>
</div>
</div>
</div>
<ul>
<li>The recurrent weights are randomly initialized and fixed throughout learning:</li>
</ul>
<p><span class="math display">w_{ij} \sim \mathcal{N}(0, \frac{g}{\sqrt{N}})</span></p>
<ul>
<li><strong>Supervised learning</strong> can be used to train the readout weights to reproduce desired targets.</li>
</ul>
<div class="footer">
<p>Tanaka et al.&nbsp;(2019). Recent advances in physical reservoir computing: A review. Neural Networks 115, 100–123. doi:10.1016/j.neunet.2019.03.005.</p>
</div>
</section>

<section id="echo-state-networks-2" class="title-slide slide level1 center">
<h1>Echo-state networks</h1>
<ul>
<li>When <span class="math inline">g&lt;1</span>, the network has no dynamics: the activity quickly fades to 0 when the input is removed.</li>
</ul>

<img data-src="img/reservoir-dynamics-0.png" class="r-stretch quarto-figure-center"></section>

<section id="echo-state-networks-3" class="title-slide slide level1 center">
<h1>Echo-state networks</h1>
<ul>
<li>For <span class="math inline">g=1</span>, the reservoir exhibits some <strong>transcient dynamics</strong> but eventually fades to 0 (echo-state property).</li>
</ul>

<img data-src="img/reservoir-dynamics-1.png" class="r-stretch quarto-figure-center"></section>

<section id="echo-state-networks-4" class="title-slide slide level1 center">
<h1>Echo-state networks</h1>
<ul>
<li>For <span class="math inline">g=1.5</span>, the reservoir exhibits many <strong>stable attractors</strong> due to its rich dynamics.</li>
</ul>

<img data-src="img/reservoir-dynamics-2.png" class="r-stretch quarto-figure-center"></section>

<section id="echo-state-networks-5" class="title-slide slide level1 center">
<h1>Echo-state networks</h1>
<ul>
<li>For higher values of <span class="math inline">g</span>, there are no stable attractors anymore: <strong>chaotic behavior</strong>.</li>
</ul>

<img data-src="img/reservoir-dynamics-3.png" class="r-stretch quarto-figure-center"></section>

<section id="representational-power-at-the-edge-of-chaos" class="title-slide slide level1 center">
<h1>Representational power at the edge of chaos</h1>
<ul>
<li>For <span class="math inline">g = 1.5</span>, different inputs (initial states) lead to different attractors.</li>
</ul>

<img data-src="img/reservoir-dynamics-represent.png" class="r-stretch quarto-figure-center"></section>

<section id="chaotic-behavior-for-high-values-of-g" class="title-slide slide level1 center">
<h1>Chaotic behavior for high values of <span class="math inline">g</span></h1>
<ul>
<li>In the chaotic regime, the slightest uncertainty on the initial conditions (or the presence of noise) produces very different trajectories on the long-term.</li>
</ul>

<img data-src="img/reservoir-dynamics-reproduce2.png" class="r-stretch quarto-figure-center"></section>

<section id="edge-of-chaos" class="title-slide slide level1 center">
<h1>Edge of chaos</h1>
<ul>
<li><p>The chaotic regime appears for <span class="math inline">g &gt; 1.5</span>.</p></li>
<li><p><span class="math inline">g=1.5</span> is the <strong>edge of chaos</strong>: the dynamics are very rich, but the network is not chaotic yet.</p></li>
</ul>

<img data-src="img/reservoir-dynamics-chaos.png" style="width:60.0%" class="r-stretch quarto-figure-center"></section>

<section id="lorenz-attractor" class="title-slide slide level1 center">
<h1>Lorenz attractor</h1>
<ul>
<li><p>The <strong>Lorenz attractor</strong> is a famous example of a chaotic attractor.</p></li>
<li><p>The position <span class="math inline">x, y, z</span> of a particle is describe by a set of 3 <strong>deterministic</strong> ordinary differential equations:</p></li>
</ul>
<div class="columns">
<div class="column" style="width:70%;">
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/dP3qAq9RNLg" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
</div><div class="column" style="width:30%;">
<p><span class="math display">
\begin{cases}
\dfrac{dx}{dt} = \sigma \, (y -  x) \\
\\
\dfrac{dy}{dt} = x \, (\rho - z) - y \\
\\
\dfrac{dz}{dt} = x\, y - \beta \, z \\
\end{cases}
</span></p>
</div>
</div>
<ul>
<li><p>The resulting trajectories over time have complex dynamics and are <strong>chaotic</strong>:</p>
<ul>
<li>The slightest change in the initial conditions generates different trajectories.</li>
</ul></li>
</ul>
</section>

<section id="training-the-readout-neurons" class="title-slide slide level1 center">
<h1>Training the readout neurons</h1>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/reservoir-fit.png"></p>
</figure>
</div>
</div><div class="column" style="width:50%;">
<ul>
<li>Using the reservoir as input, the linear readout neurons can be trained to reproduce <strong>any non-linear</strong> target signal over time:</li>
</ul>
<p><span class="math display">
    z_k(t) = \sum_j W^\text{OUT}_{jk} \, r_j(t)
</span></p>
<ul>
<li>As it is a regression problem, the <strong>delta learning rule</strong> (LMS) is often enough.</li>
</ul>
<p><span class="math display">
    \Delta W^\text{OUT}_{jk} = \eta \, (t_k(t) - z_k(t)) \, r_j(t)
</span></p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> linear_model</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>reg <span class="op">=</span> linear_model.LinearRegression()</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>reg.fit(r, t)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li>Reservoirs are <strong>universal approximators</strong>: given enough neurons in the reservoir and dynamics at the edge of the chaos, a RC network can approximate any non-linear function between an input signal <span class="math inline">\mathbf{I}(t)</span> and a target signal <span class="math inline">\mathbf{t}(t)</span>.</li>
</ul>
</div>
</div>
</section>

<section id="pattern-separation" class="title-slide slide level1 center">
<h1>Pattern separation</h1>
<ul>
<li><p>The reservoir projects a low-dimensional input into a high-dimensional <strong>spatio-temporal feature space</strong> where trajectories becomes linearly separable.</p></li>
<li><p>The reservoir increases the distance between the input patterns.</p></li>
<li><p>Input patterns are separated in both space (neurons) and time: the readout neurons need much less weights than the equivalent MLP: <strong>better generalization and faster learning</strong>.</p></li>
<li><p>The only drawback is that it does not deal very well with high-dimensional inputs (images).</p></li>
</ul>

<img data-src="img/rc-patternseparation.png" style="width:70.0%" class="r-stretch quarto-figure-center"><div class="footer">
<p>Seoane, L. F. (2019). Evolutionary aspects of reservoir computing. Philosophical Transactions of the Royal Society B. doi:10.1098/rstb.2018.0377.</p>
</div>
</section>

<section id="applications-of-reservoir-computing" class="title-slide slide level1 center">
<h1>Applications of Reservoir Computing</h1>
<ul>
<li><strong>Forecasting</strong>: ESN are able to predict the future of chaotic systems (stock market, weather) much better than static RNN.</li>
</ul>

<img data-src="img/rc-forecasting.png" class="r-stretch quarto-figure-center"><p class="caption">Source: <a href="https://towardsdatascience.com/predicting-stock-prices-with-echo-state-networks-f910809d23d4" class="uri">https://towardsdatascience.com/predicting-stock-prices-with-echo-state-networks-f910809d23d4</a></p></section>

<section id="applications-of-reservoir-computing-1" class="title-slide slide level1 center">
<h1>Applications of Reservoir Computing</h1>
<ul>
<li><strong>Physics:</strong> RC networks can be used to predict the evolution of chaotic systems (Lorenz, Mackey-Glass, Kuramoto-Sivashinsky) at very long time scales (8 times the Lyapunov time).</li>
</ul>

<img data-src="img/rc-flame.gif" class="r-stretch quarto-figure-center"><div class="footer">
<p>Pathak et al.&nbsp;(2018). Model-Free Prediction of Large Spatiotemporally Chaotic Systems from Data: A Reservoir Computing Approach. Physical Review Letters 120, 024102–024102. doi:10.1103/PhysRevLett.120.024102.</p>
</div>
</section>

<section id="physical-reservoir-computing" class="title-slide slide level1 center">
<h1>Physical Reservoir Computing</h1>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li><p>The cool thing with reservoirs is that they do not have to be simulated by classical von Neumann architectures (CPU, GPU).</p></li>
<li><p>Anything able to exhibit dynamics at the edge of chaos can be used:</p>
<ul>
<li>VLSI (memristors), magnetronics, photonics (lasers), spintronics (nanoscale electronics)…</li>
</ul></li>
<li><p>This can limit drastically the energy consumption of ML algorithms (200W for a GPU).</p></li>
<li><p>Even biological or physical systems can be used…</p></li>
</ul>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/rc-memristor.jpg"></p>
</figure>
</div>
</div>
</div>
<div class="footer">
<p>Tanaka et al.&nbsp;(2018). Recent Advances in Physical Reservoir Computing: A Review. arXiv:1808.04962</p>
</div>
</section>

<section id="pattern-recognition-in-a-bucket" class="title-slide slide level1 center">
<h1>Pattern recognition in a bucket</h1>
<div class="columns">
<div class="column" style="width:40%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/liquidbrain.png"></p>
</figure>
</div>
</div><div class="column" style="width:60%;">
<ul>
<li><p>A bucket of water can be used as a reservoir.</p></li>
<li><p>Different motors provide inputs to the reservoir by creating weights.</p></li>
<li><p>The surface of the bucket is recorded and used as an input to a linear algorithm.</p></li>
<li><p>It can learn non-linear operations (XOR) or even speech recognition.</p></li>
</ul>
</div>
</div>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/liquidbrain2.png" style="width:70.0%"></p>
</figure>
</div>
<div class="footer">
<p>Fernando, C., and Sojakka, S. (2003). Pattern Recognition in a Bucket. doi:10.1007/978-3-540-39432-7_63.</p>
</div>
</section>

<section id="rc-with-a-in-silico-culture-of-biological-neurons" class="title-slide slide level1 center">
<h1>RC with a in-silico culture of biological neurons</h1>
<ul>
<li>Real biological neurons can be kept alive in a culture and stimulated /recorded to implement a reservoir.</li>
</ul>

<img data-src="img/rc-culture2.jpg" class="r-stretch quarto-figure-center"><div class="footer">
<p>Frega et al.&nbsp;(2014). Network dynamics of 3D engineered neuronal cultures: a new experimental model for in-vitro electrophysiology. Scientific Reports 4, 1–14. doi:10.1038/srep05489.</p>
</div>
</section>

<section id="rc-in-cultures-of-e.coli-bacteria" class="title-slide slide level1 center">
<h1>RC in cultures of E.Coli bacteria</h1>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/rc-ecoli.png"></p>
</figure>
</div>
</div><div class="column" style="width:50%;">
<ul>
<li><p>Escherichia Coli bacteria change their mRNA in response to various external factors (temperature, chemical products, etc) and interact with each other.</p></li>
<li><p>Their mRNA encode a dynamical trajectory reflecting the inputs.</p></li>
<li><p>By placing them on a microarray, one can linearly learn to perform non-linear operations on the inputs.</p></li>
</ul>
</div>
</div>
<div class="footer">
<p>Jones, B., Stekel, D., Rowe, J., and Fernando, C. (2007). Is there a Liquid State Machine in the Bacterium Escherichia Coli? doi:10.1109/ALIFE.2007.367795.</p>
</div>
</section>

<section id="spiking-networks-and-neuromorphic-computing" class="title-slide slide level1 center">
<h1>3 - Spiking networks and Neuromorphic computing</h1>

</section>

<section id="biological-neurons-communicate-through-spikes" class="title-slide slide level1 center">
<h1>Biological neurons communicate through spikes</h1>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/spiketrain.jpg"></p>
</figure>
</div>
</div><div class="column" style="width:50%;">
<ul>
<li><p>The two important dimensions of the information exchanged by neurons are:</p>
<ul>
<li><p>The instantaneous <strong>frequency</strong> or <strong>firing rate</strong>: number of spikes per second (Hz).</p></li>
<li><p>The precise <strong>timing</strong> of the spikes.</p></li>
</ul></li>
<li><p>The shape of the spike (amplitude, duration) does not matter much.</p></li>
<li><p>Spikes are binary signals (0 or 1) at precise moments of time.</p></li>
<li><p><strong>Rate-coded neurons</strong> only represent the firing rate of a neuron and ignore spike timing.</p></li>
<li><p><strong>Spiking neurons</strong> represent explicitly spike timing, but omit the details of action potentials.</p></li>
</ul>
</div>
</div>
<div class="footer">
<p>Rossant et al.&nbsp;(2011). Fitting Neuron Models to Spike Trains. Front. Neurosci. 5. doi:10.3389/fnins.2011.00009</p>
</div>
</section>

<section id="the-leaky-integrate-and-fire-neuron-lapicque-1907" class="title-slide slide level1 center">
<h1>The leaky integrate-and-fire neuron (Lapicque, 1907)</h1>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li>The <strong>leaky integrate-and-fire</strong> (LIF) neuron has a <strong>membrane potential</strong> <span class="math inline">v(t)</span> that integrates its input current <span class="math inline">I(t)</span>:</li>
</ul>
<p><span class="math display">
    C \, \frac{dv(t)}{dt} = - g_L \, (v(t) - V_L) + I(t)
</span></p>
<ul>
<li><p><span class="math inline">C</span> is the membrane capacitance, <span class="math inline">g_L</span> the leak conductance and <span class="math inline">V_L</span> the resting potential.</p></li>
<li><p>In the absence of input current (<span class="math inline">I=0</span>), the membrane potential is equal to the resting potential.</p></li>
<li><p>When the membrane potential exceeds a threshold <span class="math inline">V_T</span>, the neuron emits a spike and the membrane potential is reset to the reset potential <span class="math inline">V_r</span> for a fixed refractory period <span class="math inline">t_\text{ref}</span>.</p></li>
</ul>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/LIF-threshold.png"></p>
</figure>
</div>
</div>
</div>
<p><span class="math display">
    \text{if} \; v(t) &gt; V_T \; \text{: emit a spike and set} \, v(t) = V_r \; \text{for} \, t_\text{ref} \, \text{ms.}
</span></p>
</section>

<section id="hebbian-learning" class="title-slide slide level1 center">
<h1>Hebbian learning</h1>
<ul>
<li><strong>Hebbian learning</strong> postulates that synapses strengthen based on the <strong>correlation</strong> between the activity of the pre- and post-synaptic neurons:</li>
</ul>
<blockquote>
<p>When an axon of cell A is near enough to excite a cell B and repeatedly or persistently takes part in firing it, some growth process or metabolic change takes place in one or both cells such that A’s efficiency, as one of the cells firing B, is increased.</p>
<p><strong>Donald Hebb</strong>, 1949</p>
</blockquote>

<img data-src="img/hebb.png" class="r-stretch quarto-figure-center"><p class="caption">Source: <a href="https://slideplayer.com/slide/11511675/" class="uri">https://slideplayer.com/slide/11511675/</a></p></section>

<section id="stdp-spike-timing-dependent-plasticity" class="title-slide slide level1 center">
<h1>STDP: Spike-timing dependent plasticity</h1>
<ul>
<li><p>Synaptic efficiencies actually evolve depending on the the <strong>causation</strong> between the neuron’s firing patterns:</p>
<ul>
<li><p>If the pre-synaptic neuron fires <strong>before</strong> the post-synaptic one, the weight is increased (<strong>long-term potentiation</strong>). Pre causes Post to fire.</p></li>
<li><p>If it fires <strong>after</strong>, the weight is decreased (<strong>long-term depression</strong>). Pre does not cause Post to fire.</p></li>
</ul></li>
</ul>

<img data-src="img/stdp.jpg" style="width:70.0%" class="r-stretch quarto-figure-center"><div class="footer">
<p>Bi, G. and Poo, M. (2001). Synaptic modification of correlated activity: Hebb’s postulate revisited. Ann. Rev.&nbsp;Neurosci., 24:139-166.</p>
</div>
</section>

<section id="stdp-spike-timing-dependent-plasticity-1" class="title-slide slide level1 center">
<h1>STDP: Spike-timing dependent plasticity</h1>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li>The STDP (spike-timing dependent plasticity) plasticity rule describes how the weight of a synapse evolves when the pre-synaptic neuron fires at <span class="math inline">t_\text{pre}</span> and the post-synaptic one fires at <span class="math inline">t_\text{post}</span>.</li>
</ul>
<p><span class="math display"> \Delta w =
\begin{cases}
A^+ \, \exp - \dfrac{t_\text{pre} - t_\text{post}}{\tau^+} \; \text{if} \; t_\text{post} &gt; t_\text{pre} \\
\\
A^- \, \exp - \dfrac{t_\text{pre} - t_\text{post}}{\tau^-} \; \text{if} \; t_\text{pre} &gt; t_\text{post} \\
\end{cases}
</span></p>
<ul>
<li><p>STDP can be implemented online using traces.</p></li>
<li><p>More complex variants of STDP (triplet STDP) exist, but this is the main model of synaptic plasticity in spiking networks.</p></li>
</ul>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/stdp2.png"></p>
</figure>
</div>
</div>
</div>
<div class="footer">
<p>Bi, G. and Poo, M. (2001). Synaptic modification of correlated activity: Hebb’s postulate revisited. Ann. Rev.&nbsp;Neurosci., 24:139-166.</p>
</div>
</section>

<section id="deep-convolutional-spiking-networks" class="title-slide slide level1 center">
<h1>Deep convolutional spiking networks</h1>
<ul>
<li><p>A lot of work has lately focused on deep spiking networks, either using a modified version of backpropagation or using STDP.</p></li>
<li><p>The Masquelier lab has proposed a deep spiking convolutional network learning to extract features using STDP (<strong>unsupervised learning</strong>).</p></li>
<li><p>A simple classifier (SVM) then learns to predict classes.</p></li>
</ul>

<img data-src="img/masquelier-architecture.png" class="r-stretch quarto-figure-center"><div class="footer">
<p>Kheradpisheh et al.&nbsp;(2018). STDP-based spiking deep convolutional neural networks for object recognition. Neural Networks 99, 56–67. doi:10.1016/j.neunet.2017.12.005.</p>
</div>
</section>

<section id="deep-convolutional-spiking-networks-1" class="title-slide slide level1 center">
<h1>Deep convolutional spiking networks</h1>
<ul>
<li><p>The convolutional and pooling layers work just as in regular CNNs (shared weights), except the neurons are <strong>integrate-and-fire</strong> (IF).</p></li>
<li><p>There is additionally a <strong>temporal coding scheme</strong>, where the first neuron to emit a spike at a particular location (i.e.&nbsp;over all feature maps) <strong>inhibits</strong> all the others.</p></li>
<li><p>This ensures selectivity of the features through <strong>sparse coding</strong>: only one feature can be detected at a given location.</p></li>
<li><p>STDP allows to learn <strong>causation</strong> between the features and to extract increasingly complex features.</p></li>
</ul>

<img data-src="img/masquelier-architecture.png" style="width:80.0%" class="r-stretch quarto-figure-center"><div class="footer">
<p>Kheradpisheh et al.&nbsp;(2018). STDP-based spiking deep convolutional neural networks for object recognition. Neural Networks 99, 56–67. doi:10.1016/j.neunet.2017.12.005.</p>
</div>
</section>

<section id="deep-convolutional-spiking-networks-2" class="title-slide slide level1 center">
<h1>Deep convolutional spiking networks</h1>

<img data-src="img/masquelier2.png" style="width:80.0%" class="r-stretch quarto-figure-center"></section>

<section id="deep-convolutional-spiking-networks-3" class="title-slide slide level1 center">
<h1>Deep convolutional spiking networks</h1>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/u32Xnz2hDkE" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
</section>

<section id="deep-convolutional-spiking-networks-4" class="title-slide slide level1 center">
<h1>Deep convolutional spiking networks</h1>
<ul>
<li><p>The network is trained <strong>unsupervisedly</strong> on various datasets and obtains accuracies close to the state of the art:</p>
<ul>
<li>Caltech face/motorbike dataset.</li>
<li>ETH-80</li>
<li>MNIST</li>
</ul></li>
</ul>

<img data-src="img/masquelier3.png" style="width:80.0%" class="r-stretch quarto-figure-center"><div class="footer">
<p>Kheradpisheh et al.&nbsp;(2018). STDP-based spiking deep convolutional neural networks for object recognition. Neural Networks 99, 56–67. doi:10.1016/j.neunet.2017.12.005.</p>
</div>
</section>

<section id="deep-convolutional-spiking-networks-5" class="title-slide slide level1 center">
<h1>Deep convolutional spiking networks</h1>
<ul>
<li>The performance on MNIST is in line with classical 3-layered CNNs, but without backpropagation!</li>
</ul>

<img data-src="img/masquelier4.png" class="r-stretch quarto-figure-center"><div class="footer">
<p>Kheradpisheh et al.&nbsp;(2018). STDP-based spiking deep convolutional neural networks for object recognition. Neural Networks 99, 56–67. doi:10.1016/j.neunet.2017.12.005.</p>
</div>
</section>

<section id="event-based-cameras" class="title-slide slide level1 center">
<h1>Event-based cameras</h1>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/kPCZESVfHoQ" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
</section>

<section id="event-based-cameras-1" class="title-slide slide level1 center">
<h1>Event-based cameras</h1>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/eomALySSGVU" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
</section>

<section id="neuromorphic-computing" class="title-slide slide level1 center">
<h1>Neuromorphic computing</h1>
<ul>
<li>Event-based cameras are inspired from the retina (<strong>neuromorphic</strong>) and emit spikes corresponding to luminosity changes.</li>
</ul>

<img data-src="img/eventbased-spike.jpg" class="r-stretch quarto-figure-center"><p class="caption">Source: <a href="https://www.researchgate.net/publication/280600732_A_Computational_Model_of_Innate_Directional_Selectivity_Refined_by_Visual_Experience" class="uri">https://www.researchgate.net/publication/280600732_A_Computational_Model_of_Innate_Directional_Selectivity_Refined_by_Visual_Experience</a></p><ul>
<li><p>Classical von Neumann computers cannot cope with the high fps of event-based cameras.</p></li>
<li><p>Spiking neural networks can be used to process the events (classification, control, etc). But do we have the hardware for that?</p></li>
</ul>
</section>

<section id="intel-loihi" class="title-slide slide level1 center">
<h1>Intel Loihi</h1>

<img data-src="img/lohihi-overview.png" class="r-stretch quarto-figure-center"><div class="footer">
<p><a href="https://en.wikichip.org/wiki/intel/loihi" class="uri">https://en.wikichip.org/wiki/intel/loihi</a></p>
</div>
</section>

<section id="intel-loihi-1" class="title-slide slide level1 center">
<h1>Intel Loihi</h1>
<ul>
<li>Loihi implements 128 neuromorphic cores, each containing 1,024 primitive spiking neural units grouped into tree-like structures in order to simplify the implementation.</li>
</ul>

<img data-src="img/loihi_core.png" class="r-stretch quarto-figure-center"><div class="footer">
<p><a href="https://en.wikichip.org/wiki/intel/loihi" class="uri">https://en.wikichip.org/wiki/intel/loihi</a></p>
</div>
</section>

<section id="intel-loihi-2" class="title-slide slide level1 center">
<h1>Intel Loihi</h1>
<div class="columns">
<div class="column" style="width:40%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/loihi_spikes.gif"></p>
</figure>
</div>
</div><div class="column" style="width:60%;">
<ul>
<li><p>Each neuromorphic core transits spikes to the other cores.</p></li>
<li><p>Fortunately, the firing rates in a spiking network are usually low (10 Hz), what limits the communication costs inside the chip.</p></li>
<li><p>Synapses are <strong>learnable</strong> with STDP mechanisms (memristors).</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/loihi-algos.png"></p>
</figure>
</div>
</div>
</div>
<div class="footer">
<p><a href="https://en.wikichip.org/wiki/intel/loihi" class="uri">https://en.wikichip.org/wiki/intel/loihi</a></p>
</div>
</section>

<section id="neuromorphic-computing-1" class="title-slide slide level1 center">
<h1>Neuromorphic computing</h1>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li><p>Intel Loihi consumes 1/1000th of the energy needed by a modern GPU.</p>
<ul>
<li>Finding suitable algorithms would solve the power consumption of current deep learning-based algorithms.</li>
</ul></li>
<li><p>Alternatives to Intel Loihi are:</p>
<ul>
<li><p>FPGA</p></li>
<li><p>IBM TrueNorth</p></li>
<li><p>Spinnaker (University of Manchester and Dresden).</p></li>
<li><p>Brainchip</p></li>
<li><p>Intel Loihi 2 (2022)</p></li>
</ul></li>
<li><p>The number of simulated neurons and synapses is still very far away from the human brain, but getting closer!</p></li>
</ul>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/loihi-comp.png"></p>
<p></p><figcaption>Source: <a href="https://fuse.wikichip.org/news/2519/intel-labs-builds-a-neuromorphic-system-with-64-to-768-loihi-chips-8-million-to-100-million-neurons/" class="uri">https://fuse.wikichip.org/news/2519/intel-labs-builds-a-neuromorphic-system-with-64-to-768-loihi-chips-8-million-to-100-million-neurons/</a></figcaption><p></p>
</figure>
</div>
</div>
</div>
</section>

<section id="self-organization" class="title-slide slide level1 center">
<h1>4 - Self-organization</h1>

</section>

<section id="self-organization-1" class="title-slide slide level1 center">
<h1>Self-organization</h1>

<img data-src="img/fish.jpg" style="width:40.0%" class="r-stretch quarto-figure-center"><ul>
<li><p>There are two complementary approaches to unsupervised learning:</p>
<ul>
<li><p>the <strong>statistical approach</strong>, which tries to extract the most relevant information from the distribution of unlabeled data (autoencoders, etc).</p></li>
<li><p><strong>self-organization</strong>, which tries to understand the principles of organization of natural systems and use them to create efficient algorithms.</p></li>
</ul></li>
<li><p>Self-organization is a generic process relying on four basic principles: locality of computations, learning, competition and cooperation.</p></li>
</ul>
</section>

<section id="self-organization-2" class="title-slide slide level1 center">
<h1>Self-organization</h1>
<ul>
<li><p><strong>Self-organization</strong> is observed in a wide range of natural processes:</p>
<ul>
<li><p>Physics: formation of crystals, star formation, chemical reactions…</p></li>
<li><p>Biology: folding of proteins, social insects, flocking behavior, brain functioning, Gaia hypothesis…</p></li>
<li><p>Social science: critical mass, group thinking, herd behavior…</p></li>
</ul></li>
</ul>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/dune.jpg" style="width:70.0%"></p>
</figure>
</div>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/flock.jpg" style="width:70.0%"></p>
</figure>
</div>
</div>
</div>
</section>

<section id="self-organization-locality-of-computations-and-learning" class="title-slide slide level1 center">
<h1>Self-organization : locality of computations and learning</h1>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Not self-organized:</strong></p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/orchestra.jpg"></p>
</figure>
</div>
</div><div class="column" style="width:50%;">
<p><strong>Self-organized:</strong></p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/biologicalneurons.jpg" style="width:60.0%"></p>
</figure>
</div>
</div>
</div>
<ul>
<li><p>A self-organizing system is composed of elementary units (particles, cells, neurons, organs, individuals…) which all perform similar deterministic functions (rule of behavior) on a small part of the available information.</p></li>
<li><p>There is <strong>no central supervisor</strong> or coordinator that knows everything and tells each unit what to do:</p>
<ul>
<li>they have their own rule of behavior and apply it to the information they receive.</li>
</ul></li>
<li><p>The units are able to adapt their behavior to the available information: principle of <strong>localized learning</strong>.</p></li>
<li><p>There is no <strong>explicit loss function</strong> specifying what the system should do: <strong>emergence</strong>.</p></li>
</ul>
</section>

<section id="example-conways-game-of-life." class="title-slide slide level1 center">
<h1>Example: Conway’s game of life.</h1>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/gameoflife2.gif"></p>
<p></p><figcaption>Source: <a href="https://www.jakubkonka.com/2015/03/15/game-of-life.html" class="uri">https://www.jakubkonka.com/2015/03/15/game-of-life.html</a></figcaption><p></p>
</figure>
</div>
</div><div class="column" style="width:50%;">
<ul>
<li><p>The rules of Conway’s <strong>Game of Life</strong> (1970) are extremely simple:</p>
<ul>
<li><p>A cell is either <strong>dead</strong> or <strong>alive</strong>.</p></li>
<li><p>A living cell with less than 1 neighbor dies.</p></li>
<li><p>A living cell with more than 4 neighbors dies.</p></li>
<li><p>A dead cell with 3 neighbors relives.</p></li>
</ul></li>
</ul>
</div>
</div>
<ul>
<li><p>Despite this simplicity, GoL can exhibit very complex patterns (fractals, spaceships, pulsars).</p></li>
<li><p>The GoL is an example of self-organizing <strong>cellular automata</strong>.</p></li>
</ul>
<div class="footer">
<p><a href="https://en.wikipedia.org/wiki/Conway%27s_Game_of_Life" class="uri">https://en.wikipedia.org/wiki/Conway%27s_Game_of_Life</a></p>
</div>
</section>

<section id="key-differences-between-deep-networks-and-the-brain" class="title-slide slide level1 center">
<h1>Key differences between deep networks and the brain</h1>
<p><strong>Bio-inspired AI</strong> has to tackle many challenges.</p>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li><p><strong>No backpropagation</strong> in the brain, at least in its current form.</p></li>
<li><p>Information processing is <strong>local</strong> to each neuron and synapse.</p></li>
<li><p>Complex <strong>recurrent</strong> architecture (feedback connections).</p></li>
<li><p>Neurons have <strong>non-linear dynamics</strong>, especially as populations (edge of chaos).</p></li>
<li><p><strong>Emergence</strong> of functions: the whole is more than the sum of its parts</p></li>
<li><p><strong>Self-organization</strong>. There is no explicit loss function to minimize: the only task of the brain is to ensure survival of the organism (homeostasis).</p></li>
<li><p><strong>Embodiment</strong>: the brain is part of a body.</p></li>
</ul>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/ai-brain.jpg"></p>
<p></p><figcaption>Source: <a href="https://www.wsj.com/articles/should-artificial-intelligence-copy-the-human-brain-1533355265" class="uri">https://www.wsj.com/articles/should-artificial-intelligence-copy-the-human-brain-1533355265</a></figcaption><p></p>
</figure>
</div>
</div>
</div>
<div class="footer footer-default">

</div>
</section>
    </div>
  </div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="8.2-Beyond_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="8.2-Beyond_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="8.2-Beyond_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="8.2-Beyond_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="8.2-Beyond_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="8.2-Beyond_files/libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="8.2-Beyond_files/libs/revealjs/plugin/reveal-pointer/pointer.js"></script>
  <script src="8.2-Beyond_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="8.2-Beyond_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="8.2-Beyond_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="8.2-Beyond_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'smaller': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":false},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, RevealPointer, QuartoSupport,

          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const clipboard = new window.ClipboardJS('.code-copy-button', {
        target: function(trigger) {
          return trigger.previousElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      });
      function tippyHover(el, contentFn) {
        const config = {
          allowHTML: true,
          content: contentFn,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'quarto-reveal',
          placement: 'bottom-start'
        };
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          return note.innerHTML;
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>