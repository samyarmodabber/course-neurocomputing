<!DOCTYPE html>
<html lang="en"><head>
<script src="9.5-Hebbian_files/libs/clipboard/clipboard.min.js"></script>
<script src="9.5-Hebbian_files/libs/quarto-html/tabby.min.js"></script>
<script src="9.5-Hebbian_files/libs/quarto-html/popper.min.js"></script>
<script src="9.5-Hebbian_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="9.5-Hebbian_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="9.5-Hebbian_files/libs/quarto-html/quarto-html.min.css" rel="stylesheet" data-mode="light">
<link href="9.5-Hebbian_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.1.175">

  <meta name="author" content="Michael Teichmann">
  <title>Neurocomputing</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="9.5-Hebbian_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="9.5-Hebbian_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="9.5-Hebbian_files/libs/revealjs/dist/theme/quarto.css" id="theme">
  <link href="9.5-Hebbian_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="9.5-Hebbian_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="9.5-Hebbian_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="9.5-Hebbian_files/libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="9.5-Hebbian_files/libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="9.5-Hebbian_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-captioned.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-captioned) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-captioned.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-captioned .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-captioned .callout-caption  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-captioned.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-captioned.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-caption {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-caption {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-caption {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-captioned .callout-body > .callout-content > :last-child {
    margin-bottom: 0.5rem;
  }

  .callout.callout-captioned .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-captioned) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-caption {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-caption {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-caption {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-caption {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-caption {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" data-background-image="img/tuc.svg" data-background-opacity="1" data-background-position="top" data-background-size="30%" class="center">
  <h1 class="title">Neurocomputing</h1>
  <p class="subtitle">Unsupervised Hebbian learning</p>
  <p class="author">Michael Teichmann</p>
  <p class="institute">Professur für Künstliche Intelligenz - Fakultät für Informatik</p>
  <p class="date"><a href="https://tu-chemnitz.de/informatik/KI/edu/neurocomputing" class="uri">https://tu-chemnitz.de/informatik/KI/edu/neurocomputing</a></p>
</section>

<section id="what-is-hebbian-learning" class="title-slide slide level1 center">
<h1>1- What is Hebbian learning?</h1>

</section>

<section id="hebbian-postulate" class="title-slide slide level1 center">
<h1>Hebbian postulate</h1>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/Donald_Hebb.gif" height="200"></p>
<p></p><figcaption>Donald O. Hebb</figcaption><p></p>
</figure>
</div>
<div class="footer">
<p>https://en.wikipedia.org/w/index.php?curid=17594522</p>
</div>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/220px-The_Organization_of_Behavior.jpg" height="200"></p>
<p></p><figcaption>The Organization of Behavior (1949)</figcaption><p></p>
</figure>
</div>
<div class="footer">
<p>https://en.wikipedia.org/w/index.php?curid=41633389</p>
</div>
</div>
</div>
<p>Donald Hebb postulates in 1949 in its book <em>The Organization of Behavior</em> how long lasting cellular changes are induced in the nervous system:</p>
<pre><code>"When an axon of cell A is near enough to excite a cell B and repeatedly or persistently takes part in firing it, some growth process or metabolic change takes place in one or both cells such that A's efficiency, as one of the cells firing B, is increased."</code></pre>
<p>Often simplified to:</p>
<pre><code>Neurons wire together if they fire together.</code></pre>
</section>

<section id="what-is-hebbian-learning-1" class="title-slide slide level1 center">
<h1>What is Hebbian learning?</h1>

<img data-src="img/hebb-single.svg" style="width:30.0%" class="r-stretch quarto-figure-center"><ul>
<li>Based on this principle, a basic computational rule can be formulated, where the weight change is proportional to the product of activation values:</li>
</ul>
<p><span class="math display">\[
\Delta w_{ij} = \eta \, r_i \, r_j
\]</span></p>
<p>where <span class="math inline">\(r_i\)</span> is the pre-synaptic activity of neuron <span class="math inline">\(i\)</span>, <span class="math inline">\(r_j\)</span> the post-synaptic activity of neuron <span class="math inline">\(j\)</span> and <span class="math inline">\(w_{ij}\)</span> the weight from neuron <span class="math inline">\(i\)</span> to <span class="math inline">\(j\)</span>.</p>
<ul>
<li><p>Hebbian learning requires no other information than the activities, such as labels or error signals: it is an <strong>unsupervised learning</strong> method.</p></li>
<li><p>Hebbian learning is not a concrete learning rule, it is a postulate on the fundamental principle of biological learning.</p></li>
<li><p>Because of its unsupervised nature, it will rather learn frequent properties of the input statistics than task-specific properties. It is also called a <strong>correlation-based</strong> learning rule.</p></li>
</ul>
</section>

<section id="properties-of-hebbian-learning-rules" class="title-slide slide level1 center">
<h1>Properties of Hebbian learning rules</h1>
<ul>
<li>A useful Hebbian-based learning rule has to respect several criteria:</li>
</ul>
<ol type="1">
<li><p><strong>Locality:</strong> The weight change should only depend on the activity of the two neurons and the synaptic weight itself. <span class="math display">\[
\Delta w_{ij} = F(w_{ij}; r_i; r_j)
\]</span></p></li>
<li><p><strong>Cooperativity:</strong> Hebb’s postulate says cell <em>A</em> “takes part in firing it”, which implicates that both neurons must be active to induce a weight increase.</p></li>
<li><p><strong>Synaptic depression:</strong> whilw Hebb’s postulate refers only to conditions to strengthen the synapses, a mechanism for decreasing weights is necessary for any useful learning rule.</p></li>
<li><p><strong>Boundedness:</strong> To be realistic, weights should remain bounded in a certain range. The dependence of the learning on <span class="math inline">\(w_{ij}\)</span> or <span class="math inline">\(r_j\)</span> can be used for bounding the weights.</p></li>
<li><p><strong>Competition:</strong> The weights grow at the expense of other weights. This can be implemented by a local form of weight vector normalization.</p></li>
<li><p><strong>Long-term stability:</strong> For adaptive systems, care must be taken that previously learned information is not lost. This is called the “stability-plasticity dilemma”.</p></li>
</ol>
<div class="footer">
<p>Gerstner, W. and Kistler, W. M. (2002). Mathematical formulations of Hebbian learning. Biological Cybernetics, 87(5–6), 404–415</p>
</div>
</section>

<section id="implementations-of-hebbian-learning-rules" class="title-slide slide level1 center">
<h1>2 - Implementations of Hebbian learning rules</h1>

</section>

<section id="simple-hebbian-learning-rule" class="title-slide slide level1 center">
<h1>Simple Hebbian learning rule</h1>
<ul>
<li>In the most basic formulation, learning depends only on the presynaptic <span class="math inline">\(r_i\)</span> and postsynaptic <span class="math inline">\(r_j\)</span> firing rates and a learning rate <span class="math inline">\(\eta\)</span> (correlation-based learning principle):</li>
</ul>
<p><span class="math display">\[
\Delta w_{ij} = \eta \, r_i \, r_j
\]</span></p>
<ul>
<li>If the postsynaptic activity is computed over multiple input synapses:</li>
</ul>
<p><span class="math display">\[
r_j = \sum_i w_{ij} \, r_i = \mathbf{r}^T \times \mathbf{w}_j \\
\]</span></p>
<p>then the learning rule accumulates the auto-correlation matrix <span class="math inline">\(Q\)</span> of the input vector <span class="math inline">\(\mathbf{r}\)</span>:</p>
<p><span class="math display">\[
\Delta \mathbf{w}_j = \eta \, \mathbf{r} \, r_j = \eta \, \mathbf{r}  \times \mathbf{r}^T \times \mathbf{w}_j = \eta \, Q \times \mathbf{w}_j
\]</span></p>
<ul>
<li>When multiple input vectors are presented, <span class="math inline">\(Q\)</span> represents the correlation matrix of the inputs:</li>
</ul>
<p><span class="math display">\[
Q = \mathbb{E}_{\mathbf{r}} [\mathbf{r}  \times \mathbf{r}^T]
\]</span></p>
<ul>
<li>Thus, Hebbian plasticity is assigning strong weights to frequently co-occurring input elements.</li>
</ul>
</section>

<section id="covariance-based-hebbian-learning" class="title-slide slide level1 center">
<h1>Covariance-based Hebbian learning</h1>
<ul>
<li>The covariance between two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is defined by:</li>
</ul>
<p><span class="math display">\[\begin{aligned}
  \text{cov}(X,Y) &amp;= \mathbb{E}(X- \mathbb{E}(X)) \, \mathbb{E}(Y-\mathbb{E}(Y))\\
                  &amp;\\
                  &amp;= \mathbb{E}(XY)- \mathbb{E}(X) \, \mathbb{E}(Y) \\
\end{aligned}\]</span></p>
<ul>
<li><p>One property of the covariance is that it is zero for independent variables and positive for dependent variables.</p></li>
<li><p>However, it just measures linear independence and ignores higher order dependencies.</p></li>
<li><p>It is useful to learn meaningful weights only between statistically dependent neurons.</p></li>
</ul>
</section>

<section id="covariance-based-hebbian-learning-1" class="title-slide slide level1 center">
<h1>Covariance-based Hebbian learning</h1>
<ul>
<li>With the following formulation, the weight change is relative to the covariance of the activity of the connected neurons:</li>
</ul>
<p><span class="math display">\[\begin{aligned}
\Delta w_{ij} &amp;= \eta \, (r_i-\theta_i) \, (r_j-\theta_j)\\
\end{aligned}\]</span></p>
<p>where <span class="math inline">\(\theta_i\)</span> and <span class="math inline">\(\theta_j\)</span> are estimates of the expectation of the pre- and post-synaptic activities, for example through a moving average:</p>
<p><span class="math display">\[\begin{aligned}
  \theta_i &amp; = \alpha \, \theta_i + (1 - \alpha) \, r_i \\
  &amp;\\
  \theta_j &amp; = \beta \, \theta_j + (1 - \beta) \, r_j \\
\end{aligned}\]</span></p>
<ul>
<li><p>Note that with covariance-based learning, weight can both increase (LTP) and decrease (LTD).</p></li>
<li><p>Some variants of covariance-based Hebbian only use a threshold on one of the terms:</p></li>
</ul>
<p><span class="math display">\[\begin{aligned}
\Delta w_{ij} &amp;= \eta \, r_i \, (r_j-\theta) = \eta \, (r_i \, r_j -  \theta \, r_i)\\
&amp;\\
\Delta w_{ij} &amp;= \eta \, (r_i-\theta) \, r_j = \eta (r_i \, r_j- \theta \, r_j)
\end{aligned}\]</span></p>
</section>

<section id="boundedness---normalization" class="title-slide slide level1 center">
<h1>Boundedness - Normalization</h1>
<ul>
<li><p>The previous implementations lack any bound for the weight increase.</p></li>
<li><p>Since the correlation of input and output increases through learning the weight would grow without limits.</p></li>
<li><p>In the case of anti-correlated neurons, the weights could also become negative for covariance-based learning.</p></li>
<li><p>There are several ways to bound the weights:</p></li>
</ul>
<ol type="1">
<li>Hard bounds</li>
</ol>
<p><span class="math display">\[
w_{min}\le w_{ij} \le w_{max}
\]</span></p>
<ol start="2" type="1">
<li>Soft bounds</li>
</ol>
<p><span class="math display">\[
\Delta w_{ij} = \eta \, r_i \, r_j \, (w_{ij} - w_{min}) \, (w_{max}-w_{ij})
\]</span></p>
<ol start="3" type="1">
<li>Normalized weight vector length (Oja, 1982)</li>
</ol>
<p><span class="math display">\[
\Delta w_{ij} = \eta \, (r_i \, r_j - \alpha \, r_j^2 \, w_{ij})
\]</span></p>
<ol start="4" type="1">
<li>Rate-based threshold adaption (Bienenstock, Cooper, &amp; Monroe, 1982)</li>
</ol>
</section>

<section id="oja-learning-rule" class="title-slide slide level1 center">
<h1>Oja learning rule</h1>
<ul>
<li>Normalized weight vector length (Oja, 1982)</li>
</ul>
<p><span class="math display">\[
\Delta w_{ij} = \eta \, (r_i \, r_j - \alpha \, r_j^2 \, w_{ij})
\]</span></p>
<ul>
<li><p>Erkki Oja found a formulation which normalizes the length of a weight vector by a <strong>local</strong> operation, fulfilling the first criterium for Hebbian learning.</p></li>
<li><p><span class="math inline">\(\alpha \, r_j^2 \, w_{ij}\)</span> is a <strong>regularization term</strong>: when the weight <span class="math inline">\(w_{ij}\)</span> or the postsynaptic activity <span class="math inline">\(r_j\)</span> are too high, the term cancels the “Hebbian” part <span class="math inline">\(r_i \, r_j\)</span> and decreases the weight.</p></li>
<li><p>Oja has shown that with this equation the norm of the weight vector converges to a constant value determined by the parameter <span class="math inline">\(\alpha\)</span>:</p></li>
</ul>
<p><span class="math display">\[
||\mathbf{w}||^2 = \frac{1}{\alpha}
\]</span></p>
<ul>
<li>To come to the solution the relation between input and output <span class="math inline">\(r_j = \mathbf{r} \times \mathbf{w}^T\)</span> and a Taylor expansion over <span class="math inline">\(\mathbf{w}\)</span> has been used.</li>
</ul>
<p><br></p>
<div class="footer">
<p>Oja, E. (1982). A simplified neuron model as a principal component analyzer. Journal of Mathematical Biology, 15(3), 267–273. <a href="https://doi.org/10.1007/BF00275687" class="uri">https://doi.org/10.1007/BF00275687</a></p>
</div>
</section>

<section id="bienenstock-cooper-monroe-bcm-learning-rule" class="title-slide slide level1 center">
<h1>Bienenstock-Cooper-Monroe (BCM) learning rule</h1>
<ul>
<li>Rate-based threshold adaption (Bienenstock, Cooper, &amp; Monroe, 1982; Intrator, Cooper, 1992)</li>
</ul>
<div class="columns">
<div class="column" style="width:60%;">
<p><span class="math display">\[\begin{aligned}
\Delta w_{ij} &amp;= \eta \, r_i \, (r_j - \theta) \, r_j \,  f'(net)\\
&amp;\\
\theta &amp;= \mathbb{E} [r_j^2] \\
\end{aligned}\]</span></p>
<ul>
<li><p>In the BCM learning rule, the threshold <span class="math inline">\(\theta\)</span> averages the square of the post-synaptic activity, i.e.&nbsp;its second moment (<span class="math inline">\(\approx\)</span> variance).</p></li>
<li><p>When the short-term trace <span class="math inline">\(\theta\)</span> over the past activities of <span class="math inline">\(r_j\)</span> increases, the fraction of events leading to synaptic depression increases.</p></li>
</ul>
</div><div class="column" style="width:40%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/300px-BCM_Main_figure.png"></p>
</figure>
</div>
<div class="footer">
<p>Source: <a href="http://www.scholarpedia.org/article/BCM_theory" class="uri">http://www.scholarpedia.org/article/BCM_theory</a></p>
</div>
</div>
</div>
<div class="footer">
<p>Bienenstock, E. L., Cooper, L. N., &amp; Munro, P. W. (1982). Theory for the development of neuron selectivity: orientation specificity and binocular interaction in visual cortex. Journal of Neuroscience, 2(1), 32–48. <a href="http://www.jneurosci.org/cgi/content/abstract/2/1/32" class="uri">http://www.jneurosci.org/cgi/content/abstract/2/1/32</a></p>
</div>
<div class="footer">
<p>Intrator, N., &amp; Cooper, L. N. (1992). Objective function formulation of the BCM theory of visual cortical plasticity: Statistical connections, stability conditions. Neural Networks, 5(1), 3–17. https://doi.org/10.1016/S0893-6080(05)80003-6</p>
</div>
</section>

<section id="spike-time-dependent-plasticity-stdp" class="title-slide slide level1 center">
<h1>Spike-Time-Dependent Plasticity (STDP)</h1>
<div class="columns">
<div class="column" style="width:65%;">
<ul>
<li><p>The brain transmits neuronal activity majorly via generation of short electrical impulses, called spikes.</p></li>
<li><p>The timing of these spikes might convey additional information over the firing rate, which we regarded before.</p></li>
<li><p>Spike-based neural networks are also a technical way to transmit information with a very high energy efficiency (neuromorphic hardware).</p></li>
<li><p>An important aspect of STDP is the temporal asymmetric learning window.</p>
<ul>
<li>A spike that arrives slightly before the postsynaptic spike is likely to cause this one.</li>
</ul></li>
<li><p>Thus, STDP learning rules can incorporate temporal aspects implying causality, an important implicit aspect of the cooperativity property of Hebbian learning.</p></li>
</ul>
</div><div class="column" style="width:35%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/Shulz_Feldman_2013-Forms_STDP.jpg"></p>
</figure>
</div>
</div>
</div>
<div class="footer">
<p>Shulz, D. E., &amp; Feldman, D. E. (2013). Chapter 9 - Spike Timing-Dependent Plasticity. In J. L. R. Rubenstein &amp; P. Rakic (Eds.), Neural Circuit Development and Function in the Brain. Elsevier. https://doi.org/10.1016/C2011-0-07732-3</p>
</div>
</section>

<section id="hebbian-neural-networks" class="title-slide slide level1 center">
<h1>3 - Hebbian Neural Networks</h1>

</section>

<section id="perceptron" class="title-slide slide level1 center">
<h1>Perceptron</h1>
<ul>
<li>What does a layer of multiple neurons learn with Hebbian learning?</li>
</ul>
<div class="columns">
<div class="column" style="width:80%;">
<ul>
<li><p>Erkki Oja (1982) has shown that his learning rule converges for linear neurons to the first principle component of the input data.</p></li>
<li><p>A principle component is an eigenvector of the covariance matrix of the input data. The first principle component is the eigenvector with the largest variance, having the highest eigenvalue.</p></li>
<li><p>A network of these neurons appears not very useful, as all neurons will just learn the first principle component. An additional element is required providing differentiation between the neurons.</p></li>
</ul>
</div><div class="column" style="width:20%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/perceptron_FF.svg"></p>
</figure>
</div>
</div>
</div>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/Dayan_Abbot_2005-Principle_Component.png" style="width:65.0%"></p>
</figure>
</div>
<div class="footer">
<p>Dayan, P., &amp; Abbott, L. F. (2005). Theoretical Neuroscience: Computational and Mathematical Modeling of Neural Systems. MIT Press Cambridge, MA, USA. P. 297, section 8.3.</p>
</div>
</section>

<section id="perceptron-1" class="title-slide slide level1 center">
<h1>Perceptron</h1>
<div class="columns">
<div class="column" style="width:78%;">
<ul>
<li>There are several methods existing differentiating the neuron responses, e.g.:</li>
</ul>
<ol type="1">
<li><p>Winner-take-all competition</p>
<ul>
<li><p>Only the neuron with the highest response is selected for learning.</p></li>
<li><p>In practice k-winner-take-all is often used, letting the k strongest neurons learn.</p></li>
</ul></li>
<li><p>A recurrent circuit providing a competitive signal.</p>
<ul>
<li><p>The neurons compete with their neighbors to become active to learn.</p></li>
<li><p>In the brain this is not done directly, it is done via a special neuron type, called <strong>inhibitory neurons</strong>.</p></li>
<li><p>Inhibitory neurons form only synapses reducing the activity of the postsynaptic neuron (Dale’s law).</p></li>
<li><p>Inhibition can be implemented in different manners.</p></li>
</ul></li>
</ol>
</div><div class="column" style="width:22%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img\perceptron_R.svg"></p>
</figure>
</div>
</div>
</div>
</section>

<section id="recurrent-e-i-network" class="title-slide slide level1 center">
<h1>Recurrent E-I Network</h1>
<div class="columns">
<div class="column" style="width:65%;">
<ul>
<li><p><strong>Modulatory inhibition</strong> divides the excitation of the postsynaptic neuron by the received inhibition coming from the neighboring units.</p></li>
<li><p>Scaling the neuronal gain and non-linearly separating the activity values of the neurons in the way that high activities remain high, but lower activities are suppressed.</p></li>
</ul>
<p><span class="math display">\[
  r_j(\sigma,E,I)=\frac{E}{\sigma+E+I}
\]</span></p>
<p>with excitation E, inhibition I, and <span class="math inline">\(\sigma\)</span> scaling the strength of the normalization.</p>
</div><div class="column" style="width:33%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img\Graham2011_Normalization.png"></p>
<p></p><figcaption>Models of normalization as feedback circuit for V1 cells</figcaption><p></p>
</figure>
</div>
</div>
</div>
<div class="footer">
<p>Graham, N. V. (2011). Beyond multiple pattern analyzers modeled as linear filters (as classical V1 simple cells): useful additions of the last 25 years. Vision Research, 51(13), 1397–1430. https://doi.org/10.1016/j.visres.2011.02.007</p>
</div>
</section>

<section id="recurrent-e-i-network-1" class="title-slide slide level1 center">
<h1>Recurrent E-I Network</h1>
<ul>
<li><p><strong>Subtractive inhibition</strong> means that the inhibitory currents are subtracted from the excitatory one.</p></li>
<li><p>In a recurrent circuit highly active neurons reduce the activity of their neighboring neurons and limit their ability to inhibit other neurons. This is called <strong>shunting inhibition</strong>.</p></li>
</ul>
<p><span class="math display">\[
    r_j(E,I)=E-I
\]</span></p>
<ul>
<li>Depending how the weights are arranged this can implement a continuum from winner-take-all competition (equal and strong weights) to very specific competition between particular neurons (e.g.&nbsp;penalizing similar neurons).</li>
</ul>
</section>

<section id="anti-hebbian-learning" class="title-slide slide level1 center">
<h1>Anti-Hebbian Learning</h1>
<ul>
<li><p>A method to learn weights providing a penalty for similarly active neurons is <strong>anti-Hebbian learning</strong>.</p></li>
<li><p>Hebbian learning can easily turned into anti-Hebbian learning by switching the sign of the weight change or switching the effect of the weight from excitatory to inhibitory.</p></li>
<li><p>Covariance-based weight change (Vogels et al., 2011):</p></li>
</ul>
<p><span class="math display">\[
\Delta c_{ij} = r_i r_j - r_i \rho_0
\]</span></p>
<ul>
<li>Weight relative to covariance</li>
</ul>
<p><span class="math display">\[
  \Delta c_{ij} = r_i r_j - r_i\rho_0 (1 + c_{ij})
\]</span></p>
<ul>
<li><p>The equilibrium point of the equation is reached, when the weight indicates by which factor the product of the expectation values <span class="math inline">\(r_i \rho_0\)</span> has to be multiplied to be equal to the expectation value of the product of the activities <span class="math inline">\(r_i r_j\)</span>.</p></li>
<li><p>From a theoretical viewpoint anti-Hebbian learned competition aims to minimize linear dependencies between the activities. When having independent neural activities than the information encoded by a population of neurons is maximal (Simoncelli and Olshausen, 2001).</p></li>
</ul>
<div class="footer">
<p>Vogels, T. P., Sprekeler, H., Zenke, F., Clopath, C., Gerstner, W. (2011). Inhibitory Plasticity Balances Excitation and Inhibition in Sensory Pathways and Memory Networks. Science, 334(6062), 1569–1573. https://doi.org/10.1126/science.1211095</p>
</div>
<div class="footer">
<p>Simoncelli, E. P., Olshausen, B. A. (2001). Natural image statistics and neural representation. Annual Review of Neuroscience, 24(1), 1193–1216. <a href="https://doi.org/10.1146/annurev.neuro.24.1.1193" class="uri">https://doi.org/10.1146/annurev.neuro.24.1.1193</a></p>
</div>
</section>

<section id="anti-hebbian-learning-1" class="title-slide slide level1 center">
<h1>Anti-Hebbian Learning</h1>
<ul>
<li><p>What about the boundedness issue?</p></li>
<li><p>Oja normalization is based on the fact that the postsynaptic activity is caused by the presynaptic activity and the weight strength. This is only true for excitatory weights.</p></li>
<li><p>Inhibitory weights reduce the activity. This causes a softbound effect:</p>
<ul>
<li><p>When the inhibitory weight increases, the activity decreases.</p></li>
<li><p>With lower activities the weight change gets slower, until it stops when the neuron remains inactive.</p></li>
<li><p>Formulations where the weight is relative to the covariance additionally saturate at their equilibrium point.</p></li>
</ul></li>
</ul>
</section>

<section id="information-representation" class="title-slide slide level1 center">
<h1>4 - Information representation</h1>

</section>

<section id="information-representation-1" class="title-slide slide level1 center">
<h1>Information representation</h1>
<ul>
<li><p>Since Hebbian and anti-Hebbian learning are restricted to use only local information, there is no global objective what a population of these neurons should represent.</p></li>
<li><p>Competition between the neurons induce differences in their response patterns, but might not control that all neurons convey information.</p></li>
<li><p>There are two issues:</p></li>
</ul>
<ol type="1">
<li>A single pattern can get dominant because of differences in the activity.</li>
</ol>
<p><span class="math display">\[
  E(w_{ij}) = E(r_i r_j)-E(r_i)E(r_j)
\]</span></p>
<p>If the activity of a particular input neuron <span class="math inline">\(r_i\)</span> is by average higher than the activity of other input neurons, then its weight value gets higher than the weight of a similarly correlated but less active neuron.</p>
<p>This effect is strengthen over multiple layers, causing a dominance of a few patterns. Thus, the activities have to be balanced to avoid an imbalanced input to the next layer.</p>
<ol start="2" type="1">
<li>Neurons can become permanently inactive or unresponsive to changing input.</li>
</ol>
<ul>
<li><p>This can be avoided by aiming for a similar operating point of the neurons, by keeping:</p>
<ul>
<li><p>All neurons active, to use the full capacity of the neuronal population.</p></li>
<li><p>All neurons in a similar range, so that no neuron can dominate the learnings of subsequent layers.</p></li>
</ul></li>
</ul>
</section>

<section id="mechanisms-to-achieve-a-balanced-neural-code" class="title-slide slide level1 center">
<h1>Mechanisms to achieve a balanced neural code</h1>
<ul>
<li><p><strong>Mechanism 1:</strong> Controlling the operating point via setpoints in the plasticity rule (<strong>homeostasis</strong>).</p></li>
<li><p>In Hebbian learning, the amount of weight decrease and increase can be regulated to achieve a certain activity range. Clopath et al.&nbsp;(2010) regulate the strength of the weight decrease <span class="math inline">\(A_{LTD}\)</span> by relating the average membrane potential <span class="math inline">\(\bar{\bar u}\)</span> to a reference value <span class="math inline">\(u_{ref}^2\)</span>, defining a target activity with that:</p></li>
</ul>
<p><span class="math display">\[
    A_{LTD}(\bar{\bar u})=A_{LTD} \frac{\bar{\bar u}^2}{u_{ref}^2}
\]</span></p>
<ul>
<li>BCM learning adapts the threshold based on the average activity of the neuron, facilitating or impeding weight increases and decreases:</li>
</ul>
<p><span class="math display">\[
\Delta w_{ij} = \eta \, r_i \, (r_j - \theta) \, r_j \, f'(net)
\]</span> <span class="math display">\[
  \theta = \mathbb{E} [r_j^2]
\]</span></p>
<ul>
<li>In anti-Hebbian learning, the amount of inhibition a neuron receives is up or downregulated to achieve a certain average activity. Vogels et al.&nbsp;(2011) define a target activity of the postsynaptic neuron <span class="math inline">\(\rho_0\)</span>, the amount of inhibition is up or downregulated to reach this activity:</li>
</ul>
<p><span class="math display">\[
\Delta w_{ij} = r_i \, r_j - r_i \, \rho_0
\]</span></p>
<div class="footer">
<p>Clopath, C., Büsing, L., Vasilaki, E., &amp; Gerstner, W. (2010). Connectivity reflects coding: a model of voltage-based STDP with homeostasis. Nature Neuroscience, 13(3), 344–352. <a href="https://doi.org/10.1038/nn.2479" class="uri">https://doi.org/10.1038/nn.2479</a></p>
</div>
<div class="footer">
<p>Vogels, T. P., Sprekeler, H., Zenke, F., Clopath, C., &amp; Gerstner, W. (2011). Inhibitory Plasticity Balances Excitation and Inhibition in Sensory Pathways and Memory Networks. Science, 334(6062), 1569–1573. <a href="https://doi.org/10.1126/science.1211095" class="uri">https://doi.org/10.1126/science.1211095</a></p>
</div>
</section>

<section id="mechanisms-to-achieve-a-balanced-neural-code-1" class="title-slide slide level1 center">
<h1>Mechanisms to achieve a balanced neural code</h1>
<ul>
<li><p><strong>Mechanism 2:</strong> Controlling the operating point via <strong>intrinsic plasticity</strong>, by forcing a certain activity distribution, through adapting the activation function.</p></li>
<li><p>Joshi and Triesch (2009) adapt the parameters of the transfer function <span class="math inline">\(g(u)\)</span>, by minimizing the Kullback-Leibler divergence between the neuron’s activity and an exponential distribution. The update rules for the parameters <span class="math inline">\(r_0, u_0, u_\alpha\)</span> have been derived via stochastic gradient descent:</p></li>
</ul>
<p><span class="math display">\[
    g(u) = r_0 \, \log \left( 1 + e^\frac{u-u_0}{u_\alpha} \right)
\]</span> <span class="math display">\[
    \Delta r_0 = \frac{\eta}{r_0} \left( 1- \frac{g}{\mu} \right)
\]</span> <span class="math display">\[
    \Delta u_0 = \frac{\eta}{u_\alpha} \left( \left( 1+\frac{r_0}{\mu} \right) \left( 1-e^\frac{-g}{r_0} \right) -1 \right)
\]</span> <span class="math display">\[
    \Delta u_\alpha = \frac{\eta}{u_\alpha} \left( \frac{u-u_0}{u_\alpha} \left( \left( 1+\frac{r_0}{\mu} \right) \left( 1-e^\frac{-g}{r_0} \right) -1 \right) -1 \right)
\]</span></p>
<div class="footer">
<p>Joshi, P., &amp; Triesch, J. (2009). Rules for Information Maximization in Spiking Neurons Using Intrinsic Plasticity. Neural Networks, 2009. IJCNN 2009., 8, 1456–1461. <a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5178625" class="uri">http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5178625</a></p>
</div>
</section>

<section id="mechanisms-to-achieve-a-balanced-neural-code-2" class="title-slide slide level1 center">
<h1>Mechanisms to achieve a balanced neural code</h1>
<ul>
<li><p><strong>Mechanism 2:</strong> Controlling the operating point via <strong>intrinsic plasticity</strong>, by regulating the first moments of activity (mean, variance) and by adapting the activation function.</p></li>
<li><p>Teichmann and Hamker (2015) adapt the parameters of a rectified linear transfer function, by regulating the threshold <span class="math inline">\(\theta_j\)</span> and slope <span class="math inline">\(a_j\)</span>, to achieve a similar mean and variance of all neurons within a layer:</p></li>
</ul>
<p><span class="math display">\[
    \Delta r_j = a_j \left( \sum_i w_{ij} \, r_i - \sum_{k, k \ne j} c_{kj} \, r_k - \theta_j \right) -r_j
\]</span> <span class="math display">\[
    \Delta \theta_j = (r_j - \theta_{target})
\]</span> <span class="math display">\[
    \Delta a_j = (a_{target} -r_j^2)
\]</span></p>
<div class="footer">
<p>Teichmann, M. and Hamker, F. H. (2015). Intrinsic Plasticity: A Simple Mechanism to Stabilize Hebbian Learning in Multilayer Neural Networks. In T. Villmann &amp; F.-M. Schleif (Eds.), Machine Learning Reports 03/2015 (pp.&nbsp;103–111). https://www.techfak.uni-bielefeld.de/~fschleif/mlr/mlr_03_2015.pdf</p>
</div>
</section>

<section id="supervised-hebbian-learning" class="title-slide slide level1 center">
<h1>Supervised Hebbian learning</h1>
<ul>
<li><p>Large parts of the plasticity in the brain is thought to be Hebbian, this means it uses local information and learns unsupervised.</p></li>
<li><p>However the brain also is largely recurrent, information flows in all directions and this information could guide neighboring or preceding processing stages.</p></li>
<li><p>If such an information influences the activity of a neuron then it also influences the learning on the other synapses of this neuron.</p></li>
</ul>

<img data-src="img/Schmidt_Albada_2018-VC.png" style="width:70.0%" class="r-stretch quarto-figure-center"><div class="footer">
<p>Schmidt, M., Diesmann, M., &amp; Albada, S. J. Van. (2018). Multi-scale account of the network structure of macaque visual cortex. Brain Structure and Function, 223(3), 1409–1435. https://doi.org/10.1007/s00429-017-1554-4</p>
</div>
</section>

<section id="supervised-hebbian-learning-1" class="title-slide slide level1 center">
<h1>Supervised Hebbian learning</h1>
<div class="columns">
<div class="column" style="width:70%;">
<ul>
<li>In supervised Hebbian learning the postsynaptic activity is fully controlled. With that the subset of inputs which should evoke activity can be selected.</li>
</ul>
<p><span class="math display">\[
  \Delta w_{ij} = r_i \, t - \alpha \, w_{ij}
\]</span></p>
<ul>
<li><p>The supervised Hebbian learning principle can be extended to a form of top-down learning.</p></li>
<li><p>A top-down signal, conveying additional modulatory information, modulates or contributes partially to the neuronal activity.</p></li>
</ul>
</div><div class="column" style="width:30%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/Grossberg88_TopDown.png"></p>
</figure>
</div>
<div class="footer">
<p>Grossberg, S. (1988). Nonlinear neural networks: Principles, mechanisms, and architectures. Neural Networks, 1(1), 17–61. https://doi.org/10.1016/0893-6080(88)90021-4</p>
</div>
</div>
</div>
<ul>
<li><p>We can illustrate the effect by splitting the plasticity term into bottom-up and the top-down parts. <span class="math display">\[
r'_j= \gamma \, r_j + (1-\gamma) \, t
\]</span> <span class="math display">\[
\Delta w_{ij} = r_i \, r'_j- \alpha \, {r'_j}^2 \, w_{ij}
\]</span></p></li>
<li><p>Depending on <span class="math inline">\(\gamma\)</span>, the top-down signal contributes to the activity, it implements a continuum between unsupervised and supervised Hebbian learning. The weight change does not depend on the actual performance, therefore error correcting learning rules are required.</p></li>
</ul>
<div class="footer">
<p>Dayan, P. and Abbott, L. F. (2005). Theoretical Neuroscience: Computational and Mathematical Modeling of Neural Systems. MIT Press Cambridge, MA, USA. Pp. 313, section 8.4.</p>
</div>
</section>

<section id="summary" class="title-slide slide level1 center">
<h1>Summary</h1>
<ul>
<li><p>Hebbian learning postulates properties of biological learning.</p></li>
<li><p>There is no concrete implementation of Hebbian learning. Algorithms have to fulfill the properties: locality, cooperativity, synaptic depression, boundedness, competition, and long term stability.</p></li>
<li><p>Hebbian learning exploits the statistics of its input and learns frequent patterns. Like the first principle component.</p></li>
<li><p>Beside differences from random initialization of the weights, all neurons would learn the same pattern, when having the same inputs. Thus, Hebbian learning requires a mechanism for competition for differentiation.</p></li>
<li><p>Recurrent inhibitory connections induce competition by penalizing similar activities of the neurons. With that dependencies are reduced and the neural code gets efficient in terms of the information it conveys.</p></li>
</ul>
</section>

<section id="summary-1" class="title-slide slide level1 center">
<h1>Summary</h1>
<ul>
<li><p>However, imbalances in the activity can harm Hebbian learning in subsequent layers. Or inactive neurons reduce the information. Thus, the operating point of the neurons has to be adjusted.</p></li>
<li><p>The operating point can be modified by set points in the Hebbian or anti-Hebbian learning rules. Or by regulating the transfer function of the neurons to achieve either a particular activity distribution or similar response properties like mean and variance.</p></li>
<li><p>Hebbian learning can be extended by top-down signals and implement a continuum between supervised and unsupervised learning. This might help to reduce the dependency on large amounts of labeled data of supervised learning algorithms.</p></li>
</ul>
<div class="footer footer-default">

</div>
</section>
    </div>
  </div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="9.5-Hebbian_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="9.5-Hebbian_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="9.5-Hebbian_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="9.5-Hebbian_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="9.5-Hebbian_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="9.5-Hebbian_files/libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="9.5-Hebbian_files/libs/revealjs/plugin/reveal-pointer/pointer.js"></script>
  <script src="9.5-Hebbian_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="9.5-Hebbian_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="9.5-Hebbian_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="9.5-Hebbian_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="9.5-Hebbian_files/libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'smaller': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":false},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, RevealPointer, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const clipboard = new window.ClipboardJS('.code-copy-button', {
        target: function(trigger) {
          return trigger.previousElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        setTimeout(function() {
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      });
      function tippyHover(el, contentFn) {
        const config = {
          allowHTML: true,
          content: contentFn,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'quarto-reveal',
          placement: 'bottom-start'
        };
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          return note.innerHTML;
        });
      }
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const cites = ref.parentNode.getAttribute('data-cites').split(' ');
        tippyHover(ref, function() {
          var popup = window.document.createElement('div');
          cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    });
    </script>
    

</body></html>