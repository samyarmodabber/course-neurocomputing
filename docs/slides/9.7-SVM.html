<!DOCTYPE html>
<html lang="en"><head>
<script src="9.7-SVM_files/libs/clipboard/clipboard.min.js"></script>
<script src="9.7-SVM_files/libs/quarto-html/tabby.min.js"></script>
<script src="9.7-SVM_files/libs/quarto-html/popper.min.js"></script>
<script src="9.7-SVM_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="9.7-SVM_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="9.7-SVM_files/libs/quarto-html/quarto-html.min.css" rel="stylesheet" data-mode="light">
<link href="9.7-SVM_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.1.175">

  <meta name="author" content="Julien Vitay">
  <title>Neurocomputing</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="9.7-SVM_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="9.7-SVM_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="9.7-SVM_files/libs/revealjs/dist/theme/quarto.css" id="theme">
  <link href="9.7-SVM_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="9.7-SVM_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="9.7-SVM_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="9.7-SVM_files/libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="9.7-SVM_files/libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="9.7-SVM_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-captioned.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-captioned) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-captioned.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-captioned .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-captioned .callout-caption  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-captioned.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-captioned.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-caption {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-caption {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-caption {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-captioned .callout-body > .callout-content > :last-child {
    margin-bottom: 0.5rem;
  }

  .callout.callout-captioned .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-captioned) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-caption {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-caption {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-caption {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-caption {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-caption {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" data-background-image="img/tuc.svg" data-background-opacity="1" data-background-position="top" data-background-size="30%" class="center">
  <h1 class="title">Neurocomputing</h1>
  <p class="subtitle">Support-Vector Machines</p>
  <p class="author">Julien Vitay</p>
  <p class="institute">Professur für Künstliche Intelligenz - Fakultät für Informatik</p>
  <p class="date"><a href="https://tu-chemnitz.de/informatik/KI/edu/neurocomputing" class="uri">https://tu-chemnitz.de/informatik/KI/edu/neurocomputing</a></p>
</section>

<section id="drawbacks-of-neural-networks" class="title-slide slide level1 center">
<h1>Drawbacks of neural networks</h1>
<ul>
<li><p>Neural networks are an empirical method to solve classification or regression problems: you add or remove some components and see whether it improves the performance or not.</p></li>
<li><p>The number of layers/neurons in a NN is free: it has to be found through cross-validation.</p></li>
<li><p>If the number of parameters increases, the training error becomes small, but the generalization error increases.</p></li>
<li><p>The computational complexity of both learning and usage (inference) is quickly untractable if you have a low budget.</p></li>
</ul>
</section>

<section id="support-vector-machines" class="title-slide slide level1 center">
<h1>Support-vector machines</h1>
<ul>
<li>If one could work in a virtually infinite feature space but keep a finite VC dimension, the training and generalization errors would both be kept small.</li>
</ul>
<p><span class="math display">\[
  \epsilon(h) \leq \hat{\epsilon}_{\mathcal{S}}(h) + \sqrt{\frac{\text{VC}_\text{dim} (\mathcal{H}) \cdot (1 + \log(\frac{2\cdot N}{\text{VC}_\text{dim} (\mathcal{H})})) - \log(\frac{\delta}{4})}{N}}
\]</span></p>
<ul>
<li><p>This is the main idea of <em>support-vector machines</em>, created by Vladimir Vapnik in the 60’s, but developped and used in the 90’s when coupled with the kernel trick.</p></li>
<li><p>It is a nice mathematical framework explaining how and <strong>why</strong> the algorithm will converge.</p></li>
</ul>
<div class="footer">
<p>Support Vector Machines and other kernel-based learning methods, Nello Cristianini and John Shawe-Taylor, Cambridge Press.</p>
</div>
</section>

<section id="margins" class="title-slide slide level1 center">
<h1>1 - Margins</h1>

</section>

<section id="margins-1" class="title-slide slide level1 center">
<h1>Margins</h1>

<img data-src="img/fatmargin-1.png" style="width:70.0%" class="r-stretch quarto-figure-center"><p>Neural networks do not care about their generalization error: they only concentrate on the training set.</p>
</section>

<section id="margins-2" class="title-slide slide level1 center">
<h1>Margins</h1>

<img data-src="img/fatmargin-2.png" style="width:70.0%" class="r-stretch quarto-figure-center"><p>A successful classifier with a very small margin is an acceptable solution.</p>
</section>

<section id="margins-3" class="title-slide slide level1 center">
<h1>Margins</h1>

<img data-src="img/fatmargin-3.png" style="width:70.0%" class="r-stretch quarto-figure-center"><p>However, classifiers with small margins are more likely to make mistakes on new examples.</p>
</section>

<section id="margins-4" class="title-slide slide level1 center">
<h1>Margins</h1>

<img data-src="img/fatmargin-4.png" style="width:70.0%" class="r-stretch quarto-figure-center"><p>A classifier with a fat margin has the same success on the training set…</p>
</section>

<section id="margins-5" class="title-slide slide level1 center">
<h1>Margins</h1>

<img data-src="img/fatmargin-5.png" style="width:70.0%" class="r-stretch quarto-figure-center"><p>… but is much less likely to make a mistake on new examples: its generalization error is smaller.</p>
</section>

<section id="effect-of-the-margin-on-the-vc-dimension" class="title-slide slide level1 center">
<h1>Effect of the margin on the VC dimension</h1>
<ul>
<li>The VC dimension is linked the number of possible dichotomies learnable by a classifier on the training set.</li>
</ul>

<img data-src="img/marginvc-1.png" style="width:50.0%" class="r-stretch quarto-figure-center"></section>

<section id="effect-of-the-margin-on-the-vc-dimension-1" class="title-slide slide level1 center">
<h1>Effect of the margin on the VC dimension</h1>
<ul>
<li>Forcing a fat margin for a classifier reduces the number of possible dichotomies, therefore reduces its VC dimension, leading to better generalization.</li>
</ul>

<img data-src="img/marginvc-2.png" style="width:50.0%" class="r-stretch quarto-figure-center"></section>

<section id="binary-linear-classification" class="title-slide slide level1 center">
<h1>Binary linear classification</h1>
<ul>
<li>In binary linear classification, <span class="math inline">\(y_i = f_{\mathbf{w}, b}(\mathbf{x}_i) = \text{sign}( \langle \mathbf{w} . \mathbf{x}_i \rangle +b)\)</span> is the <strong>predicted output</strong> for the input <span class="math inline">\(\mathbf{x}_i\)</span> by the hyperplane <span class="math inline">\((\mathbf{w}, b)\)</span>.</li>
</ul>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/projection.svg"></p>
</figure>
</div>
</div><div class="column" style="width:50%;">
<ul>
<li>The predicted output allows to compute the <strong>functional margin</strong> <span class="math inline">\(\hat{\gamma}_i\)</span> of the example:</li>
</ul>
<p><span class="math display">\[\hat{\gamma}_i = t_i \, (\langle \mathbf{w} . \mathbf{x}_i \rangle +b)\]</span></p>
<p>which determines how well is the example classified, knowing the ground truth <span class="math inline">\(t_i\)</span>.</p>
<ul>
<li><p>If <span class="math inline">\(\hat{\gamma}_i &gt; 0\)</span>, the example is well classified (<span class="math inline">\(t_i\)</span> and <span class="math inline">\(y_i\)</span> have the same sign). Otherwise, it is badly classified.</p></li>
<li><p>The highest <span class="math inline">\(|\hat{\gamma}_i|\)</span>, the further away from the hyperplane is the example.</p></li>
</ul>
</div>
</div>
<ul>
<li>A good linear classifier is an hyperplane where each example in the training set has a positive functional margin.</li>
</ul>
<p><span class="math display">\[\hat{\gamma} = \min_i \hat{\gamma}_i = \min_i t_i \, (\langle \mathbf{w} . \mathbf{x}_i \rangle +b) &gt; 0\]</span></p>
</section>

<section id="functional-and-geometric-margins" class="title-slide slide level1 center">
<h1>Functional and geometric margins</h1>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/linearclassification.svg" style="width:80.0%"></p>
</figure>
</div>
<ul>
<li><p>The decision function <span class="math inline">\(F(x) = \langle \mathbf{w} . \mathbf{x}\rangle + b\)</span> is not influenced by the norm of the vector.</p></li>
<li><p>For the same classification, the <strong>functional margin</strong> will be higher when the weight vector has a bigger norm.</p></li>
<li><p>The <strong>geometric margin</strong> is a better measurement of the margin of a classifier.</p></li>
</ul>
</div><div class="column" style="width:45%;">
<ul>
<li><p>Margins for an example:</p>
<ul>
<li>Functional margin:</li>
</ul>
<p><span class="math display">\[ \hat{\gamma}_i = t_i \cdot (\langle \mathbf{w} . \mathbf{x}_i \rangle +b) \]</span></p>
<ul>
<li>Geometric margin:</li>
</ul>
<p><span class="math display">\[ \gamma_i = t_i \cdot ( \frac{\langle \mathbf{w} . \mathbf{x}_i \rangle +b}{\|\mathbf{w} \|}) \]</span></p></li>
<li><p>Margins on the whole training set:</p>
<ul>
<li>Functional margin:</li>
</ul>
<p><span class="math display">\[ \hat{\gamma} = \min_i \hat{\gamma}_i \]</span></p>
<ul>
<li>Geometric margin:</li>
</ul>
<p><span class="math display">\[ \gamma = \min_i \gamma_i \]</span></p></li>
</ul>
</div>
</div>
</section>

<section id="maximal-margin-classifier-for-linearly-separable-data" class="title-slide slide level1 center">
<h1>Maximal margin classifier for linearly separable data</h1>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/linearclassification.svg"></p>
</figure>
</div>
</div><div class="column" style="width:50%;">
<ul>
<li>The goal of a maximal margin classifier is the find the hyperplane <span class="math inline">\(F(x) = \langle \mathbf{w} . \mathbf{x}\rangle + b\)</span> which has the maximal geometric margin while correctly classifying every example:</li>
</ul>
<p><span class="math display">\[
\begin{align*}
    \text{maximize}_{\mathbf{w}, b} \quad \gamma &amp; = \min_i \, t_i \cdot ( \frac{\langle \mathbf{w} . \mathbf{x}_i \rangle +b}{\|\mathbf{w} \|}) \\
    &amp;=  \frac{\hat{\gamma}}{\|\mathbf{w} \|}
\end{align*}
\]</span></p>
<p><span class="math display">\[
    \text{s.t.} \quad \hat{\gamma}_i = t_i \cdot (\langle \mathbf{w} . \mathbf{x}_i \rangle +b) \geq \hat{\gamma} \quad \forall i.
\]</span></p>
</div>
</div>
</section>

<section id="maximal-margin-classifier-for-linearly-separable-data-1" class="title-slide slide level1 center">
<h1>Maximal margin classifier for linearly separable data</h1>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/linearclassification.svg"></p>
</figure>
</div>
</div><div class="column" style="width:50%;">
<ul>
<li><p>Problem: the geometric margin is a non-convex function of <span class="math inline">\(\mathbf{w}\)</span>, therefore very hard to optimize (local maxima).</p></li>
<li><p>We can reformulate the problem by forcing the functional margin to be 1:</p></li>
</ul>
<p><span class="math display">\[
    \text{maximize}_{\mathbf{w}, b} \quad \gamma = \frac{1}{\|\mathbf{w} \|}
\]</span> <span class="math display">\[
    \text{s.t.} \quad \hat{\gamma}_i = t_i \cdot (\langle \mathbf{w} . \mathbf{x}_i \rangle +b) \geq 1 \quad \forall i.
\]</span></p>
</div>
</div>
<ul>
<li><p>For a given hyperplane, it is always possible to “re-normalize” the weight vector so that the functional margin becomes 1.</p></li>
<li><p>This is only a convention to ease mathematical analysis, but it does not change the result of the classification.</p></li>
</ul>
</section>

<section id="maximal-margin-classifier-for-linearly-separable-data-2" class="title-slide slide level1 center">
<h1>Maximal margin classifier for linearly separable data</h1>
<ul>
<li>The optimization problem is now:</li>
</ul>
<p><span class="math display">\[
    \text{maximize}_{\mathbf{w}, b} \quad \gamma = \frac{1}{\|\mathbf{w} \|}
\]</span></p>
<p><span class="math display">\[
    \text{s.t.} \quad t_i \cdot (\langle \mathbf{w} . \mathbf{x}_i \rangle +b) \geq 1 \quad \forall i.
\]</span></p>
<ul>
<li>Or alternatively:</li>
</ul>
<p><span class="math display">\[
    \text{minimize}_{\mathbf{w}, b} \quad \frac{1}{2} \cdot \|\mathbf{w} \|^2 = \frac{1}{2} \cdot \langle \mathbf{w} . \mathbf{w} \rangle
\]</span></p>
<p><span class="math display">\[
    \text{s.t.} \quad t_i \cdot (\langle \mathbf{w} . \mathbf{x}_i \rangle +b) \geq 1 \quad \forall i.
\]</span></p>
<ul>
<li><p>The <strong>maximal margin classifier</strong> aims at finding the smallest weight vector having a functional margin of 1 on the training set.</p></li>
<li><p>The optimization problem is now a quadratic optimization problem, having therefore only one solution and being very easy to compute.</p></li>
<li><p>The linear inequality constraints restrict the correct values for the weight vector (a zero weight vector would have the minimal norm, but would not satisfy the constraints).</p></li>
</ul>
</section>

<section>
<section id="lagrange-optimization" class="title-slide slide level1 center">
<h1>2 - Lagrange optimization</h1>

</section>
<section class="slide level2">

<p><strong>Global optimization:</strong></p>
<p><span class="math display">\[
  \text{minimize}_{\mathbf{x}} \quad f(\mathbf{x}) \qquad \Longleftrightarrow \qquad \frac{\partial f}{\partial \mathbf{x}} (\mathbf{x}^*) = 0
\]</span></p>
<p><strong>Optimization under linear equality constraints:</strong></p>
<p><span class="math display">\[
  \text{minimize}_{\mathbf{x}} \quad f(\mathbf{x}) \qquad \text{s.t.} \quad g(\mathbf{x}) = 0
\]</span></p>
<p>We define the Lagrange function for the constrained optimization problem, parameterized by <span class="math inline">\(\lambda\)</span>, called a Lagrange multiplier:</p>
<p><span class="math display">\[
\mathcal{L}(\mathbf{x}, \lambda) = f(\mathbf{x}) + \lambda \cdot g(\mathbf{x})
\]</span></p>
<p>and search for its minimum <span class="math inline">\((\mathbf{x}^*, \lambda^*)\)</span>:</p>
<p><span class="math display">\[
  \text{minimize}_{\mathbf{x}, \lambda} \quad \mathcal{L}(\mathbf{x}, \lambda) \qquad \Longleftrightarrow \qquad
  \begin{cases}
  \frac{\partial \mathcal{L}}{\partial \mathbf{x}} (\mathbf{x}^*, \lambda^*) = 0  \\
  \\
  \frac{\partial \mathcal{L}}{\partial \lambda} (\mathbf{x}^*, \lambda^*) = g(\mathbf{x}^*) = 0
  \end{cases}
\]</span></p>
<p><span class="math inline">\(\rightarrow \mathbf{x}^*\)</span> is the minimum of <span class="math inline">\(f\)</span> on the subspace defined by <span class="math inline">\(g(\mathbf{x}) = 0\)</span></p>
</section></section>
<section>
<section id="example" class="title-slide slide level1 center">
<h1>Example</h1>

<img data-src="img/lagrange.png" class="r-stretch quarto-figure-center"></section>
<section class="slide level2">

<p><strong>Optimization under linear inequality constraints:</strong></p>
<p><span class="math display">\[
  \text{minimize}_{\mathbf{x}} \quad f(\mathbf{x}) \qquad \text{s.t.} \quad h(\mathbf{x}) \leq 0
\]</span></p>
<p>We define the generalized Lagrange function for the constrained optimization problem, with a parameter <span class="math inline">\(\alpha \geq 0\)</span> called the <strong>Karush-Kuhn-Tucker</strong> (KKT) multiplier:</p>
<p><span class="math display">\[
\mathcal{L}(\mathbf{x}, \alpha) = f(\mathbf{x}) + \alpha \cdot h(\mathbf{x})
\]</span></p>
<p>We then search <span class="math inline">\((\mathbf{x}^*, \alpha^*)\)</span> respecting the <em>Karush-Kuhn-Tucker</em> conditions:</p>
<p><span class="math display">\[
\begin{cases}
  \frac{\partial \mathcal{L}}{\partial \mathbf{x}} (\mathbf{x}^*, \alpha^*) = 0  \\
  \\
  \alpha^* \cdot \frac{\partial \mathcal{L}}{\partial \alpha} (\mathbf{x}^*, \alpha^*) = 0 \quad \Leftrightarrow \quad \alpha^* \cdot h(\mathbf{x}^*) = 0\\
  \\
  \alpha^*  \geq 0 \\
  \\
  h(\mathbf{x}^*) \leq 0
\end{cases}
\]</span></p>
<p><span class="math inline">\(\rightarrow \mathbf{x}^*\)</span> is the minimum of <span class="math inline">\(f\)</span> on the subspace defined by <span class="math inline">\(h(\mathbf{x}) \leq 0\)</span></p>
</section>
<section class="slide level2">

<p><strong>Optimization under linear inequality constraints:</strong></p>
<div class="columns">
<div class="column" style="width:50%;">
<p><span class="math display">\[
  \alpha^* \cdot \frac{\partial \mathcal{L}}{\partial \alpha} (\mathbf{x}^*, \alpha^*) = 0 \Leftrightarrow \alpha^* \cdot h(\mathbf{x}^*) = 0
\]</span></p>
<ul>
<li><p>We obtain a complex system to solve, but the only thing to remember is that:</p>
<ul>
<li><p>Either <span class="math inline">\(\alpha^* = 0\)</span>:</p>
<ul>
<li><span class="math inline">\(h(\mathbf{x}^*)\)</span> can take any (negative) value: the solution <span class="math inline">\(\mathbf{x}^*\)</span> can be anywhere in the half-space <span class="math inline">\(h(\mathbf{x}) \leq 0\)</span>.</li>
</ul></li>
<li><p>Or <span class="math inline">\(\alpha^* &gt; 0\)</span>:</p>
<ul>
<li>the solution <span class="math inline">\(\mathbf{x}^*\)</span> <strong>has</strong> to respect <span class="math inline">\(h(\mathbf{x}) = 0\)</span> (on the “hyperplane”).</li>
</ul></li>
</ul></li>
</ul>
</div><div class="column" style="width:45%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/lagrange.png"></p>
</figure>
</div>
<ul>
<li><p>When <span class="math inline">\(\alpha^* = 0\)</span>, the constraint is <strong>free</strong>.</p></li>
<li><p>When <span class="math inline">\(\alpha^* &gt; 0\)</span>, the constraint is <strong>saturated</strong>.</p></li>
</ul>
</div>
</div>
</section></section>
<section id="optimization-under-constraints-example-1" class="title-slide slide level1 center">
<h1>Optimization under constraints : example 1</h1>
<ul>
<li>Goal: minimize the quadratic function in 2D</li>
</ul>
<p><span class="math display">\[
  \text{minimize}_{x, y} \quad f(x, y) = x^2 + y^2
\]</span></p>
<ul>
<li>We only need to find where the partial derivates of this function are zero:</li>
</ul>
<p><span class="math display">\[
\begin{cases}
\frac{\partial f}{\partial x} (x^*, y^*) = 2\cdot x^* = 0 \\
\frac{\partial f}{\partial y} (x^*, y^*) = 2\cdot y^* = 0 \\
\end{cases}
\]</span></p>
<ul>
<li>The minimum of this function is therefore obtained for <span class="math inline">\((x^*, y^*) =(0,0)\)</span>.</li>
</ul>

<img data-src="img/optimization.png" style="width:40.0%" class="r-stretch quarto-figure-center"></section>

<section id="optimization-under-constraints-example-2" class="title-slide slide level1 center">
<h1>Optimization under constraints : example 2</h1>
<ul>
<li>Goal: minimize the quadratic function in 2D under linear equality constraint:</li>
</ul>
<p><span class="math display">\[
  \text{minimize}_{x, y} \quad f(x, y) = x^2 + y^2
\]</span></p>
<p><span class="math display">\[
  \text{s.t.}\quad g(x, y) = x + 2y +1 = 0
\]</span></p>

<img data-src="img/lagrange.png" style="width:60.0%" class="r-stretch quarto-figure-center"></section>

<section id="optimization-under-constraints-example-2-1" class="title-slide slide level1 center">
<h1>Optimization under constraints : example 2</h1>
<p><span class="math display">\[
  \text{minimize}_{x, y} \quad f(x, y) = x^2 + y^2
\]</span></p>
<p><span class="math display">\[
  \text{s.t.}\quad g(x, y) = x + 2y +1 = 0
\]</span></p>
<p>We define the Lagrange function of this optimization problem:</p>
<p><span class="math display">\[
\mathcal{L}(x, y, \lambda) = f(x, y) + \lambda \cdot g(x, y) = x^2 + y^2 + \lambda\cdot (x + 2y +1)
\]</span></p>
<p>and we search for its minimum <span class="math inline">\((x^*, y^*, \lambda^*)\)</span>:</p>
<p><span class="math display">\[
\frac{\partial \mathcal{L}}{\partial x} (x^*, y^*, \lambda^*) = 2\cdot x^* + \lambda^*= 0
\]</span> <span class="math display">\[
\frac{\partial \mathcal{L}}{\partial y} (x^*, y^*, \lambda^*) = 2\cdot y^* +2\lambda^*= 0
\]</span> <span class="math display">\[
\frac{\partial \mathcal{L}}{\partial \lambda} (x^*, y^*, \lambda^*) = x^* + 2y^* +1= 0
\]</span></p>
<p>We obtain a system of 3 linear equations with 3 variables.</p>
<p>We obtain the only solution <span class="math inline">\((x^*, y^*, \lambda^*) = (-\frac{1}{5}, -\frac{2}{5}, \frac{2}{5})\)</span>.</p>
<p>This solution verifies <span class="math inline">\(x^* + 2y^* +1= 0\)</span> and we have <span class="math inline">\(f(x^*, y^*)= \frac{1}{5}\)</span></p>
</section>

<section id="optimization-under-constraints-example-3" class="title-slide slide level1 center">
<h1>Optimization under constraints : example 3</h1>
<ul>
<li>Goal: minimize the quadratic function in 2D under linear inequality constraint:</li>
</ul>
<p><span class="math display">\[
  \text{minimize}_{x, y} \quad f(x, y) = x^2 + y^2
\]</span> <span class="math display">\[
  \text{s.t.}\quad h(x, y) = x + 2y +1 \leq 0
\]</span></p>

<img data-src="img/lagrange.png" style="width:60.0%" class="r-stretch quarto-figure-center"></section>

<section id="optimization-under-constraints-example-3-1" class="title-slide slide level1 center">
<h1>Optimization under constraints : example 3</h1>
<p><span class="math display">\[
  \text{minimize}_{x, y} \quad f(x, y) = x^2 + y^2
\]</span> <span class="math display">\[
  \text{s.t.}\quad h(x, y) = x + 2y +1 \leq 0
\]</span></p>
<p>We define the generalized Lagrangian of this optimization problem:</p>
<p><span class="math display">\[
\mathcal{L}(x, y, \alpha) = f(x, y) + \alpha \cdot h(x, y) = x^2 + y^2 + \alpha\cdot (x + 2y +1)
\]</span></p>
<p>and we search for a tuple <span class="math inline">\((x^*, y^*, \alpha^*)\)</span> satisfying the Karush-Kuhn-Tucker conditions:</p>
<p><span class="math display">\[
\begin{cases}
\frac{\partial \mathcal{L}}{\partial x} (x^*, y^*, \alpha^*) = 2\cdot x^* + \alpha^*= 0 \\
\frac{\partial \mathcal{L}}{\partial y} (x^*, y^*, \alpha^*) = 2\cdot y^* +2\alpha^*= 0 \\
\alpha^* \cdot  h(x^*, y^*) = \alpha^* \cdot  (x^* + 2y^* +1)= 0 \\
\alpha^* \geq 0 \\
h(x^*, y^*) = x^* + 2y^* +1 \leq 0 \\
\end{cases}
\]</span></p>
</section>

<section id="optimization-under-constraints-example-3-2" class="title-slide slide level1 center">
<h1>Optimization under constraints : example 3</h1>
<p><span class="math display">\[
  \text{minimize}_{x, y} \quad f(x, y) = x^2 + y^2
\]</span> <span class="math display">\[
  \text{s.t.}\quad h(x, y) = x + 2y +1 \leq 0
\]</span></p>
<p>The third KKT condition admits two cases:</p>
<p><span class="math display">\[
\alpha^* \cdot  h(x^*, y^*) = \alpha^* \cdot  (x^* + 2y^* +1)= 0
\]</span></p>
<ul>
<li><span class="math inline">\(\alpha^* = 0\)</span> : it implies <span class="math inline">\((x^* + 2y^* +1)\)</span> can have any value. The two first conditions become: <span class="math display">\[
2\cdot x^* + \alpha^*= 2\cdot x^* = 0 \]</span> <span class="math display">\[
2\cdot y^* + 2\alpha^*= 2\cdot y^* = 0
\]</span></li>
</ul>
<p>which have the solution <span class="math inline">\((x^*, y^*) = (0, 0)\)</span> but which is in conflict with the last KKT condition <span class="math inline">\(x^* + 2y^* +1 \leq 0\)</span>. The system is then not solvable, and it is not possible that <span class="math inline">\(\alpha^* = 0\)</span>.</p>
</section>

<section id="optimization-under-constraints-example-3-3" class="title-slide slide level1 center">
<h1>Optimization under constraints : example 3</h1>
<p><span class="math display">\[
  \text{minimize}_{x, y} \quad f(x, y) = x^2 + y^2
\]</span> <span class="math display">\[
  \text{s.t.}\quad h(x, y) = x + 2y +1 \leq 0
\]</span></p>
<p>The third KKT condition admits two cases:</p>
<p><span class="math display">\[
\alpha^* \cdot  h(x^*, y^*) = \alpha^* \cdot  (x^* + 2y^* +1)= 0
\]</span></p>
<ul>
<li><span class="math inline">\(\alpha^* &gt; 0\)</span> : it implies <span class="math inline">\(x^* + 2y^* +1 = 0\)</span>. We now have the system: <span class="math display">\[
2\cdot x^* + \alpha^* = 0
\]</span> <span class="math display">\[
2\cdot y^* + 2\alpha^* = 0
\]</span> <span class="math display">\[
x^* + 2y^* +1 = 0
\]</span></li>
</ul>
<p>which has the solution <span class="math inline">\((x^*, y^*, \alpha^*) = (-\frac{1}{5}, -\frac{2}{5}, \frac{2}{5})\)</span>. This solution verifies <span class="math inline">\(x^* + 2y^* +1 = 0 \leq 0\)</span>.</p>
<p>When <span class="math inline">\(\alpha\)</span> is strictly positive, it means the solution is on the border of the inequality: the constraint is said “saturated”.</p>
</section>

<section id="optimization-under-constraints-example-4" class="title-slide slide level1 center">
<h1>Optimization under constraints : example 4</h1>
<ul>
<li>Goal: minimize the quadratic function in 2D under linear inequality constraint:</li>
</ul>
<p><span class="math display">\[
  \text{minimize}_{x, y} \quad f(x, y) = x^2 + y^2
\]</span> <span class="math display">\[
  \text{s.t.}\quad h(x, y) = -x - 2\, y -1 \leq 0
\]</span></p>

<img data-src="img/lagrange.png" style="width:50.0%" class="r-stretch quarto-figure-center"></section>

<section id="optimization-under-constraints-example-4-1" class="title-slide slide level1 center">
<h1>Optimization under constraints : example 4</h1>
<p><span class="math display">\[
  \text{minimize}_{x, y} \quad f(x, y) = x^2 + y^2
\]</span> <span class="math display">\[
  \text{s.t.}\quad h(x, y) = -x - 2 \, y -1 \leq 0
\]</span></p>
<p>We define the generalized Lagrangian of this optimization problem:</p>
<p><span class="math display">\[
\mathcal{L}(x, y, \alpha) = f(x, y) + \alpha \cdot h(x, y) = x^2 + y^2 + \alpha\cdot (-x - 2y -1)
\]</span></p>
<p>and we search for a tuple <span class="math inline">\((x^*, y^*, \alpha^*)\)</span> satisfying the Karush-Kuhn-Tucker conditions:</p>
<p><span class="math display">\[
\begin{cases}
\frac{\partial \mathcal{L}}{\partial x} (x^*, y^*, \alpha^*) = 2\cdot x^* + \alpha^*= 0 \\
\frac{\partial \mathcal{L}}{\partial y} (x^*, y^*, \alpha^*) = 2\cdot y^* +2\alpha^*= 0 \\
\alpha^* \cdot  h(x^*, y^*) = \alpha^* \cdot  (-x^* - 2y^* -1)= 0 \\
\alpha^* \geq 0 \\
h(x^*, y^*) = - x^* - 2y^* -1 \leq 0 \\
\end{cases}
\]</span></p>
</section>

<section id="optimization-under-constraints-example-4-2" class="title-slide slide level1 center">
<h1>Optimization under constraints : example 4</h1>
<p><span class="math display">\[
  \text{minimize}_{x, y} \quad f(x, y) = x^2 + y^2
\]</span> <span class="math display">\[
  \text{s.t.}\quad h(x, y) = -x - 2y -1 \leq 0
\]</span></p>
<p>The third KKT condition admits two cases:</p>
<p><span class="math display">\[
\alpha^* \cdot  h(x^*, y^*) = \alpha^* \cdot  (-x^* - 2y^* -1)= 0
\]</span></p>
<ul>
<li><span class="math inline">\(\alpha^* = 0\)</span> : it implies <span class="math inline">\((x^* + 2y^* +1)\)</span> can have any value. The two first conditions become:</li>
</ul>
<p><span class="math display">\[
2\cdot x^* + \alpha^*= 2\cdot x^* = 0 \]</span> <span class="math display">\[
2\cdot y^* + 2\alpha^*= 2\cdot y^* = 0
\]</span></p>
<p>which have the solution <span class="math inline">\((x^*, y^*) = (0, 0)\)</span>. This solution now fits with the last KKT condition <span class="math inline">\(-x^* -2y^* -1 \leq 0\)</span>. This is an admissible solution.</p>
</section>

<section>
<section id="optimization-under-constraints-example-4-3" class="title-slide slide level1 center">
<h1>Optimization under constraints : example 4</h1>
<p><span class="math display">\[
  \text{minimize}_{x, y} \quad f(x, y) = x^2 + y^2
\]</span> <span class="math display">\[
  \text{s.t.}\quad h(x, y) = -x - 2y -1 \leq 0
\]</span></p>
<p>The third KKT condition admits two cases:</p>
<p><span class="math display">\[
\alpha^* \cdot  h(x^*, y^*) = \alpha^* \cdot  (-x^* - 2y^* -1)= 0
\]</span></p>
<ul>
<li><span class="math inline">\(\alpha^* &gt; 0\)</span> : it implies <span class="math inline">\(-x^* - 2y^* - 1 = 0\)</span>. We now have the system:</li>
</ul>
<p><span class="math display">\[
2\cdot x^* - \alpha^* = 0
\]</span> <span class="math display">\[
2\cdot y^* - 2\alpha^* = 0
\]</span> <span class="math display">\[
x^* + 2y^* +1 = 0
\]</span></p>
<p>which has the solution <span class="math inline">\((x^*, y^*, \alpha^*) = (-\frac{1}{5}, -\frac{2}{5}, - \frac{2}{5})\)</span>. This solution is in conflict with the KKT condition <span class="math inline">\(\alpha^* &gt; 0\)</span>. It is therefore not valid.</p>
<p>When <span class="math inline">\(\alpha\)</span> is zero, it means the solution is largely within the inequality constraint: the constraint is said “free”.</p>
</section>
<section class="slide level2">

<p><strong>Generalized Lagrange Method</strong></p>
<ul>
<li>We can have as many linear equalities and inequalities as we want:</li>
</ul>
<p><span class="math display">\[
  \text{minimize}_{\mathbf{x}} \quad f(\mathbf{x})
\]</span> <span class="math display">\[
  \text{s.t.}\quad g_i (\mathbf{x}) = 0 \quad  \forall i \in [1, l]
\]</span> <span class="math display">\[
  \text{s.t.}\quad h_j (\mathbf{x}) \leq 0 \quad  \forall j \in [1, k]
\]</span></p>
<ul>
<li>We only need to write the <strong>generalized Lagrangian function</strong>:</li>
</ul>
<p><span class="math display">\[
\mathcal{L}(\mathbf{x}, \lambda_1, ... , \lambda_l, \alpha_1, ... , \alpha_k) = f(\mathbf{x}) + \sum_{i=1}^l \lambda_i \cdot g_i(\mathbf{x}) + \sum_{j=1}^{k}\alpha_j \cdot h_j(\mathbf{x})
\]</span></p>
<p>with one parameter per contraint.</p>
</section>
<section class="slide level2">

<p><strong>Generalized Lagrange Method</strong></p>
<p><span class="math display">\[
  \text{minimize}_{\mathbf{x}} \quad f(\mathbf{x})
\]</span> <span class="math display">\[
  \text{s.t.}\quad g_i (\mathbf{x}) = 0 \quad  \forall i \in [1, l]
\]</span> <span class="math display">\[
  \text{s.t.}\quad h_j (\mathbf{x}) \leq 0 \quad  \forall j \in [1, k]
\]</span></p>
<ul>
<li>We then find the variables <span class="math inline">\((\mathbf{x}^*, \lambda_1^*, ... , \lambda_l^*, \alpha_1^*, ... , \alpha_k^*)\)</span> which satisfy the KKT confitions:</li>
</ul>
<p><span class="math display">\[
\begin{cases}
  \frac{\partial \mathcal{L}}{\partial \mathbf{x}} (\mathbf{x}^*, \lambda_1^*, ... , \lambda_l^*, \alpha_1^*, ... , \alpha_k^*) = 0  \\
  \\
  \frac{\partial \mathcal{L}}{\partial \lambda_i} (\mathbf{x}^*, \lambda_1^*, ... , \lambda_l^*, \alpha_1^*, ... , \alpha_k^*) = 0 \quad  \forall i \in [1, l]\\
  \\
  \alpha_j^* \cdot \frac{\partial \mathcal{L}}{\partial \alpha_j} (\mathbf{x}^*, \lambda_1^*, ... , \lambda_l^*, \alpha_1^*, ... , \alpha_k^*) = 0 \quad  \forall j \in [1, k]\\
  \\
  \alpha_j^* \geq 0 \quad  \forall j \in [1, k]\\
  \\
  h_j(\mathbf{x}^*) \leq 0 \quad  \forall j \in [1, k]
\end{cases}
\]</span></p>
</section></section>
<section>
<section id="maximal-margin-classifier" class="title-slide slide level1 center">
<h1>3 - Maximal Margin Classifier</h1>

</section>
<section class="slide level2">

<p><strong>Primal form of the maximal margin classifier</strong></p>
<p>Given a linearly separable training set:</p>
<p><span class="math display">\[\mathcal{S} = \{(\mathbf{x}_1, y_1), ... , (\mathbf{x}_N, y_N) \}\]</span></p>
<p>the optimal hyperplane <span class="math inline">\((\mathbf{w}^*, b^*)\)</span> that solves the optimization problem:</p>
<p><span class="math display">\[
\text{minimize}_{\mathbf{w}, b} \quad \frac{1}{2} \cdot \|\mathbf{w} \|^2 = \frac{1}{2} \cdot \langle \mathbf{w} . \mathbf{w} \rangle
\]</span></p>
<p><span class="math display">\[
\text{s.t.} \quad 1 - t_i \cdot (\langle \mathbf{w} . \mathbf{x}_i\rangle +b) \leq 0 \quad \forall i \in [1, N]
\]</span></p>
<p>realizes the maximal margin hyperplane with geometric margin:</p>
<p><span class="math display">\[
\gamma = \frac{1}{\|\mathbf{w} \|}
\]</span></p>
</section></section>
<section id="lagrangian-of-the-maximal-margin-classifier" class="title-slide slide level1 center">
<h1>Lagrangian of the maximal margin classifier</h1>
<p><span class="math display">\[
\mathcal{L}(\mathbf{w}, b, \alpha_1, .. , \alpha_N) = \frac{1}{2} \cdot \langle \mathbf{w} . \mathbf{w} \rangle + \sum_{i=1}^N \alpha_i \cdot (1 - t_i \cdot (\langle \mathbf{w} . \mathbf{x}_i\rangle +b))
\]</span></p>
<ul>
<li>The optimal solution <span class="math inline">\((\mathbf{w}^*, b^*, \alpha_1^*, .. , \alpha_N^*)\)</span> satisfies the Karush-Kuhn-Tucker conditions:</li>
</ul>
<p><span class="math display">\[
\begin{cases}
  \frac{\partial \mathcal{L}}{\partial \mathbf{w}} (\mathbf{w}^*, b^*, \alpha_1^*, .. , \alpha_N^*) = \mathbf{w}^* - \sum_{i=1}^N \alpha_i^* \cdot t_i \cdot \mathbf{x}_i = 0  \\
  \\
  \frac{\partial \mathcal{L}}{\partial b} (\mathbf{w}^*, b^*, \alpha_1^*, .. , \alpha_N^*) = - \sum_{i=1}^N \alpha_i^* \cdot  t_i  = 0  \\
  \\
  \alpha_i^* \cdot (1 - t_i \cdot (\langle \mathbf{w}^* . \mathbf{x}_i\rangle +b^*)) = 0 \quad  \forall i \in [1, N]\\
  \\
  \alpha_i^* \geq 0 \quad  \forall i \in [1, N]\\
  \\
  (1 - t_i \cdot (\langle \mathbf{w}^* . \mathbf{x}_i\rangle +b^*)) \leq 0 \quad  \forall i \in [1, N]
\end{cases}
\]</span></p>
</section>

<section id="the-weight-vector-depends-on-the-data" class="title-slide slide level1 center">
<h1>The weight vector depends on the data</h1>
<ul>
<li>The first KKT condition tells us that the optimal weight vector will be a linear combination of the training data:</li>
</ul>
<p><span class="math display">\[
\mathbf{w}^* = \sum_{i=1}^N \alpha_i^* \cdot t_i \cdot \mathbf{x}_i
\]</span></p>
<ul>
<li>If we know the <span class="math inline">\(\alpha\)</span> values, we do not need the weight vector anymore to compute the decision function:</li>
</ul>
<p><span class="math display">\[
    F(\mathbf{x}) = \sum_{i=1}^{N} \alpha_i^* \cdot t_i \cdot  \langle \mathbf{x}_i . \mathbf{x}\rangle + b^*
\]</span></p>
<ul>
<li>The intercept term <span class="math inline">\(b^*\)</span> disappeared from the conditions, but can be found through:</li>
</ul>
<p><span class="math display">\[
b^* = - \frac{\max_{i \in \mathcal{C}^-} \langle \mathbf{w}^* . \mathbf{x}_i\rangle + \min_{i \in \mathcal{C}^+} \langle \mathbf{w}^* . \mathbf{x}_i\rangle}{2}
\]</span></p>
</section>

<section id="dual-form-of-the-perceptron-algorithm" class="title-slide slide level1 center">
<h1>Dual form of the Perceptron algorithm</h1>
<ul>
<li><p>Primal form of the learning rule: <span class="math inline">\(\mathbf{w} \gets \textbf{w} + \eta \cdot (t_i - f_{\mathbf{w}}( \mathbf{x}_i)) \cdot \mathbf{x}_i\)</span></p></li>
<li><p>If <span class="math inline">\(\mathbf{w}(0) = 0\)</span>, the weight vector converges towards a linear combination of the examples:</p></li>
</ul>
<p><span class="math display">\[ \mathbf{w} = \sum_{j=1}^{N} \alpha_j \cdot y_j \cdot \mathbf{x_j}
\]</span></p>
<ul>
<li><p><span class="math inline">\(\alpha_j\)</span> is proportional to the number of times when the example was misclassified: embedding strength.</p></li>
<li><p>The hypothesis can be rewritten in the dual form:</p></li>
</ul>
<p><span class="math display">\[\begin{eqnarray*}
        f_{\mathbf{w}}( \mathbf{x})   &amp;=&amp; \text{sign}( \langle \mathbf{w} . \mathbf{x}\rangle ) \\
                        &amp;=&amp; \text{sign}( \langle \sum_{j=1}^{N} \alpha_j \cdot y_j \cdot \mathbf{x_j} . \mathbf{x}\rangle ) \\
                        &amp;=&amp; \text{sign}( \sum_{j=1}^{N} \alpha_j \cdot y_j \cdot  \langle \mathbf{x_j} . \mathbf{x}\rangle ) \\
\end{eqnarray*}\]</span></p>
</section>

<section id="dual-form-of-the-perceptron-algorithm-1" class="title-slide slide level1 center">
<h1>Dual form of the Perceptron algorithm</h1>
<p><span class="math inline">\(\mathbf{\alpha} \gets 0^N\)</span></p>
<p><span class="math inline">\(\textbf{while } \hat{\gamma} &lt; 0 :\)</span></p>
<p><span class="math inline">\(\qquad \textbf{forall } \text{ examples } (\mathbf{x}_i, t_i) :\)</span></p>
<p><span class="math inline">\(\qquad \qquad \hat{\gamma}_i = t_i \cdot \sum_{j=1}^{N} \alpha_j \cdot y_j \cdot \langle \mathbf{x}_j . \mathbf{x}_i \rangle\)</span></p>
<p><span class="math inline">\(\qquad \qquad \textbf{if } \hat{\gamma}_i &lt; 0 :\)</span></p>
<p><span class="math inline">\(\qquad \qquad \qquad \alpha_i = \alpha_i + 1\)</span></p>
<p><span class="math inline">\(\qquad \hat{\gamma} = \min_{i} \hat{\gamma}_i\)</span></p>
<p>The dual form of an algorithm only relies on the Gram matrix of the training examples:</p>
<p><span class="math display">\[ G = X^T \cdot X = ( \langle x_i . x_j \rangle)_{i,j=1..N} \]</span></p>
</section>

<section>
<section id="dual-form-of-the-maximal-margin-classifier" class="title-slide slide level1 center">
<h1>Dual form of the maximal margin classifier</h1>
<ul>
<li><p>We are searching for <span class="math inline">\((\mathbf{w}^*, b^*, \alpha_1^*, .. , \alpha_N^*)\)</span>, but <span class="math inline">\(\mathbf{w}^*\)</span> is entirely defined by the <span class="math inline">\(\alpha_i\)</span>.</p></li>
<li><p>If we plug back the known value of <span class="math inline">\(\mathbf{w}^*= \sum_{i=1}^N \alpha_i^* \cdot t_i \cdot \mathbf{x}_i\)</span> into the Lagrangian, we obtain:</p></li>
</ul>
<p><span class="math display">\[\begin{eqnarray*}
\mathcal{L}(\alpha_1, .. , \alpha_N) &amp;=&amp; \frac{1}{2} \cdot \langle \mathbf{w} . \mathbf{w} \rangle + \sum_{i=1}^N \alpha_i \cdot (1 - t_i \cdot (\langle \mathbf{w} . \mathbf{x}_i\rangle +b)) \\
            &amp;=&amp; \frac{1}{2} \cdot \langle (\sum_{i=1}^N \alpha_i \cdot t_i \cdot \mathbf{x}_i) . (\sum_{j=1}^N \alpha_j \cdot y_j \cdot \mathbf{x_j}) \rangle \\
            &amp;&amp; + \sum_{i=1}^N \alpha_i \cdot (1 - t_i \cdot (\langle (\sum_{j=1}^N \alpha_j \cdot y_j \cdot \mathbf{x_j}) . \mathbf{x}_i\rangle +b)) \\
            &amp;=&amp; \sum_{i=1}^N \alpha_i - \frac{1}{2} \cdot \sum_{i=1}^N \sum_{j=1}^N \alpha_i \cdot \alpha_j \cdot t_i \cdot y_j \cdot \langle x_i . x_j \rangle
\end{eqnarray*}\]</span></p>
</section>
<section class="slide level2">

<p><strong>Dual form of the maximal margin classifier</strong></p>
<p>Given a linearly separable training set <span class="math inline">\(\mathcal{S} = \{(\mathbf{x}_1, y_1), ... , (\mathbf{x}_N, y_N) \}\)</span>, the parameters <span class="math inline">\((\alpha_1, ..., \alpha_N)\)</span> that solve the optimization problem:</p>
<p><span class="math display">\[
\text{maximize}_{\alpha_1, .. , \alpha_N} \quad \mathcal{Q}(\alpha_1, .. , \alpha_N) = \sum_{i=1}^N \alpha_i - \frac{1}{2} \cdot \sum_{i=1}^N \sum_{j=1}^N \alpha_i \cdot \alpha_j \cdot t_i \cdot y_j \cdot \langle \mathbf{x}_i . \mathbf{x}_j \rangle
\]</span><span class="math display">\[
\text{s.t.} \quad \sum_{i=1}^N \alpha_i \cdot t_i = 0
\]</span><span class="math display">\[
\text{s.t.} \quad \alpha_i \geq 0 \quad \forall i \in [1, N]
\]</span></p>
<p>define a decision rule <span class="math inline">\(\text{sign}(F(\mathbf{x}))\)</span> where:</p>
<p><span class="math display">\[F(\mathbf{x}) = \sum_{i=1}^{N} \alpha_i \cdot t_i \cdot  \langle \mathbf{x}_i . \mathbf{x}\rangle +b\]</span></p>
<p>that is equivalent to the maximal margin hyperplane with the geometric margin <span class="math inline">\(\gamma = \frac{1}{\sqrt{\sum_{i=1}^{N} \alpha_i}}\)</span>.</p>
</section></section>
<section id="support-vectors" class="title-slide slide level1 center">
<h1>Support vectors</h1>
<ul>
<li>For both the primal and dual forms of the maximal margin classifier, we are searching for the KKT multiplier <span class="math inline">\(\alpha_i\)</span> of each training example.</li>
</ul>
<div class="columns">
<div class="column" style="width:40%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/supportvectors.png"></p>
</figure>
</div>
</div><div class="column" style="width:55%;">
<ul>
<li>The weight vector of the hyperplane is entirely defined by the <span class="math inline">\(\alpha_i\)</span> and the training data:</li>
</ul>
<p><span class="math display">\[\mathbf{w}^*= \sum_{i=1}^N \alpha_i^* \cdot t_i \cdot \mathbf{x}_i\]</span></p>
<ul>
<li>However, only the training examples whose constraint is saturated:</li>
</ul>
<p><span class="math display">\[t_i \cdot (\langle \mathbf{w}^* . \mathbf{x}_i\rangle +b^*) = 1\]</span></p>
<p>will have a KKT multiplier <span class="math inline">\(\alpha_i^*\)</span> different from 0.</p>
</div>
</div>
<ul>
<li><p>Examples with a functional margin of 1 (the “closest” to the hyperplane) will have a strictly positive <span class="math inline">\(\alpha_i\)</span>, all others will have 0.</p></li>
<li><p>The examples with non-zero KKT multipliers are called <strong>support vectors</strong>.</p></li>
</ul>
</section>

<section id="support-vectors-1" class="title-slide slide level1 center">
<h1>Support vectors</h1>
<div class="columns">
<div class="column" style="width:40%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/supportvectors.png"></p>
</figure>
</div>
</div><div class="column" style="width:55%;">
<ul>
<li>The <span class="math inline">\(N_{SV} &lt;&lt; N\)</span> support vectors and their KKT multipliers are the only relevant information after learning:</li>
</ul>
<p><span class="math display">\[
\mathbf{w}^* = \sum_{i=1}^{N_{SV}} \alpha_i^* \cdot t_i \cdot \mathbf{x}_i
\]</span></p>
<p><span class="math display">\[
b^* = 1 -  \sum_{i=1}^{N_{SV}} \alpha_i \cdot t_i \cdot \langle \mathbf{x}_i . \mathbf{x}_{\text{SV}}^+ \rangle
\]</span></p>
</div>
</div>
<ul>
<li>The decision function only depends on the support vectors, not the weight vector:</li>
</ul>
<p><span class="math display">\[
    F(\mathbf{x}) = \sum_{i=1}^{N_{SV}} \alpha_i \cdot t_i \cdot  \langle \mathbf{x}_i . \mathbf{x}\rangle + 1 -  \sum_{i=1}^{N_{SV}} \alpha_i \cdot t_i \cdot \langle \mathbf{x}_i . \mathbf{x}_{\text{SV}}^+ \rangle
\]</span></p>
<ul>
<li>The support vectors “support” or “carry” the hyperplane, i.e.&nbsp;they are sufficient to define it entirely.</li>
</ul>
</section>

<section id="maximal-margin-classifier-generalization" class="title-slide slide level1 center">
<h1>Maximal margin classifier : generalization</h1>
<div class="columns">
<div class="column" style="width:40%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/supportvectors.png"></p>
</figure>
</div>
</div><div class="column" style="width:55%;">
<p><span class="math display">\[
F(\mathbf{x}) = \sum_{i=1}^{N_{SV}} \alpha_i \cdot t_i \cdot  \langle \mathbf{x}_i . \mathbf{x}\rangle +b^*
\]</span></p>
<p><span class="math display">\[
\sum_{i=1}^{N_{SV}} \alpha_i \cdot t_i = 0
\]</span></p>
<p><span class="math display">\[
\alpha_i &gt; 0 \quad \forall i \in [1, N_{SV}]
\]</span></p>
</div>
</div>
<ul>
<li><p>The decision function does not depend on the weights, but only on the support vectors and their multipliers.</p></li>
<li><p>The complexity of the classifier is therefore only dependent on the number of support vectors.</p></li>
<li><p><strong>If we have less support vectors than input dimensions, we have reduced the number of free parameters!</strong></p></li>
<li><p>The training examples which are not support vectors are useless after learning: they can be suppressed without changing the result.</p></li>
</ul>
</section>

<section id="maximal-margin-classifier-generalization-1" class="title-slide slide level1 center">
<h1>Maximal margin classifier : generalization</h1>
<div class="columns">
<div class="column" style="width:40%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/supportvectors.png"></p>
</figure>
</div>
</div><div class="column" style="width:55%;">
<p><span class="math display">\[
F(\mathbf{x}) = \sum_{i=1}^{N_{SV}} \alpha_i \cdot t_i \cdot  \langle \mathbf{x}_i . \mathbf{x}\rangle +b^*
\]</span></p>
<p><span class="math display">\[
\sum_{i=1}^{N_{SV}} \alpha_i \cdot t_i = 0
\]</span></p>
<p><span class="math display">\[
\alpha_i &gt; 0 \quad \forall i \in [1, N_{SV}]
\]</span></p>
</div>
</div>
<ul>
<li><p>Generalization error with probability (1-<span class="math inline">\(\delta\)</span>): <span class="math display">\[
\epsilon (F) \leq \frac{1}{N - N_{SV}} \cdot (N_{SV}\cdot (1 - \log \frac{N_{SV}}{N}) + \log \frac{N}{\delta} )
\]</span></p></li>
<li><p>Expected generalization error: <span class="math display">\[
\epsilon = \frac{N_{SV}}{N}
\]</span></p></li>
<li><p><strong>The fewer support vectors, the simpler the model, the better is the generalization.</strong></p></li>
</ul>
</section>

<section id="maximal-margin-classifier-generalization-2" class="title-slide slide level1 center">
<h1>Maximal margin classifier : generalization</h1>
<div class="columns">
<div class="column" style="width:40%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/supportvectors.png"></p>
</figure>
</div>
</div><div class="column" style="width:55%;">
<p><span class="math display">\[
F(\mathbf{x}) = \sum_{i=1}^{N_{SV}} \alpha_i \cdot t_i \cdot  \langle \mathbf{x}_i . \mathbf{x}\rangle +b^*
\]</span></p>
<p><span class="math display">\[
\sum_{i=1}^{N_{SV}} \alpha_i \cdot t_i = 0
\]</span></p>
<p><span class="math display">\[
\alpha_i &gt; 0 \quad \forall i \in [1, N_{SV}]
\]</span></p>
</div>
</div>
<ul>
<li>Vapnik and Chervonenkis were able to define a bound on the VC dimension:</li>
</ul>
<p><span class="math display">\[
    \text{VC}_\text{dim}(\text{SVM}) = \min ( \lceil  \frac{\max_{(i, j) \in [1, N_{\text{SV}}]^2} \|\mathbf{x}_i - \mathbf{x}_j \|^2}{2 \cdot \gamma}\rceil, d) + 1
\]</span></p>
<ul>
<li>By normalizing the input and controlling the geometric margin <span class="math inline">\(\gamma = \frac{1}{\sqrt{\sum_{i=1}^{N} \alpha_i}}\)</span>, one can obtain a classifier of smaller VC dimension that the equivalent linear classifier.</li>
</ul>
</section>

<section id="maximal-margin-classifier-summary" class="title-slide slide level1 center">
<h1>Maximal margin classifier : summary</h1>
<div class="columns">
<div class="column" style="width:40%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/supportvectors.png"></p>
</figure>
</div>
</div><div class="column" style="width:55%;">
<ul>
<li><p>The <strong>maximal margin classifier</strong> tries to find the smallest weight vector with a functional margin of 1.</p></li>
<li><p>The primal and dual forms of the algorithm associate a KKT multiplier <span class="math inline">\(\alpha_i\)</span> to each training example.</p></li>
<li><p>Only a few examples will have a non-zero KKT multiplier: they are the closest points to the hyperplane and are called <strong>support vectors</strong>.</p></li>
</ul>
</div>
</div>
<ul>
<li>The support vectors define completely the hyperplane:</li>
</ul>
<p><span class="math display">\[
    F(\mathbf{x}) = \sum_{i=1}^{N_{SV}} \alpha_i \cdot t_i \cdot  \langle \mathbf{x}_i . \mathbf{x}\rangle + 1 -  \sum_{i=1}^{N_{SV}} \alpha_i \cdot t_i \cdot \langle \mathbf{x}_i . \mathbf{x}_{\text{SV}}^+ \rangle
\]</span></p>
<ul>
<li><p>The complexity of the maximal margin classifier does not depend on the input dimensions, but on the number of support vectors.</p></li>
<li><p>Reducing the number of support vectors leads to better generalization.</p></li>
</ul>
</section>

<section>
<section id="soft-margin-classifier" class="title-slide slide level1 center">
<h1>4 - Soft Margin Classifier</h1>

</section>
<section class="slide level2">


<img data-src="img/softmargin.png" class="r-stretch quarto-figure-center"></section>
<section class="slide level2">


<img data-src="img/softmargin2.png" class="r-stretch quarto-figure-center"></section>
<section class="slide level2">


<img data-src="img/softmargin3.png" class="r-stretch quarto-figure-center"></section></section>
<section id="soft-margin-classifier-introducing-slack-variables" class="title-slide slide level1 center">
<h1>Soft margin classifier: introducing slack variables</h1>
<div class="columns">
<div class="column" style="width:40%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/softmargin3.png"></p>
</figure>
</div>
</div><div class="column" style="width:50%;">
<ul>
<li>The limit on the functional margin for a maximal margin classifier is hard:</li>
</ul>
<p><span class="math display">\[
   t_i \cdot (\langle \mathbf{w} . \mathbf{x}_i\rangle +b) \geq 1
\]</span></p>
<ul>
<li>This limit is violated when the data is noisy (the classification is still correct, but the geometric margin is small) or when there exist outliers (the classification can not be correct as the data is not linearly separable).</li>
</ul>
</div>
</div>
<ul>
<li>The soft-margin classifier adds some flexibility to the optimization algorithm by allowing some points to violate the functional margin condition.</li>
</ul>
<p><span class="math display">\[
            t_i \cdot (\langle \mathbf{w} . \mathbf{x}_i\rangle +b) \geq 1 - \xi_i \quad \forall i \in [1, N]
\]</span></p>
</section>

<section id="soft-margin-classifier-introducing-slack-variables-1" class="title-slide slide level1 center">
<h1>Soft margin classifier: introducing slack variables</h1>
<div class="columns">
<div class="column" style="width:40%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/softmargin3.png"></p>
</figure>
</div>
</div><div class="column" style="width:50%;">
<ul>
<li>The limit on the functional margin is now variable:</li>
</ul>
<p><span class="math display">\[
            t_i \cdot (\langle \mathbf{w} . \mathbf{x}_i\rangle +b) \geq 1 - \xi_i \quad \forall i \in [1, N]
\]</span></p>
<ul>
<li><p>The parameters <span class="math inline">\(\xi_i \geq 0\)</span> are called the <strong>slack variables</strong>: they tell how much each example violates the functional margin.</p>
<ul>
<li><span class="math inline">\(\xi_i = 0\)</span> : no problem</li>
<li><span class="math inline">\(0 &lt; \xi_i &lt; 1\)</span> : good classification but inside the margin</li>
<li><span class="math inline">\(\xi_i &gt;1\)</span> : misclassification.</li>
</ul></li>
</ul>
</div>
</div>
</section>

<section id="soft-margin-classifier-introducing-slack-variables-2" class="title-slide slide level1 center">
<h1>Soft margin classifier: introducing slack variables</h1>

<img data-src="img/hinge.png" style="width:40.0%" class="r-stretch quarto-figure-center"><ul>
<li>The slack variables can be expressed using the <strong>Hinge loss</strong> function:</li>
</ul>
<p><span class="math display">\[
    \xi_i = \max(0, 1 - t_i \cdot (\langle \mathbf{w} . \mathbf{x}_i\rangle +b))
\]</span></p>
<p><span class="math display">\[t_i \cdot (\langle \mathbf{w} . \mathbf{x}_i\rangle +b) &gt; 1 \rightarrow \xi_i = 0\]</span></p>
<p><span class="math display">\[t_i \cdot (\langle \mathbf{w} . \mathbf{x}_i\rangle +b) &lt; 1 \rightarrow \xi_i &gt; 0\]</span></p>
<div class="footer">
<p>Source: <a href="https://en.wikipedia.org/wiki/Hinge_loss" class="uri">https://en.wikipedia.org/wiki/Hinge_loss</a></p>
</div>
</section>

<section id="primal-form-of-the-1-norm-soft-margin-optimization" class="title-slide slide level1 center">
<h1>Primal form of the 1-norm soft-margin optimization</h1>
<div class="columns">
<div class="column" style="width:40%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/softmargin3.png"></p>
</figure>
</div>
</div><div class="column" style="width:50%;">
<ul>
<li><p>The goal of a soft-margin classifier is to <strong>maximize the geometric margin</strong> (or minimize the norm of the weight vector) while <strong>minimizing the slack variables</strong>.</p></li>
<li><p>This trade-off between two conflicting objectives is called <strong>regularization</strong>. Same principle as for NN: the main goal is to have a small training error, but adding a constraint on the free parameters forces a better generalization.</p></li>
</ul>
</div>
</div>
<p><strong>Primal form the soft margin classifier</strong></p>
<p><span class="math display">\[
\text{minimize}_{\mathbf{w}, b} \qquad \frac{1}{2} \cdot \langle \mathbf{w} . \mathbf{w} \rangle  + C \cdot \sum_{i=1}^N \xi_i
\]</span></p>
<p><span class="math display">\[
\text{s.t.} \quad (1 - \xi_i) - t_i \cdot (\langle \mathbf{w} . \mathbf{x}_i\rangle +b) \leq 0 \quad \forall i \in [1, N]
\]</span></p>
<p><span class="math display">\[
\text{s.t.} \quad \xi_i \geq 0 \quad \forall i \in [1, N]
\]</span></p>
</section>

<section>
<section id="soft-margin-classification-uses-regularization" class="title-slide slide level1 center">
<h1>Soft margin classification uses regularization</h1>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/C.png"></p>
</figure>
</div>
</div><div class="column" style="width:50%;">
<p><span class="math display">\[
\text{minimize}_{\mathbf{w}, b} \qquad \frac{1}{2} \cdot \langle \mathbf{w} . \mathbf{w} \rangle  + C \cdot \sum_{i=1}^N \xi_i
\]</span></p>
<p><span class="math display">\[
\text{s.t.} \quad (1 - \xi_i) - t_i \cdot (\langle \mathbf{w} . \mathbf{x}_i\rangle +b) \leq 0 \quad \forall i \in [1, N]
\]</span></p>
<p><span class="math display">\[
\text{s.t.} \quad \xi_i \geq 0 \quad \forall i \in [1, N]
\]</span></p>
</div>
</div>
<ul>
<li><p>The user-defined parameter <span class="math inline">\(C\)</span> controls the trade-off between the complexity of the machine (its number of support vectors) and the number on non-separable examples.</p></li>
<li><p>If <span class="math inline">\(C\)</span> is high, the slack variables will be minimized in priority, so the soft-margin classifier behaves like a maximal margin classifier.</p></li>
<li><p>If <span class="math inline">\(C\)</span> is low, the slack variables can grow, leading to misclassifications, but obtaining a fat margin.</p></li>
</ul>
<div class="footer">
<p>Source: <a href="https://stats.stackexchange.com/questions/31066/what-is-the-influence-of-c-in-svms-with-linear-kernel" class="uri">https://stats.stackexchange.com/questions/31066/what-is-the-influence-of-c-in-svms-with-linear-kernel</a></p>
</div>
</section>
<section class="slide level2">

<p><strong>Dual form of the 1-norm soft-margin optimization</strong></p>
<p><span class="math display">\[
    \text{maximize}_{\alpha_1, ... , \alpha_N} \qquad \mathcal{Q}(\alpha_1, .. , \alpha_N) = \sum_{i=1}^N \alpha_i - \frac{1}{2} \cdot \sum_{i=1}^N \sum_{j=1}^N \alpha_i \cdot \alpha_j \cdot t_i \cdot y_j \cdot \langle \mathbf{x}_i . \mathbf{x}_j \rangle
\]</span></p>
<p><span class="math display">\[
\text{s.t.} \quad \sum_{i=1}^N \alpha_i \cdot t_i = 0
\]</span></p>
<p><span class="math display">\[
\text{s.t.} \quad 0 \leq \alpha_i \leq C \quad \forall i \in [1, N]
\]</span></p>
<ul>
<li>The dual form leads to the same optimization problem as for the maximal-margin classifier, except that the KKT multipliers <span class="math inline">\(\alpha_i\)</span> are now upper-bounded by C.</li>
</ul>
</section></section>
<section id="dual-form-of-the-1-norm-soft-margin-optimization" class="title-slide slide level1 center">
<h1>Dual form of the 1-norm soft-margin optimization</h1>

<img data-src="img/soft.png" style="width:30.0%" class="r-stretch quarto-figure-center"><ul>
<li><p>The examples where <span class="math inline">\(\alpha_i = C\)</span> lie within the margin of separation, i.e.&nbsp;they have a non-zero slack variable.</p></li>
<li><p>The support vectors are characterized by <span class="math inline">\(0 &lt; \alpha_i &lt; C\)</span>, they still have a functional margin of exactly one.</p></li>
<li><p>The decision function is computed using the support vectors only, not the examples with a functional margin smaller than one.</p></li>
<li><p><strong>Regularization reduces the number of support vectors</strong> by tolerating some misclassifications, hence decreasing the complexity of the model and increasing the geometric margin.</p></li>
</ul>
</section>

<section id="kernel-trick" class="title-slide slide level1 center">
<h1>5 - Kernel trick</h1>

</section>

<section id="use-of-a-feature-space" class="title-slide slide level1 center">
<h1>Use of a feature space</h1>
<ul>
<li><p>The methods seen before only work for (almost) linearly separable problems.</p></li>
<li><p>For non-linear problems, we can project the input data into a higher dimensional <strong>feature space</strong> through a function <span class="math inline">\(\varphi\)</span>, and hope that the problem becomes there linearly separable by a soft-margin classifier.</p></li>
</ul>

<img data-src="img/featurespace.png" style="width:50.0%" class="r-stretch quarto-figure-center"></section>

<section id="kernel-function-to-project-on-a-feature-space" class="title-slide slide level1 center">
<h1>Kernel function to project on a feature space</h1>
<ul>
<li>The soft margin classifier depends on the dot product between an input <span class="math inline">\(\mathbf{x}\)</span> and the support vectors <span class="math inline">\(\mathbf{x}_i\)</span>:</li>
</ul>
<p><span class="math display">\[
F(\mathbf{x}) = \sum_{i=1}^{N_{SV}} \alpha_i \cdot t_i \cdot  \langle \mathbf{x}_i . \mathbf{x} \rangle + b
\]</span></p>
<ul>
<li>When working in the feature space, it becomes:</li>
</ul>
<p><span class="math display">\[
\begin{align*}
F(\mathbf{x}) &amp;= \sum_{i=1}^{N_{SV}} \alpha_i \cdot t_i \cdot  \langle \varphi(\mathbf{x}_i) . \varphi(\mathbf{x}) \rangle + b   \\
    &amp;= \sum_{i=1}^{N_{SV}} \alpha_i \cdot t_i \cdot  K( \mathbf{x}_i,  \mathbf{x}) +b  \\
\end{align*}
\]</span></p>
<ul>
<li><p>The function <span class="math inline">\(K(\mathbf{x}, \mathbf{z}) = \langle \varphi(\mathbf{x}) . \varphi(\mathbf{z}) \rangle\)</span> is called a <strong>kernel</strong>.</p></li>
<li><p>It implicitely performs the dot product between two feature representations <span class="math inline">\(\varphi(\mathbf{x})\)</span> and <span class="math inline">\(\varphi(\mathbf{z})\)</span>, but using the original vectors <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{z}\)</span>.</p></li>
</ul>
</section>

<section id="example-of-the-polynomial-kernel" class="title-slide slide level1 center">
<h1>Example of the polynomial kernel</h1>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li>Let’s consider the quadratic kernel in <span class="math inline">\(\Re^3\)</span>:</li>
</ul>
<p><span class="math display">\[\begin{eqnarray*}
\forall (x, z) \in \Re^3 \times \Re^3 &amp;&amp; \\
&amp;&amp; \\
  K(x,z) &amp;=&amp; ( \langle x . z \rangle)^2 \\
                                            &amp;=&amp;  (\sum_{i=1}^3 x_i \cdot z_i) \cdot (\sum_{j=1}^3 x_j \cdot z_j) \\
                                            &amp;=&amp;  \sum_{i=1}^3 \sum_{j=1}^3 (x_i \cdot x_j) \cdot ( z_i \cdot z_j) \\
                                            &amp;=&amp;  \langle \varphi(x) . \varphi(z) \rangle
\end{eqnarray*}\]</span></p>
</div><div class="column" style="width:50%;">
<p><span class="math display">\[
  \text{with:} \qquad  \varphi(x) = \begin{bmatrix}
                            x_1 \cdot x_1 \\
                            x_1 \cdot x_2 \\
                            x_1 \cdot x_3 \\
                            x_2 \cdot x_1 \\
                            x_2 \cdot x_2 \\
                            x_2 \cdot x_3 \\
                            x_3 \cdot x_1 \\
                            x_3 \cdot x_2 \\
                            x_3 \cdot x_3 \end{bmatrix}
\]</span></p>
</div>
</div>
<ul>
<li>The quadratic kernel implicitely transforms an input space with three dimensions into a feature space of 9 dimensions.</li>
</ul>
</section>

<section>
<section id="example-of-the-polynomial-kernel-1" class="title-slide slide level1 center">
<h1>Example of the polynomial kernel</h1>
<ul>
<li>More generally, the polynomial kernel in <span class="math inline">\(\Re^d\)</span> of degree <span class="math inline">\(p\)</span>:</li>
</ul>
<p><span class="math display">\[
\begin{align*}
\forall (x, z) \in \Re^d \times \Re^d \qquad  K(x,z) &amp;= ( \langle x . z \rangle)^p \\
                                            &amp;=  \langle \varphi(x) . \varphi(z) \rangle
\end{align*}
\]</span></p>
<p>transforms the input from a space with <span class="math inline">\(d\)</span> dimensions into a feature space of <span class="math inline">\(d^p\)</span> dimensions.</p>
<ul>
<li><p>While the inner product in the feature space would require <span class="math inline">\(O(d^p)\)</span> operations, the calculation of the kernel directly in the input space only requires <span class="math inline">\(O(d)\)</span> operations.</p></li>
<li><p>This is called the <strong>kernel trick</strong>: when a linear algorithm only relies on the inner product between input vectors, it can be safely projected into a higher dimensional feature space through a kernel function, without increasing too much its computational complexity, and without ever computing the values in the feature space.</p></li>
</ul>
</section>
<section class="slide level2">

<p><strong>Definition of the support vector machine</strong></p>
<p>For a valid kernel function <span class="math inline">\(K\)</span> and a training set <span class="math inline">\(\mathcal{S} = \{(\mathbf{x}_1, y_1), ... , (\mathbf{x}_N, y_N) \}\)</span>, the KKT multipliers that solve the 1-norm optimization problem:</p>
<p><span class="math display">\[ \text{maximize}_{\alpha_1, ... , \alpha_N} \qquad \mathcal{Q}(\alpha_1, .. , \alpha_N) = \sum_{i=1}^N \alpha_i - \frac{1}{2} \cdot \sum_{i=1}^N \sum_{j=1}^N \alpha_i \cdot \alpha_j \cdot t_i \cdot y_j \cdot K(\mathbf{x}_i,  \mathbf{x}_j)
\]</span> <span class="math display">\[
\text{s.t.} \quad \sum_{i=1}^N \alpha_i \cdot t_i = 0
\]</span> <span class="math display">\[
\text{s.t.} \quad 0 \leq \alpha_i \leq C \quad \forall i \in [1, N]
\]</span></p>
<p>realize a decision function that can correctly classify the training set <span class="math display">\[
  F(\mathbf{x}) = \sum_{i=1}^{N_{SV}} \alpha_i \cdot t_i \cdot  K( \mathbf{x}_i,  \mathbf{x}) +b^*
\]</span></p>
</section></section>
<section id="what-is-a-valid-kernel" class="title-slide slide level1 center">
<h1>What is a valid kernel?</h1>
<p><strong>Mercer’s theorem (1909)</strong></p>
<p>Let <span class="math inline">\(K : \Re^d \times \Re^d \rightarrow \Re\)</span> be a function and <span class="math inline">\(\{\mathbf{x}_1, ... , \mathbf{x}_N \}\)</span> a set of points in <span class="math inline">\(\Re^d\)</span>. <span class="math inline">\(K\)</span> is a valid kernel if the Kernel matrix <span class="math inline">\(\mathcal{K}\)</span> defined by:</p>
<p><span class="math display">\[
\mathcal{K} = \{ K(\mathbf{x}_i, \mathbf{x}_j)_{(i,j) \in [1, N]^2}\}
\]</span></p>
<p>is symmetric positive semi-definite.</p>
<p><span class="math display">\[
    \forall \mathbf{x} \in \Re^d, \qquad \mathbf{x}^T \cdot \mathcal{K} \cdot \mathbf{x} \geq 0
\]</span></p>
<ul>
<li><p>The validity of a kernel depends on the feature mapping itself, but also on the data set. The kernel matrix can be tested on the training data before learning in order to know if the SVM will converge or not.</p></li>
<li><p>Any valid kernel can be used in a SVM to classify the training data. Only the generalization error will depend on the choice of the Kernel function.</p></li>
</ul>
</section>

<section id="examples-of-kernels-used-in-svms" class="title-slide slide level1 center">
<h1>Examples of kernels used in SVMs</h1>
<ul>
<li><strong>Linear kernel</strong>: dimension of the feature space = <span class="math inline">\(d\)</span>.</li>
</ul>
<p><span class="math display">\[
K(\mathbf{x},\mathbf{z}) = \langle \mathbf{x} \cdot \mathbf{z} \rangle
\]</span></p>
<ul>
<li><strong>Polynomial kernel</strong>: dimension of the feature space = <span class="math inline">\(d^p\)</span>.</li>
</ul>
<p><span class="math display">\[
K(\mathbf{x},\mathbf{z}) = (\langle \mathbf{x} \cdot \mathbf{z} \rangle)^p
\]</span></p>
<ul>
<li><strong>Gaussian kernel</strong> (or RBF kernel): dimension of the feature space= <span class="math inline">\(\infty\)</span>.</li>
</ul>
<p><span class="math display">\[
K(\mathbf{x},\mathbf{z}) = \exp(-\frac{\| \mathbf{x} - \mathbf{z} \|^2}{2\sigma^2})
\]</span></p>
<ul>
<li><strong>Hyperbolic tangent kernel</strong>: dimension of the feature space = <span class="math inline">\(\infty\)</span></li>
</ul>
<p><span class="math display">\[
k(\mathbf{x},\mathbf{z})=\tanh(\langle \kappa \mathbf{x} \cdot \mathbf{z} \rangle +c)
\]</span></p>
</section>

<section id="examples-of-kernels-used-in-svms-1" class="title-slide slide level1 center">
<h1>Examples of kernels used in SVMs</h1>

<img data-src="img/kernels.png" class="r-stretch quarto-figure-center"><ul>
<li><p>In practice, the choice of the kernel family depends more on the nature of data (text, image…) and its distribution than on the complexity of the learning problem.</p></li>
<li><p>RBF kernels tend to “group” positive examples together.</p></li>
<li><p>Polynomial kernels are more like “distorted” hyperplanes.</p></li>
<li><p>Kernels have parameters (<span class="math inline">\(p\)</span>, <span class="math inline">\(\sigma\)</span>…) which have to found using cross-validation.</p></li>
</ul>
<div class="footer">
<p>Source: <a href="http://beta.cambridgespark.com/courses/jpm/05-module.html" class="uri">http://beta.cambridgespark.com/courses/jpm/05-module.html</a></p>
</div>
</section>

<section id="why-svms-are-a-brilliant-idea" class="title-slide slide level1 center">
<h1>Why SVMs are a brilliant idea</h1>
<ul>
<li><p>Using a kernel, we can simply project the input data to a feature space with many dimensions (e.g.&nbsp;Gaussian kernel: <span class="math inline">\(+\infty\)</span>), ensuring a small training according to Cover’s theorem.</p></li>
<li><p>The VC dimension depends on the <strong>geometric margin</strong> <span class="math inline">\(\gamma = \frac{1}{\sqrt{\sum_{i=1}^{N_{SV}} \alpha_i}}\)</span>, i.e.&nbsp;on the number and strength of the support vectors:</p></li>
</ul>
<p><span class="math display">\[
  \begin{align*}
    \text{VC}_\text{dim}(\text{SVM}) &amp;= \min ( \lceil  \frac{\max_{(i, j) \in [1, N_{\text{SV}}]^2} \|\mathbf{x}_i - \mathbf{x}_j \|^2}{2 \cdot \gamma}\rceil, d) + 1 \\
    &amp; \approx  \frac{1}{\gamma} = \sqrt{\sum_{i=1}^{N_{SV}} \alpha_i} \\
  \end{align*}
\]</span></p>
<ul>
<li><p>The soft-margin <strong>regularization</strong> parameter <span class="math inline">\(C\)</span> can control the number and strength of support vectors: <span class="math inline">\(\gamma\)</span> can be increased by tolerating some training errors, reducing the VC dimension.</p></li>
<li><p><strong>SVMs can work in a feature space with infinite dimensions, while keeping the VC dimension finite!</strong></p></li>
<li><p>Both the training error and the generalization error can be kept small, depending on the value of C.</p></li>
</ul>
</section>

<section id="learning-procedures-in-a-svm" class="title-slide slide level1 center">
<h1>6 - Learning procedures in a SVM</h1>

</section>

<section id="optimization-procedure-for-the-primal-form" class="title-slide slide level1 center">
<h1>Optimization procedure for the primal form</h1>
<ul>
<li>In the primal form of the soft-margin classifier, we had the following optimization problem:</li>
</ul>
<p><span class="math display">\[
\text{minimize}_{\mathbf{w}, b} \quad \frac{1}{2} \cdot \|\mathbf{w} \|^2 + C \, \sum_{i=1}^N \xi_i
\]</span></p>
<p><span class="math display">\[
\text{s.t.} \quad 1 - \xi_i - t_i \cdot (\langle \mathbf{w} . \mathbf{x}_i\rangle +b) \leq 0 \quad \forall i \in [1, N]
\]</span></p>
<ul>
<li>It can be rewritten as a simple optimisation problem without constraint:</li>
</ul>
<p><span class="math display">\[
\text{minimize}_{\mathbf{w}, b} \quad \frac{1}{2} \cdot \|\mathbf{w} \|^2 + C \, \sum_{i=1}^N \max(0, 1 - t_i \cdot (\langle \mathbf{w} . \mathbf{x}_i\rangle +b))
\]</span></p>
<ul>
<li><p>Minimizing <span class="math inline">\(\max(0, 1 - t_i \cdot (\langle \mathbf{w} . \mathbf{x}_i\rangle +b))\)</span> implies making sure that <span class="math inline">\(1 - t_i \cdot (\langle \mathbf{w} . \mathbf{x}_i\rangle +b)\)</span> is negative, i.e.&nbsp;that the functional margin is higher than 1.</p></li>
<li><p>It is the same as minimizing the slack variables directly, but we do not need to care about the constraints!</p></li>
</ul>
</section>

<section id="hinge-loss" class="title-slide slide level1 center">
<h1>Hinge loss</h1>
<ul>
<li>For a single example, <span class="math inline">\(\mathcal{l}_i(\mathbf{w}, b) = \max(0, 1 - t_i \cdot (\langle \mathbf{w} . \mathbf{x}_i\rangle +b))\)</span> is called the <strong>Hinge loss</strong>:</li>
</ul>
<p><span class="math display">\[
\text{minimize}_{\mathbf{w}, b} \quad \frac{1}{2} \cdot \|\mathbf{w} \|^2 + C \, \sum_{i=1}^N \mathcal{l}_i(\mathbf{w}, b)
\]</span></p>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/hinge.png"></p>
</figure>
</div>
</div><div class="column" style="width:50%;">
<ul>
<li><p>The Hinge loss is positive for examples with a functional margin smaller than 1, and zero otherwise.</p></li>
<li><p>Its gradient w.r.t the weight vector is known:</p></li>
</ul>
<p><span class="math display">\[
    \frac{\partial \mathcal{l}_i(\mathbf{w}, b)}{\partial \mathbf{w}} = \begin{cases} - t_i \, \mathbf{x}_i \qquad \text{if} \quad \langle \mathbf{w} . \mathbf{x}_i\rangle +b &lt; 1 \\
    0 \qquad \text{otherwise.}
    \end{cases}
\]</span></p>
<ul>
<li>We can simply apply <strong>gradient descent</strong> to find the weight vector!</li>
</ul>
</div>
</div>
</section>

<section id="regularization-comparison-with-neural-networks" class="title-slide slide level1 center">
<h1>Regularization : comparison with neural networks</h1>
<ul>
<li>Remember the loss function for <strong>neural networks</strong> with L2 regularization:</li>
</ul>
<p><span class="math display">\[
\text{minimize}_{\theta} \quad  \frac{1}{2} \, \sum_{i=1}^N ||\mathbf{t}_i - \mathbf{y}_i||^2 + \frac{\lambda}{2} \cdot ||\theta||^2
\]</span></p>
<ul>
<li>Training the primal form of a soft-margin classifier obeys to the same logic:</li>
</ul>
<p><span class="math display">\[
\text{minimize}_{\mathbf{w}, b} \quad \frac{1}{2} \cdot \|\mathbf{w} \|^2 + C \, \sum_{i=1}^N \max(0, 1 - t_i \cdot (\langle \mathbf{w} . \mathbf{x}_i\rangle +b))
\]</span></p>
<ul>
<li><p>One part of the function to minimize punishes the <strong>misclassifications</strong> (mse / cross-entropy / Hinge).</p></li>
<li><p>The other <strong>regularizes</strong> learning, by forcing the free parameters to be kept small.</p></li>
<li><p><strong>But</strong> things get uglier when using kernels (local optima, long convergence times…). See <a href="http://www.kyb.mpg.de/fileadmin/user_upload/files/publications/attachments/primal_%5b0%5d.pdf" class="uri">http://www.kyb.mpg.de/fileadmin/user_upload/files/publications/attachments/primal_%5b0%5d.pdf</a> for a discussion.</p></li>
</ul>
</section>

<section id="optimization-procedure-for-the-dual-form" class="title-slide slide level1 center">
<h1>Optimization procedure for the dual form</h1>
<ul>
<li>For the dual form of the SVM, we only need to find the <span class="math inline">\(\alpha_i\)</span> value of each training example that maximize the objective function:</li>
</ul>
<p><span class="math display">\[ \text{maximize}_{\alpha_1, ... , \alpha_N} \qquad \mathcal{Q}(\alpha_1, .. , \alpha_N) = \sum_{i=1}^N \alpha_i - \frac{1}{2} \cdot \sum_{i=1}^N \sum_{j=1}^N \alpha_i \cdot \alpha_j \cdot t_i \cdot y_j \cdot K(\mathbf{x}_i,  \mathbf{x}_j)
\]</span> <span class="math display">\[
\text{s.t.} \quad \sum_{i=1}^N \alpha_i \cdot t_i = 0
\]</span> <span class="math display">\[
\text{s.t.} \quad 0 \leq \alpha_i \leq C \quad \forall i \in [1, N]
\]</span></p>
<ul>
<li>The functional <span class="math inline">\(\mathcal{Q}(\mathbf{\alpha})\)</span> is quadratic in <span class="math inline">\(\alpha\)</span> (convex), so learning should always converge efficiently if the kernel is valid.</li>
</ul>
</section>

<section id="naive-stochastic-gradient-ascent" class="title-slide slide level1 center">
<h1>Naive stochastic gradient ascent</h1>
<p><span class="math display">\[
    \mathcal{Q}(\alpha_1, .. , \alpha_N) = \sum_{i=1}^N \alpha_i - \frac{1}{2} \cdot \sum_{i=1}^N \sum_{j=1}^N \alpha_i \cdot \alpha_j \cdot t_i \cdot y_j \cdot K(\mathbf{x}_i,  \mathbf{x}_j)
\]</span></p>
<div class="columns">
<div class="column" style="width:35%;">
<ul>
<li>The easiest way to maximize the functional is to use <strong>gradient ascent</strong> to iteratively modify each KKT multiplier:</li>
</ul>
<p><span class="math display">\[
  \Delta \alpha_i = \eta \cdot \frac{\partial \mathcal{Q}(\alpha_1, .. , \alpha_N)}{\partial \alpha_i}
\]</span></p>
<ul>
<li>After applying the update rule, we make sure that each KKT multiplier is bounded by 0 and <span class="math inline">\(C\)</span>.</li>
</ul>
</div><div class="column" style="width:60%;">
<p><strong>Algorithm:</strong></p>
<p><span class="math inline">\(\mathbf{\alpha} \gets 0^N\)</span></p>
<p><span class="math inline">\(\textbf{while } \text{not converged:}\)</span></p>
<p><span class="math inline">\(\qquad \textbf{forall } \text{examples}\quad (\mathbf{x}_i, t_i)\)</span></p>
<p><span class="math inline">\(\qquad \qquad \alpha_i \gets \alpha_i + \eta_i \cdot (1 - t_i\cdot \sum_{j=1}^N \alpha_j \cdot y_j \cdot K(\mathbf{x}_i, \mathbf{x_j}) )\)</span></p>
<p><span class="math inline">\(\qquad \qquad \textbf{if} \quad \alpha_i &lt; 0 \quad : \quad \alpha_i \gets 0\)</span></p>
<p><span class="math inline">\(\qquad \qquad \textbf{else if} \quad \alpha_i &gt; C \quad : \quad \alpha_i \gets C\)</span></p>
</div>
</div>
</section>

<section id="naive-stochastic-gradient-ascent-1" class="title-slide slide level1 center">
<h1>Naive stochastic gradient ascent</h1>
<ul>
<li>This algorithm does not care about the constraint:</li>
</ul>
<p><span class="math display">\[
\sum_{i=1}^N \alpha_i \cdot t_i = 0
\]</span></p>
<p>as it would be impossible to modify only one <span class="math inline">\(\alpha_i\)</span> at a time. The solution will not be optimal.</p>
<ul>
<li><p>The value of <span class="math inline">\(b^*\)</span> is unknown and must be found by another algorithm.</p></li>
<li><p>The stopping criterion can be computed by checking all the KKT conditions. When they are all achieved within a certain tolerance criterion, the algorithm stops.</p></li>
<li><p>The drawback of this algorithm is that it needs to store the Kernel Matrix in memory: can become a problem when the number of examples increases.</p></li>
<li><p>A good choice for the individual learning rates <span class="math inline">\(\eta_i\)</span> is <span class="math inline">\(\frac{1}{K(\mathbf{x}_i, \mathbf{x}_i)}\)</span></p></li>
<li><p>It is better to sort the training examples during each epoch in the descending order of their <span class="math inline">\(\alpha_i\)</span>.</p></li>
</ul>
</section>

<section id="chunking-algorithm-for-the-1-norm-margin" class="title-slide slide level1 center">
<h1>Chunking algorithm for the 1-norm margin</h1>
<ul>
<li>The idea is to leave out as soon as possible the examples which will never acquire a positive <span class="math inline">\(\alpha\)</span>, in order to save memory space.</li>
</ul>
<p><span class="math inline">\(\mathbf{\alpha} \gets 0^N\)</span></p>
<p><span class="math inline">\(\text{Select a working subset } \hat{\mathcal{S}} \text{ of } \mathcal{S}\)</span></p>
<p><span class="math inline">\(\textbf{while } \text{not converged}:\)</span></p>
<p><span class="math inline">\(\qquad \text{Solve optimization problem on } \hat{\mathcal{S}}\)</span></p>
<p><span class="math inline">\(\qquad \text{Leave out all the non-support vectors from } \hat{\mathcal{S}}\)</span></p>
<p><span class="math inline">\(\qquad \text{Test the found classifier on the rest of } \mathcal{S}\)</span></p>
<p><span class="math inline">\(\qquad \text{Add the } M \text{ points which violates the most the KKT conditions to } \hat{\mathcal{S}}\)</span></p>
<ul>
<li>This heuristic works better in practice than the online version and needs less memory.</li>
</ul>
</section>

<section id="sequential-minimal-optimization-smo" class="title-slide slide level1 center">
<h1>Sequential Minimal Optimization (SMO)</h1>
<ul>
<li>Stochastic gradient ascent can not consider the KKT constraint:</li>
</ul>
<p><span class="math display">\[
  \sum_{i=1}^N \alpha_i \cdot t_i = 0
\]</span></p>
<p>because it updates the <span class="math inline">\(\alpha_i\)</span> values one after another. Each <span class="math inline">\(\alpha_i\)</span> is constrained by the other values:</p>
<p><span class="math display">\[
  \alpha_j = - y_j \cdot \sum_{i\neq j} \alpha_i \cdot t_i
\]</span></p>
<ul>
<li>The idea of SMO is to select two examples from the dataset using an adequate heuristic (e.g.&nbsp;the two examples that would modify the most the functional <span class="math inline">\(\mathcal{Q}(\mathbf{\alpha})\)</span>) and update the <span class="math inline">\(\alpha\)</span> values only for these two examples, the others being fixed.</li>
</ul>
<p><span class="math display">\[
  y_1 \cdot \alpha_1 + y_2 \cdot \alpha_2 = - \sum_{i \geq 3} \alpha_i \cdot t_i = \xi
\]</span></p>
<p><span class="math display">\[
  \alpha_2 = y_2 \cdot ( \xi -  y_1 \cdot \alpha_1 )
\]</span></p>
</section>

<section id="sequential-minimal-optimization-smo-1" class="title-slide slide level1 center">
<h1>Sequential Minimal Optimization (SMO)</h1>
<p><span class="math display">\[ \text{maximize}_{\alpha_1, \alpha_2} \qquad \mathcal{Q}(\alpha_1, \alpha_2) = a\cdot \alpha_1^2 + b\cdot \alpha_2^2 + c\cdot \alpha_1 \cdot \alpha_2 + d\cdot \alpha_1 + e\cdot \alpha_2 + f
\]</span></p>
<p><span class="math display">\[
\text{s.t.} \quad y_1 \cdot \alpha_1 + y_2 \cdot \alpha_2 = \xi
\]</span></p>
<p><span class="math display">\[
\text{s.t.} \quad 0 \leq \alpha_1 \leq C
\]</span></p>
<p><span class="math display">\[
\text{s.t.} \quad 0 \leq \alpha_2 \leq C
\]</span></p>

<img data-src="img/smo.png" style="width:50.0%" class="r-stretch quarto-figure-center"></section>

<section id="sequential-minimal-optimization-smo-2" class="title-slide slide level1 center">
<h1>Sequential Minimal Optimization (SMO)</h1>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/smo.png"></p>
</figure>
</div>
</div><div class="column" style="width:50%;">
<p><span class="math inline">\(\mathbf{\alpha} \gets 0^N\)</span></p>
<p><span class="math inline">\(\textbf{while } \text{not converged:}\)</span></p>
<p><span class="math inline">\(\qquad \text{Select two examples } (\alpha_1, \alpha_2) \text{ according}\)</span> <span class="math inline">\(\qquad \qquad \text{ to a heuristic.}\)</span></p>
<p><span class="math inline">\(\qquad \text{Optimize the functional } \mathcal{Q}(\alpha_1, \alpha_2) \text{ only}\)</span> <span class="math inline">\(\qquad \qquad \text{ regarding } (\alpha_1, \alpha_2) \text{, the other } \alpha_i \text{ being fixed, }\)</span> <span class="math inline">\(\qquad \qquad \text{ while ensuring that:}\)</span></p>
<p><span class="math display">\[y_1 \cdot \alpha_1 + y_2 \cdot \alpha_2 = \xi \]</span></p>
<p><span class="math display">\[ 0 \leq \alpha_1 \leq C \]</span></p>
<p><span class="math display">\[ 0 \leq \alpha_2 \leq C \]</span></p>
</div>
</div>
</section>

<section id="summary-of-svm" class="title-slide slide level1 center">
<h1>Summary of SVM</h1>
<ul>
<li><p>The dual form of a SVM always finds a global and efficient maxima, unlike neural networks (NN).</p></li>
<li><p>The solution found by a SVM can be made sparse (small number of support vectors) through <strong>regularization</strong>.</p></li>
<li><p>The complexity (VC dimension) of a SVM does not depend on the dimension of the input space: typical choice for high-dimensional data.</p></li>
<li><p>The training time can become prohibitive for large datasets.</p></li>
<li><p>There exists a version of SVM for regression: SVR, which is slightly different conceptually.</p></li>
<li><p>For a discussion on whether to optimize the primal or the dual, see <a href="http://www.kyb.mpg.de/fileadmin/user_upload/files/publications/attachments/primal_%5b0%5d.pdf" class="uri">http://www.kyb.mpg.de/fileadmin/user_upload/files/publications/attachments/primal_%5b0%5d.pdf</a>.</p></li>
<li><p>The mathematical foundation of SVM is strong, but rather complicated. However, already-implemented algorithms with various optimizations are freely available. Don’t code your own SVM! Example: SVMlight, libsvm, scikit-learn…</p></li>
<li><p>SVMs were the standard classification method before deep learning arrived, but are still a weapon of choice for small datasets.</p></li>
</ul>
<div class="footer">
<p><a href="http://cs229.stanford.edu/notes/cs229-notes3.pdf" class="uri">http://cs229.stanford.edu/notes/cs229-notes3.pdf</a></p>
</div>
<div class="footer footer-default">

</div>
</section>
    </div>
  </div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="9.7-SVM_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="9.7-SVM_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="9.7-SVM_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="9.7-SVM_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="9.7-SVM_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="9.7-SVM_files/libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="9.7-SVM_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="9.7-SVM_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="9.7-SVM_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="9.7-SVM_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="9.7-SVM_files/libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'smaller': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":false},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const clipboard = new window.ClipboardJS('.code-copy-button', {
        target: function(trigger) {
          return trigger.previousElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        setTimeout(function() {
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      });
      function tippyHover(el, contentFn) {
        const config = {
          allowHTML: true,
          content: contentFn,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'quarto-reveal',
          placement: 'bottom-start'
        };
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          return note.innerHTML;
        });
      }
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const cites = ref.parentNode.getAttribute('data-cites').split(' ');
        tippyHover(ref, function() {
          var popup = window.document.createElement('div');
          cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    });
    </script>
    

</body></html>