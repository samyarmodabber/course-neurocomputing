<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.251">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>linearregression</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="3-LinearRegression_files/libs/clipboard/clipboard.min.js"></script>
<script src="3-LinearRegression_files/libs/quarto-html/quarto.js"></script>
<script src="3-LinearRegression_files/libs/quarto-html/popper.min.js"></script>
<script src="3-LinearRegression_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="3-LinearRegression_files/libs/quarto-html/anchor.min.js"></script>
<link href="3-LinearRegression_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="3-LinearRegression_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="3-LinearRegression_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="3-LinearRegression_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="3-LinearRegression_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">



<section id="linear-regression" class="level1">
<h1>Linear regression</h1>
<p>The goal of this exercise is to implement the least mean squares algorithm (LMS) for linear regression seen in the course.</p>
<p>We start by importing numpy and matplotlib.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="least-mean-squares" class="level2">
<h2 class="anchored" data-anchor-id="least-mean-squares">Least mean squares</h2>
<p>To generate the data for the exercise, we will use the <code>scikit-learn</code> library <a href="https://scikit-learn.org" class="uri">https://scikit-learn.org</a>. It provides a huge selection of already implemented machine learning algorithms for classification, regression or clustering.</p>
<p>If you use Colab, <code>scikit-learn</code> should already be installed. Otherwise, install it with either <code>conda</code> or <code>pip</code> (you may need to restart this notebook afterwards):</p>
<pre><code>conda install scikit-learn
# or:
pip install scikit-learn</code></pre>
<p>We will use the method <code>sklearn.datasets.make_regression</code> to generate the data. The documentation of this method is available at <a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html" class="uri">https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html</a>.</p>
<p>The following cell imports the method:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_regression</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can now generate the data. We start with the simplest case where the inputs have only one dimension. We will generate 100 samples <span class="math inline">\((x_i, t_i)\)</span> linked by a linear relationship and some noise.</p>
<p>The following code generates the data:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>X, t <span class="op">=</span> make_regression(n_samples<span class="op">=</span>N, n_features<span class="op">=</span><span class="dv">1</span>, noise<span class="op">=</span><span class="fl">15.0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><code>n_samples</code> is the number of samples generates, <code>n_features</code> is the number of input variables and <code>noise</code> quantifies how the points deviate from the linear relationship.</p>
<p><strong>Q:</strong> Print the shape of the arrays <code>X</code> and <code>t</code> to better understand what is generated. Visualize the dataset using matplotlib (<code>plt.scatter</code>). Vary the value of the <code>noise</code> argument in the previous cell and visualize the data again.</p>
<p>Now is the time to implement the LMS algorithm with numpy.</p>
<p>Remember the LMS algorithm from the course:</p>
<ul>
<li><p><span class="math inline">\(w=0 \quad;\quad b=0\)</span></p></li>
<li><p><strong>for</strong> M epochs:</p>
<ul>
<li><p><span class="math inline">\(dw=0 \quad;\quad db=0\)</span></p></li>
<li><p><strong>for</strong> each sample <span class="math inline">\((x_i, t_i)\)</span>:</p>
<ul>
<li><p><span class="math inline">\(y_i = w \, x_i + b\)</span></p></li>
<li><p><span class="math inline">\(dw = dw + (t_i - y_i) \, x_i\)</span></p></li>
<li><p><span class="math inline">\(db = db + (t_i - y_i)\)</span></p></li>
</ul></li>
<li><p><span class="math inline">\(\Delta w = \eta \, \frac{1}{N} dw\)</span></p></li>
<li><p><span class="math inline">\(\Delta b = \eta \, \frac{1}{N} db\)</span></p></li>
</ul></li>
</ul>
<p>Our linear model <span class="math inline">\(y = w \, x + b\)</span> predicts outputs for an input <span class="math inline">\(x\)</span>. The error <span class="math inline">\(t-y\)</span> between the prediction and the data is used to adapt the weight <span class="math inline">\(w\)</span> and the bias <span class="math inline">\(b\)</span> at the end of each epoch.</p>
<p><strong>Q:</strong> Implement the LMS algorithm and apply it to the generated data. The Python code that you will write is almost a line-by-line translation of the pseudo-code above. You will use a learning rate <code>eta = 0.1</code> at first, but you will vary this value later. Start by running a single epoch, as it will be easier to debug it, and then increase the number of epochs to 100. Print the value of the weight and bias at the end.</p>
<p><strong>Q:</strong> Visualize the quality of the fit by superposing the learned model to the data with matplotlib.</p>
<p><em>Tip</em>: you can get the extreme values of the xaxis with <code>X.min()</code> and <code>X.max()</code>. To visualize the model, you just need to plot a line between the points <code>(X.min(), w*X.min()+b)</code> and <code>(X.max(), w*X.max()+b)</code>.</p>
<p>Another option is to predict a value for all inputs and plot this vector <span class="math inline">\(y\)</span> against the desired values <span class="math inline">\(t\)</span>.</p>
<p><strong>Q:</strong> Make a scatter plot where <span class="math inline">\(t\)</span> is the x-axis and <span class="math inline">\(y = w\, x + b\)</span> is the y-axis. How should the points be arranged in the ideal case? Also plot what this ideal relationship should be.</p>
<p>A much better method to analyse the result of the learning algorithm is to track the <strong>mean squared error</strong> (mse) after each epoch, i.e.&nbsp;the loss function which we actually want to minimize. The MSE is defined as:</p>
<p><span class="math display">\[\text{mse} = \frac{1}{N} \, \sum_{i=1}^N (t_i - y_i)^2\]</span></p>
<p><strong>Q:</strong> Modify your LMS algorithm (either directly or copy it in the next cell) to track the mse after each epoch. After each epoch, append the mse on the training set to a list (initially empty) and plot it at the end. How does the mse evolve? Which value does it get in the end? Why? How many epochs do you actually need?</p>
<p>Let’s now study the influence of the learning rate <code>eta=0.1</code> seemed to work, but is it the best value?</p>
<p><strong>Q:</strong> Iterate over multiple values of <code>eta</code> using a logarithmic scale and plot the final mse after 100 epochs as a function of the learning rate. Conclude.</p>
<p><em>Hint:</em> the logarithmic scale means that you will try values such as <span class="math inline">\(10^{-5}\)</span>, <span class="math inline">\(10^{-4}\)</span>, <span class="math inline">\(10^{-3}\)</span>, etc. until 1.0. In Python, you can either write explictly 0.0001 or use the notation <code>1e-4</code>. For the plot, use <code>np.log10(eta)</code> to only display the exponent on the X-axis.</p>
</section>
<section id="scikit-learn" class="level2">
<h2 class="anchored" data-anchor-id="scikit-learn">Scikit-learn</h2>
<p>The code that you have written is functional, but extremely slow, as you use for loops in Python. For so little data samples, it does not make a difference, but if you had millions of samples, this would start to be a problem.</p>
<p>The solution is to use optimized implementations of the algorithms, running in C++ or FORTRAN under the hood. We will use here the LMS algorithm provided by <code>scikit-learn</code> as you have already installed it and it is very simple to use. Note that one could use tensorflow too, but that would be killing a fly with a sledgehammer.</p>
<p><code>scikit-learn</code> provides a <code>LinearRegression</code> object that implements LMS. The documentation is at: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html" class="uri">https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html</a>.</p>
<p>You simply import it with:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>You create the object with:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>reg <span class="op">=</span> LinearRegression()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><code>reg</code> is now an object with different methods (<code>fit()</code>, <code>predict()</code>) that accept any kind of data and performs linear regression.</p>
<p>To train the model on the data <span class="math inline">\((X, t)\)</span>, simply use:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>reg.fit(X, t)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The parameters of the model are obtained with <code>reg.coef_</code> for <span class="math inline">\(w\)</span> and <code>reg.intercept_</code> for <span class="math inline">\(b\)</span>.</p>
<p>You can predict outputs for new inputs using:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> reg.predict(X)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Q:</strong> Apply linear regression on the data using <code>scikit-learn</code>. Check the model parameters after learning and compare them to what you obtained previously. Print the mse and make a plot comparing the predictions with the data.</p>
</section>
<section id="delta-learning-rule" class="level2">
<h2 class="anchored" data-anchor-id="delta-learning-rule">Delta learning rule</h2>
<p>Let’s now implement the online version of LMS, the <strong>delta learning rule</strong>. The only difference is that the parameter updates are applied immediately after each example is evaluated, not at the end of training.</p>
<ul>
<li><p><span class="math inline">\(w=0 \quad;\quad b=0\)</span></p></li>
<li><p><strong>for</strong> M epochs:</p>
<ul>
<li><p><strong>for</strong> each sample <span class="math inline">\((x_i, t_i)\)</span>:</p>
<ul>
<li><p><span class="math inline">\(y_i = w \, x_i + b\)</span></p></li>
<li><p><span class="math inline">\(\Delta w = \eta \, (t_i - y_i ) \, x_i\)</span></p></li>
<li><p><span class="math inline">\(\Delta b = \eta \, (t_i - y_i)\)</span></p></li>
</ul></li>
</ul></li>
</ul>
<p><strong>Q:</strong> Implement the delta learning rule for the regression problem with <code>eta = 0.1</code>. Plot the evolution of the mse and compare it to LMS.</p>
<p><strong>Q:</strong> Vary the learning rate logarithmically as for LMS and conclude.</p>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>