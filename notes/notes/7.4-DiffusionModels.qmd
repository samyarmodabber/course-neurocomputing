# Diffusion Probabilistic Models

Slides: [html](../slides/7.4-DiffusionModels.html){target="_blank"} [pdf](../slides/pdf/7.4-DiffusionModels.pdf){target="_blank"}

## Diffusion Probabilistic models

Generative modeling is about learning a mapping function from a simple probability distribution (e.g. normal) to a complex one (images).

![Source: <https://towardsdatascience.com/understanding-diffusion-probabilistic-models-dpms-1940329d6048>](../slides/img/diffusion-concept.webp){width=70%}


VAE and GAN generators transform simple Gaussian noise to complex distributions:

![Source: <https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73>](../slides/img/vae-structure.png){width=70%}


![Source: <https://towardsdatascience.com/understanding-generative-adversarial-networks-gans-cd6e4651a29>](../slides/img/gan-principle.png)


Destroying information is easier than creating it.

![Source: <https://towardsdatascience.com/understanding-diffusion-probabilistic-models-dpms-1940329d6048>](../slides/img/diffusion-complexity.webp)

Stochastic processes can destroy information. Iteratively adding normal noise to a signal creates a **stochastic differential equation** (SDE).

$$
    X_t = \sqrt{1 - p} \, X_{t-1} + \sqrt{p}   \, \sigma \qquad\qquad \text{where} \qquad\qquad  \sigma \sim \mathcal{N}(0, 1)
$$

Under some conditions, any probability distribution converges to a normal distribution.

![Source: <https://towardsdatascience.com/understanding-diffusion-probabilistic-models-dpms-1940329d6048>](../slides/img/diffusion-noiseconvergence.webp)

A **diffusion process** can therefore iteratively destruct all information in an image through a Markov chain.

![Source: <https://towardsdatascience.com/understanding-diffusion-probabilistic-models-dpms-1940329d6048>](../slides/img/diffusion-idea.webp)

It should be possible to **reverse** each diffusion step by removing the noise using a form of denoising autoencoder.

![Source: <https://towardsdatascience.com/understanding-diffusion-probabilistic-models-dpms-1940329d6048>](../slides/img/diffusion-steps.webp)

We will not get into details, but learning the reverse diffusion step implies Bayesian inference, KL divergence and so on. As we have the images at $t$ and $t+1$, it should be possible to learn, right?

![Source: <https://towardsdatascience.com/understanding-diffusion-probabilistic-models-dpms-1940329d6048>](../slides/img/diffusion-steps4.webp)


![Source: <http://adityaramesh.com/posts/dalle2/dalle2.html>](../slides/img/diffusion.gif)


## Dall-e

Dall-e is a very famous text-to-image generative model. See  <https://openai.com/dall-e-2/>.

![Source: <https://towardsdatascience.com/understanding-diffusion-probabilistic-models-dpms-1940329d6048>](../slides/img/diffusion-dall-e.webp)

### CLIP: Contrastive Language-Image Pre-training

Embeddings for text and images are learned using **Transformer encoders** and **contrastive learning**. 

For each pair (text, image) in the training set, their representation should be made similar, while being different from the others. 

<video controls width=100%><source src="../slides/img/dall-e-2.webm" type="video/webm"></video>

### GLIDE

GLIDE is a **reverse diffusion process** conditioned on the encoding of an image.

![Source: <https://www.assemblyai.com/blog/how-dall-e-2-actually-works/>](../slides/img/CLIP_to_GLIDE-1.png)

### Dall-e

A prior network learns to map text embeddings into image embeddings:

![Source: <https://www.assemblyai.com/blog/how-dall-e-2-actually-works/>](../slides/img/dalle-text_to_image_encoding_2.png){width=40%}

Complete Dall-e architecture:

![Source: <https://www.assemblyai.com/blog/how-dall-e-2-actually-works/>](../slides/img/dall-e-complete.png){width=80%}

