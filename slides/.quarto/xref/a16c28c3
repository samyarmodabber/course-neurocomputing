{"entries":[],"headings":["multi-layer-perceptron","multi-layer-perceptron-1","fully-connected-layer","activation-functions","modern-activation-functions","softmax-activation-function","why-non-linear-activation-functions","training-a-mlp-loss-functions","training-a-mlp-optimizer","backpropagation","backpropagation-on-a-shallow-network","gradient-of-the-loss-function","gradient-in-the-output-layer","gradient-in-the-hidden-layer","gradient-in-the-hidden-layer-1","backpropagated-error","backpropagation-for-a-shallow-mlp","derivative-of-the-activation-functions","what-is-backpropagated","what-is-backpropagated-1","what-is-backpropagated-2","mlp-the-universal-approximation-theorem","properties-of-mlp","deep-neural-networks","deep-neural-network","backpropagation-for-deep-neural-networks","gradient-of-a-fully-connected-layer","gradient-of-a-fully-connected-layer-1","training-a-deep-neural-network-with-backpropagation","example","mlp-example","mlp-example-1","mlp-example-2","automatic-differentiation-deep-learning-frameworks","example-of-a-shallow-neural-network-with-keras","example-of-a-shallow-neural-network-with-keras-1","example-of-a-shallow-neural-network-with-keras-2","example-of-a-shallow-neural-network-with-keras-3","example-of-a-shallow-neural-network-with-keras-4"]}